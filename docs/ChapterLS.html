<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 1 Linear Regressions | Micro-Econometrics</title>
<meta name="author" content="Jean-Paul Renne">
<meta name="description" content="Definition 1.1 A linear regression model is of the form: \[\begin{equation} y_i = \boldsymbol\beta'\mathbf{x}_{i} + \varepsilon_i,\tag{1.1} \end{equation}\] where...">
<meta name="generator" content="bookdown 0.34 with bs4_book()">
<meta property="og:title" content="Chapter 1 Linear Regressions | Micro-Econometrics">
<meta property="og:type" content="book">
<meta property="og:description" content="Definition 1.1 A linear regression model is of the form: \[\begin{equation} y_i = \boldsymbol\beta'\mathbf{x}_{i} + \varepsilon_i,\tag{1.1} \end{equation}\] where...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 1 Linear Regressions | Micro-Econometrics">
<meta name="twitter:description" content="Definition 1.1 A linear regression model is of the form: \[\begin{equation} y_i = \boldsymbol\beta'\mathbf{x}_{i} + \varepsilon_i,\tag{1.1} \end{equation}\] where...">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.5.0/transition.js"></script><script src="libs/bs3compat-0.5.0/tabs.js"></script><script src="libs/bs3compat-0.5.0/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
<style type="text/css">
    /* Used with Pandoc 2.11+ new --citeproc when CSL is used */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
        }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
<link rel="stylesheet" href="my-style.css">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Micro-Econometrics</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Micro-Econometrics</a></li>
<li><a class="active" href="ChapterLS.html"><span class="header-section-number">1</span> Linear Regressions</a></li>
<li><a class="" href="Panel.html"><span class="header-section-number">2</span> Panel regressions</a></li>
<li><a class="" href="estimation-methods.html"><span class="header-section-number">3</span> Estimation Methods</a></li>
<li><a class="" href="binary-choice-models.html"><span class="header-section-number">4</span> Binary-choice models</a></li>
<li><a class="" href="multiple-choice-models.html"><span class="header-section-number">5</span> Multiple Choice Models</a></li>
<li><a class="" href="tobit-and-sample-selection-models.html"><span class="header-section-number">6</span> Tobit and sample-selection models</a></li>
<li><a class="" href="models-of-count-data.html"><span class="header-section-number">7</span> Models of Count Data</a></li>
<li><a class="" href="append.html"><span class="header-section-number">8</span> Appendix</a></li>
<li><a class="" href="references.html">References</a></li>
</ul>

        <div class="book-extra">
          
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="ChapterLS" class="section level1" number="1">
<h1>
<span class="header-section-number">1</span> Linear Regressions<a class="anchor" aria-label="anchor" href="#ChapterLS"><i class="fas fa-link"></i></a>
</h1>
<!-- This chapter covers linear regressions. It is organized as followed: Section \@ref(specif) -->
<div class="definition">
<p><span id="def:essai" class="definition"><strong>Definition 1.1  </strong></span>A linear regression model is of the form:
<span class="math display" id="eq:linearspecif">\[\begin{equation}
y_i = \boldsymbol\beta'\mathbf{x}_{i} + \varepsilon_i,\tag{1.1}
\end{equation}\]</span>
where <span class="math inline">\(\mathbf{x}_{i}=[x_{i,1},\dots,x_{i,K}]'\)</span> is a vector of dimension <span class="math inline">\(K \times 1\)</span>.</p>
</div>
<p>For entity <span class="math inline">\(i\)</span>, the <span class="math inline">\(x_{i,k}\)</span>’s, for <span class="math inline">\(k \in \{1,\dots,K\}\)</span>, are explanatory variables, regressors, or covariates. The variable of interest, <span class="math inline">\(y_i\)</span>, is often called dependent variable, or regressand. The last term of the specification, namely <span class="math inline">\(\varepsilon_i\)</span>, is called error, or disturbance.</p>
<p>The researcher is usually interested in the components of vector <span class="math inline">\(\boldsymbol\beta\)</span>, denoted by <span class="math inline">\(\beta_k\)</span>, <span class="math inline">\(k \in \{1,\dots,K\}\)</span>. She usually aims at estimating these coefficients based on observations of <span class="math inline">\(\{y_i,\mathbf{x}_{i}\}\)</span>, <span class="math inline">\(i \in \{1,\dots,n\}\)</span>, which constitutes a <em>sample</em>. In the following, we will denote the sample length by <span class="math inline">\(n\)</span>.</p>
<p>To have an intercept in the specification <a href="ChapterLS.html#eq:linearspecif">(1.1)</a>, one has to set <span class="math inline">\(x_{i,1}=1\)</span> for all <span class="math inline">\(i\)</span>; <span class="math inline">\(\beta_1\)</span> then corresponds to the intercept.</p>
<div id="linearHyp" class="section level2" number="1.1">
<h2>
<span class="header-section-number">1.1</span> Hypotheses<a class="anchor" aria-label="anchor" href="#linearHyp"><i class="fas fa-link"></i></a>
</h2>
<p>In this section, we introduce different assumptions regarding the covariates and/or the errors. The properties of the estimators used by the researcher depend on which of these assumptions are satisfied.</p>
<div class="hypothesis">
<p><span id="hyp:fullrank" class="hypothesis"><strong>Hypothesis 1.1  (Full rank) </strong></span>There is no exact linear relationship among the independent variables (the <span class="math inline">\(x_{i,k}\)</span>’s, for a given <span class="math inline">\(i \in \{1,\dots,n\}\)</span>).</p>
</div>
<p>Intuitively, when Hypothesis <a href="ChapterLS.html#hyp:fullrank">1.1</a> is satisfied, then the estimation of the model parameters is unfeasible since, for any value of <span class="math inline">\(\boldsymbol\beta\)</span>, some changes in the explanatory variables will be exactly compensated by other changes in another set of explanatory variables, preventing the identification of these effects.</p>
<p>Let us denote by <span class="math inline">\(\mathbf{X}\)</span> the matrix containing all explanatory variables, of dimension <span class="math inline">\(n \times K\)</span>. (That is, row <span class="math inline">\(i\)</span> of <span class="math inline">\(\mathbf{X}\)</span> is <span class="math inline">\(\mathbf{x}_i'\)</span>.) The following hypothesis concerns the relationship between the errors (gathered in <span class="math inline">\(\boldsymbol\varepsilon\)</span>, a <span class="math inline">\(n\)</span>-dimensional vector) and the explanatory variables <span class="math inline">\(\mathbf{X}\)</span>:</p>
<div class="hypothesis">
<p><span id="hyp:exogeneity" class="hypothesis"><strong>Hypothesis 1.2  (Conditional mean-zero assumption) </strong></span><span class="math display">\[\begin{equation}
\mathbb{E}(\boldsymbol\varepsilon|\mathbf{X}) = 0.
\end{equation}\]</span></p>
</div>
<p>Hypothesis <a href="ChapterLS.html#hyp:exogeneity">1.2</a> has important implications:</p>
<div class="proposition">
<p><span id="prp:implicationExog" class="proposition"><strong>Proposition 1.1  </strong></span>Under Hypothesis <a href="ChapterLS.html#hyp:exogeneity">1.2</a>:</p>
<ol style="list-style-type: lower-roman">
<li>
<span class="math inline">\(\mathbb{E}(\varepsilon_{i})=0\)</span>;</li>
<li>The <span class="math inline">\(x_{ij}\)</span>’s and the <span class="math inline">\(\varepsilon_{i}\)</span>’s are uncorrelated, i.e. <span class="math inline">\(\forall i,\,j \quad \mathbb{C}orr(x_{ij},\varepsilon_{i})=0\)</span>.</li>
</ol>
</div>
<div class="proof">
<p><span id="unlabeled-div-1" class="proof"><em>Proof</em>. </span>Let us prove (i) and (ii):</p>
<ol style="list-style-type: lower-roman">
<li>By the law of iterated expectations:
<span class="math display">\[
\mathbb{E}(\boldsymbol\varepsilon)=\mathbb{E}(\mathbb{E}(\boldsymbol\varepsilon|\mathbf{X}))=\mathbb{E}(0)=0.
\]</span>
</li>
<li>
<span class="math inline">\(\mathbb{E}(x_{ij}\varepsilon_i)=\mathbb{E}(\mathbb{E}(x_{ij}\varepsilon_i|\mathbf{X}))=\mathbb{E}(x_{ij}\underbrace{\mathbb{E}(\varepsilon_i|\mathbf{X})}_{=0})=0\)</span>.</li>
</ol>
</div>
<p>The next two hypotheses (<a href="ChapterLS.html#hyp:homoskedasticity">1.3</a> and <a href="ChapterLS.html#hyp:noncorrelResid">1.4</a>) concern the stochastic properties of the errors <span class="math inline">\(\varepsilon_i\)</span>:</p>
<div class="hypothesis">
<p><span id="hyp:homoskedasticity" class="hypothesis"><strong>Hypothesis 1.3  (Homoskedasticity) </strong></span><span class="math display">\[
\forall i, \quad \mathbb{V}ar(\varepsilon_i|\mathbf{X}) = \sigma^2.
\]</span></p>
</div>
<p>The following lines of code generate a figure comparing two situations: Panel (a) of Figure <a href="ChapterLS.html#fig:heteroskedasticity">1.1</a> corresponds to a situation of homoskedasticity, and Panel (b) corresponds to a situation of heteroskedasticity. Let us be more specific. In the two plots, we have <span class="math inline">\(X_i \sim \mathcal{N}(0,1)\)</span> and <span class="math inline">\(\varepsilon^*_i \sim \mathcal{N}(0,1)\)</span>. In Panel (a) (homoskedasticity):
<span class="math display">\[
Y_i = 2 + 2X_i + \varepsilon^*_i.
\]</span>
In Panel (b) (heteroskedasticity):
<span class="math display">\[
Y_i = 2 + 2X_i + \left(2\mathbb{I}_{\{X_i&lt;0\}}+0.2\mathbb{I}_{\{X_i\ge0\}}\right)\varepsilon^*_i\]</span>.</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">N</span> <span class="op">&lt;-</span> <span class="fl">200</span></span>
<span><span class="va">X</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">N</span><span class="op">)</span>;<span class="va">eps</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">N</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>mfrow<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>,<span class="fl">2</span><span class="op">)</span>,plt<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">.2</span>,<span class="fl">.95</span>,<span class="fl">.2</span>,<span class="fl">.8</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">Y</span> <span class="op">&lt;-</span> <span class="fl">2</span> <span class="op">+</span> <span class="fl">2</span><span class="op">*</span><span class="va">X</span> <span class="op">+</span> <span class="va">eps</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">X</span>,<span class="va">Y</span>,pch<span class="op">=</span><span class="fl">19</span>,main<span class="op">=</span><span class="st">"(a) Homoskedasticity"</span>,</span>
<span>     las<span class="op">=</span><span class="fl">1</span>,cex.lab<span class="op">=</span><span class="fl">.8</span>,cex.axis<span class="op">=</span><span class="fl">.8</span>,cex.main<span class="op">=</span><span class="fl">.8</span>,<span class="op">)</span></span>
<span><span class="va">Y</span> <span class="op">&lt;-</span> <span class="fl">2</span> <span class="op">+</span> <span class="fl">2</span><span class="op">*</span><span class="va">X</span> <span class="op">+</span> <span class="va">eps</span><span class="op">*</span><span class="op">(</span> <span class="op">(</span><span class="va">X</span><span class="op">&lt;</span><span class="fl">0</span><span class="op">)</span><span class="op">*</span><span class="fl">2</span> <span class="op">+</span> <span class="op">(</span><span class="va">X</span><span class="op">&gt;=</span><span class="fl">0</span><span class="op">)</span><span class="op">*</span><span class="fl">.2</span> <span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">X</span>,<span class="va">Y</span>,pch<span class="op">=</span><span class="fl">19</span>,main<span class="op">=</span><span class="st">"(b) Heteroskedasticity"</span>,</span>
<span>     las<span class="op">=</span><span class="fl">1</span>,cex.lab<span class="op">=</span><span class="fl">.8</span>,cex.axis<span class="op">=</span><span class="fl">.8</span>,cex.main<span class="op">=</span><span class="fl">.8</span>,<span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: left-aligned">
<span style="display:block;" id="fig:heteroskedasticity"></span>
<img src="MicroEc_files/figure-html/heteroskedasticity-1.png" alt="Homoskedasticity vs heteroskedasticity. See text for the exact specifications." width="90%"><p class="caption">
Figure 1.1: Homoskedasticity vs heteroskedasticity. See text for the exact specifications.
</p>
</div>
<p>Figure <a href="ChapterLS.html#fig:exmpSalarayPhDSHP">1.2</a> shows a real-data situation of heteroskedasticity, based on data taken from the <a href="https://forscenter.ch/projects/swiss-household-panel/">Swiss Household Panel</a>. The sample is restricted to persons (i) that are younger than 35 year in 2019, and (ii) that have completed at least 19 years of study. The figure shows that the dispersion of yearly income increases with age.</p>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">AEC</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/table.html">table</a></span><span class="op">(</span><span class="va">shp</span><span class="op">$</span><span class="va">edyear19</span><span class="op">)</span></span></code></pre></div>
<pre><code>## 
##    8    9   10   12   13   14   16   19   21 
##   70  325  350 1985  454  117  990 1263  168</code></pre>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">shp_higherEd</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/subset.html">subset</a></span><span class="op">(</span><span class="va">shp</span>,<span class="op">(</span><span class="va">edyear19</span><span class="op">&gt;</span><span class="fl">18</span><span class="op">)</span><span class="op">&amp;</span><span class="va">age19</span><span class="op">&lt;</span><span class="fl">35</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">i19wyg</span><span class="op">/</span><span class="fl">1000</span><span class="op">~</span><span class="va">age19</span>,data<span class="op">=</span><span class="va">shp_higherEd</span>,pch<span class="op">=</span><span class="fl">19</span>,las<span class="op">=</span><span class="fl">1</span>,</span>
<span>     xlab<span class="op">=</span><span class="st">"Age"</span>,ylab<span class="op">=</span><span class="st">"Yearly work income"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/abline.html">abline</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">i19wyg</span><span class="op">/</span><span class="fl">1000</span><span class="op">~</span><span class="va">age19</span>,data<span class="op">=</span><span class="va">shp_higherEd</span><span class="op">)</span>,col<span class="op">=</span><span class="st">"red"</span>,lwd<span class="op">=</span><span class="fl">2</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: left-aligned">
<span style="display:block;" id="fig:exmpSalarayPhDSHP"></span>
<img src="MicroEc_files/figure-html/exmpSalarayPhDSHP-1.png" alt="Income versus age. Data are from the Swiss Household Panel. The sample is restricted to persons that have completed at least 19 years of study. The figure shows that the dispersion of yearly income increases with age." width="95%"><p class="caption">
Figure 1.2: Income versus age. Data are from the Swiss Household Panel. The sample is restricted to persons that have completed at least 19 years of study. The figure shows that the dispersion of yearly income increases with age.
</p>
</div>
<!-- x -->
<!-- ```{r exmpSalarayPhD, fig.cap="Salary versus years after PhD", fig.asp = .6, out.width = "90%", fig.align = 'left-aligned'} -->
<!-- # load data into R -->
<!-- data(Salaries, package = "carData") -->
<!-- # first six rows of the data -->
<!-- head(Salaries) -->
<!-- # Regression: -->
<!-- eq <- lm(salary~.,data=Salaries) -->
<!-- summary(eq) -->
<!-- par(mfrow=c(1,1)) -->
<!-- par(plt=c(.2,.95,.2,.95)) -->
<!-- plot(salary/1000~yrs.since.phd,pch=19,xlab="years since PhD",ylab="Salary",data=Salaries,las=1) -->
<!-- abline(lm(salary/1000~yrs.since.phd,data=Salaries),col="red",lwd=2) -->
<!-- ``` -->
<p>The next assumption concerns the correlation of the errors across entities.</p>
<div class="hypothesis">
<p><span id="hyp:noncorrelResid" class="hypothesis"><strong>Hypothesis 1.4  (Uncorrelated errors) </strong></span><span class="math display">\[
\forall i \ne j, \quad \mathbb{C}ov(\varepsilon_i,\varepsilon_j|\mathbf{X})=0.
\]</span></p>
</div>
<p>We will often need to work with the covariance matrix of the errors. Proposition <a href="ChapterLS.html#prp:Sigma">1.2</a> give the specific form of the covariance matrix of the errors —conditional on <span class="math inline">\(\mathbf{X}\)</span>— when both Hypotheses <a href="ChapterLS.html#hyp:homoskedasticity">1.3</a> and <a href="ChapterLS.html#hyp:noncorrelResid">1.4</a> are satisfied:</p>
<div class="proposition">
<p><span id="prp:Sigma" class="proposition"><strong>Proposition 1.2  </strong></span>If Hypotheses <a href="ChapterLS.html#hyp:homoskedasticity">1.3</a> and <a href="ChapterLS.html#hyp:noncorrelResid">1.4</a> hold, then:
<span class="math display">\[
\mathbb{V}ar(\boldsymbol\varepsilon|\mathbf{X})= \sigma^2 Id,
\]</span>
where <span class="math inline">\(Id\)</span> is the <span class="math inline">\(n \times n\)</span> identity matrix.</p>
</div>
<p>We will sometimes assume that errors are Gaussian—or normal. We will then invoke Hypothesis <a href="ChapterLS.html#hyp:normality">1.5</a>:</p>
<div class="hypothesis">
<p><span id="hyp:normality" class="hypothesis"><strong>Hypothesis 1.5  (Normal distribution) </strong></span><span class="math display">\[
\forall i, \quad \varepsilon_i \sim \mathcal{N}(0,\sigma^2).
\]</span></p>
</div>
</div>
<div id="LSquares" class="section level2" number="1.2">
<h2>
<span class="header-section-number">1.2</span> Least square estimation<a class="anchor" aria-label="anchor" href="#LSquares"><i class="fas fa-link"></i></a>
</h2>
<div id="derivation-of-the-ols-formula" class="section level3" number="1.2.1">
<h3>
<span class="header-section-number">1.2.1</span> Derivation of the OLS formula<a class="anchor" aria-label="anchor" href="#derivation-of-the-ols-formula"><i class="fas fa-link"></i></a>
</h3>
<p>In this section, we will present and study the properties of the most popular estimation approach, namely the <strong>Ordinary Least Squares (OLS)</strong> approach. As suggested by its name, the OLS estimator of <span class="math inline">\(\boldsymbol\beta\)</span> is defined as the vector <span class="math inline">\(\mathbf{b}\)</span> that minimizes the sum of squared residuals. (The <em>residuals</em> are the estimates of the <em>errors</em> <span class="math inline">\(\varepsilon_i\)</span>.)</p>
<p>For a given vector of coefficients <span class="math inline">\(\mathbf{b}=[b_1,\dots,b_K]'\)</span>, the sum of squared residuals is:
<span class="math display">\[
f(\mathbf{b}) =\sum_{i=1}^n \left(y_i - \sum_{j=1}^K x_{i,j} b_j \right)^2 = \sum_{i=1}^n (y_i - \mathbf{x}_i' \mathbf{b})^2.
\]</span>
Minimizing this sum amounts to minimizing:
<span class="math display">\[
f(\mathbf{b}) = (\mathbf{y} - \mathbf{X}\mathbf{b})'(\mathbf{y} - \mathbf{X}\mathbf{b}).
\]</span></p>
<p>Since:<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;see Proposition &lt;a href="append.html#prp:partial"&gt;8.7&lt;/a&gt;.&lt;/p&gt;'><sup>1</sup></a>
<span class="math display">\[
\frac{\partial f}{\partial \mathbf{b}}(\mathbf{b}) = - 2 \mathbf{X}'\mathbf{y} + 2 \mathbf{X}'\mathbf{X}\mathbf{b},
\]</span>
it comes that a necessary first-order condition (FOC) is:
<span class="math display" id="eq:OLSFOC">\[\begin{equation}
\mathbf{X}'\mathbf{X}\mathbf{b} = \mathbf{X}'\mathbf{y}.\tag{1.2}
\end{equation}\]</span>
Under Assumption <a href="ChapterLS.html#hyp:fullrank">1.1</a>, <span class="math inline">\(\mathbf{X}'\mathbf{X}\)</span> is invertible. Hence:
<span class="math display">\[
\boxed{\mathbf{b} = (\mathbf{X}'\mathbf{X})^{-1} \mathbf{X}'\mathbf{y}.}
\]</span>
Vector <span class="math inline">\(\mathbf{b}\)</span> minimizes the sum of squared residuals. (<span class="math inline">\(f\)</span> is a non-negative quadratic function, it therefore admits a minimum.)</p>
<p>We have:
<span class="math display">\[
\mathbf{y} = \underbrace{\mathbf{X}\mathbf{b}}_{\mbox{fitted values } (\hat{\mathbf{y}})} + \underbrace{\mathbf{e}}_{\mbox{residuals}}
\]</span></p>
<p>The estimated residuals are:
<span class="math display" id="eq:Mres">\[\begin{equation}
\mathbf{e} = \mathbf{y} - \mathbf{X} (\mathbf{X}'\mathbf{X})^{-1} \mathbf{X}' \mathbf{y} = \mathbf{M} \mathbf{y},\tag{1.3}
\end{equation}\]</span>
where <span class="math inline">\(\mathbf{M} := \mathbf{I} - \mathbf{X} (\mathbf{X}'\mathbf{X})^{-1} \mathbf{X}'\)</span> is called the <strong>residual maker</strong> matrix.</p>
<p>Moreover, the fitted values <span class="math inline">\(\hat{\mathbf{y}}\)</span> are given by:
<span class="math display" id="eq:Proj">\[\begin{equation}
\hat{\mathbf{y}}=\mathbf{X} (\mathbf{X}'\mathbf{X})^{-1} \mathbf{X}' \mathbf{y} = \mathbf{P} \mathbf{y},\tag{1.4}
\end{equation}\]</span>
where <span class="math inline">\(\mathbf{P}=\mathbf{X} (\mathbf{X}'\mathbf{X})^{-1} \mathbf{X}'\)</span> is a <strong>projection matrix</strong>.</p>
<p>These matrices <span class="math inline">\(\mathbf{M}\)</span> and <span class="math inline">\(\mathbf{P}\)</span> are such that:</p>
<ul>
<li>
<span class="math inline">\(\mathbf{M} \mathbf{X} = \mathbf{0}\)</span>: if one regresses one of the explanatory variables on <span class="math inline">\(\mathbf{X}\)</span>, the residuals are null.</li>
<li>
<span class="math inline">\(\mathbf{M}\mathbf{y}=\mathbf{M}\boldsymbol\varepsilon\)</span> (because <span class="math inline">\(\mathbf{y} = \mathbf{X}\boldsymbol\beta + \boldsymbol\varepsilon\)</span> and <span class="math inline">\(\mathbf{M} \mathbf{X} = \mathbf{0}\)</span>).</li>
</ul>
<p>Here are some additional properties of <span class="math inline">\(\mathbf{M}\)</span> and <span class="math inline">\(\mathbf{P}\)</span>:</p>
<ul>
<li>
<span class="math inline">\(\mathbf{M}\)</span> is symmetric (<span class="math inline">\(\mathbf{M} = \mathbf{M}'\)</span>) and idempotent (<span class="math inline">\(\mathbf{M} = \mathbf{M}^2 = \mathbf{M}^k\)</span> for <span class="math inline">\(k&gt;0\)</span>).</li>
<li>
<span class="math inline">\(\mathbf{P}\)</span> is symmetric and idempotent.</li>
<li>
<span class="math inline">\(\mathbf{P}\mathbf{X} = \mathbf{X}\)</span>.</li>
<li>
<span class="math inline">\(\mathbf{P} \mathbf{M} = \mathbf{M} \mathbf{P} = 0\)</span>.</li>
<li>
<span class="math inline">\(\mathbf{y} = \mathbf{P}\mathbf{y} + \mathbf{M}\mathbf{y}\)</span> (decomposition of <span class="math inline">\(\mathbf{y}\)</span> into two orthogonal parts).</li>
</ul>
<p>It is easily checked that <span class="math inline">\(\mathbf{X}'\mathbf{e}=0\)</span>. Each column of <span class="math inline">\(\mathbf{X}\)</span> is therefore orthogonal to <span class="math inline">\(\mathbf{e}\)</span>. In particular, if an intercept is included in the regression (<span class="math inline">\(x_{i,1} \equiv 1\)</span> for all <span class="math inline">\(i\)</span>’s, i.e., the first column of <span class="math inline">\(\mathbf{X}\)</span> is filled with ones), the average of the residuals is null.</p>
<div class="example">
<p><span id="exm:bivar" class="example"><strong>Example 1.1  (Bivariate case) </strong></span>Consider a bivariate situation, where we regress <span class="math inline">\(y_i\)</span> on a constant and an explanatory variable <span class="math inline">\(w_i\)</span>. We have <span class="math inline">\(K=2\)</span>, and <span class="math inline">\(\mathbf{X}\)</span> is a <span class="math inline">\(n \times 2\)</span> matrix whose <span class="math inline">\(i^{th}\)</span> row is <span class="math inline">\([x_{i,1},x_{i,2}]\)</span>, with <span class="math inline">\(x_{i,1}=1\)</span> (to account for the intercept) and with <span class="math inline">\(w_i = x_{i,2}\)</span> (say).</p>
<p>We have:
<span class="math display">\[\begin{eqnarray*}
\mathbf{X}'\mathbf{X} &amp;=&amp;
\left[\begin{array}{cc}
n &amp; \sum_i w_i \\
\sum_i w_i &amp; \sum_i w_i^2
\end{array}
\right],\\
(\mathbf{X}'\mathbf{X})^{-1} &amp;=&amp;
\frac{1}{n\sum_i w_i^2-(\sum_i w_i)^2}
\left[\begin{array}{cc}
\sum_i w_i^2 &amp; -\sum_i w_i \\
-\sum_i w_i &amp; n
\end{array}
\right],\\
(\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\mathbf{y} &amp;=&amp;
\frac{1}{n\sum_i w_i^2-(\sum_i w_i)^2}
\left[\begin{array}{c}
\sum_i w_i^2\sum_i y_i -\sum_i w_i \sum_i w_iy_i \\
-\sum_i w_i \sum_i y_i + n \sum_i w_i y_i
\end{array}
\right]\\
&amp;=&amp; \frac{1}{\frac{1}{n}\sum_i(w_i - \bar{w})^2}
\left[\begin{array}{c}
\frac{\bar{y}}{n}\sum_i w_i^2 -\frac{\bar{w}}{n}\sum_i w_iy_i \\
\frac{1}{n}\sum_i (w_i-\bar{w})(y_i-\bar{y})
\end{array}
\right].
\end{eqnarray*}\]</span></p>
<p>It can be seen that the second element of <span class="math inline">\(\mathbf{b}=(\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\mathbf{y}\)</span> is:
<span class="math display">\[
b_2 = \frac{\overline{\mathbb{C}ov(W,Y)}}{\overline{\mathbb{V}ar(W)}},
\]</span>
where <span class="math inline">\(\overline{\mathbb{C}ov(W,Y)}\)</span> and <span class="math inline">\(\overline{\mathbb{V}ar(W)}\)</span> are sample estimates.</p>
<p>Since there is a constant in the regression, we have <span class="math inline">\(b_1 = \bar{y} - b_2 \bar{w}\)</span>.</p>
</div>
</div>
<div id="properties-of-the-ols-estimate-small-sample" class="section level3" number="1.2.2">
<h3>
<span class="header-section-number">1.2.2</span> Properties of the OLS estimate (small sample)<a class="anchor" aria-label="anchor" href="#properties-of-the-ols-estimate-small-sample"><i class="fas fa-link"></i></a>
</h3>
<p>The OLS properties stated in Proposition <a href="ChapterLS.html#prp:propOLS">1.3</a> are valid for any sample size <span class="math inline">\(n\)</span>:</p>
<div class="proposition">
<p><span id="prp:propOLS" class="proposition"><strong>Proposition 1.3  (Properties of the OLS estimator) </strong></span>We have:</p>
<ol style="list-style-type: lower-roman">
<li><p>Under Assumptions <a href="ChapterLS.html#hyp:fullrank">1.1</a> and <a href="ChapterLS.html#hyp:exogeneity">1.2</a>, the OLS estimator is linear and unbiased.</p></li>
<li><p>Under Hypotheses <a href="ChapterLS.html#hyp:fullrank">1.1</a> to <a href="ChapterLS.html#hyp:noncorrelResid">1.4</a>, the conditional covariance matrix of <span class="math inline">\(\mathbf{b}\)</span> is: <span class="math inline">\(\mathbb{V}ar(\mathbf{b}|\mathbf{X}) = \sigma^2 (\mathbf{X}'\mathbf{X})^{-1}\)</span>.</p></li>
</ol>
</div>
<div class="proof">
<p><span id="unlabeled-div-2" class="proof"><em>Proof</em>. </span>Under Hypothesis <a href="ChapterLS.html#hyp:fullrank">1.1</a>, <span class="math inline">\(\mathbf{X}'\mathbf{X}\)</span> can be inverted. We have:
<span class="math display">\[
\mathbf{b} = (\mathbf{X}'\mathbf{X})^{-1} \mathbf{X}'\mathbf{y} = \boldsymbol\beta + (\mathbf{X}'\mathbf{X})^{-1} \mathbf{X}' \mathbf{\varepsilon}.
\]</span></p>
<ol style="list-style-type: lower-roman">
<li>Let us consider the expectation of the last term, i.e. <span class="math inline">\(\mathbb{E}((\mathbf{X}'\mathbf{X})^{-1} \mathbf{X}' \mathbf{\varepsilon})\)</span>. Using the law of iterated expectations, we obtain:
<span class="math display">\[
\mathbb{E}((\mathbf{X}'\mathbf{X})^{-1} \mathbf{X}' \mathbf{\varepsilon}) = \mathbb{E}(\mathbb{E}[(\mathbf{X}'\mathbf{X})^{-1} \mathbf{X}' \mathbf{\varepsilon}|\mathbf{X}]) = \mathbb{E}((\mathbf{X}'\mathbf{X})^{-1} \mathbf{X}'\mathbb{E}[\mathbf{\varepsilon}|\mathbf{X}]).
\]</span>
By Hypothesis <a href="ChapterLS.html#hyp:exogeneity">1.2</a>, we have <span class="math inline">\(\mathbb{E}[\mathbf{\varepsilon}|\mathbf{X}]=0\)</span>. Hence <span class="math inline">\(\mathbb{E}((\mathbf{X}'\mathbf{X})^{-1} \mathbf{X}' \mathbf{\varepsilon}) =0\)</span> and result (i) follows.</li>
<li>
<span class="math inline">\(\mathbb{V}ar(\mathbf{b}|\mathbf{X}) = (\mathbf{X}'\mathbf{X})^{-1} \mathbf{X}' \mathbb{E}(\boldsymbol\varepsilon\boldsymbol\varepsilon'|\mathbf{X}) \mathbf{X} (\mathbf{X}'\mathbf{X})^{-1}\)</span>.
By Prop. <a href="ChapterLS.html#prp:Sigma">1.2</a>, if <a href="ChapterLS.html#hyp:homoskedasticity">1.3</a> and <a href="ChapterLS.html#hyp:noncorrelResid">1.4</a> hold, then we have <span class="math inline">\(\mathbb{E}(\boldsymbol\varepsilon\boldsymbol\varepsilon'|\mathbf{X})=\mathbb{V}ar(\boldsymbol\varepsilon|\mathbf{X})=\sigma^2 Id\)</span>.</li>
</ol>
</div>
<p>Together, Hypotheses <a href="ChapterLS.html#hyp:fullrank">1.1</a> to <a href="ChapterLS.html#hyp:noncorrelResid">1.4</a> form the so-called Gauss-Markov set of assumptions. Under these assumptions, the OLS estimator feature the lowest possible variance within the family of linear unbiased estimates of <span class="math inline">\(\boldsymbol\beta\)</span>:</p>
<div class="theorem">
<p><span id="thm:GaussMarkov" class="theorem"><strong>Theorem 1.1  (Gauss-Markov Theorem) </strong></span>Under Assumptions <a href="ChapterLS.html#hyp:fullrank">1.1</a> to <a href="ChapterLS.html#hyp:noncorrelResid">1.4</a>, for any vector <span class="math inline">\(w\)</span>, the minimum-variance linear unbiased estimator of <span class="math inline">\(w' \boldsymbol\beta\)</span> is <span class="math inline">\(w' \mathbf{b}\)</span>, where <span class="math inline">\(\mathbf{b}\)</span> is the least squares estimator. (BLUE: Best Linear Unbiased Estimator.)</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-3" class="proof"><em>Proof</em>. </span>Consider <span class="math inline">\(\mathbf{b}^* = C \mathbf{y}\)</span>, another linear unbiased estimator of <span class="math inline">\(\boldsymbol\beta\)</span>. Since it is unbiased, we must have <span class="math inline">\(\mathbb{E}(C\mathbf{y}|\mathbf{X}) = \mathbb{E}(C\mathbf{X}\boldsymbol\beta + C\boldsymbol\varepsilon|\mathbf{X}) = \boldsymbol\beta\)</span>. We have <span class="math inline">\(\mathbb{E}(C\boldsymbol\varepsilon|\mathbf{X})=C\mathbb{E}(\boldsymbol\varepsilon|\mathbf{X})=0\)</span> (by <a href="ChapterLS.html#hyp:exogeneity">1.2</a>). Therefore <span class="math inline">\(\mathbf{b}^*\)</span> is unbiased if <span class="math inline">\(\mathbb{E}(C\mathbf{X})\boldsymbol\beta=\boldsymbol\beta\)</span>. This has to be the case for any <span class="math inline">\(\boldsymbol\beta\)</span>, which implies that we must have <span class="math inline">\(C\mathbf{X}=\mathbf{I}\)</span>. Let us compute <span class="math inline">\(\mathbb{V}ar(\mathbf{b^*}|\mathbf{X})\)</span>. For this, we introduce <span class="math inline">\(D = C - (\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\)</span>, which is such that <span class="math inline">\(D\mathbf{y}=\mathbf{b}^*-\mathbf{b}\)</span>. The fact that <span class="math inline">\(C\mathbf{X}=\mathbf{I}\)</span> implies that <span class="math inline">\(D\mathbf{X} = \mathbf{0}\)</span>. We have <span class="math inline">\(\mathbb{V}ar(\mathbf{b^*}|\mathbf{X}) = \mathbb{V}ar(C \mathbf{y}|\mathbf{X}) =\mathbb{V}ar(C \boldsymbol\varepsilon|\mathbf{X}) = \sigma^2CC'\)</span> (by Assumptions <a href="ChapterLS.html#hyp:homoskedasticity">1.3</a> and <a href="ChapterLS.html#hyp:noncorrelResid">1.4</a>, see Prop. <a href="ChapterLS.html#prp:Sigma">1.2</a>). Using <span class="math inline">\(C=D+(\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\)</span> and exploiting the fact that <span class="math inline">\(D\mathbf{X} = \mathbf{0}\)</span> leads to:
<span class="math display">\[
\mathbb{V}ar(\mathbf{b^*}|\mathbf{X}) =\sigma^2\left[(D+(\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}')(D+(\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}')'\right] = \mathbb{V}ar(\mathbf{b}|\mathbf{X}) + \sigma^2 \mathbf{D}\mathbf{D}'.
\]</span>
Therefore, we have
<span class="math display">\[\begin{eqnarray*}
&amp;&amp;\mathbb{V}ar(w'\mathbf{b^*}|\mathbf{X})=w'\mathbb{V}ar(\mathbf{b}|\mathbf{X})w + \sigma^2 w'\mathbf{D}\mathbf{D}'w\\
&amp;\ge&amp; w'\mathbb{V}ar(\mathbf{b}|\mathbf{X})w=\mathbb{V}ar(w'\mathbf{b}|\mathbf{X}).
\end{eqnarray*}\]</span></p>
</div>
<p>The Frish-Waugh theorem (Theorem <a href="ChapterLS.html#thm:FW">1.2</a>) reveals the relationship between the OLS estimator and the notion of partial correlation coefficient. Consider the linear least square regression of <span class="math inline">\(\mathbf{y}\)</span> on <span class="math inline">\(\mathbf{X}\)</span>. We introduce the notations:</p>
<ul>
<li>
<span class="math inline">\(\mathbf{b}^{\mathbf{y}/\mathbf{X}}\)</span>: OLS estimates of <span class="math inline">\(\boldsymbol\beta\)</span>,</li>
<li>
<span class="math inline">\(\mathbf{M}^{\mathbf{X}}\)</span>: residual-maker matrix of any regression on <span class="math inline">\(\mathbf{X}\)</span> (given by <span class="math inline">\(\mathbf{I} - \mathbf{X} (\mathbf{X}'\mathbf{X})^{-1} \mathbf{X}'\)</span>),</li>
<li>
<span class="math inline">\(\mathbf{P}^{\mathbf{X}}\)</span>: projection matrix of any regression on <span class="math inline">\(\mathbf{X}\)</span> (given by <span class="math inline">\(\mathbf{X} (\mathbf{X}'\mathbf{X})^{-1} \mathbf{X}'\)</span>).</li>
</ul>
<p>Let us split the set of explanatory variables into two: <span class="math inline">\(\mathbf{X} = [\mathbf{X}_1,\mathbf{X}_2]\)</span>. With obvious notations: <span class="math inline">\(\mathbf{b}^{\mathbf{y}/\mathbf{X}}=[\mathbf{b}_1',\mathbf{b}_2']'\)</span>.</p>
<div class="theorem">
<p><span id="thm:FW" class="theorem"><strong>Theorem 1.2  (Frisch-Waugh Theorem) </strong></span>We have:
<span class="math display">\[
\mathbf{b}_2 = \mathbf{b}^{\mathbf{M^{\mathbf{X}_1}y}/\mathbf{M^{\mathbf{X}_1}\mathbf{X}_2}}.
\]</span></p>
</div>
<div class="proof">
<p><span id="unlabeled-div-4" class="proof"><em>Proof</em>. </span>The minimization of the least squares leads to (these are first-order conditions, see Eq. <a href="ChapterLS.html#eq:OLSFOC">(1.2)</a>):
<span class="math display">\[
\left[ \begin{array}{cc} \mathbf{X}_1'\mathbf{X}_1 &amp; \mathbf{X}_1'\mathbf{X}_2 \\ \mathbf{X}_2'\mathbf{X}_1 &amp; \mathbf{X}_2'\mathbf{X}_2\end{array}\right]
\left[ \begin{array}{c} \mathbf{b}_1 \\ \mathbf{b}_2\end{array}\right] =
\left[ \begin{array}{c} \mathbf{X}_1' \mathbf{y} \\ \mathbf{X}_2' \mathbf{y} \end{array}\right].
\]</span>
Use the first-row block of equations to solve for <span class="math inline">\(\mathbf{b}_1\)</span> first; it comes as a function of <span class="math inline">\(\mathbf{b}_2\)</span>. Then use the second set of equations to solve for <span class="math inline">\(\mathbf{b}_2\)</span>, which leads to:
<span class="math display">\[\begin{eqnarray*}
\mathbf{b}_2 &amp;=&amp; [\mathbf{X}_2'\mathbf{X}_2 - \mathbf{X}_2'\mathbf{X}_1(\mathbf{X}_1'\mathbf{X}_1)\mathbf{X}_1'\mathbf{X}_2]^{-1}\mathbf{X}_2'(Id - \mathbf{X}_1(\mathbf{X}_1'\mathbf{X}_1)\mathbf{X}_1')\mathbf{y}\\
&amp;=&amp; [\mathbf{X}_2' \mathbf{M}^{\mathbf{X}_1}\mathbf{X}_2]^{-1}\mathbf{X}_2'\mathbf{M}^{\mathbf{X}_1}\mathbf{y}.
\end{eqnarray*}\]</span>
Using the fact that <span class="math inline">\(\mathbf{M}^{\mathbf{X}_1}\)</span> is idempotent and symmetric leads to the result.</p>
</div>
<p>This suggests a second way of estimating <span class="math inline">\(\mathbf{b}_2\)</span>:</p>
<ol style="list-style-type: decimal">
<li>Regress <span class="math inline">\(Y\)</span> on <span class="math inline">\(X_1\)</span>, regress <span class="math inline">\(X_2\)</span> on <span class="math inline">\(X_1\)</span>.</li>
<li>Regress the residuals associated with the former regression on those associated with the latter regressions.</li>
</ol>
<p>This is illustrated by the following code, where we run different regressions involving the number of Google searches for “parapluie” (<em>umbrella</em> in French). In the broad specification, we regress it on French precipitations and month dummies. Next, we deseasonalize both the dependent variable and the precipitations by regressing them on the month dummies. As stated by Theorem <a href="ChapterLS.html#thm:FW">1.2</a>, regressing deseasonalized Google searches on deseasonalized precipitations give the same coefficient as in the baseline regression.</p>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">AEC</span><span class="op">)</span></span>
<span><span class="va">dummies</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">as.matrix</a></span><span class="op">(</span><span class="va">parapluie</span><span class="op">[</span>,<span class="fl">4</span><span class="op">:</span><span class="fl">14</span><span class="op">]</span><span class="op">)</span></span>
<span><span class="va">eq_all</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">parapluie</span><span class="op">~</span><span class="va">dummies</span><span class="op">+</span><span class="va">precip</span>,data<span class="op">=</span><span class="va">parapluie</span><span class="op">)</span></span>
<span><span class="va">deseas_parapluie</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">parapluie</span><span class="op">~</span><span class="va">dummies</span>,data<span class="op">=</span><span class="va">parapluie</span><span class="op">)</span><span class="op">$</span><span class="va">residuals</span></span>
<span><span class="va">deseas_precip</span>    <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">precip</span><span class="op">~</span><span class="va">dummies</span>,data<span class="op">=</span><span class="va">parapluie</span><span class="op">)</span><span class="op">$</span><span class="va">residuals</span></span>
<span><span class="va">eq_frac</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">deseas_parapluie</span><span class="op">~</span><span class="va">deseas_precip</span><span class="op">-</span><span class="fl">1</span><span class="op">)</span></span>
<span><span class="fu">stargazer</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/stargazer/man/stargazer.html">stargazer</a></span><span class="op">(</span><span class="va">eq_all</span>,<span class="va">eq_frac</span>,omit<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="fl">11</span>,<span class="st">"Constant"</span><span class="op">)</span>,type<span class="op">=</span><span class="st">"text"</span>,</span>
<span>                     omit.stat <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"f"</span>,<span class="st">"ser"</span><span class="op">)</span>,digits<span class="op">=</span><span class="fl">5</span>,</span>
<span>                     add.lines<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">'Monthly dummy'</span>,<span class="st">'Yes'</span>,<span class="st">'No'</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<pre><code>## 
## ==========================================
##                   Dependent variable:     
##               ----------------------------
##                parapluie  deseas_parapluie
##                   (1)           (2)       
## ------------------------------------------
## precip        0.13001***                  
##                (0.03594)                  
##                                           
## deseas_precip                0.13001***   
##                              (0.03277)    
##                                           
## ------------------------------------------
## Monthly dummy     Yes            No       
## Observations      72             72       
## R2              0.51793       0.18148     
## Adjusted R2     0.41988       0.16995     
## ==========================================
## Note:          *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01</code></pre>
<p>When <span class="math inline">\(b_2\)</span> is scalar (and then <span class="math inline">\(\mathbf{X}_2\)</span> is of dimension <span class="math inline">\(n \times 1\)</span>), Theorem <a href="ChapterLS.html#thm:FW">1.2</a> gives the expression of the <strong>partial regression coefficient</strong> <span class="math inline">\(b_2\)</span>:
<span class="math display">\[
b_2 = \frac{\mathbf{X}_2'M^{\mathbf{X}_1}\mathbf{y}}{\mathbf{X}_2'M^{\mathbf{X}_1}\mathbf{X}_2}.
\]</span></p>
</div>
<div id="goodness-of-fit" class="section level3" number="1.2.3">
<h3>
<span class="header-section-number">1.2.3</span> Goodness of fit<a class="anchor" aria-label="anchor" href="#goodness-of-fit"><i class="fas fa-link"></i></a>
</h3>
<p>Define the total variation in <span class="math inline">\(y\)</span> as the sum of squared deviations (from the sample mean):
<span class="math display">\[
TSS = \sum_{i=1}^{n} (y_i - \bar{y})^2.
\]</span>
We have:
<span class="math display">\[
\mathbf{y} = \mathbf{X}\mathbf{b} + \mathbf{e} = \hat{\mathbf{y}} + \mathbf{e}
\]</span>
In the following, we assume that the regression includes a constant (i.e. for all <span class="math inline">\(i\)</span>, <span class="math inline">\(x_{i,1}=1\)</span>). Denote by <span class="math inline">\(\mathbf{M}^0\)</span> the matrix that transforms observations into deviations from sample means. Using that <span class="math inline">\(\mathbf{M}^0 \mathbf{e} = \mathbf{e}\)</span> and that <span class="math inline">\(\mathbf{X}' \mathbf{e}=0\)</span>, we have:
<span class="math display">\[\begin{eqnarray*}
\underbrace{\mathbf{y}'\mathbf{M}^0\mathbf{y}}_{\mbox{Total sum of sq.}} &amp;=&amp; (\mathbf{X}\mathbf{b} + \mathbf{e})' \mathbf{M}^0 (\mathbf{X}\mathbf{b} + \mathbf{e})\\
&amp;=&amp; \underbrace{\mathbf{b}' \mathbf{X}' \mathbf{M}^0 \mathbf{X}\mathbf{b}}_{\mbox{"Explained" sum of sq.}} + \underbrace{\mathbf{e}'\mathbf{e}}_{\mbox{Sum of sq. residuals}}\\
TSS &amp;=&amp; Expl.SS + SSR.
\end{eqnarray*}\]</span></p>
<p>We can now define the coefficient of determination:
<span class="math display" id="eq:RR2">\[\begin{equation}
\boxed{\mbox{Coefficient of determination} = \frac{Expl.SS}{TSS} = 1 - \frac{SSR}{TSS} = 1 - \frac{\mathbf{e}'\mathbf{e}}{\mathbf{y}'\mathbf{M}^0\mathbf{y}}.}\tag{1.5}
\end{equation}\]</span></p>
<p>It can be shown (<span class="citation">Greene (<a href="references.html#ref-Greene2003Econometric">2003</a>)</span>, Section 3.5) that:
<span class="math display">\[
\mbox{Coefficient of determination} = \frac{[\sum_{i=1}^n(y_i - \bar{y})(\hat{y_i} - \bar{y})]^2}{\sum_{i=1}^n(y_i - \bar{y})^2 \sum_{i=1}^n(\hat{y_i} - \bar{y})^2}.
\]</span>
That is, the <span class="math inline">\(R^2\)</span> is the sample squared correlation between <span class="math inline">\(y\)</span> and the (regression-implied) <span class="math inline">\(y\)</span>’s predictions.</p>
<!-- Figure \@ref(fig:R2) compares two situations: in Panel XXXX -->
<!-- ```{r R2} -->
<!-- par(mfrow=c(1,2)) -->
<!-- par(plt=c(.3,.95,.2,.85)) -->
<!-- N <- 100 -->
<!-- eps <- rnorm(N);X <- rnorm(N) -->
<!-- Y <- 1 + X + eps -->
<!-- plot(X,Y,pch=19,main="(a) Low R2") -->
<!-- Y <- 1 + X + .1*eps -->
<!-- plot(X,Y,pch=19,main="(b) High R2") -->
<!-- ``` -->
<p>The hgher the <span class="math inline">\(R^2\)</span>, the higher the goodness of fit of a model. One however has to be cautious with <span class="math inline">\(R^2\)</span>. Indeed, it is easy to increase it: it suffices to add explanatory variables. As stated by Proposition <a href="ChapterLS.html#prp:chgeInR2">1.5</a>, adding an explanatory variable (even if it does not truly relate to the dependent variable) mechanically results in an increase in the <span class="math inline">\(R^2\)</span>. In the limit, taking any set of <span class="math inline">\(n\)</span> non-linearly-dependent explanatory variables (i.e., variables satisfying Hypothesis <a href="ChapterLS.html#hyp:fullrank">1.1</a>) results in a <span class="math inline">\(R^2\)</span> equal to one.</p>
<div class="proposition">
<p><span id="prp:chgeR2" class="proposition"><strong>Proposition 1.4  (Change in SSR when a variable is added) </strong></span>We have:
<span class="math display" id="eq:uu">\[\begin{equation}
\mathbf{u}'\mathbf{u} = \mathbf{e}'\mathbf{e} - c^2(\mathbf{z^*}'\mathbf{z^*}) \qquad (\le \mathbf{e}'\mathbf{e}) \tag{1.6}
\end{equation}\]</span>
where (i) <span class="math inline">\(\mathbf{u}\)</span> and <span class="math inline">\(\mathbf{e}\)</span> are the residuals in the regressions of <span class="math inline">\(\mathbf{y}\)</span> on <span class="math inline">\([\mathbf{X},\mathbf{z}]\)</span> and of <span class="math inline">\(\mathbf{y}\)</span> on <span class="math inline">\(\mathbf{X}\)</span>, respectively, (ii) <span class="math inline">\(c\)</span> is the regression coefficient on <span class="math inline">\(\mathbf{z}\)</span> in the former regression and where <span class="math inline">\(\mathbf{z}^*\)</span> are the residuals in the regression of <span class="math inline">\(\mathbf{z}\)</span> on <span class="math inline">\(\mathbf{X}\)</span>.</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-5" class="proof"><em>Proof</em>. </span>The OLS estimates <span class="math inline">\([\mathbf{d}',\mathbf{c}]'\)</span> in the regression of <span class="math inline">\(\mathbf{y}\)</span> on <span class="math inline">\([\mathbf{X},\mathbf{z}]\)</span> satisfies (first-order cond., Eq. <a href="ChapterLS.html#eq:OLSFOC">(1.2)</a>):
<span class="math display">\[
\left[ \begin{array}{cc} \mathbf{X}'\mathbf{X} &amp; \mathbf{X}'\mathbf{z} \\ \mathbf{z}'\mathbf{X} &amp; \mathbf{z}'\mathbf{z}\end{array}\right]
\left[ \begin{array}{c} \mathbf{d} \\ c\end{array}\right] =
\left[ \begin{array}{c} \mathbf{X}' \mathbf{y} \\ \mathbf{z}' \mathbf{y} \end{array}\right].
\]</span>
Hence, in particular <span class="math inline">\(\mathbf{d} = \mathbf{b} - (\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\mathbf{z}c\)</span>, where <span class="math inline">\(\mathbf{b}\)</span> is the OLS of <span class="math inline">\(\mathbf{y}\)</span> on <span class="math inline">\(\mathbf{X}\)</span>. Substituting in <span class="math inline">\(\mathbf{u} = \mathbf{y} - \mathbf{X}\mathbf{d} - \mathbf{z}c\)</span>, we get <span class="math inline">\(\mathbf{u} = \mathbf{e} - \mathbf{z}^*c\)</span>. We therefore have:
<span class="math display" id="eq:uuu">\[\begin{equation}
\mathbf{u}'\mathbf{u} = (\mathbf{e} - \mathbf{z}^*c)(\mathbf{e} - \mathbf{z}^*c)= \mathbf{e}'\mathbf{e} + c^2(\mathbf{z^*}'\mathbf{z^*}) - 2 c\mathbf{z^*}'\mathbf{e}.\tag{1.7}
\end{equation}\]</span>
Now <span class="math inline">\(\mathbf{z^*}'\mathbf{e} = \mathbf{z^*}'(\mathbf{y} - \mathbf{X}\mathbf{b}) = \mathbf{z^*}'\mathbf{y}\)</span> because <span class="math inline">\(\mathbf{z}^*\)</span> are the residuals in an OLS regression on <span class="math inline">\(\mathbf{X}\)</span>. Since <span class="math inline">\(c = (\mathbf{z^*}'\mathbf{z^*})^{-1}\mathbf{z^*}'\mathbf{y^*}\)</span> (by an application of Theorem <a href="ChapterLS.html#thm:FW">1.2</a>), we have <span class="math inline">\((\mathbf{z^*}'\mathbf{z^*})c = \mathbf{z^*}'\mathbf{y^*}\)</span> and, therefore, <span class="math inline">\(\mathbf{z^*}'\mathbf{e} = (\mathbf{z^*}'\mathbf{z^*})c\)</span>. Inserting this in Eq. <a href="ChapterLS.html#eq:uuu">(1.7)</a> leads to the results.</p>
</div>
<div class="proposition">
<p><span id="prp:chgeInR2" class="proposition"><strong>Proposition 1.5  (Change in the coefficient of determination when a variable is added) </strong></span>Denoting by <span class="math inline">\(R_W^2\)</span> the coefficient of determination in the regression of <span class="math inline">\(\mathbf{y}\)</span> on some variable <span class="math inline">\(\mathbf{W}\)</span>, we have:
<span class="math display">\[
R_{\mathbf{X},\mathbf{z}}^2 = R_{\mathbf{X}}^2 + (1-R_{\mathbf{X}}^2)(r_{yz}^\mathbf{X})^2,
\]</span>
where <span class="math inline">\(r_{yz}^\mathbf{X}\)</span> is the coefficient of partial correlation (see Definition <a href="append.html#def:partialcorrel">8.5</a>).</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-6" class="proof"><em>Proof</em>. </span>Let’s use the same notations as in Prop. <a href="ChapterLS.html#prp:chgeR2">1.4</a>. Theorem <a href="ChapterLS.html#thm:FW">1.2</a> implies that <span class="math inline">\(c = (\mathbf{z^*}'\mathbf{z^*})^{-1}\mathbf{z^*}'\mathbf{y^*}\)</span>. Using this in Eq. <a href="ChapterLS.html#eq:uu">(1.6)</a> gives <span class="math inline">\(\mathbf{u}'\mathbf{u} = \mathbf{e}'\mathbf{e} - (\mathbf{z^*}'\mathbf{y^*})^2/(\mathbf{z^*}'\mathbf{z^*})\)</span>. Using the definition of the partial correlation (Eq. <a href="append.html#eq:pc">(8.1)</a>), we get <span class="math inline">\(\mathbf{u}'\mathbf{u} = \mathbf{e}'\mathbf{e}\left(1 - (r_{yz}^\mathbf{X})^2\right)\)</span>. The results is obtained by dividing both sides of the previous equation by <span class="math inline">\(\mathbf{y}'\mathbf{M}_0\mathbf{y}\)</span>.</p>
</div>
<p>Figure <a href="ChapterLS.html#fig:R2issue">1.3</a>, below, illustrates the fact that one can obtain an <span class="math inline">\(R^2\)</span> of one by regressing a sample of length <span class="math inline">\(n\)</span> on any set of <span class="math inline">\(n\)</span> linearly-independent variables.</p>
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">n</span> <span class="op">&lt;-</span> <span class="fl">30</span>;<span class="va">Y</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span><span class="op">)</span>;<span class="va">X</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span>,<span class="va">n</span>,<span class="va">n</span><span class="op">)</span></span>
<span><span class="va">all_R2</span> <span class="op">&lt;-</span> <span class="cn">NULL</span>;<span class="va">all_adjR2</span> <span class="op">&lt;-</span> <span class="cn">NULL</span></span>
<span><span class="kw">for</span><span class="op">(</span><span class="va">j</span> <span class="kw">in</span> <span class="fl">0</span><span class="op">:</span><span class="op">(</span><span class="va">n</span><span class="op">-</span><span class="fl">1</span><span class="op">)</span><span class="op">)</span><span class="op">{</span></span>
<span>  <span class="kw">if</span><span class="op">(</span><span class="va">j</span><span class="op">==</span><span class="fl">0</span><span class="op">)</span><span class="op">{</span><span class="va">eq</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">Y</span><span class="op">~</span><span class="fl">1</span><span class="op">)</span><span class="op">}</span><span class="kw">else</span><span class="op">{</span><span class="va">eq</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">Y</span><span class="op">~</span><span class="va">X</span><span class="op">[</span>,<span class="fl">1</span><span class="op">:</span><span class="va">j</span><span class="op">]</span><span class="op">)</span><span class="op">}</span></span>
<span>  <span class="va">all_R2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">all_R2</span>,<span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">eq</span><span class="op">)</span><span class="op">$</span><span class="va">r.squared</span><span class="op">)</span></span>
<span>  <span class="va">all_adjR2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">all_adjR2</span>,<span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">eq</span><span class="op">)</span><span class="op">$</span><span class="va">adj.r.squared</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>plt<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">.15</span>,<span class="fl">.95</span>,<span class="fl">.25</span>,<span class="fl">.95</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">all_R2</span>,pch<span class="op">=</span><span class="fl">19</span>,ylim<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Extremes.html">min</a></span><span class="op">(</span><span class="va">all_adjR2</span>,na.rm <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>,<span class="fl">1</span><span class="op">)</span>,</span>
<span>     xlab<span class="op">=</span><span class="st">"number of regressors"</span>,ylab<span class="op">=</span><span class="st">"R2"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/points.html">points</a></span><span class="op">(</span><span class="va">all_adjR2</span>,pch<span class="op">=</span><span class="fl">3</span><span class="op">)</span>;<span class="fu"><a href="https://rdrr.io/r/graphics/abline.html">abline</a></span><span class="op">(</span>h<span class="op">=</span><span class="fl">0</span>,col<span class="op">=</span><span class="st">"light grey"</span>,lwd<span class="op">=</span><span class="fl">2</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/legend.html">legend</a></span><span class="op">(</span><span class="st">"topleft"</span>,<span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"R2"</span>,<span class="st">"Adjusted R2"</span><span class="op">)</span>,</span>
<span>       lty<span class="op">=</span><span class="cn">NaN</span>,col<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"black"</span><span class="op">)</span>,pch<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">19</span>,<span class="fl">3</span><span class="op">)</span>,lwd<span class="op">=</span><span class="fl">2</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: left-aligned">
<span style="display:block;" id="fig:R2issue"></span>
<img src="MicroEc_files/figure-html/R2issue-1.png" alt="This figure illustrates the monotonous increase in the $R^2$ as a function of the number of explanatory variables. In the true model, there is no explanatory variables, i.e., $y_i = \varepsilon_i$. We then take (independent) regressors and regress $y$ on the latter, progressively increasing the set of regressors." width="90%"><p class="caption">
Figure 1.3: This figure illustrates the monotonous increase in the <span class="math inline">\(R^2\)</span> as a function of the number of explanatory variables. In the true model, there is no explanatory variables, i.e., <span class="math inline">\(y_i = \varepsilon_i\)</span>. We then take (independent) regressors and regress <span class="math inline">\(y\)</span> on the latter, progressively increasing the set of regressors.
</p>
</div>
<p>In order to address the risk of adding irrelevant explanatory variables, measures of <strong>adjusted <span class="math inline">\(R^2\)</span></strong> have been proposed. Compared to the standard <span class="math inline">\(R^2\)</span>, these measures add penalties that depend on the number of covariates employed in the regression. A common adjusted <span class="math inline">\(R^2\)</span> measure, denoted by <span class="math inline">\(\bar{R}^2\)</span>, is the following:
<span class="math display">\[\begin{equation*}
\boxed{\bar{R}^2 = 1 - \frac{\mathbf{e}'\mathbf{e}/(n-K)}{\mathbf{y}'\mathbf{M}^0\mathbf{y}/(n-1)} = 1 - \frac{n-1}{n-K}(1-R^2).}
\end{equation*}\]</span></p>
</div>
<div id="inference-and-confidence-intervals-in-small-sample" class="section level3" number="1.2.4">
<h3>
<span class="header-section-number">1.2.4</span> Inference and confidence intervals (in small sample)<a class="anchor" aria-label="anchor" href="#inference-and-confidence-intervals-in-small-sample"><i class="fas fa-link"></i></a>
</h3>
<p>Under the normality assumption (Assumption <a href="ChapterLS.html#hyp:normality">1.5</a>), we know the distribution of <span class="math inline">\(\mathbf{b}\)</span> (conditional on <span class="math inline">\(\mathbf{X}\)</span>). Indeed, <span class="math inline">\(\mathbf{b} = \boldsymbol\beta + (\mathbf{X}'\mathbf{X})^{-1} \mathbf{X}'\boldsymbol\varepsilon\)</span>. Therefore, conditional on <span class="math inline">\(\mathbf{X}\)</span>, vector <span class="math inline">\(\mathbf{b}\)</span> is an affine combination of Gaussian variables—the components of <span class="math inline">\(\boldsymbol\varepsilon\)</span>. As a result, it is also Gaussian. Its distribution is therefore completely characterized by its mean and variance, and we have:
<span class="math display" id="eq:distriBcondi">\[\begin{equation}
\mathbf{b}|\mathbf{X} \sim \mathcal{N}(\boldsymbol\beta,\sigma^2(\mathbf{X}'\mathbf{X})^{-1}).\tag{1.8}
\end{equation}\]</span></p>
<p>Eq. <a href="ChapterLS.html#eq:distriBcondi">(1.8)</a> can be used to conduct inference and tests. However, in practice, we do not know <span class="math inline">\(\sigma^2\)</span> (which is a population parameter). The following proposition gives an unbiased estimate of <span class="math inline">\(\sigma^2\)</span>.</p>
<div class="proposition">
<p><span id="prp:expects2" class="proposition"><strong>Proposition 1.6  </strong></span>Under <a href="ChapterLS.html#hyp:fullrank">1.1</a> to <a href="ChapterLS.html#hyp:noncorrelResid">1.4</a>, an unbiased estimate of <span class="math inline">\(\sigma^2\)</span> is given by:
<span class="math display" id="eq:s2">\[\begin{equation}
s^2 = \frac{\mathbf{e}'\mathbf{e}}{n-K}.\tag{1.9}
\end{equation}\]</span>
(It is sometimes denoted by <span class="math inline">\(\sigma^2_{OLS}\)</span>.)</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-7" class="proof"><em>Proof</em>. </span>We have:
<span class="math display">\[\begin{eqnarray*}
\mathbb{E}(\mathbf{e}'\mathbf{e}|\mathbf{X})&amp;=&amp;\mathbb{E}(\boldsymbol{\varepsilon}'\mathbf{M}\boldsymbol{\varepsilon}|\mathbf{X})=\mathbb{E}(\mbox{Tr}(\boldsymbol{\varepsilon}'\mathbf{M}\boldsymbol{\varepsilon})|\mathbf{X}))\\
&amp;=&amp;\mbox{Tr}(\mathbf{M}\mathbb{E}(\boldsymbol{\varepsilon}\boldsymbol{\varepsilon}'|\mathbf{X}))=\sigma^2 \mbox{Tr}(\mathbf{M}).
\end{eqnarray*}\]</span>
(Note that we have <span class="math inline">\(\mathbb{E}(\boldsymbol{\varepsilon}\boldsymbol{\varepsilon}'|\mathbf{X})=\sigma^2Id\)</span> by Assumptions <a href="ChapterLS.html#hyp:homoskedasticity">1.3</a> and <a href="ChapterLS.html#hyp:noncorrelResid">1.4</a>, see Prop. <a href="ChapterLS.html#prp:Sigma">1.2</a>.) Moreover:
<span class="math display">\[\begin{eqnarray*}
\mbox{Tr}(\mathbf{M})&amp;=&amp;n-\mbox{Tr}(\mathbf{X}(\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}')\\
&amp;=&amp;n-\mbox{Tr}((\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\mathbf{X})=n-\mbox{Tr}(Id_{K \times K}),
\end{eqnarray*}\]</span>
which leads to the result.</p>
</div>
<p>Two results will prove important to produce inference:</p>
<ol style="list-style-type: lower-roman">
<li>We know the conditional distribution of <span class="math inline">\(s^2\)</span> (Prop. <a href="ChapterLS.html#prp:s2distri">1.7</a>).</li>
<li>
<span class="math inline">\(s^2\)</span> and <span class="math inline">\(\mathbf{b}\)</span> are independent random variables (Prop. <a href="ChapterLS.html#prp:indeps2b">1.8</a>).</li>
</ol>
<div class="proposition">
<p><span id="prp:s2distri" class="proposition"><strong>Proposition 1.7  </strong></span>Under <a href="ChapterLS.html#hyp:fullrank">1.1</a> to <a href="ChapterLS.html#hyp:normality">1.5</a>, we have: <span class="math inline">\(\dfrac{s^2}{\sigma^2} | \mathbf{X} \sim \frac{1}{n-K}\chi^2(n-K)\)</span>.</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-8" class="proof"><em>Proof</em>. </span>We have <span class="math inline">\(\mathbf{e}'\mathbf{e}=\boldsymbol\varepsilon'\mathbf{M}\boldsymbol\varepsilon\)</span>. <span class="math inline">\(\mathbf{M}\)</span> is an idempotent symmetric matrix. Therefore it can be decomposed as <span class="math inline">\(PDP'\)</span> where <span class="math inline">\(D\)</span> is a diagonal matrix and <span class="math inline">\(P\)</span> is an orthogonal matrix. As a result <span class="math inline">\(\mathbf{e}'\mathbf{e} = (P'\boldsymbol\varepsilon)'D(P'\boldsymbol\varepsilon)\)</span>, i.e. <span class="math inline">\(\mathbf{e}'\mathbf{e}\)</span> is a weighted sum of independent squared Gaussian variables (the entries of <span class="math inline">\(P'\boldsymbol\varepsilon\)</span> are independent because they are Gaussian —under <a href="ChapterLS.html#hyp:normality">1.5</a>— and uncorrelated). The variance of each of these i.i.d. Gaussian variable is <span class="math inline">\(\sigma^2\)</span>. Because <span class="math inline">\(\mathbf{M}\)</span> is an idempotent symmetric matrix, its eigenvalues are either 0 or 1, and its rank equals its trace (see Propositions <a href="append.html#prp:rootsidempotent">8.3</a> and <a href="append.html#prp:chi2idempotent">8.4</a>). Further, its trace is equal to <span class="math inline">\(n-K\)</span> (see proof of Eq. <a href="ChapterLS.html#eq:s2">(1.9)</a>). Therefore <span class="math inline">\(D\)</span> has <span class="math inline">\(n-K\)</span> entries equal to 1 and <span class="math inline">\(K\)</span> equal to 0. Hence, <span class="math inline">\(\mathbf{e}'\mathbf{e} = (P'\boldsymbol\varepsilon)'D(P'\boldsymbol\varepsilon)\)</span> is a sum of <span class="math inline">\(n-K\)</span> squared independent Gaussian variables of variance <span class="math inline">\(\sigma^2\)</span>. Therefore <span class="math inline">\(\frac{\mathbf{e}'\mathbf{e}}{\sigma^2} = (n-K)\frac{s^2}{\sigma^2}\)</span> is a sum of <span class="math inline">\(n-k\)</span> squared i.i.d. standard normal variables. The result follows by the definition of the chi-square distribution (see Def. <a href="append.html#def:chi2">8.13</a>).</p>
</div>
<div class="proposition">
<p><span id="prp:indeps2b" class="proposition"><strong>Proposition 1.8  </strong></span>Under Hypotheses <a href="ChapterLS.html#hyp:fullrank">1.1</a> to <a href="ChapterLS.html#hyp:normality">1.5</a>, <span class="math inline">\(\mathbf{b}\)</span> and <span class="math inline">\(s^2\)</span> are independent.</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-9" class="proof"><em>Proof</em>. </span>We have <span class="math inline">\(\mathbf{b}=\boldsymbol\beta + [\mathbf{X}'{\mathbf{X}}]^{-1}\mathbf{X}\boldsymbol\varepsilon\)</span> and <span class="math inline">\(s^2 = \boldsymbol\varepsilon' \mathbf{M} \boldsymbol\varepsilon/(n-K)\)</span>. Hence <span class="math inline">\(\mathbf{b}\)</span> is an affine combination of <span class="math inline">\(\boldsymbol\varepsilon\)</span> and <span class="math inline">\(s^2\)</span> is a quadratic combination of the same Gaussian shocks. One can write <span class="math inline">\(s^2\)</span> as <span class="math inline">\(s^2 = (\mathbf{M}\boldsymbol\varepsilon)' \mathbf{M} \boldsymbol\varepsilon/(n-K)\)</span> and <span class="math inline">\(\mathbf{b}\)</span> as <span class="math inline">\(\boldsymbol\beta + \mathbf{T}\boldsymbol\varepsilon\)</span>. Since <span class="math inline">\(\mathbf{T}\mathbf{M}=0\)</span>, <span class="math inline">\(\mathbf{T}\boldsymbol\varepsilon\)</span> and <span class="math inline">\(\mathbf{M}\boldsymbol\varepsilon\)</span> are independent (because two uncorrelated Gaussian variables are independent), therefore <span class="math inline">\(\mathbf{b}\)</span> and <span class="math inline">\(s^2\)</span>, which are functions of two sets of independent variables, are independent.</p>
</div>
<p>Consistently with Eq. <a href="ChapterLS.html#eq:distriBcondi">(1.8)</a>, under Hypotheses <a href="ChapterLS.html#hyp:fullrank">1.1</a> to <a href="ChapterLS.html#hyp:normality">1.5</a>, the <span class="math inline">\(k^{th}\)</span> entry of <span class="math inline">\(\mathbf{b}\)</span> satisfies:
<span class="math display">\[
b_k | \mathbf{X} \sim \mathcal{N}(\beta_k,\sigma^2 v_k),
\]</span>
where <span class="math inline">\(v_k\)</span> is the k<span class="math inline">\(^{th}\)</span> component of the diagonal of <span class="math inline">\((\mathbf{X}'\mathbf{X})^{-1}\)</span>.</p>
<p>Moreover, we have (Prop. <a href="ChapterLS.html#prp:s2distri">1.7</a>):
<span class="math display">\[
\frac{(n-K)s^2}{\sigma^2} | \mathbf{X} \sim \chi ^2 (n-K).
\]</span></p>
<p>As a result (using Propositions <a href="ChapterLS.html#prp:s2distri">1.7</a> and <a href="ChapterLS.html#prp:indeps2b">1.8</a>), we have:
<span class="math display" id="eq:resultstudentt">\[\begin{equation}
\boxed{t_k = \frac{\frac{b_k - \beta_k}{\sqrt{\sigma^2 v_k}}}{\sqrt{\frac{(n-K)s^2}{\sigma^2(n-K)}}} = \frac{b_k - \beta_k}{\sqrt{s^2v_k}} \sim t(n-K),}\tag{1.10}
\end{equation}\]</span>
where <span class="math inline">\(t(n-K)\)</span> denotes a Student <span class="math inline">\(t\)</span> distribution with <span class="math inline">\(n-K\)</span> degrees of freedom (see Def. <a href="append.html#def:tStudent">8.12</a>).<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;We have &lt;span class="math inline"&gt;\(\frac{b_k - \beta_k}{\sqrt{\sigma^2 v_k}} | \mathbf{X} \sim \mathcal{N}(0,1)\)&lt;/span&gt; and &lt;span class="math inline"&gt;\(\frac{(n-K)s^2}{\sigma^2} | \mathbf{X} \sim \chi ^2 (n-K)\)&lt;/span&gt;. These two distributions do not depend on &lt;span class="math inline"&gt;\(\mathbf{X}\)&lt;/span&gt; &lt;span class="math inline"&gt;\(\Rightarrow\)&lt;/span&gt; the &lt;em&gt;marginal distribution&lt;/em&gt; of &lt;span class="math inline"&gt;\(t_k\)&lt;/span&gt; is also &lt;span class="math inline"&gt;\(t\)&lt;/span&gt;.&lt;/p&gt;'><sup>2</sup></a></p>
<p>Note that <span class="math inline">\(s^2 v_k\)</span> is not exactly the conditional variance of <span class="math inline">\(b_k\)</span>: The variance of <span class="math inline">\(b_k\)</span> conditional on <span class="math inline">\(\mathbf{X}\)</span> is <span class="math inline">\(\sigma^2 v_k\)</span>. However <span class="math inline">\(s^2 v_k\)</span> is an unbiased estimate of <span class="math inline">\(\sigma^2 v_k\)</span> (by Prop. <a href="ChapterLS.html#prp:expects2">1.6</a>).</p>
<p>The previous result (Eq. <a href="ChapterLS.html#eq:resultstudentt">(1.10)</a>) can be extended to any linear combinations of elements of <span class="math inline">\(\mathbf{b}\)</span>. (Eq. <a href="ChapterLS.html#eq:resultstudentt">(1.10)</a> is for its <span class="math inline">\(k^{th}\)</span> component only.) Let us consider <span class="math inline">\(\boldsymbol\alpha'\mathbf{b}\)</span>, the OLS estimate of <span class="math inline">\(\boldsymbol\alpha'\boldsymbol\beta\)</span>. From Eq. <a href="ChapterLS.html#eq:distriBcondi">(1.8)</a>, we have:
<span class="math display">\[
\boldsymbol\alpha'\mathbf{b} | \mathbf{X} \sim \mathcal{N}(\boldsymbol\alpha'\boldsymbol\beta,\sigma^2 \boldsymbol\alpha'(\mathbf{X}'\mathbf{X})^{-1}\boldsymbol\alpha).
\]</span>
Therefore:
<span class="math display">\[
\frac{\boldsymbol\alpha'\mathbf{b} - \boldsymbol\alpha'\boldsymbol\beta}{\sqrt{\sigma^2 \boldsymbol\alpha'(\mathbf{X}'\mathbf{X})^{-1}\boldsymbol\alpha}} | \mathbf{X} \sim \mathcal{N}(0,1).
\]</span>
Using the same approach as the one used to derive Eq. <a href="ChapterLS.html#eq:resultstudentt">(1.10)</a>, one can show that Props. <a href="ChapterLS.html#prp:s2distri">1.7</a> and <a href="ChapterLS.html#prp:indeps2b">1.8</a> also imply that:
<span class="math display" id="eq:resultstudentt2">\[\begin{equation}
\boxed{\frac{\boldsymbol\alpha'\mathbf{b} - \boldsymbol\alpha'\boldsymbol\beta}{\sqrt{s^2\boldsymbol\alpha'(\mathbf{X}'\mathbf{X})^{-1}\boldsymbol\alpha}} \sim t(n-K).}\tag{1.11}
\end{equation}\]</span></p>
<div class="figure">
<span style="display:block;" id="fig:chartStudent"></span>
<img src="MicroEc_files/figure-html/chartStudent-1.png" alt="The higher the degree of freedom, the closer the distribution of $t(\nu)$ gets to the normal distribution. (Convergence in distribution.)" width="672"><p class="caption">
Figure 1.4: The higher the degree of freedom, the closer the distribution of <span class="math inline">\(t(\nu)\)</span> gets to the normal distribution. (Convergence in distribution.)
</p>
</div>
<p>What precedes is widely exploited for statistical inference in the context of linear regressions. Indeed, Eq. <a href="ChapterLS.html#eq:resultstudentt">(1.10)</a> gives a sense of the distances between <span class="math inline">\(b_k\)</span> and <span class="math inline">\(\beta_k\)</span> that can be deemed as “likely” (or, conversely, “unlikely”). For instance, it implies that, if <span class="math inline">\(\sqrt{v_k s^2}\)</span> is equal to 1 (say), then the probability to obtain <span class="math inline">\(b_k\)</span> smaller than <span class="math inline">\(\beta_k-\)</span> 4.587 <span class="math inline">\(\times \sqrt{v_k s^2}\)</span> or larger than <span class="math inline">\(\beta_k+\)</span> 4.587 <span class="math inline">\(\times \sqrt{v_k s^2}\)</span> is equal to 0.1% when <span class="math inline">\(n-K=10\)</span>.</p>
<p>That means for instance that, under the assumption that <span class="math inline">\(\beta_k=0\)</span>, it would be extremely unlikely to have obtained <span class="math inline">\(b_k/\sqrt{v_k s^2}\)</span> smaller than -4.587 or larger than 4.587. More generally, this shows that the <strong>t-statistic</strong>, i.e., the ratio <span class="math inline">\(b_k/\sqrt{v_k s^2}\)</span>, is the test statistic associated with the null hypothesis:
<span class="math display">\[
H_0: \beta_k=0.
\]</span>
Under the null hypothesis, the test statistic follows a Student-t distribution with <span class="math inline">\(n-K\)</span> degrees of freedom. The <strong>t-statistic</strong> is therefore of particular importance, and, as a result, it is routinely reported in regression outputs (see Example <a href="ChapterLS.html#exm:SHP0001">1.2</a>).</p>
<div class="example">
<p><span id="exm:SHP0001" class="example"><strong>Example 1.2  (Education and income) </strong></span>Consider regression that aims at determining covariates of households’ income. This example makes use of data from the <a href="https://forscenter.ch/projects/swiss-household-panel/">Swiss Household Panel (SHP)</a>; <code>edyear19</code> is the number of years of education and <code>age19</code> is the age of the respondent, as of 2019.</p>
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">AEC</span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://sandwich.R-Forge.R-project.org/">sandwich</a></span><span class="op">)</span></span>
<span><span class="va">shp</span><span class="op">$</span><span class="va">income</span> <span class="op">&lt;-</span> <span class="va">shp</span><span class="op">$</span><span class="va">i19ptotn</span><span class="op">/</span><span class="fl">1000</span></span>
<span><span class="va">shp</span><span class="op">$</span><span class="va">female</span> <span class="op">&lt;-</span> <span class="fl">1</span><span class="op">*</span><span class="op">(</span><span class="va">shp</span><span class="op">$</span><span class="va">sex19</span><span class="op">==</span><span class="fl">2</span><span class="op">)</span></span>
<span><span class="va">eq</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">income</span> <span class="op">~</span> <span class="va">edyear19</span> <span class="op">+</span> <span class="va">age19</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/base/AsIs.html">I</a></span><span class="op">(</span><span class="va">age19</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span> <span class="op">+</span> <span class="va">female</span>,data<span class="op">=</span><span class="va">shp</span><span class="op">)</span></span>
<span><span class="fu">lmtest</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/lmtest/man/coeftest.html">coeftest</a></span><span class="op">(</span><span class="va">eq</span><span class="op">)</span></span></code></pre></div>
<pre><code>## 
## t test of coefficients:
## 
##                Estimate  Std. Error t value  Pr(&gt;|t|)    
## (Intercept) -71.9738073   5.7082456 -12.609 &lt; 2.2e-16 ***
## edyear19      4.8442661   0.2172320  22.300 &lt; 2.2e-16 ***
## age19         3.2386215   0.2183812  14.830 &lt; 2.2e-16 ***
## I(age19^2)   -0.0289498   0.0020915 -13.842 &lt; 2.2e-16 ***
## female      -31.8089006   1.4578004 -21.820 &lt; 2.2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</code></pre>
</div>
<p>The last two columns of the previous table give the t-statistic and the p-values associated with t-tests, whose size-<span class="math inline">\(\alpha\)</span> critical region is:
<span class="math display">\[
\left]-\infty,-\Phi^{-1}_{t(n-K)}\left(1-\frac{\alpha}{2}\right)\right] \cup \left[\Phi^{-1}_{t(n-K)}\left(1-\frac{\alpha}{2}\right),+\infty\right[.
\]</span></p>
<p>We recall that the <strong>p-value</strong> is defined as the probability that <span class="math inline">\(|Z| &gt; |t|\)</span>, where <span class="math inline">\(t\)</span> is the (computed) t-statistics and where <span class="math inline">\(Z \sim t(n-K)\)</span>. That is, in the context of the t-test, the p-value is given by <span class="math inline">\(2(1 - \Phi_{t(n-K)}(|t_k|))\)</span>. See <a href="https://jrenne.shinyapps.io/tests/">this webpage</a> for details regarding the link between critical regions, p-value, and test outcomes.</p>
<p>Now, suppose we want to compute a (symmetrical) <em>confidence interval</em> <span class="math inline">\([I_{d,1-\alpha},I_{u,1-\alpha}]\)</span> that is such that <span class="math inline">\(\mathbb{P}(\beta_k \in [I_{d,1-\alpha},I_{u,1-\alpha}])=1-\alpha\)</span>. That is, we want to have: <span class="math inline">\(\mathbb{P}(\beta_k &lt; I_{d,1-\alpha})=\frac{\alpha}{2}\)</span> and <span class="math inline">\(\mathbb{P}(\beta_k &gt; I_{u,1-\alpha})=\frac{\alpha}{2}\)</span>. Let us focus on <span class="math inline">\(I_{d,1-\alpha}\)</span> to start with. Using Eq. <a href="ChapterLS.html#eq:resultstudentt">(1.10)</a>, i.e., <span class="math inline">\(t_k = \frac{b_k - \beta_k}{\sqrt{s^2v_k}} \sim t(n-K)\)</span>, we have:
<span class="math display">\[\begin{eqnarray*}
\mathbb{P}(\beta_k &lt; I_{d,1-\alpha})=\frac{\alpha}{2} &amp;\Leftrightarrow&amp; \\
\mathbb{P}\left(\frac{b_k - \beta_k}{\sqrt{s^2v_k}} &gt; \frac{b_k - I_{d,1-\alpha}}{\sqrt{s^2v_k}}\right)=\frac{\alpha}{2} &amp;\Leftrightarrow&amp; \mathbb{P}\left(t_k &gt; \frac{b_k - I_{d,1-\alpha}}{\sqrt{s^2v_k}}\right)=\frac{\alpha}{2} \Leftrightarrow\\
1 - \mathbb{P}\left(t_k \le \frac{b_k - I_{d,1-\alpha}}{\sqrt{s^2v_k}}\right)=\frac{\alpha}{2} &amp;\Leftrightarrow&amp; \frac{b_k - I_{d,1-\alpha}}{\sqrt{s^2v_k}} = \Phi^{-1}_{t(n-K)}\left(1-\frac{\alpha}{2}\right),
\end{eqnarray*}\]</span>
where <span class="math inline">\(\Phi_{t(n-K)}(\alpha)\)</span> is the c.d.f. of the <span class="math inline">\(t(n-K)\)</span> distribution (Table <a href="append.html#tab:Student">8.2</a>).</p>
<p>Doing the same for <span class="math inline">\(I_{u,1-\alpha}\)</span>, we obtain:
<span class="math display">\[\begin{eqnarray*}
&amp;&amp;[I_{d,1-\alpha},I_{u,1-\alpha}] =\\
&amp;&amp;\left[b_k - \Phi^{-1}_{t(n-K)}\left(1-\frac{\alpha}{2}\right)\sqrt{s^2v_k},b_k + \Phi^{-1}_{t(n-K)}\left(1-\frac{\alpha}{2}\right)\sqrt{s^2v_k}\right].
\end{eqnarray*}\]</span></p>
<p>Using the results presented in Example <a href="ChapterLS.html#exm:SHP0001">1.2</a>, we can compute lower and upper bounds of 95% confidence intervals for the estimated parameters as follows:</p>
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">n</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">eq</span><span class="op">$</span><span class="va">residuals</span><span class="op">)</span>; <span class="va">K</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">eq</span><span class="op">$</span><span class="va">coefficients</span><span class="op">)</span></span>
<span><span class="va">lower.b</span> <span class="op">&lt;-</span> <span class="va">eq</span><span class="op">$</span><span class="va">coefficients</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/stats/TDist.html">qt</a></span><span class="op">(</span><span class="fl">.025</span>,df<span class="op">=</span><span class="va">n</span><span class="op">-</span><span class="va">K</span><span class="op">)</span><span class="op">*</span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/diag.html">diag</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/vcov.html">vcov</a></span><span class="op">(</span><span class="va">eq</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">upper.b</span> <span class="op">&lt;-</span> <span class="va">eq</span><span class="op">$</span><span class="va">coefficients</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/stats/TDist.html">qt</a></span><span class="op">(</span><span class="fl">.975</span>,df<span class="op">=</span><span class="va">n</span><span class="op">-</span><span class="va">K</span><span class="op">)</span><span class="op">*</span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/diag.html">diag</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/vcov.html">vcov</a></span><span class="op">(</span><span class="va">eq</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cbind.html">cbind</a></span><span class="op">(</span><span class="va">lower.b</span>,<span class="va">upper.b</span><span class="op">)</span></span></code></pre></div>
<pre><code>##                  lower.b      upper.b
## (Intercept) -83.16413225 -60.78348237
## edyear19      4.41840914   5.27012310
## age19         2.81051152   3.66673148
## I(age19^2)   -0.03304986  -0.02484977
## female      -34.66674188 -28.95105932</code></pre>
</div>
<div id="Ftest" class="section level3" number="1.2.5">
<h3>
<span class="header-section-number">1.2.5</span> Testing a set of linear restrictions<a class="anchor" aria-label="anchor" href="#Ftest"><i class="fas fa-link"></i></a>
</h3>
<p>We sometimes want to test if a set of restrictions are <em>jointly</em> consistent with the data at hand. Let us formalize such a set of (<span class="math inline">\(J\)</span>) linear restrictions:
<span class="math display">\[\begin{equation}\label{eq:restrictions}
\begin{array}{ccc}
r_{1,1} \beta_1 + \dots + r_{1,K} \beta_K &amp;=&amp; q_1\\
\vdots &amp;&amp; \vdots\\
r_{J,1} \beta_1 + \dots + r_{J,K} \beta_K &amp;=&amp; q_J.
\end{array}
\end{equation}\]</span>
In matrix form, we get:
<span class="math display">\[\begin{equation}
\mathbf{R}\boldsymbol\beta = \mathbf{q}.
\end{equation}\]</span></p>
<p>Define the <em>discrepancy vector</em> <span class="math inline">\(\mathbf{m} = \mathbf{R}\mathbf{b} - \mathbf{q}\)</span>. Under the null hypothesis:
<span class="math display">\[\begin{eqnarray*}
\mathbb{E}(\mathbf{m}|\mathbf{X}) &amp;=&amp; \mathbf{R}\boldsymbol\beta - \mathbf{q} = 0 \quad \mbox{and} \\
\mathbb{V}ar(\mathbf{m}|\mathbf{X}) &amp;=&amp; \mathbf{R} \mathbb{V}ar(\mathbf{b}|\mathbf{X}) \mathbf{R}'.
\end{eqnarray*}\]</span></p>
<p>With these notations, the assumption to test is:
<span class="math display" id="eq:H0Ftest">\[\begin{equation}
\boxed{H_0: \mathbf{R}\boldsymbol\beta - \mathbf{q} = 0 \mbox{ against } H_1: \mathbf{R}\boldsymbol\beta - \mathbf{q} \ne 0.}\tag{1.12}
\end{equation}\]</span></p>
<p>Under Hypotheses <a href="ChapterLS.html#hyp:fullrank">1.1</a> to <a href="ChapterLS.html#hyp:noncorrelResid">1.4</a>, we have <span class="math inline">\(\mathbb{V}ar(\mathbf{m}|\mathbf{X}) = \sigma^2 \mathbf{R} (\mathbf{X}'\mathbf{X})^{-1} \mathbf{R}'\)</span> (see Prop. <a href="ChapterLS.html#prp:propOLS">1.3</a>). If we add the normality assumption (Hypothesis <a href="ChapterLS.html#hyp:normality">1.5</a>), we have:
<span class="math display" id="eq:W1">\[\begin{equation}
W = \mathbf{m}'\mathbb{V}ar(\mathbf{m}|\mathbf{X})^{-1}\mathbf{m} \sim \chi^2(J). \tag{1.13}
\end{equation}\]</span></p>
<p>If <span class="math inline">\(\sigma^2\)</span> was known, we could then conduct a <em>Wald test</em> (directly exploiting Eq. <a href="ChapterLS.html#eq:W1">(1.13)</a>). But this is not the case in practice and we cannot compute <span class="math inline">\(W\)</span>. We can, however, approximate it be replacing <span class="math inline">\(\sigma^2\)</span> by <span class="math inline">\(s^2\)</span> (given in Eq. <a href="ChapterLS.html#eq:s2">(1.9)</a>). The distribution of this new statistic is not <span class="math inline">\(\chi^2(J)\)</span> any more; it is an <em><span class="math inline">\(\mathcal{F}\)</span> distribution</em> (whose quantiles are shown in Table <a href="append.html#tab:Fstat">8.4</a>), and the test is called <em><span class="math inline">\(F\)</span> test</em>.</p>
<div class="proposition">
<p><span id="prp:Ftest1" class="proposition"><strong>Proposition 1.9  </strong></span>Under Hypotheses <a href="ChapterLS.html#hyp:fullrank">1.1</a> to <a href="ChapterLS.html#hyp:normality">1.5</a> and if Eq. <a href="ChapterLS.html#eq:H0Ftest">(1.12)</a> holds, we have:
<span class="math display" id="eq:defFstatistics">\[\begin{equation}
F = \frac{W}{J}\frac{\sigma^2}{s^2} = \frac{\mathbf{m}'(\mathbf{R}(\mathbf{X}'\mathbf{X})^{-1}\mathbf{R}')^{-1}\mathbf{m}}{s^2J} \sim \mathcal{F}(J,n-K),\tag{1.14}
\end{equation}\]</span>
where <span class="math inline">\(\mathcal{F}\)</span> is the distribution of the F-statistic (see Def. <a href="append.html#def:fstatistics">8.11</a>).</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-10" class="proof"><em>Proof</em>. </span>According to Eq. <a href="ChapterLS.html#eq:W1">(1.13)</a>, <span class="math inline">\(W/J \sim \chi^2(J)/J\)</span>. Moreover, the denominator (<span class="math inline">\(s^2/\sigma^2\)</span>) is <span class="math inline">\(\sim \chi^2(n-K)\)</span>. Therefore, <span class="math inline">\(F\)</span> is the ratio of a r.v. distributed as <span class="math inline">\(\chi^2(J)/J\)</span> and another distributed as <span class="math inline">\(\chi^2(n-K)/(n-K)\)</span>. It remains to verify that these r.v. are independent. Under <span class="math inline">\(H_0\)</span>, we have <span class="math inline">\(\mathbf{m} = \mathbf{R}(\mathbf{b}-\boldsymbol\beta) = \mathbf{R}(\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\boldsymbol\varepsilon\)</span>.
Therefore <span class="math inline">\(\mathbf{m}'(\mathbf{R}(\mathbf{X}'\mathbf{X})^{-1}\mathbf{R}')^{-1}\mathbf{m}\)</span> is of the form <span class="math inline">\(\boldsymbol\varepsilon'\mathbf{T}\boldsymbol\varepsilon\)</span> with <span class="math inline">\(\mathbf{T}=\mathbf{D}'\mathbf{C}\mathbf{D}\)</span> where <span class="math inline">\(\mathbf{D}=\mathbf{R}(\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\)</span> and <span class="math inline">\(\mathbf{C}=(\mathbf{R}(\mathbf{X}'\mathbf{X})^{-1}\mathbf{R}')^{-1}\)</span>. Under Hypotheses <a href="ChapterLS.html#hyp:fullrank">1.1</a> to <a href="ChapterLS.html#hyp:noncorrelResid">1.4</a>, the covariance between <span class="math inline">\(\mathbf{T}\boldsymbol\varepsilon\)</span> and <span class="math inline">\(\mathbf{M}\boldsymbol\varepsilon\)</span> is <span class="math inline">\(\sigma^2\mathbf{T}\mathbf{M} = \mathbf{0}\)</span>. Therefore, under <a href="ChapterLS.html#hyp:normality">1.5</a>, these variables are Gaussian variables with 0 covariance. Hence they are independent.</p>
</div>
<p>For large <span class="math inline">\(n-K\)</span>, the <span class="math inline">\(\mathcal{F}_{J,n-K}\)</span> distribution converges to <span class="math inline">\(\mathcal{F}_{J,\infty}=\chi^2(J)/J\)</span>. This implies that, in large samples, the F-statistic approximately has a <span class="math inline">\(\chi^2\)</span> distribution. In other words, one can approximately employ Eq. <a href="ChapterLS.html#eq:W1">(1.13)</a> to perform a Wald test (one just has to replace <span class="math inline">\(\sigma^2\)</span> with <span class="math inline">\(s^2\)</span> when computing <span class="math inline">\(\mathbb{V}ar(\mathbf{m}|\mathbf{X})\)</span>).</p>
<p>The following proposition proposes another equivalent computation of the F-statistic, based on the <span class="math inline">\(R^2\)</span> of the restricted and unrestricted linear models.</p>
<div class="proposition">
<p><span id="prp:Ftest" class="proposition"><strong>Proposition 1.10  </strong></span>The F-statistic defined by Eq. <a href="ChapterLS.html#eq:defFstatistics">(1.14)</a> is also equal to:
<span class="math display" id="eq:defFstatistics2">\[\begin{equation}
F = \frac{(R^2-R_*^2)/J}{(1-R^2)/(n-K)} =  \frac{(SSR_{restr}-SSR_{unrestr})/J}{SSR_{unrestr}/(n-K)},\tag{1.15}
\end{equation}\]</span>
where <span class="math inline">\(R_*^2\)</span> is the coef. of determination (Eq. <a href="ChapterLS.html#eq:RR2">(1.5)</a>) of the “restricted regression” <em>(SSR: sum of squared residuals.)</em></p>
</div>
<div class="proof">
<p><span id="unlabeled-div-11" class="proof"><em>Proof</em>. </span>Let’s denote by <span class="math inline">\(\mathbf{e}_*=\mathbf{y}-\mathbf{X}\mathbf{b}_*\)</span> the vector of residuals associated to the <em>restricted regression</em> (i.e. <span class="math inline">\(\mathbf{R}\mathbf{b}_*=\mathbf{q}\)</span>).
We have <span class="math inline">\(\mathbf{e}_*=\mathbf{e} - \mathbf{X}(\mathbf{b}_*-\mathbf{b})\)</span>. Using <span class="math inline">\(\mathbf{e}'\mathbf{X}=0\)</span>, we get <span class="math inline">\(\mathbf{e}_*'\mathbf{e}_*=\mathbf{e}'\mathbf{e} + (\mathbf{b}_*-\mathbf{b})'\mathbf{X}'\mathbf{X}(\mathbf{b}_*-\mathbf{b}) \ge \mathbf{e}'\mathbf{e}\)</span>.</p>
<p>By Proposition <a href="append.html#prp:constrainedLS">8.5</a> (in Appendix <a href="append.html#LinAlgebra">8.1</a>), we have: <span class="math inline">\(\mathbf{b}_*-\mathbf{b}=-(\mathbf{X}'\mathbf{X})^{-1} \mathbf{R}'\{\mathbf{R}(\mathbf{X}'\mathbf{X})^{-1}\mathbf{R}'\}^{-1}(\mathbf{R}\mathbf{b} - \mathbf{q})\)</span>. Therefore:
<span class="math display">\[
\mathbf{e}_*'\mathbf{e}_* - \mathbf{e}'\mathbf{e} = (\mathbf{R}\mathbf{b} - \mathbf{q})'[\mathbf{R}(\mathbf{X}'\mathbf{X})^{-1}\mathbf{R}']^{-1}(\mathbf{R}\mathbf{b} - \mathbf{q}).
\]</span>
This implies that the F statistic defined in Prop. <a href="ChapterLS.html#prp:Ftest1">1.9</a> is also equal to:
<span class="math display">\[
\frac{(\mathbf{e}_*'\mathbf{e}_* - \mathbf{e}'\mathbf{e})/J}{\mathbf{e}'\mathbf{e}/(n-K)},
\]</span>
which leads to the result.</p>
</div>
<p>The null hypothesis <span class="math inline">\(H_0\)</span> (Eq. <a href="ChapterLS.html#eq:H0Ftest">(1.12)</a>) of the F-test is rejected if <span class="math inline">\(F\)</span> —defined by Eq. <a href="ChapterLS.html#eq:defFstatistics">(1.14)</a> or <a href="ChapterLS.html#eq:defFstatistics2">(1.15)</a>— is higher than <span class="math inline">\(\mathcal{F}_{1-\alpha}(J,n-K)\)</span>. (Hence, this test is a one-sided test.)</p>
</div>
<div id="largeSample" class="section level3" number="1.2.6">
<h3>
<span class="header-section-number">1.2.6</span> Large Sample Properties<a class="anchor" aria-label="anchor" href="#largeSample"><i class="fas fa-link"></i></a>
</h3>
<p>Even if we relax the normality assumption (Hypothesis <a href="ChapterLS.html#hyp:normality">1.5</a>), we can approximate the finite-sample behavior of the estimators by using <em>large-sample</em> or <em>asymptotic properties</em>.</p>
<p>To begin with, we proceed under Hypothesis <a href="ChapterLS.html#hyp:fullrank">1.1</a> to <a href="ChapterLS.html#hyp:noncorrelResid">1.4</a>. (We will see, later on, how to deal with —partial— relaxations of Hypothesis <a href="ChapterLS.html#hyp:homoskedasticity">1.3</a> and <a href="ChapterLS.html#hyp:noncorrelResid">1.4</a>.)</p>
<p>Under regularity assumptions, and under Hypotheses <a href="ChapterLS.html#hyp:fullrank">1.1</a> to <a href="ChapterLS.html#hyp:noncorrelResid">1.4</a>, even if the residuals are not normally-distributed, the least square estimators can be <em>asymptotically normal</em> and inference can be performed in the same way as in small samples when Hypotheses <a href="ChapterLS.html#hyp:fullrank">1.1</a> to <a href="ChapterLS.html#hyp:normality">1.5</a> hold. This derives from Prop. <a href="ChapterLS.html#prp:asymptOLS">1.11</a> (below). The F-test (Prop. <a href="ChapterLS.html#prp:Ftest">1.10</a>) and the t-test (Eq. <a href="ChapterLS.html#eq:resultstudentt">(1.10)</a>) can then be performed.</p>
<div class="proposition">
<p><span id="prp:asymptOLS" class="proposition"><strong>Proposition 1.11  </strong></span>Under Assumptions <a href="ChapterLS.html#hyp:fullrank">1.1</a> to <a href="ChapterLS.html#hyp:noncorrelResid">1.4</a>, and assuming further that:
<span class="math display" id="eq:Qasympt">\[\begin{equation}
Q = \mbox{plim}_{n \rightarrow \infty} \frac{\mathbf{X}'\mathbf{X}}{n},\tag{1.16}
\end{equation}\]</span>
and that the <span class="math inline">\((\mathbf{x}_i,\varepsilon_i)\)</span>’s are independent (across entities <span class="math inline">\(i\)</span>), we have:
<span class="math display" id="eq:convgceOLS">\[\begin{equation}
\sqrt{n}(\mathbf{b} - \boldsymbol\beta)\overset{d} {\rightarrow} \mathcal{N}\left(0,\sigma^2Q^{-1}\right).\tag{1.17}
\end{equation}\]</span></p>
</div>
<div class="proof">
<p><span id="unlabeled-div-12" class="proof"><em>Proof</em>. </span>Since <span class="math inline">\(\mathbf{b} = \boldsymbol\beta + \left( \frac{\mathbf{X}'\mathbf{X}}{n}\right)^{-1}\left(\frac{\mathbf{X}'\boldsymbol\varepsilon}{n}\right)\)</span>, we have: <span class="math inline">\(\sqrt{n}(\mathbf{b} - \boldsymbol\beta) = \left( \frac{\mathbf{X}'\mathbf{X}}{n}\right)^{-1} \left(\frac{1}{\sqrt{n}}\right)\mathbf{X}'\boldsymbol\varepsilon\)</span>. Since <span class="math inline">\(f:A \rightarrow A^{-1}\)</span> is a continuous function (for <span class="math inline">\(A \ne \mathbf{0}\)</span>), <span class="math inline">\(\mbox{plim}_{n \rightarrow \infty} \left(\frac{\mathbf{X}'\mathbf{X}}{n}\right)^{-1} = \mathbf{Q}^{-1}\)</span> (see Prop. <a href="append.html#prp:Slutsky">8.12</a>). Let us denote by <span class="math inline">\(V_i\)</span> the vector <span class="math inline">\(\mathbf{x}_i \varepsilon_i\)</span>. Because the <span class="math inline">\((\mathbf{x}_i,\varepsilon_i)\)</span>’s are independent, the <span class="math inline">\(V_i\)</span>’s are independent as well. Their covariance matrix is <span class="math inline">\(\sigma^2\mathbb{E}(\mathbf{x}_i \mathbf{x}_i')=\sigma^2Q\)</span>. Applying the multivariate central limit theorem on vectors <span class="math inline">\(V_i\)</span> gives <span class="math inline">\(\sqrt{n}\left(\frac{1}{n}\sum_{i=1}^n \mathbf{x}_i \varepsilon_i\right) = \left(\frac{1}{\sqrt{n}}\right)\mathbf{X}'\boldsymbol\varepsilon \overset{d}{\rightarrow} \mathcal{N}(0,\sigma^2Q)\)</span>. An application of Slutsky’s theorem (Prop. <a href="append.html#prp:Slutsky">8.12</a>) then leads to the results.</p>
</div>
<p>In practice, <span class="math inline">\(\sigma^2\)</span> is approximated by <span class="math inline">\(s^2=\frac{\mathbf{e}'\mathbf{e}}{n-K}\)</span> (Eq. <a href="ChapterLS.html#eq:s2">(1.9)</a>) and <span class="math inline">\(\mathbf{Q}^{-1}\)</span> by <span class="math inline">\(\left(\frac{\mathbf{X}'\mathbf{X}}{n}\right)^{-1}\)</span>. That is, the covariance matrix of the estimator is approximated by:
<span class="math display" id="eq:sXX">\[\begin{equation}
\boxed{\widehat{\mathbb{V}ar}(\mathbf{b}) = s^2 (\mathbf{X}'\mathbf{X})^{-1}.}\tag{1.18}
\end{equation}\]</span></p>
<p>Eqs. <a href="ChapterLS.html#eq:Qasympt">(1.16)</a> and <a href="ChapterLS.html#eq:convgceOLS">(1.17)</a> respectively correspond to convergences in probability and in distribution (see Definitions <a href="append.html#def:convergenceproba">8.16</a> and <a href="append.html#def:cvgceDistri">8.19</a>, respectively).</p>
</div>
</div>
<div id="CommonPitfalls" class="section level2" number="1.3">
<h2>
<span class="header-section-number">1.3</span> Common pitfalls in linear regressions<a class="anchor" aria-label="anchor" href="#CommonPitfalls"><i class="fas fa-link"></i></a>
</h2>
<div id="multicollinearity" class="section level3" number="1.3.1">
<h3>
<span class="header-section-number">1.3.1</span> Multicollinearity<a class="anchor" aria-label="anchor" href="#multicollinearity"><i class="fas fa-link"></i></a>
</h3>
<p>Consider the model: <span class="math inline">\(y_i = \beta_1 x_{i,1} + \beta_2 x_{i,2} + \varepsilon_i\)</span>, where all variables are zero-mean and <span class="math inline">\(\mathbb{V}ar(\varepsilon_i)=\sigma^2\)</span>. We have:
<span class="math display">\[
\mathbf{X}'\mathbf{X} = \left[ \begin{array}{cc}
\sum_i x_{i,1}^2 &amp; \sum_i x_{i,1} x_{i,2} \\
\sum_i x_{i,1} x_{i,2} &amp; \sum_i x_{i,2}^2
\end{array}\right],
\]</span>
therefore:
<span class="math display">\[\begin{eqnarray*}
(\mathbf{X}'\mathbf{X})^{-1} &amp;=&amp; \frac{1}{\sum_i x_{i,1}^2\sum_i x_{i,2}^2 - (\sum_i x_{i,1} x_{i,2})^2} \left[ \begin{array}{cc}
\sum_i x_{i,2}^2 &amp; -\sum_i x_{i,1} x_{i,2} \\
-\sum_i x_{i,1} x_{i,2} &amp; \sum_i x_{i,1}^2
\end{array}\right].
\end{eqnarray*}\]</span>
The inverse of the upper-left parameter of <span class="math inline">\((\mathbf{X}'\mathbf{X})^{-1}\)</span> is:
<span class="math display" id="eq:multicollin">\[\begin{equation}
\sum_i x_{i,1}^2 - \frac{(\sum_i x_{i,1} x_{i,2})^2}{\sum_i x_{i,2}^2} = \sum_i x_{i,1}^2(1 - correl_{1,2}^2),\tag{1.19}
\end{equation}\]</span>
where <span class="math inline">\(correl_{1,2}\)</span> is the sample correlation between <span class="math inline">\(\mathbf{x}_{1}\)</span> and <span class="math inline">\(\mathbf{x}_{2}\)</span>.</p>
<p>Hence, the closer to one <span class="math inline">\(correl_{1,2}\)</span>, the higher the variance of <span class="math inline">\(b_1\)</span> (recall that the variance of <span class="math inline">\(b_1\)</span> is the upper-left component of <span class="math inline">\(\sigma^2(\mathbf{X}'\mathbf{X})^{-1}\)</span>). That is, if some of our regressors are close to a linear conbination of the other ones, then the confidence intervals will tend to be wide, which typically reduces the power of the t-test (we tend to fail to reject the null hypothesis that the coefficients are different from zero).</p>
</div>
<div id="Omitted" class="section level3" number="1.3.2">
<h3>
<span class="header-section-number">1.3.2</span> Omitted variables<a class="anchor" aria-label="anchor" href="#Omitted"><i class="fas fa-link"></i></a>
</h3>
<p>Consider the following model (the “True model”):
<span class="math display">\[
\mathbf{y} = \underbrace{\mathbf{X}_1}_{n \times K_1}\underbrace{\boldsymbol\beta_1}_{K_1 \times 1} + \underbrace{\mathbf{X}_2}_{n\times K_2}\underbrace{\boldsymbol\beta_2}_{K_2 \times 1} + \boldsymbol\varepsilon
\]</span>
If one computes <span class="math inline">\(\mathbf{b}_1\)</span> by regressing <span class="math inline">\(\mathbf{y}\)</span> on <span class="math inline">\(\mathbf{X}_1\)</span> only, one gets:
<span class="math display">\[
\mathbf{b}_1 = (\mathbf{X}_1'\mathbf{X}_1)^{-1}\mathbf{X}_1'\mathbf{y} = \boldsymbol\beta_1 + (\mathbf{X}_1'\mathbf{X}_1)^{-1}\mathbf{X}_1'\mathbf{X}_2\boldsymbol\beta_2 +
(\mathbf{X}_1'\mathbf{X}_1)^{-1}\mathbf{X}_1'\boldsymbol\varepsilon.
\]</span></p>
<p>This results in the omitted-variable formula:
<span class="math display">\[
\mathbb{E}(\mathbf{b}_1|\mathbf{X}) = \boldsymbol\beta_1 + \underbrace{(\mathbf{X}_1'\mathbf{X}_1)^{-1}(\mathbf{X}_1'\mathbf{X}_2)}_{K_1 \times K_2}\boldsymbol\beta_2.
\]</span>
(Each column of <span class="math inline">\((\mathbf{X}_1'\mathbf{X}_1)^{-1}(\mathbf{X}_1'\mathbf{X}_2)\)</span> are the OLS regressors obtained when regressing the columns of <span class="math inline">\(\mathbf{X}_2\)</span> on <span class="math inline">\(\mathbf{X}_1\)</span>.) Unless the variables included in <span class="math inline">\(\mathbf{X}_1\)</span> are orthogonal to those in <span class="math inline">\(\mathbf{X}_2\)</span>, we obtain a bias. A way to address this potential pitfall is to introduce “controls” in the specification.</p>
<!-- XXXXX -->
<!-- :::{.example #wageeduc} -->
<!-- Consider the "true model": -->
<!-- \begin{equation} -->
<!-- wage_i = \beta_0 +\beta_1 edu_i + \beta_2 ability_i + \varepsilon_i, \quad \varepsilon_i \sim i.i.d.\,\mathcal{N}(0,\sigma_\varepsilon^2) -->
<!-- \end{equation} -->
<!-- Further, we assume that the $edu$ variable is correlated to the $ability$. Specifically: -->
<!-- $$ -->
<!-- edu_i = \alpha_0 +\alpha_1 ability_i + \eta_i, \quad \eta_i \sim i.i.d.\,\mathcal{N}(0,\sigma_\eta^2). -->
<!-- $$ -->
<!-- Assume we mistakingly run the regression omitting the $ability$ variable: -->
<!-- \begin{equation} -->
<!-- wage_i = \gamma_0 +\gamma_1 edu_i + \xi_i. -->
<!-- \end{equation} -->
<!-- It can be seen that $\xi_i = \varepsilon_i - (\beta_2/\alpha_1) \eta_i \sim i.i.d.\,\mathcal{N}(0,\sigma_\varepsilon^2+(\beta_2/\alpha_1)^2\sigma_\eta^2)$ and that the population regression coefficient is $\gamma_1 = \beta_1 + \beta_2/\alpha_1 \ne \beta_1$. -->
<!-- ::: -->
<div class="example">
<p><span id="exm:CASchools" class="example"><strong>Example 1.3  </strong></span>Let us use the <a href="https://rdrr.io/cran/AER/man/CASchools.html">California Test Score dataset</a> (in the package <code>AER</code>). Assume we want to measure the effect of the students-to-teacher ratio (<code>str</code>) on student test scores (<code>testscr</code>). The following regressions show that the effect is lower when controls are added.</p>
<div class="sourceCode" id="cb13"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">AER</span><span class="op">)</span>; <span class="fu"><a href="https://rdrr.io/r/utils/data.html">data</a></span><span class="op">(</span><span class="st">"CASchools"</span><span class="op">)</span></span>
<span><span class="va">CASchools</span><span class="op">$</span><span class="va">str</span> <span class="op">&lt;-</span> <span class="va">CASchools</span><span class="op">$</span><span class="va">students</span><span class="op">/</span><span class="va">CASchools</span><span class="op">$</span><span class="va">teachers</span></span>
<span><span class="va">CASchools</span><span class="op">$</span><span class="va">testscr</span> <span class="op">&lt;-</span> <span class="fl">.5</span> <span class="op">*</span> <span class="op">(</span><span class="va">CASchools</span><span class="op">$</span><span class="va">math</span> <span class="op">+</span> <span class="va">CASchools</span><span class="op">$</span><span class="va">read</span><span class="op">)</span></span>
<span><span class="va">eq1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">testscr</span><span class="op">~</span><span class="va">str</span>,data<span class="op">=</span><span class="va">CASchools</span><span class="op">)</span></span>
<span><span class="va">eq2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">testscr</span><span class="op">~</span><span class="va">str</span><span class="op">+</span><span class="va">lunch</span>,data<span class="op">=</span><span class="va">CASchools</span><span class="op">)</span></span>
<span><span class="va">eq3</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">testscr</span><span class="op">~</span><span class="va">str</span><span class="op">+</span><span class="va">lunch</span><span class="op">+</span><span class="va">english</span>,data<span class="op">=</span><span class="va">CASchools</span><span class="op">)</span></span>
<span><span class="fu">stargazer</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/stargazer/man/stargazer.html">stargazer</a></span><span class="op">(</span><span class="va">eq1</span>,<span class="va">eq2</span>,<span class="va">eq3</span>,type<span class="op">=</span><span class="st">"text"</span>,</span>
<span>                     no.space <span class="op">=</span> <span class="cn">TRUE</span>,omit.stat<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"f"</span>,<span class="st">"ser"</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<pre><code>## 
## =============================================
##                    Dependent variable:       
##              --------------------------------
##                          testscr             
##                 (1)        (2)        (3)    
## ---------------------------------------------
## str          -2.280***  -1.117***  -0.998*** 
##               (0.480)    (0.240)    (0.239)  
## lunch                   -0.600***  -0.547*** 
##                          (0.017)    (0.022)  
## english                            -0.122*** 
##                                     (0.032)  
## Constant     698.933*** 702.911*** 700.150***
##               (9.467)    (4.700)    (4.686)  
## ---------------------------------------------
## Observations    420        420        420    
## R2             0.051      0.767      0.775   
## Adjusted R2    0.049      0.766      0.773   
## =============================================
## Note:             *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01</code></pre>
</div>
</div>
<div id="irrelevant" class="section level3" number="1.3.3">
<h3>
<span class="header-section-number">1.3.3</span> Irrelevant variable<a class="anchor" aria-label="anchor" href="#irrelevant"><i class="fas fa-link"></i></a>
</h3>
<p>Consider the <em>true model</em>:
<span class="math display">\[
\mathbf{y} = \mathbf{X}_1\boldsymbol\beta_1 + \boldsymbol\varepsilon,
\]</span>
while the <em>estimated model</em> is:
<span class="math display">\[
\mathbf{y} = \mathbf{X}_1\boldsymbol\beta_1 + \mathbf{X}_2\boldsymbol\beta_2 + \boldsymbol\varepsilon
\]</span></p>
<p>The estimates are unbiased. However, adding irrelevant explanatory variables increases the variance of the estimate of <span class="math inline">\(\boldsymbol\beta_1\)</span> (compared to the case where one uses the correct explanatory variables). This is the case unless the correlation between <span class="math inline">\(\mathbf{X}_1\)</span> and <span class="math inline">\(\mathbf{X}_2\)</span> is null, see Eq. <a href="ChapterLS.html#eq:multicollin">(1.19)</a>.</p>
<p>In other words, the estimator is <em>inefficient</em>, i.e., there exists an alternative consistent estimator whose variance is lower. The inefficiency problem can have serious consequences when testing hypotheses such as <span class="math inline">\(H_0: \beta_1 = 0\)</span>. Due to the loss of power, we might wrongly infer that the <span class="math inline">\(\mathbf{X}_1\)</span> variables are not “relevant” (<em>Type-II error, False Negative</em>).</p>
</div>
</div>
<div id="IV" class="section level2" number="1.4">
<h2>
<span class="header-section-number">1.4</span> Instrumental Variables<a class="anchor" aria-label="anchor" href="#IV"><i class="fas fa-link"></i></a>
</h2>
<!-- [Nice of interpretation of tests](http://www.learneconometrics.com/class/6243/notes/IVtests.pdf) -->
<p>The conditional mean zero assumption (Hypothesis <a href="ChapterLS.html#hyp:exogeneity">1.2</a>), according to which <span class="math inline">\(\mathbb{E}(\boldsymbol\varepsilon|\mathbf{X})=0\)</span> —which implies in particular that <span class="math inline">\(\mathbf{x}_i\)</span> and <span class="math inline">\(\varepsilon_i\)</span> are uncorrelated— is sometimes not consistent with the considered economic framework. When it is the case, the parameters of interest may still be estimated consistently by resorting to instrumental variable techniques.</p>
<p>Consider the following model:
<span class="math display" id="eq:modelIV">\[\begin{equation}
y_i = \mathbf{x_i}'\boldsymbol\beta + \varepsilon_i, \quad \mbox{where } \mathbb{E}(\varepsilon_i)=0  \mbox{ and } \mathbf{x_i}\not\perp \varepsilon_i.\tag{1.20}
\end{equation}\]</span></p>
<p>Let us illustrate how this situation may result in biased OLS estimate. Consider for instance the situation where:
<span class="math display" id="eq:exmIV">\[\begin{equation}
\mathbb{E}(\varepsilon_i)=0 \quad \mbox{and} \quad \mathbb{E}(\varepsilon_i \mathbf{x_i})=\boldsymbol\gamma,\tag{1.21}
\end{equation}\]</span>
in which case we have <span class="math inline">\(\mathbf{x}_i\not\perp \varepsilon_i\)</span> (consistently with Eq. <a href="ChapterLS.html#eq:modelIV">(1.20)</a>).</p>
<p>By the law of large numbers, <span class="math inline">\(\mbox{plim}_{n \rightarrow \infty} \mathbf{X}'\boldsymbol\varepsilon / n = \boldsymbol\gamma\)</span>. If <span class="math inline">\(\mathbf{Q}_{xx} := \mbox{plim } \mathbf{X}'\mathbf{X}/n\)</span>, the OLS estimator is not consistent because
<span class="math display">\[
\mathbf{b} = \boldsymbol\beta + (\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\boldsymbol\varepsilon \overset{p}{\rightarrow} \boldsymbol\beta + \mathbf{Q}_{xx}^{-1}\boldsymbol\gamma \ne \boldsymbol\beta.
\]</span></p>
<p>Let us now introduce the notion of instruments.</p>
<div class="definition">
<p><span id="def:instruments" class="definition"><strong>Definition 1.2  (Instrumental variables) </strong></span>The <span class="math inline">\(L\)</span>-dimensional random variable <span class="math inline">\(\mathbf{z}_i\)</span> is a valid set of instruments if:</p>
<ol style="list-style-type: lower-alpha">
<li>
<span class="math inline">\(\mathbf{z}_i\)</span> is correlated to <span class="math inline">\(\mathbf{x}_i\)</span>;</li>
<li>we have <span class="math inline">\(\mathbb{E}(\boldsymbol\varepsilon|\mathbf{Z})=0\)</span> and</li>
<li>the orthogonal projections of the <span class="math inline">\(\mathbf{x}_i\)</span>’s on the <span class="math inline">\(\mathbf{z}_i\)</span>’s are not multicollinear.</li>
</ol>
</div>
<p>Point c implies in particular that the dimension of <span class="math inline">\(\mathbf{z}_i\)</span> has to be at least as large as that of <span class="math inline">\(\mathbf{x}_i\)</span>. If <span class="math inline">\(\mathbf{z}_i\)</span> is a valid set of instruments, we have:
<span class="math display">\[
\mbox{plim}\left( \frac{\mathbf{Z}'\mathbf{y}}{n} \right) =\mbox{plim}\left( \frac{\mathbf{Z}'(\mathbf{X}\boldsymbol\beta + \boldsymbol\varepsilon)}{n} \right) = \mbox{plim}\left( \frac{\mathbf{Z}'\mathbf{X}}{n} \right)\boldsymbol\beta.
\]</span>
Indeed, by the law of large numbers, <span class="math inline">\(\frac{\mathbf{Z}'\boldsymbol\varepsilon}{n} \overset{p}{\rightarrow}\mathbb{E}(\mathbf{z}_i\varepsilon_i)=0\)</span>.</p>
<p>If <span class="math inline">\(L = K\)</span>, the matrix <span class="math inline">\(\frac{\mathbf{Z}'\mathbf{X}}{n}\)</span> is of dimension <span class="math inline">\(K \times K\)</span> and we have:
<span class="math display">\[
\left[\mbox{plim }\left( \frac{\mathbf{Z}'\mathbf{X}}{n} \right)\right]^{-1}\mbox{plim }\left( \frac{\mathbf{Z}'\mathbf{y}}{n} \right) = \boldsymbol\beta.
\]</span>
By continuity of the inverse function (everywhere but at 0): <span class="math inline">\(\left[\mbox{plim }\left( \frac{\mathbf{Z}'\mathbf{X}}{n} \right)\right]^{-1}=\mbox{plim }\left( \frac{\mathbf{Z}'\mathbf{X}}{n} \right)^{-1}\)</span>.
The Slutsky Theorem (Prop. <a href="append.html#prp:Slutsky">8.12</a>) further implies that:
<span class="math display">\[
\mbox{plim }\left( \frac{\mathbf{Z}'\mathbf{X}}{n} \right)^{-1} \mbox{plim }\left( \frac{\mathbf{Z}'\mathbf{y}}{n} \right)  = \mbox{plim }\left( \left( \frac{\mathbf{Z}'\mathbf{X}}{n} \right)^{-1} \frac{\mathbf{Z}'\mathbf{y}}{n} \right).
\]</span>
Hence <span class="math inline">\(\mathbf{b}_{iv}\)</span> is consistent if it is defined by:
<span class="math display">\[
\boxed{\mathbf{b}_{iv} = (\mathbf{Z}'\mathbf{X})^{-1}\mathbf{Z}'\mathbf{y}.}
\]</span></p>
<div class="proposition">
<p><span id="prp:IV" class="proposition"><strong>Proposition 1.12  (Asymptotic distribution of the IV estimator) </strong></span>If <span class="math inline">\(\mathbf{z}_i\)</span> is a <span class="math inline">\(L\)</span>-dimensional random variable that constitutes a valid set of instruments (see Def. <a href="ChapterLS.html#def:instruments">1.2</a>) and if <span class="math inline">\(L=K\)</span>, then the asymptotic distribution of <span class="math inline">\(\mathbf{b}_{iv}\)</span> is:
<span class="math display">\[
\mathbf{b}_{iv} \overset{d}{\rightarrow} \mathcal{N}\left(\boldsymbol\beta,\frac{\sigma^2}{n}\left[Q_{xz}Q_{zz}^{-1}Q_{zx}\right]^{-1}\right)
\]</span>
where <span class="math inline">\(\mbox{plim } \mathbf{Z}'\mathbf{Z}/n =: \mathbf{Q}_{zz}\)</span>, <span class="math inline">\(\mbox{plim } \mathbf{Z}'\mathbf{X}/n =: \mathbf{Q}_{zx}\)</span>, <span class="math inline">\(\mbox{plim } \mathbf{X}'\mathbf{Z}/n =: \mathbf{Q}_{xz}\)</span>.</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-13" class="proof"><em>Proof</em>. </span>The proof is very similar to that of Prop. <a href="ChapterLS.html#prp:asymptOLS">1.11</a>, the starting point being that <span class="math inline">\(\mathbf{b}_{iv} = \boldsymbol\beta + (\mathbf{Z}'\mathbf{X})^{-1}\mathbf{Z}'\boldsymbol\varepsilon\)</span>.</p>
</div>
<p>When <span class="math inline">\(L=K\)</span>, we have:
<span class="math display">\[
\left[Q_{xz}Q_{zz}^{-1}Q_{zx}\right]^{-1}=Q_{zx}^{-1}Q_{zz}Q_{xz}^{-1}.
\]</span>
In practice, to estimate <span class="math inline">\(\mathbb{V}ar(\mathbf{b}_{iv}) = \frac{\sigma^2}{n}Q_{zx}^{-1}Q_{zz}Q_{xz}^{-1}\)</span>, we replace <span class="math inline">\(\sigma^2\)</span> by:
<span class="math display">\[
s_{iv}^2 = \frac{1}{n}\sum_{i=1}^{n} (y_i - \mathbf{x}_i'\mathbf{b}_{iv})^2.
\]</span></p>
<p>What about when <span class="math inline">\(L &gt; K\)</span>? In this case, we proceed as follows:</p>
<ol style="list-style-type: decimal">
<li>Regress <span class="math inline">\(\mathbf{X}\)</span> on the space spanned by <span class="math inline">\(\mathbf{Z}\)</span> and</li>
<li>Regress <span class="math inline">\(\mathbf{y}\)</span> on the fitted values <span class="math inline">\(\hat{\mathbf{X}}:=\mathbf{Z}(\mathbf{Z}'\mathbf{Z})^{-1}\mathbf{Z}'\mathbf{X}\)</span>.</li>
</ol>
<p>These two-step approach is called <strong>Two-Stage Least Squares (2SLS)</strong>. It results in:
<span class="math display" id="eq:IV">\[\begin{equation}
\boxed{\mathbf{b}_{iv} = [\mathbf{X}'\mathbf{Z}(\mathbf{Z}'\mathbf{Z})^{-1}\mathbf{Z}'\mathbf{X}]^{-1}\mathbf{X}'\mathbf{Z}(\mathbf{Z}'\mathbf{Z})^{-1}\mathbf{Z}'\mathbf{Y}.} \tag{1.22}
\end{equation}\]</span></p>
<p>In this case, Prop. <a href="ChapterLS.html#prp:IV">1.12</a> still holds, with <span class="math inline">\(\mathbf{b}_{iv}\)</span> given by Eq. <a href="ChapterLS.html#eq:IV">(1.22)</a>.</p>
<p>If the instruments do not properly satisfy Condition (a) in Def. <a href="ChapterLS.html#def:instruments">1.2</a> (i.e. if <span class="math inline">\(\mathbf{x}_i\)</span> and <span class="math inline">\(\mathbf{z}_i\)</span> are only loosely related), the instruments are said to be <strong>weak</strong> (see, e.g., <span class="citation">J. H. Stock and Yogo (<a href="references.html#ref-stock_yogo_2005">2005</a>)</span>, available <a href="http://scholar.harvard.edu/files/stock/files/testing_for_weak_instruments_in_linear_iv_regression.pdf">here</a> or <span class="citation">Andrews, Stock, and Sun (<a href="references.html#ref-Andrews_Stock_Sun_2019">2019</a>)</span>). A simple standard way to test for weak instruments consist in looking at the F-statistic associated with the first stage of the estimation. The easier it is to reject the null hypothesis (large test statistic), the less weak —or the stronger— the instruments.</p>
<p>The Durbin-Wu-Hausman test (<span class="citation">Durbin (<a href="references.html#ref-Durbin_1954">1954</a>)</span>, <span class="citation">Wu (<a href="references.html#ref-Wu_1973">1973</a>)</span>, <span class="citation">Hausman (<a href="references.html#ref-Hausman_1978">1978</a>)</span>) can be used to test if IV necessary. (IV techniques are required if <span class="math inline">\(\mbox{plim}_{n \rightarrow \infty} \mathbf{X}'\boldsymbol\varepsilon / n \ne 0\)</span>.) <a href="http://www.jstor.org/stable/1913827?seq=1#page_scan_tab_contents">Hausman (1978)</a> proposes a test of the efficiency of estimators. Under the null hypothesis two estimators, <span class="math inline">\(\mathbf{b}_0\)</span> and <span class="math inline">\(\mathbf{b}_1\)</span>, are consistent but <span class="math inline">\(\mathbf{b}_0\)</span> is (asymptotically) efficient relative to <span class="math inline">\(\mathbf{b}_1\)</span>. Under the alternative hypothesis, <span class="math inline">\(\mathbf{b}_1\)</span> (IV in the present case) remains consistent but not <span class="math inline">\(\mathbf{b}_0\)</span> (OLS in the present case). That is, when we reject the null hypothesis, it means that the OLS estimator is not consistent, potentially due to endogeneity issue.</p>
<p>The test statistic is:
<span class="math display">\[
H = (\mathbf{b}_1 - \mathbf{b}_0)' MPI(\mathbb{V}ar(\mathbf{b}_1) - \mathbb{V}ar(\mathbf{b}_0))(\mathbf{b}_1 - \mathbf{b}_0),
\]</span>
where <span class="math inline">\(MPI\)</span> is the Moore-Penrose pseudo-inverse. Under the null hypothesis, <span class="math inline">\(H \sim \chi^2(q)\)</span>, where <span class="math inline">\(q\)</span> is the rank of <span class="math inline">\(\mathbb{V}ar(\mathbf{b}_1) - \mathbb{V}ar(\mathbf{b}_0)\)</span>.</p>
<div class="example">
<p><span id="exm:priceElasticity" class="example"><strong>Example 1.4  (Estimation of price elasticity) </strong></span>See e.g. <a href="http://www.who.int/tobacco/economics/2_2estimatingpriceincomeelasticities.pdf?ua=1">WHO and estimation of tobacco price elasticity of demand</a>.</p>
<p>We want to estimate what is the effect on demand of an <em>exogenous increase</em> in prices of cigarettes (say).</p>
<p>The model is:
<span class="math display">\[\begin{eqnarray*}
\underbrace{q^d_t}_{\mbox{log(demand)}} &amp;=&amp; \alpha_0 + \alpha_1 \underbrace{\times p_t}_{\mbox{log(price)}} + \alpha_2 \underbrace{\times w_t}_{\mbox{income}} + \varepsilon_t^d\\
\underbrace{q^s_t}_{\mbox{log(supply)}} &amp;=&amp; \gamma_0 + \gamma_1 \times p_t + \gamma_2 \underbrace{\times \mathbf{y}_t}_{\mbox{cost factors}} + \varepsilon_t^s,
\end{eqnarray*}\]</span>
where <span class="math inline">\(\mathbf{y}_t\)</span>, <span class="math inline">\(w_t\)</span>, <span class="math inline">\(\varepsilon_t^s \sim \mathcal{N}(0,\sigma^2_s)\)</span> and <span class="math inline">\(\varepsilon_t^d \sim \mathcal{N}(0,\sigma^2_d)\)</span> are independent.</p>
<p>Equilibrium: <span class="math inline">\(q^d_t = q^s_t\)</span>. This implies that prices are <strong>endogenous</strong>:
<span class="math display">\[
p_t = \frac{\alpha_0 + \alpha_2 w_t + \varepsilon_t^d - \gamma_0 - \gamma_2 \mathbf{y}_t - \varepsilon_t^s}{\gamma_1 - \alpha_1}.
\]</span>
In particular we have <span class="math inline">\(\mathbb{E}(p_t \varepsilon_t^d) = \frac{\sigma^2_d}{\gamma_1 - \alpha_1} \ne 0\)</span> <span class="math inline">\(\Rightarrow\)</span> Regressing by OLS <span class="math inline">\(q_t^d\)</span> on <span class="math inline">\(p_t\)</span> gives biased estimates (see Eq. <a href="ChapterLS.html#eq:exmIV">(1.21)</a>).</p>
<div class="figure">
<span style="display:block;" id="fig:figureIV"></span>
<img src="MicroEc_files/figure-html/figureIV-1.png" alt="This figure illustrates the situation prevailing when estimating a price-elasticity (and the price is endogenous)." width="672"><p class="caption">
Figure 1.5: This figure illustrates the situation prevailing when estimating a price-elasticity (and the price is endogenous).
</p>
</div>
<p>Let us use IV regressions to estimate the price elasticity of cigarette demand. For that purpose, we use the <code>CigarettesSW</code> dataset of package <code>AER</code> (these data are used by <span class="citation">J. Stock and Watson (<a href="references.html#ref-Stock_Watson_2003">2003</a>)</span>). This panel dataset documents cigarette consumption for the 48 continental US States from 1985–1995. The instrument is the real tax on cigarettes arising from the state’s general sales tax. The rationale is that larger general sales tax drives cigarette prices up, but the general tax is not determined by other forces affecting <span class="math inline">\(\varepsilon_t^d\)</span>.</p>
<div class="sourceCode" id="cb15"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/utils/data.html">data</a></span><span class="op">(</span><span class="st">"CigarettesSW"</span>, package <span class="op">=</span> <span class="st">"AER"</span><span class="op">)</span></span>
<span><span class="va">CigarettesSW</span><span class="op">$</span><span class="va">rprice</span>  <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/with.html">with</a></span><span class="op">(</span><span class="va">CigarettesSW</span>, <span class="va">price</span><span class="op">/</span><span class="va">cpi</span><span class="op">)</span></span>
<span><span class="va">CigarettesSW</span><span class="op">$</span><span class="va">rincome</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/with.html">with</a></span><span class="op">(</span><span class="va">CigarettesSW</span>, <span class="va">income</span><span class="op">/</span><span class="va">population</span><span class="op">/</span><span class="va">cpi</span><span class="op">)</span></span>
<span><span class="va">CigarettesSW</span><span class="op">$</span><span class="va">tdiff</span>   <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/with.html">with</a></span><span class="op">(</span><span class="va">CigarettesSW</span>, <span class="op">(</span><span class="va">taxs</span> <span class="op">-</span> <span class="va">tax</span><span class="op">)</span><span class="op">/</span><span class="va">cpi</span><span class="op">)</span></span>
<span></span>
<span><span class="co">## model </span></span>
<span><span class="va">eq.IV1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/AER/man/ivreg.html">ivreg</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">packs</span><span class="op">)</span> <span class="op">~</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">rprice</span><span class="op">)</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">rincome</span><span class="op">)</span> <span class="op">|</span></span>
<span>                  <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">rincome</span><span class="op">)</span> <span class="op">+</span> <span class="va">tdiff</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/base/AsIs.html">I</a></span><span class="op">(</span><span class="va">tax</span><span class="op">/</span><span class="va">cpi</span><span class="op">)</span>,</span>
<span>                data <span class="op">=</span> <span class="va">CigarettesSW</span>, subset <span class="op">=</span> <span class="va">year</span> <span class="op">==</span> <span class="st">"1995"</span><span class="op">)</span></span>
<span><span class="va">eq.IV2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/AER/man/ivreg.html">ivreg</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">packs</span><span class="op">)</span> <span class="op">~</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">rprice</span><span class="op">)</span> <span class="op">|</span> <span class="va">tdiff</span>,</span>
<span>                data <span class="op">=</span> <span class="va">CigarettesSW</span>, subset <span class="op">=</span> <span class="va">year</span> <span class="op">==</span> <span class="st">"1995"</span><span class="op">)</span></span>
<span><span class="va">eq.no.IV</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">packs</span><span class="op">)</span> <span class="op">~</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">rprice</span><span class="op">)</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">rincome</span><span class="op">)</span>,</span>
<span>               data <span class="op">=</span> <span class="va">CigarettesSW</span>, subset <span class="op">=</span> <span class="va">year</span> <span class="op">==</span> <span class="st">"1995"</span><span class="op">)</span></span>
<span><span class="fu">stargazer</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/stargazer/man/stargazer.html">stargazer</a></span><span class="op">(</span><span class="va">eq.no.IV</span>,<span class="va">eq.IV1</span>,<span class="va">eq.IV2</span>,type<span class="op">=</span><span class="st">"text"</span>,no.space <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>                     omit.stat<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"f"</span>,<span class="st">"ser"</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<pre><code>## 
## ==========================================
##                   Dependent variable:     
##              -----------------------------
##                       log(packs)          
##                 OLS       instrumental    
##                             variable      
##                 (1)       (2)       (3)   
## ------------------------------------------
## log(rprice)  -1.407*** -1.277*** -1.084***
##               (0.251)   (0.263)   (0.317) 
## log(rincome)   0.344     0.280            
##               (0.235)   (0.239)           
## Constant     10.342*** 9.895***  9.720*** 
##               (1.023)   (1.059)   (1.514) 
## ------------------------------------------
## Observations    48        48        48    
## R2             0.433     0.429     0.401  
## Adjusted R2    0.408     0.404     0.388  
## ==========================================
## Note:          *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01</code></pre>
<div class="sourceCode" id="cb17"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">eq.IV1</span>,diagnostics <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span><span class="op">$</span><span class="va">diagnostics</span></span></code></pre></div>
<pre><code>##                  df1 df2   statistic      p-value
## Weak instruments   2  44 244.7337536 1.444054e-24
## Wu-Hausman         1  44   3.0678163 8.682505e-02
## Sargan             1  NA   0.3326221 5.641191e-01</code></pre>
<p>The last three tests are interpreted as follows:</p>
<ul>
<li>Since the p-value of the first test is small, we reject the null hypothesis according to which the instrument is weak.</li>
<li>The small p-value of the Wu-Hausman test implies that we reject the null hypothesis according to which the OLS estimates are consistent (at the 10% level only, though).</li>
<li>No over-identification (misspecification) is detected by the Sargan test (large p-value).</li>
</ul>
</div>
<div class="example">
<p><span id="exm:IVCollegeDistance" class="example"><strong>Example 1.5  (Education and wage) </strong></span>In this example, we make use of another dataset proposed by <span class="citation">J. Stock and Watson (<a href="references.html#ref-Stock_Watson_2003">2003</a>)</span>, namely the <code>CollegeDistance</code> dataset.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;Cross-section data from the High School and Beyond survey conducted by the Department of Education in the 80s. The survey includes students from approximately 1,100 high schools.&lt;/p&gt;"><sup>3</sup></a> the objective is to estimate the effect of education on wages. Education choice is suspected to be an endogenous variable, which calls for an IV strategy. The instrumental variable is the distance to college (see, e.g., <span class="citation">Dee (<a href="references.html#ref-DEE20041697">2004</a>)</span>).</p>
<div class="sourceCode" id="cb19"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://www.r-project.org">sem</a></span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/data.html">data</a></span><span class="op">(</span><span class="st">"CollegeDistance"</span>, package <span class="op">=</span> <span class="st">"AER"</span><span class="op">)</span></span>
<span><span class="va">eq.1st.stage</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">education</span> <span class="op">~</span> <span class="va">urban</span> <span class="op">+</span> <span class="va">gender</span> <span class="op">+</span> <span class="va">ethnicity</span> <span class="op">+</span> <span class="va">unemp</span> <span class="op">+</span> <span class="va">distance</span>,</span>
<span>                   data <span class="op">=</span> <span class="va">CollegeDistance</span><span class="op">)</span></span>
<span><span class="va">CollegeDistance</span><span class="op">$</span><span class="va">ed.pred</span><span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">eq.1st.stage</span><span class="op">)</span></span>
<span><span class="va">eq.2nd.stage</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">wage</span> <span class="op">~</span> <span class="va">urban</span> <span class="op">+</span> <span class="va">gender</span> <span class="op">+</span> <span class="va">ethnicity</span> <span class="op">+</span> <span class="va">unemp</span> <span class="op">+</span> <span class="va">ed.pred</span>,</span>
<span>                   data <span class="op">=</span> <span class="va">CollegeDistance</span><span class="op">)</span></span>
<span><span class="va">eqOLS</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">wage</span> <span class="op">~</span> <span class="va">urban</span> <span class="op">+</span> <span class="va">gender</span> <span class="op">+</span> <span class="va">ethnicity</span> <span class="op">+</span> <span class="va">unemp</span> <span class="op">+</span> <span class="va">education</span>,</span>
<span>            data<span class="op">=</span><span class="va">CollegeDistance</span><span class="op">)</span></span>
<span><span class="va">eq2SLS</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/AER/man/ivreg.html">ivreg</a></span><span class="op">(</span><span class="va">wage</span> <span class="op">~</span> <span class="va">urban</span> <span class="op">+</span> <span class="va">gender</span> <span class="op">+</span> <span class="va">ethnicity</span> <span class="op">+</span> <span class="va">unemp</span> <span class="op">+</span> <span class="va">education</span><span class="op">|</span></span>
<span>                  <span class="va">urban</span> <span class="op">+</span> <span class="va">gender</span> <span class="op">+</span> <span class="va">ethnicity</span> <span class="op">+</span> <span class="va">unemp</span> <span class="op">+</span> <span class="va">distance</span>,</span>
<span>                data<span class="op">=</span><span class="va">CollegeDistance</span><span class="op">)</span></span>
<span><span class="fu">stargazer</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/stargazer/man/stargazer.html">stargazer</a></span><span class="op">(</span><span class="va">eq.1st.stage</span>,<span class="va">eq.2nd.stage</span>,<span class="va">eq2SLS</span>,<span class="va">eqOLS</span>,</span>
<span>                     type<span class="op">=</span><span class="st">"text"</span>,no.space <span class="op">=</span> <span class="cn">TRUE</span>,omit.stat <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"f"</span>,<span class="st">"ser"</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<pre><code>## 
## ============================================================
##                              Dependent variable:            
##                   ------------------------------------------
##                   education               wage              
##                      OLS       OLS    instrumental    OLS   
##                                         variable            
##                      (1)       (2)        (3)         (4)   
## ------------------------------------------------------------
## urbanyes           -0.092     0.046      0.046       0.070  
##                    (0.065)   (0.045)    (0.060)     (0.045) 
## genderfemale       -0.025    -0.071*     -0.071    -0.085** 
##                    (0.052)   (0.037)    (0.050)     (0.037) 
## ethnicityafam     -0.524*** -0.227***   -0.227**   -0.556***
##                    (0.072)   (0.073)    (0.099)     (0.052) 
## ethnicityhispanic -0.275*** -0.351***  -0.351***   -0.544***
##                    (0.068)   (0.057)    (0.077)     (0.049) 
## unemp               0.010   0.139***    0.139***   0.133*** 
##                    (0.010)   (0.007)    (0.009)     (0.007) 
## distance          -0.087***                                 
##                    (0.012)                                  
## ed.pred                     0.647***                        
##                              (0.101)                        
## education                               0.647***     0.005  
##                                         (0.136)     (0.010) 
## Constant          14.061***  -0.359      -0.359    8.641*** 
##                    (0.083)   (1.412)    (1.908)     (0.157) 
## ------------------------------------------------------------
## Observations        4,739     4,739      4,739       4,739  
## R2                  0.023     0.117      -0.612      0.110  
## Adjusted R2         0.022     0.116      -0.614      0.109  
## ============================================================
## Note:                            *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01</code></pre>
</div>
</div>
<div id="general-regression-model-grm-and-robust-covariance-matrices" class="section level2" number="1.5">
<h2>
<span class="header-section-number">1.5</span> General Regression Model (GRM) and robust covariance matrices<a class="anchor" aria-label="anchor" href="#general-regression-model-grm-and-robust-covariance-matrices"><i class="fas fa-link"></i></a>
</h2>
<p>The statistical inference presented above relies on strong assumptions regarding the stochastic properties of the errors. Namely, they are assumed to be mutually uncorrelated (Hypothesis <a href="ChapterLS.html#hyp:noncorrelResid">1.4</a>) and homoskedastic (Hypothesis <a href="ChapterLS.html#hyp:homoskedasticity">1.3</a>).</p>
<p>The objective of this section is to present approaches aimed at adjusting the estimate of the covariance matrix of the OLS estimator (<span class="math inline">\((\mathbf{X}'\mathbf{X})^{-1}s^2\)</span>, see Eq. <a href="ChapterLS.html#eq:sXX">(1.18)</a>), when the previous hypotheses do not hold.</p>
<div id="presentation-of-the-general-regression-model-grm" class="section level3" number="1.5.1">
<h3>
<span class="header-section-number">1.5.1</span> Presentation of the General Regression Model (GRM)<a class="anchor" aria-label="anchor" href="#presentation-of-the-general-regression-model-grm"><i class="fas fa-link"></i></a>
</h3>
<p>It will prove useful to introduce the following notation:
<span class="math display" id="eq:assumGLS2">\[\begin{eqnarray}
\mathbb{V}ar(\boldsymbol\varepsilon | \mathbf{X}) = \mathbb{E}(\boldsymbol\varepsilon \boldsymbol\varepsilon'| \mathbf{X}) &amp;=&amp; \boldsymbol\Sigma. \tag{1.23}
\end{eqnarray}\]</span></p>
<p>Note that Eq. <a href="ChapterLS.html#eq:assumGLS2">(1.23)</a> is more general than Hypothesis <a href="ChapterLS.html#hyp:homoskedasticity">1.3</a> and <a href="ChapterLS.html#hyp:noncorrelResid">1.4</a> because the diagonal entries of <span class="math inline">\(\boldsymbol\Sigma\)</span> may be different (as opposed to under Hypothesis <a href="ChapterLS.html#hyp:homoskedasticity">1.3</a>), and the non-diagonal entries of <span class="math inline">\(\boldsymbol\Sigma\)</span> can be non-null (as opposed to under Hypothesis <a href="ChapterLS.html#hyp:noncorrelResid">1.4</a>).</p>
<div class="definition">
<p><span id="def:GRM" class="definition"><strong>Definition 1.3  (General Regression Model (GRM)) </strong></span>Hypothesis <a href="ChapterLS.html#hyp:fullrank">1.1</a> and <a href="ChapterLS.html#hyp:exogeneity">1.2</a>, together with Eq. <a href="ChapterLS.html#eq:assumGLS2">(1.23)</a>, form the General Regression Model (GRM) framework.</p>
</div>
<p>Naturally, a regression model where Hypotheses <a href="ChapterLS.html#hyp:fullrank">1.1</a> to <a href="ChapterLS.html#hyp:noncorrelResid">1.4</a> hold is a specific case of the GRM framework.</p>
<p>The GRM context notably encompasses situations of heteroskedasticity and autocorrelation:</p>
<ul>
<li><p>Heteroskedasticity:
<span class="math display" id="eq:heteroskedasticity">\[\begin{equation}
\boldsymbol\Sigma = \left[  \begin{array}{cccc}
\sigma_1^2 &amp; 0 &amp; \dots &amp; 0 \\
0 &amp; \sigma_2^2 &amp;  &amp; 0 \\
\vdots &amp;&amp; \ddots&amp; \vdots \\
0 &amp; \dots &amp; 0 &amp; \sigma_n^2
\end{array} \right]. \tag{1.24}
\end{equation}\]</span></p></li>
<li><p>Autocorrelation:
<span class="math display" id="eq:autocorrelation">\[\begin{equation}
\boldsymbol\Sigma = \sigma^2 \left[ \begin{array}{cccc}
1 &amp; \rho_{2,1} &amp; \dots &amp; \rho_{n,1} \\
\rho_{2,1} &amp; 1 &amp;  &amp; \vdots \\
\vdots &amp;&amp; \ddots&amp; \rho_{n,n-1} \\
\rho_{n,1} &amp; \rho_{n,2} &amp; \dots &amp; 1
\end{array} \right]. \tag{1.25}
\end{equation}\]</span></p></li>
</ul>
<div class="example">
<p><span id="exm:autocorrelaaa" class="example"><strong>Example 1.6  (Auto-regressive processes) </strong></span>Autocorrelation is common in time-series contexts (see Section <a href="#TS"><strong>??</strong></a>). In a time-series context, subscript <span class="math inline">\(i\)</span> refers to a date.</p>
<p>Assume for instance that:
<span class="math display" id="eq:usual">\[\begin{equation}
y_i = \mathbf{x}_i' \boldsymbol\beta + \varepsilon_i \tag{1.26}
\end{equation}\]</span>
with
<span class="math display" id="eq:usual2">\[\begin{equation}
\varepsilon_i = \rho \varepsilon_{i-1} + v_i, \quad v_i \sim \mathcal{N}(0,\sigma_v^2).\tag{1.27}
\end{equation}\]</span>
In this case, we are in the GRM context, with:
<span class="math display" id="eq:SigmaAutocorrel">\[\begin{equation}
\boldsymbol\Sigma =\frac{ \sigma_v^2}{1 - \rho^2} \left[    \begin{array}{cccc}
1 &amp; \rho &amp; \dots &amp; \rho^{n-1} \\
\rho &amp; 1 &amp;  &amp; \vdots \\
\vdots &amp;&amp; \ddots&amp; \rho \\
\rho^{n-1} &amp; \rho^{n-2} &amp; \dots &amp; 1
\end{array} \right].\tag{1.28}
\end{equation}\]</span></p>
</div>
<p>In some cases —in particular when one assumes a parametric formulation for <span class="math inline">\(\boldsymbol\Sigma\)</span>— one can determine a better (more accurate) estimator than the OLS one. This approach is called Generalized Least Squares (GLS), which we present below.</p>
</div>
<div id="GLS" class="section level3" number="1.5.2">
<h3>
<span class="header-section-number">1.5.2</span> Generalized Least Squares<a class="anchor" aria-label="anchor" href="#GLS"><i class="fas fa-link"></i></a>
</h3>
<p>Assume <span class="math inline">\(\boldsymbol\Sigma\)</span> is known (“feasible GLS”). Because <span class="math inline">\(\boldsymbol\Sigma\)</span> is symmetric positive, it admits a spectral decomposition of the form <span class="math inline">\(\boldsymbol\Sigma = \mathbf{C} \boldsymbol\Lambda \mathbf{C}'\)</span>, where <span class="math inline">\(\mathbf{C}\)</span> is an orthogonal matrix (i.e. <span class="math inline">\(\mathbf{C}\mathbf{C}'=Id\)</span>) and <span class="math inline">\(\boldsymbol\Lambda\)</span> is a diagonal matrix (the diagonal entries are the eigenvalues of <span class="math inline">\(\boldsymbol\Sigma\)</span>).</p>
<p>We have <span class="math inline">\(\boldsymbol\Sigma = (\mathbf{P}\mathbf{P}')^{-1}\)</span> with <span class="math inline">\(\mathbf{P} = \mathbf{C}\boldsymbol\Lambda^{-1/2}\)</span>. Consider the transformed model:
<span class="math display">\[
\mathbf{P}'\mathbf{y} = \mathbf{P}'\mathbf{X}\boldsymbol\beta + \mathbf{P}'\boldsymbol\varepsilon \quad \mbox{or} \quad \mathbf{y}^* = \mathbf{X}^*\boldsymbol\beta + \boldsymbol\varepsilon^*.
\]</span>
The variance of <span class="math inline">\(\boldsymbol\varepsilon^*\)</span> is the identity matrix <span class="math inline">\(Id\)</span>. In the transformed model, OLS is BLUE (Gauss-Markow Theorem <a href="ChapterLS.html#thm:GaussMarkov">1.1</a>).</p>
<p>The <strong>Generalized least squares</strong> estimator of <span class="math inline">\(\boldsymbol\beta\)</span> is:
<span class="math display" id="eq:betaGLS">\[\begin{equation}
\boxed{\mathbf{b}_{GLS} = (\mathbf{X}'\boldsymbol\Sigma^{-1}\mathbf{X})^{-1}\mathbf{X}'\boldsymbol\Sigma^{-1}\mathbf{y}.}\tag{1.29}
\end{equation}\]</span>
We have:
<span class="math display">\[
\mathbb{V}ar(\mathbf{b}_{GLS}|\mathbf{X}) = (\mathbf{X}'\boldsymbol\Sigma^{-1}\mathbf{X})^{-1}.
\]</span></p>
<p>However, in general, <span class="math inline">\(\boldsymbol\Sigma\)</span> is unknown. The GLS estimator is then said to be <em>infeasible</em>. Some structure is required. Assume <span class="math inline">\(\boldsymbol\Sigma\)</span> admits a parametric form <span class="math inline">\(\boldsymbol\Sigma(\theta)\)</span>. The estimation becomes <em>feasible</em> (FGLS) if one replaces <span class="math inline">\(\boldsymbol\Sigma(\theta)\)</span> by <span class="math inline">\(\boldsymbol\Sigma(\hat\theta)\)</span>, where <span class="math inline">\(\hat\theta\)</span> is a consistent estimator of <span class="math inline">\(\theta\)</span>. In that case, the FGLS is asymptotically efficient (see Example <a href="ChapterLS.html#exm:autocorrelaaa2">1.7</a>).</p>
<p>When <span class="math inline">\(\boldsymbol\Sigma\)</span> has no obvious structure: the OLS (or IV) is the only estimator available. Under regularity assumptions, it remains unbiased, consistent, and asymptotically normally distributed, but not efficient. Standard inference procedures are no longer appropriate.</p>
<div class="example">
<p><span id="exm:autocorrelaaa2" class="example"><strong>Example 1.7  (GLS in the auto-correlation case) </strong></span>Consider the case presented in Example <a href="ChapterLS.html#exm:autocorrelaaa">1.6</a>. Because the OLS estimate <span class="math inline">\(\mathbf{b}\)</span> of <span class="math inline">\(\boldsymbol\beta\)</span> is consistent, the estimates <span class="math inline">\(e_i\)</span> of the <span class="math inline">\(\varepsilon_i\)</span>’s also are. Consistent estimators of <span class="math inline">\(\rho\)</span> and <span class="math inline">\(\sigma_v\)</span> are then obtained by regressing the <span class="math inline">\(e_i\)</span>’s on the <span class="math inline">\(e_{i-1}\)</span>’s. Using these estimates in Eq. <a href="ChapterLS.html#eq:SigmaAutocorrel">(1.28)</a> provides a consistent estimate of <span class="math inline">\(\boldsymbol\Sigma\)</span>. Applying these steps recursively gives an efficient estimator of <span class="math inline">\(\boldsymbol\beta\)</span> (<span class="citation">Cochrane and Orcutt (<a href="references.html#ref-Cochrane_Orcutt_1949">1949</a>)</span>).</p>
</div>
</div>
<div id="asymptotic-properties-of-the-ols-estimator-in-the-grm-framework" class="section level3" number="1.5.3">
<h3>
<span class="header-section-number">1.5.3</span> Asymptotic properties of the OLS estimator in the GRM framework<a class="anchor" aria-label="anchor" href="#asymptotic-properties-of-the-ols-estimator-in-the-grm-framework"><i class="fas fa-link"></i></a>
</h3>
<!-- the OLS is not efficient. However, it is asymptotically consistent and normal (Props. \@ref(prp:XXX) and \@ref(prp:AsymptGRM), respectively). -->
<p>Since <span class="math inline">\(\mathbf{b} = \boldsymbol\beta + \left(\mathbf{X}'\mathbf{X}\right)^{-1} \mathbf{X}'\boldsymbol\varepsilon\)</span> and <span class="math inline">\(\mathbb{V}ar(\boldsymbol\varepsilon|\mathbf{X})=\boldsymbol\Sigma\)</span>, we have:
<span class="math display" id="eq:xsx">\[\begin{equation}
\mathbb{V}ar(\mathbf{b}|\mathbf{X}) = \frac{1}{n}\left(\frac{1}{n}\mathbf{X}'\mathbf{X}\right)^{-1}\left(\frac{1}{n}\mathbf{X}'\boldsymbol\Sigma\mathbf{X}\right)\left(\frac{1}{n}\mathbf{X}'\mathbf{X}\right)^{-1}.\tag{1.30}
\end{equation}\]</span></p>
<!-- Under Hypothesis \@ref(hyp:normality), since $\bv{b}$ is linear in $\boldsymbol\varepsilon$, we have: -->
<!-- \begin{equation} -->
<!-- \bv{b}|\bv{X} \sim \mathcal{N}\left(\boldsymbol\beta,\left(\bv{X}'\bv{X}\right)^{-1}\left(\bv{X}'\boldsymbol\Sigma\bv{X}\right)\left(\bv{X}'\bv{X}\right)^{-1}\right). -->
<!-- \end{equation} -->
<p>Therefore, the conditional covariance matrix of the OLS estimator is not <span class="math inline">\(\sigma^2 (\mathbf{X}'\mathbf{X})^{-1}\)</span> any longer, and using <span class="math inline">\(s^2 (\mathbf{X}'\mathbf{X})^{-1}\)</span> for inference may be misleading. Below, we will see how to construct appropriate estimates of the covariance matrix of <span class="math inline">\(\mathbf{b}\)</span>. Before that, let us prove that the OLS estimator remains consistent in the GRM framework.</p>
<div class="proposition">
<p><span id="prp:XXX" class="proposition"><strong>Proposition 1.13  (Consistency of the OLS estimator in the GRM framework) </strong></span>If <span class="math inline">\(\mbox{plim }(\mathbf{X}'\mathbf{X}/n)\)</span> and <span class="math inline">\(\mbox{plim }(\mathbf{X}'\boldsymbol\Sigma\mathbf{X}/n)\)</span> are finite positive definite matrices, then <span class="math inline">\(\mbox{plim }(\mathbf{b})=\boldsymbol\beta\)</span>.</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-14" class="proof"><em>Proof</em>. </span>We have <span class="math inline">\(\mathbb{V}ar(\mathbf{b})=\mathbb{E}[\mathbb{V}ar(\mathbf{b}|\mathbf{X})]+\mathbb{V}ar[\mathbb{E}(\mathbf{b}|\mathbf{X})]\)</span>. Since <span class="math inline">\(\mathbb{E}(\mathbf{b}|\mathbf{X})=\boldsymbol\beta\)</span>, <span class="math inline">\(\mathbb{V}ar[\mathbb{E}(\mathbf{b}|\mathbf{X})]=0\)</span>. Eq. <a href="ChapterLS.html#eq:xsx">(1.30)</a> implies that <span class="math inline">\(\mathbb{V}ar(\mathbf{b}|\mathbf{X}) \rightarrow 0\)</span>. Hence <span class="math inline">\(\mathbf{b}\)</span> converges in mean square, and therefore in probability (see Prop. <a href="append.html#prp:implicationsconv">8.13</a>).</p>
</div>
<p>Prop. <a href="ChapterLS.html#prp:AsymptGRM">1.14</a> gives the asymptotic distribution of the OLS estimator in the GRM framework.</p>
<div class="proposition">
<p><span id="prp:AsymptGRM" class="proposition"><strong>Proposition 1.14  (Asymptotic distribution of the OLS estimator in the GRM framework) </strong></span>If <span class="math inline">\(Q_{xx}=\mbox{plim }(\mathbf{X}'\mathbf{X}/n)\)</span> and <span class="math inline">\(Q_{x\boldsymbol\Sigma x}=\mbox{plim }(\mathbf{X}'\boldsymbol\Sigma\mathbf{X}/n)\)</span> are finite positive definite matrices, then:
<span class="math display">\[
\sqrt{n}(\mathbf{b}-\boldsymbol\beta) \overset{d}{\rightarrow} \mathcal{N}(0,Q_{xx}^{-1}Q_{x\boldsymbol\Sigma x}Q_{xx}^{-1}).
\]</span></p>
</div>
<p>The IV estimator also features a normal asymptotic distribution:</p>
<div class="proposition">
<p><span id="prp:AsymptIVGRM" class="proposition"><strong>Proposition 1.15  (Asymptotic distribution of the IV estimator in the GRM framework) </strong></span>If regressors and IV variables are “well-behaved”, then:
<span class="math display">\[
\mathbf{b}_{iv} \overset{a}{\sim} \mathcal{N}(\boldsymbol\beta,\mathbf{V}_{iv}),
\]</span>
where
<span class="math display">\[
\mathbf{V}_{iv} = \frac{1}{n}(\mathbf{Q}^*)\mbox{ plim }\left(\frac{1}{n} \mathbf{Z}'\boldsymbol\Sigma \mathbf{Z}\right)(\mathbf{Q}^*)',
\]</span>
with
<span class="math display">\[
\mathbf{Q}^* = [\mathbf{Q}_{xz}\mathbf{Q}_{zz}^{-1}\mathbf{Q}_{zx}]^{-1}\mathbf{Q}_{xz}\mathbf{Q}_{zz}^{-1}.
\]</span></p>
</div>
<p>For practical purposes, one needs to have estimates of <span class="math inline">\(\boldsymbol\Sigma\)</span> in Props. <a href="ChapterLS.html#prp:AsymptGRM">1.14</a> or <a href="ChapterLS.html#prp:AsymptIVGRM">1.15</a>. The complication comes from the fact that <span class="math inline">\(\boldsymbol\Sigma\)</span> is of dimension <span class="math inline">\(n \times n\)</span>, and its estimation —based on a sample of length <span class="math inline">\(n\)</span>— is therefore infeasible in the general case. Notwithstanding, looking at Eq. <a href="ChapterLS.html#eq:xsx">(1.30)</a>, it appears that one can focus on the estimation of <span class="math inline">\(Q_{x\boldsymbol\Sigma x}=\mbox{plim }(\mathbf{X}'\boldsymbol\Sigma\mathbf{X}/n)\)</span> (or <span class="math inline">\(\mbox{plim }\left(\frac{1}{n} \mathbf{Z}'\boldsymbol\Sigma \mathbf{Z}\right)\)</span> in the IV case). This matrix being of dimension <span class="math inline">\(K \times K\)</span>, its estimation is easier.</p>
<p>We have:
<span class="math display" id="eq:GeneralXSigmaX">\[\begin{equation}
\frac{1}{n}\mathbf{X}'\boldsymbol\Sigma\mathbf{X} = \frac{1}{n}\sum_{i=1}^{n}\sum_{j=1}^{n}\sigma_{i,j}\mathbf{x}_i\mathbf{x}'_j. \tag{1.31}
\end{equation}\]</span></p>
<p>The so-called <strong>robust covariance matrices</strong> are estimates of the previous matrix. Their computation is based on the fact that if <span class="math inline">\(\mathbf{b}\)</span> is consistent, then the <span class="math inline">\(e_i\)</span>’s are consistent estimators of the <span class="math inline">\(\varepsilon_i\)</span>’s.</p>
<p>In the following sections (<a href="ChapterLS.html#HAC">1.5.4</a> and <a href="ChapterLS.html#Clusters">1.5.5</a>), we present two types of robust covariance matrices.</p>
</div>
<div id="HAC" class="section level3" number="1.5.4">
<h3>
<span class="header-section-number">1.5.4</span> HAC-robust covariance matrices<a class="anchor" aria-label="anchor" href="#HAC"><i class="fas fa-link"></i></a>
</h3>
<p>When only heteroskedasticity prevails, i.e., when matrix <span class="math inline">\(\boldsymbol\Sigma\)</span> is as in Eq. <a href="ChapterLS.html#eq:heteroskedasticity">(1.24)</a>, then one can use the formula proposed by <span class="citation">White (<a href="references.html#ref-White_1980">1980</a>)</span> to estimate <span class="math inline">\(\frac{1}{n}\mathbf{X}'\boldsymbol\Sigma\mathbf{X}\)</span> (see Example <a href="ChapterLS.html#exm:HCheteroskedasticity">1.8</a>). When the residuals feature both heteroskedasticity and auto-correlation, then one can use the <span class="citation">Newey and West (<a href="references.html#ref-Newey_West_1987">1987</a>)</span> approach (see Example <a href="ChapterLS.html#exm:HCheterAC">1.9</a>).</p>
<div class="example">
<p><span id="exm:HCheteroskedasticity" class="example"><strong>Example 1.8  (Heteroskedasticity) </strong></span>This is the case of Eq. <a href="ChapterLS.html#eq:heteroskedasticity">(1.24)</a>. We have <span class="math inline">\(\sigma_{i,j}=0\)</span> for <span class="math inline">\(i \ne j\)</span>. Hence, in this case, we then need to estimate <span class="math inline">\(\frac{1}{n}\sum_{i=1}^{n}\sigma_{i}^2\mathbf{x}_i\mathbf{x}'_i\)</span>. <span class="citation">White (<a href="references.html#ref-White_1980">1980</a>)</span> has shown that, under general conditions:
<span class="math display" id="eq:white">\[\begin{equation}
\mbox{plim}\left( \frac{1}{n}\sum_{i=1}^{n}\sigma_{i}^2\mathbf{x}_i\mathbf{x}'_i \right) =
\mbox{plim}\left( \frac{1}{n}\sum_{i=1}^{n}e_{i}^2\mathbf{x}_i\mathbf{x}'_i \right). \tag{1.32}
\end{equation}\]</span>
The estimator of <span class="math inline">\(\frac{1}{n}\mathbf{X}'\boldsymbol\Sigma\mathbf{X}\)</span> therefore is:
<span class="math display" id="eq:White">\[\begin{equation}
M_{HC0} = \frac{1}{n}\mathbf{X}'
\left[
\begin{array}{cccc}
e_1^2 &amp; 0 &amp; \dots &amp; 0 \\
0 &amp; e_2^2 &amp;  \\
\vdots &amp; &amp; \ddots&amp;0 \\
0 &amp; \dots &amp; 0 &amp; e_n^2
\end{array}
\right]
\mathbf{X}.\tag{1.33}
\end{equation}\]</span>
where the <span class="math inline">\(e_i\)</span> are the OLS residuals of the regression. The previous estimator is often called <strong>HC0</strong>. The <strong>HC1</strong> estimator, due to <span class="citation">J. MacKinnon and White (<a href="references.html#ref-MacKinnon_White_1985">1985</a>)</span>, is obtained by applying an adjustment factor <span class="math inline">\(n/(n-K)\)</span> for the number of degrees of freedom (as in Prop. <a href="ChapterLS.html#prp:expects2">1.6</a>). That is:
<span class="math display" id="eq:WhiteHC1">\[\begin{equation}
M_{HC1} = \frac{n}{n-K}M_{HC0}.\tag{1.34}
\end{equation}\]</span></p>
<p>We can illustrate the influence of heteroskedasticity using simulations. Consider the following model:
<span class="math display">\[
y_i = x_i + \varepsilon_i, \quad \varepsilon_i \sim \mathcal{N}(0,x_i^2),
\]</span>
where the <span class="math inline">\(x_i\)</span>’s are i.i.d. <span class="math inline">\(t(6)\)</span>.</p>
<p>Here is a simulated sample (<span class="math inline">\(n=200\)</span>) of this model:</p>
<div class="sourceCode" id="cb21"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">n</span> <span class="op">&lt;-</span> <span class="fl">200</span></span>
<span><span class="va">x</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/TDist.html">rt</a></span><span class="op">(</span><span class="va">n</span>,df<span class="op">=</span><span class="fl">6</span><span class="op">)</span></span>
<span><span class="va">y</span> <span class="op">&lt;-</span> <span class="va">x</span> <span class="op">+</span> <span class="va">x</span><span class="op">*</span><span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>plt<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">.1</span>,<span class="fl">.95</span>,<span class="fl">.1</span>,<span class="fl">.95</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">x</span>,<span class="va">y</span>,pch<span class="op">=</span><span class="fl">19</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: left-aligned">
<span style="display:block;" id="fig:simulHeterosk"></span>
<img src="MicroEc_files/figure-html/simulHeterosk-1.png" alt="Situation of heteroskedasticity. The model is $y_i = x_i + \varepsilon_i, \quad \varepsilon_i \sim \mathcal{N}(0,x_i^2)$, where the $x_i$'s are i.i.d. $t(6)$." width="90%"><p class="caption">
Figure 1.6: Situation of heteroskedasticity. The model is <span class="math inline">\(y_i = x_i + \varepsilon_i, \quad \varepsilon_i \sim \mathcal{N}(0,x_i^2)\)</span>, where the <span class="math inline">\(x_i\)</span>’s are i.i.d. <span class="math inline">\(t(6)\)</span>.
</p>
</div>
<p>We simulate 1000 samples of the same model with <span class="math inline">\(n=200\)</span>. For each sample, we compute the OLS estimate of <span class="math inline">\(\beta\)</span> (<span class="math inline">\(=1\)</span>). For each of the 1000 OLS estimations, we employ (a) the standard OLS variance formula (<span class="math inline">\(s^2 (\mathbf{X}'\mathbf{X})^{-1}\)</span>) and (b) the White formula to estimate the variance of <span class="math inline">\(b\)</span>. For each formula, we compute the average of the 1000 resulting standard deviations and compare these with the standard deviation of the 1000 OLS estimate of <span class="math inline">\(\beta\)</span>.</p>
<div class="sourceCode" id="cb22"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">n</span> <span class="op">&lt;-</span> <span class="fl">200</span> <span class="co"># sample size</span></span>
<span><span class="va">N</span> <span class="op">&lt;-</span> <span class="fl">1000</span> <span class="co"># number of simulated samples</span></span>
<span><span class="va">XX</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/TDist.html">rt</a></span><span class="op">(</span><span class="va">n</span><span class="op">*</span><span class="va">N</span>,df<span class="op">=</span><span class="fl">6</span><span class="op">)</span>,<span class="va">n</span>,<span class="va">N</span><span class="op">)</span></span>
<span><span class="va">YY</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span><span class="op">(</span><span class="va">XX</span> <span class="op">+</span> <span class="va">XX</span><span class="op">*</span><span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span><span class="op">)</span>,<span class="va">n</span>,<span class="va">N</span><span class="op">)</span></span>
<span><span class="va">all_b</span>       <span class="op">&lt;-</span> <span class="cn">NULL</span>;<span class="va">all_V_OLS</span>   <span class="op">&lt;-</span> <span class="cn">NULL</span>;<span class="va">all_V_White</span> <span class="op">&lt;-</span> <span class="cn">NULL</span></span>
<span><span class="kw">for</span><span class="op">(</span><span class="va">j</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="va">N</span><span class="op">)</span><span class="op">{</span></span>
<span>  <span class="va">Y</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span><span class="op">(</span><span class="va">YY</span><span class="op">[</span>,<span class="va">j</span><span class="op">]</span>,ncol<span class="op">=</span><span class="fl">1</span><span class="op">)</span></span>
<span>  <span class="va">X</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span><span class="op">(</span><span class="va">XX</span><span class="op">[</span>,<span class="va">j</span><span class="op">]</span>,ncol<span class="op">=</span><span class="fl">1</span><span class="op">)</span></span>
<span>  <span class="va">b</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/solve.html">solve</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/t.html">t</a></span><span class="op">(</span><span class="va">X</span><span class="op">)</span><span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span><span class="va">X</span><span class="op">)</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="fu"><a href="https://rdrr.io/r/base/t.html">t</a></span><span class="op">(</span><span class="va">X</span><span class="op">)</span><span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span><span class="va">Y</span></span>
<span>  <span class="va">e</span> <span class="op">&lt;-</span> <span class="va">Y</span> <span class="op">-</span> <span class="va">X</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="va">b</span></span>
<span>  <span class="va">S</span> <span class="op">&lt;-</span> <span class="fl">1</span><span class="op">/</span><span class="va">n</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/t.html">t</a></span><span class="op">(</span><span class="va">X</span><span class="op">)</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="fu"><a href="https://rdrr.io/r/base/diag.html">diag</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">e</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span><span class="op">)</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="va">X</span></span>
<span>  <span class="va">V_OLS</span>   <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/solve.html">solve</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/t.html">t</a></span><span class="op">(</span><span class="va">X</span><span class="op">)</span><span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span><span class="va">X</span><span class="op">)</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/stats/cor.html">var</a></span><span class="op">(</span><span class="va">e</span><span class="op">)</span></span>
<span>  <span class="va">V_White</span> <span class="op">&lt;-</span> <span class="fl">1</span><span class="op">/</span><span class="va">n</span> <span class="op">*</span> <span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/solve.html">solve</a></span><span class="op">(</span><span class="fl">1</span><span class="op">/</span><span class="va">n</span><span class="op">*</span><span class="fu"><a href="https://rdrr.io/r/base/t.html">t</a></span><span class="op">(</span><span class="va">X</span><span class="op">)</span><span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span><span class="va">X</span><span class="op">)</span><span class="op">)</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="va">S</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/solve.html">solve</a></span><span class="op">(</span><span class="fl">1</span><span class="op">/</span><span class="va">n</span><span class="op">*</span><span class="fu"><a href="https://rdrr.io/r/base/t.html">t</a></span><span class="op">(</span><span class="va">X</span><span class="op">)</span><span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span><span class="va">X</span><span class="op">)</span><span class="op">)</span></span>
<span>  <span class="va">all_b</span>       <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">all_b</span>,<span class="va">b</span><span class="op">)</span></span>
<span>  <span class="va">all_V_OLS</span>   <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">all_V_OLS</span>,<span class="va">V_OLS</span><span class="op">)</span></span>
<span>  <span class="va">all_V_White</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">all_V_White</span>,<span class="va">V_White</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/sd.html">sd</a></span><span class="op">(</span><span class="va">all_b</span><span class="op">)</span>,<span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="va">all_V_OLS</span><span class="op">)</span><span class="op">)</span>,<span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="va">all_V_White</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<pre><code>## [1] 0.14024423 0.06748804 0.13973431</code></pre>
<p>The results show that the White formula yields, on average, an estimated standard deviation that is much closer to the “true” value than the standard OLS formula. The latter underestimate the standard deviation of <span class="math inline">\(b\)</span>.</p>
</div>
<p>In the following example, we regress GDP growth rates from the <span class="citation">Jordà, Schularick, and Taylor (<a href="references.html#ref-JST_2017">2017</a>)</span> database on a systemic financial crisis dummy. We compute the HC0- and HC1-based standard deviations of the parameter estimate, and compare it to the one based on the standard OLS formula. The adjusted standard deviations are close to the one provided by the non-adjusted OLS formula.</p>
<div class="sourceCode" id="cb24"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">lmtest</span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://sandwich.R-Forge.R-project.org/">sandwich</a></span><span class="op">)</span></span>
<span><span class="va">nT</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/dim.html">dim</a></span><span class="op">(</span><span class="va">JST</span><span class="op">)</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span></span>
<span><span class="va">JST</span><span class="op">$</span><span class="va">growth</span> <span class="op">&lt;-</span> <span class="cn">NaN</span></span>
<span><span class="va">JST</span><span class="op">$</span><span class="va">growth</span><span class="op">[</span><span class="fl">2</span><span class="op">:</span><span class="va">nT</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">JST</span><span class="op">$</span><span class="va">rgdpbarro</span><span class="op">[</span><span class="fl">2</span><span class="op">:</span><span class="va">nT</span><span class="op">]</span><span class="op">/</span><span class="va">JST</span><span class="op">$</span><span class="va">rgdpbarro</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="op">(</span><span class="va">nT</span><span class="op">-</span><span class="fl">1</span><span class="op">)</span><span class="op">]</span><span class="op">)</span></span>
<span><span class="va">JST.red</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/subset.html">subset</a></span><span class="op">(</span><span class="va">JST</span>,<span class="va">year</span><span class="op">&gt;</span><span class="fl">1950</span><span class="op">)</span></span>
<span><span class="va">JST.red</span><span class="op">$</span><span class="va">iso</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/factor.html">as.factor</a></span><span class="op">(</span><span class="va">JST.red</span><span class="op">$</span><span class="va">iso</span><span class="op">)</span></span>
<span><span class="va">JST.red</span><span class="op">$</span><span class="va">year</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/factor.html">as.factor</a></span><span class="op">(</span><span class="va">JST.red</span><span class="op">$</span><span class="va">year</span><span class="op">)</span></span>
<span><span class="va">eq</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">growth</span><span class="op">~</span><span class="va">crisisJST</span><span class="op">+</span><span class="va">iso</span><span class="op">+</span><span class="va">year</span>,data<span class="op">=</span><span class="va">JST.red</span><span class="op">)</span></span>
<span><span class="va">vcovHC0</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://sandwich.R-Forge.R-project.org/reference/vcovHC.html">vcovHC</a></span><span class="op">(</span><span class="va">eq</span>, type <span class="op">=</span> <span class="st">"HC0"</span><span class="op">)</span></span>
<span><span class="va">vcovHC1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://sandwich.R-Forge.R-project.org/reference/vcovHC.html">vcovHC</a></span><span class="op">(</span><span class="va">eq</span>, type <span class="op">=</span> <span class="st">"HC1"</span><span class="op">)</span></span>
<span><span class="fu">stargazer</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/stargazer/man/stargazer.html">stargazer</a></span><span class="op">(</span><span class="va">eq</span>, <span class="va">eq</span>, <span class="va">eq</span>,type <span class="op">=</span> <span class="st">"text"</span>,</span>
<span>                     column.labels <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"No HAC"</span>, <span class="st">"HC0"</span>,<span class="st">"HC1"</span><span class="op">)</span>,</span>
<span>                     omit <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"iso"</span>,<span class="st">"year"</span><span class="op">)</span>,no.space <span class="op">=</span> <span class="cn">TRUE</span>,keep.stat <span class="op">=</span> <span class="st">"n"</span>,</span>
<span>                     se <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span><span class="cn">NULL</span>,<span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/diag.html">diag</a></span><span class="op">(</span><span class="va">vcovHC0</span><span class="op">)</span><span class="op">)</span>,<span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/diag.html">diag</a></span><span class="op">(</span><span class="va">vcovHC1</span><span class="op">)</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<pre><code>## 
## ==========================================
##                   Dependent variable:     
##              -----------------------------
##                         growth            
##               No HAC      HC0       HC1   
##                 (1)       (2)       (3)   
## ------------------------------------------
## crisisJST    -0.015*** -0.015*** -0.015***
##               (0.005)   (0.005)   (0.006) 
## Constant     0.042***  0.042***  0.042*** 
##               (0.005)   (0.007)   (0.007) 
## ------------------------------------------
## Observations   1,258     1,258     1,258  
## ==========================================
## Note:          *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01</code></pre>
<div class="example">
<p><span id="exm:HCheterAC" class="example"><strong>Example 1.9  (Heteroskedasticity and Autocorrelation (HAC)) </strong></span><span class="citation">Newey and West (<a href="references.html#ref-Newey_West_1987">1987</a>)</span> have proposed a formula to address both heteroskedasticity and auto-correlation of the residuals (Eqs. <a href="ChapterLS.html#eq:heteroskedasticity">(1.24)</a> and <a href="ChapterLS.html#eq:autocorrelation">(1.25)</a>). They show that, if the correlation between terms <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> gets sufficiently small when <span class="math inline">\(|i-j|\)</span> increases:
<span class="math display" id="eq:NW">\[\begin{eqnarray}
&amp;&amp;\mbox{plim} \left( \frac{1}{n}\sum_{i=1}^{n}\sum_{j=1}^{n}\sigma_{i,j}\mathbf{x}_i\mathbf{x}'_j \right) \approx  \\
&amp;&amp;\mbox{plim} \left( \frac{1}{n}\sum_{t=1}^{n}e_{t}^2\mathbf{x}_t\mathbf{x}'_t +
\frac{1}{n}\sum_{\ell=1}^{L}\sum_{t=\ell+1}^{n}w_\ell e_{t}e_{t-\ell}(\mathbf{x}_t\mathbf{x}'_{t-\ell} + \mathbf{x}_{t-\ell}\mathbf{x}'_{t})
\right), \nonumber \tag{1.35}
\end{eqnarray}\]</span>
where <span class="math inline">\(w_\ell = 1 - \ell/(L+1)\)</span> (with <span class="math inline">\(L\)</span> large).</p>
<p>Let us illustrate the influence of autocorrelation using simulations. We consider the following model:
<span class="math display" id="eq:simul11">\[\begin{equation}
y_i = x_i + \varepsilon_i, \quad \varepsilon_i \sim \mathcal{N}(0,x_i^2),\tag{1.36}
\end{equation}\]</span>
where the <span class="math inline">\(x_i\)</span>’s and the <span class="math inline">\(\varepsilon_i\)</span>’s are such that:
<span class="math display" id="eq:simul22">\[\begin{equation}
x_i = 0.8 x_{i-1} + u_i \quad and \quad \varepsilon_i = 0.8 \varepsilon_{i-1} + v_i, \tag{1.37}
\end{equation}\]</span>
where the <span class="math inline">\(u_i\)</span>’s and the <span class="math inline">\(v_i\)</span>’s are i.i.d. <span class="math inline">\(\mathcal{N}(0,1)\)</span>.</p>
<p>We simulate 500 samples of the same model with <span class="math inline">\(n=200\)</span>. For each sample, we compute the OLS estimate of <span class="math inline">\(\beta\)</span> (=1). For each of the 1000 OLS estimations, we employ (a) the standard OLS variance formula (<span class="math inline">\(s^2 (\mathbf{X}'\mathbf{X})^{-1}\)</span>), (b) the White formula, and (c) the Newey-West formula to estimate the variance of <span class="math inline">\(b\)</span>. For each formula, we compute the average of the 500 resulting standard deviations and compare these with the standard deviation of the 500 OLS estimate of <span class="math inline">\(\beta\)</span>.</p>
<div class="sourceCode" id="cb26"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">AEC</span><span class="op">)</span></span>
<span><span class="va">n</span> <span class="op">&lt;-</span> <span class="fl">100</span> <span class="co"># sample length</span></span>
<span><span class="va">nb.sim</span> <span class="op">&lt;-</span> <span class="fl">500</span> <span class="co"># number of simulated samples</span></span>
<span><span class="va">all.b</span> <span class="op">&lt;-</span> <span class="cn">NULL</span>;<span class="va">all.OLS.stdv.b</span> <span class="op">&lt;-</span> <span class="cn">NULL</span></span>
<span><span class="va">all.Whi.stdv.b</span> <span class="op">&lt;-</span> <span class="cn">NULL</span>;<span class="va">all.NW.stdv.b</span> <span class="op">&lt;-</span> <span class="cn">NULL</span></span>
<span><span class="kw">for</span><span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="va">nb.sim</span><span class="op">)</span><span class="op">{</span></span>
<span>  <span class="va">eps</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span><span class="op">)</span>;<span class="va">x</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span><span class="op">)</span></span>
<span>  <span class="kw">for</span><span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fl">2</span><span class="op">:</span><span class="va">n</span><span class="op">)</span><span class="op">{</span></span>
<span>    <span class="va">eps</span><span class="op">[</span><span class="va">i</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="va">eps</span><span class="op">[</span><span class="va">i</span><span class="op">]</span> <span class="op">+</span> <span class="fl">.8</span><span class="op">*</span><span class="va">eps</span><span class="op">[</span><span class="va">i</span><span class="op">-</span><span class="fl">1</span><span class="op">]</span></span>
<span>    <span class="va">x</span><span class="op">[</span><span class="va">i</span><span class="op">]</span>   <span class="op">&lt;-</span> <span class="va">x</span><span class="op">[</span><span class="va">i</span><span class="op">]</span>   <span class="op">+</span> <span class="fl">.8</span><span class="op">*</span><span class="va">x</span><span class="op">[</span><span class="va">i</span><span class="op">-</span><span class="fl">1</span><span class="op">]</span></span>
<span>  <span class="op">}</span></span>
<span>  <span class="va">y</span> <span class="op">&lt;-</span> <span class="va">x</span> <span class="op">+</span> <span class="va">eps</span></span>
<span>  <span class="va">eq</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">y</span><span class="op">~</span><span class="va">x</span><span class="op">)</span></span>
<span>  <span class="va">all.b</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">all.b</span>,<span class="va">eq</span><span class="op">$</span><span class="va">coefficients</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span><span class="op">)</span></span>
<span>  <span class="va">all.OLS.stdv.b</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">all.OLS.stdv.b</span>,<span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">eq</span><span class="op">)</span><span class="op">$</span><span class="va">coefficients</span><span class="op">[</span><span class="fl">2</span>,<span class="fl">2</span><span class="op">]</span><span class="op">)</span></span>
<span>  <span class="va">X</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/cbind.html">cbind</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="fl">1</span>,<span class="va">n</span><span class="op">)</span>,<span class="va">x</span><span class="op">)</span></span>
<span>  <span class="va">XX</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/t.html">t</a></span><span class="op">(</span><span class="va">X</span><span class="op">)</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="va">X</span>;<span class="va">XX_1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/solve.html">solve</a></span><span class="op">(</span><span class="va">XX</span><span class="op">)</span></span>
<span>  <span class="va">E2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/diag.html">diag</a></span><span class="op">(</span><span class="va">eq</span><span class="op">$</span><span class="va">residuals</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span></span>
<span>  <span class="va">White.V</span> <span class="op">&lt;-</span> <span class="va">XX_1</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/t.html">t</a></span><span class="op">(</span><span class="va">X</span><span class="op">)</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="va">E2</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="va">X</span><span class="op">)</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="va">XX_1</span></span>
<span>  <span class="va">all.Whi.stdv.b</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">all.Whi.stdv.b</span>,<span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="va">White.V</span><span class="op">[</span><span class="fl">2</span>,<span class="fl">2</span><span class="op">]</span><span class="op">)</span><span class="op">)</span></span>
<span>  <span class="co"># HAC:</span></span>
<span>  <span class="va">U</span> <span class="op">&lt;-</span> <span class="va">X</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/cbind.html">cbind</a></span><span class="op">(</span><span class="va">eq</span><span class="op">$</span><span class="va">residuals</span>,<span class="va">eq</span><span class="op">$</span><span class="va">residuals</span><span class="op">)</span></span>
<span>  <span class="va">XSigmaX</span> <span class="op">&lt;-</span> <span class="fu">NW.LongRunVariance</span><span class="op">(</span><span class="va">U</span>,<span class="fl">5</span><span class="op">)</span></span>
<span>  <span class="va">NW.V</span> <span class="op">&lt;-</span> <span class="fl">1</span><span class="op">/</span><span class="va">n</span> <span class="op">*</span> <span class="op">(</span><span class="va">n</span><span class="op">*</span><span class="va">XX_1</span><span class="op">)</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="va">XSigmaX</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="op">(</span><span class="va">n</span><span class="op">*</span><span class="va">XX_1</span><span class="op">)</span></span>
<span>  <span class="va">all.NW.stdv.b</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">all.NW.stdv.b</span>,<span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="va">NW.V</span><span class="op">[</span><span class="fl">2</span>,<span class="fl">2</span><span class="op">]</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cbind.html">cbind</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/sd.html">sd</a></span><span class="op">(</span><span class="va">all.b</span><span class="op">)</span>,<span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">all.OLS.stdv.b</span><span class="op">)</span>,</span>
<span>      <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">all.Whi.stdv.b</span><span class="op">)</span>,<span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">all.NW.stdv.b</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<pre><code>##          [,1]      [,2]      [,3]     [,4]
## [1,] 0.201172 0.1013048 0.0962689 0.146974</code></pre>
<p>The results show that the Newey-West formula yields, on average, an estimated standard deviation that is closer to the “true” value than the standard OLS formula. The latter underestimate the standard deviation of <span class="math inline">\(b\)</span>.</p>
</div>
<p>What precedes suggest that, when the residuals feature autocorrelation, it is important to use appropriately adjusted covariance matrices to make statistical inference. How to detect autocorrelation in residuals? A popular test has been proposed by <span class="citation">Durbin and Watson (<a href="references.html#ref-Durbin_Watson_1950">1950</a>)</span> and <span class="citation">Durbin and Watson (<a href="references.html#ref-Durbin_Watson_1951">1951</a>)</span>. The Durbin-Watson test statistic is:
<span class="math display">\[
DW = \frac{\sum_{i=2}^{n}(e_i - e_{i-1})^2}{\sum_{i=1}^{n}e_i^2}= 2(1 - r) - \underbrace{\frac{e_1^2 + e_n^2}{\sum_{i=1}^{n}e_i^2}}_{\overset{p}{\rightarrow} 0},
\]</span>
where <span class="math inline">\(r\)</span> is the slope in the regression of the <span class="math inline">\(e_i\)</span>’s on the <span class="math inline">\(e_{i-1}\)</span>’s, i.e.:
<span class="math display">\[
r = \frac{\sum_{i=2}^{n}e_i e_{i-1}}{\sum_{i=1}^{n-1}e_i^2}.
\]</span>
(<span class="math inline">\(r\)</span> is a consistent estimator of <span class="math inline">\(\mathbb{C}or(\varepsilon_i,\varepsilon_{i-1})\)</span>, i.e. <span class="math inline">\(\rho\)</span> in Eq. <a href="ChapterLS.html#eq:usual2">(1.27)</a>.)</p>
<p>The one-sided test for <span class="math inline">\(H_0\)</span>: <span class="math inline">\(\rho=0\)</span> against <span class="math inline">\(H_1\)</span>: <span class="math inline">\(\rho&gt;0\)</span> is carried out by comparing <span class="math inline">\(DW\)</span> to values <span class="math inline">\(d_L(T, K)\)</span> and <span class="math inline">\(d_U(T, K)\)</span>:
<span class="math display">\[
\left\{
\begin{array}{ll}
\mbox{If $DW &lt; d_L$,}&amp;\mbox{ the null hypothesis is rejected;}\\
\mbox{if $DW &gt; d_U$,}&amp;\mbox{ the hypothesis is not rejected;}\\
\mbox{If $d_L \le DW \le d_U$,} &amp;\mbox{ no conclusion is drawn.}
\end{array}
\right.
\]</span></p>
<div class="example">
<p><span id="exm:DurbinWats" class="example"><strong>Example 1.10  (Durbin-Watson test) </strong></span>We regress the short-term nominal US interest rate on inflation. We then employ the Durbin-Watson test to see whether the residuals are auto-correlated (which is quite obviously the case).</p>
<div class="sourceCode" id="cb28"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://r-forge.r-project.org/projects/car/">car</a></span><span class="op">)</span></span>
<span><span class="va">data</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/subset.html">subset</a></span><span class="op">(</span><span class="va">JST</span>,<span class="va">iso</span><span class="op">==</span><span class="st">"USA"</span><span class="op">)</span>;<span class="cn">T</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/dim.html">dim</a></span><span class="op">(</span><span class="va">data</span><span class="op">)</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span></span>
<span><span class="va">data</span><span class="op">$</span><span class="va">infl</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="cn">NaN</span>,<span class="fl">100</span><span class="op">*</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">data</span><span class="op">$</span><span class="va">cpi</span><span class="op">[</span><span class="fl">2</span><span class="op">:</span><span class="cn">T</span><span class="op">]</span><span class="op">/</span><span class="va">data</span><span class="op">$</span><span class="va">cpi</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="op">(</span><span class="cn">T</span><span class="op">-</span><span class="fl">1</span><span class="op">)</span><span class="op">]</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">data</span><span class="op">$</span><span class="va">infl</span><span class="op">[</span><span class="op">(</span><span class="va">data</span><span class="op">$</span><span class="va">infl</span><span class="op">&lt;</span> <span class="op">-</span><span class="fl">5</span><span class="op">)</span><span class="op">|</span><span class="op">(</span><span class="va">data</span><span class="op">$</span><span class="va">infl</span><span class="op">&gt;</span><span class="fl">10</span><span class="op">)</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="cn">NaN</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>mfrow<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>,<span class="fl">2</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">data</span><span class="op">$</span><span class="va">year</span>,<span class="va">data</span><span class="op">$</span><span class="va">stir</span>,ylim<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="op">-</span><span class="fl">10</span>,<span class="fl">20</span><span class="op">)</span>,type<span class="op">=</span><span class="st">"l"</span>,lwd<span class="op">=</span><span class="fl">2</span>,xlab<span class="op">=</span><span class="st">""</span>,</span>
<span>     ylab<span class="op">=</span><span class="st">""</span>,main<span class="op">=</span><span class="st">"Nominal rate and inflation"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/lines.html">lines</a></span><span class="op">(</span><span class="va">data</span><span class="op">$</span><span class="va">year</span>,<span class="va">data</span><span class="op">$</span><span class="va">infl</span>,col<span class="op">=</span><span class="st">"red"</span>,lwd<span class="op">=</span><span class="fl">2</span><span class="op">)</span></span>
<span><span class="va">eq</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">stir</span><span class="op">~</span><span class="va">infl</span>,data<span class="op">=</span><span class="va">data</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">eq</span><span class="op">$</span><span class="va">residuals</span>,type<span class="op">=</span><span class="st">"l"</span>,col<span class="op">=</span><span class="st">"blue"</span>,main<span class="op">=</span><span class="st">"Residuals"</span>,xlab<span class="op">=</span><span class="st">""</span>,ylab<span class="op">=</span><span class="st">""</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="MicroEc_files/figure-html/DB-1.png" width="672"></div>
<div class="sourceCode" id="cb29"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/pkg/car/man/durbinWatsonTest.html">durbinWatsonTest</a></span><span class="op">(</span><span class="va">eq</span><span class="op">)</span></span></code></pre></div>
<pre><code>##  lag Autocorrelation D-W Statistic p-value
##    1       0.7321902     0.4984178       0
##  Alternative hypothesis: rho != 0</code></pre>
</div>
<!-- ## Summary -->
<!-- |                                                      | Under Assumptions \@ref(hyp:fullrank)+ | $\bv{b}$ normal in small sample (Eq. \@ref(eq:distriBcondi)) | $\bv{b}$ is BLUE (Thm \@ref(thm:GaussMarkov)) | $\bv{b}$ unbiased in small sample (Prop. \@ref(prp:propOLS)) | $\bv{b}$ consistent (Prop. \@ref(prp:XXX))$^*$ | $\bv{b}$ $\sim$ normal in large sample (Prop. \@ref(prp:AsymptGRM))$^*$ | -->
<!-- |------------------------------------------------------|-----------------------------------------|-------------------------------------------------------------|----------------------------------------------|--------------------------------------------------------------|------------------------------------------------|--------------------------------------------------------------------------| -->
<!-- | \rotatebox[origin=c]{90}{ Condit. mean-zero}         | \@ref(hyp:exogeneity)                  | X                                                           | X                                            | X                                                            | X                                              | X                                                                        | -->
<!-- | \rotatebox[origin=c]{90}{ Homoskedasticity}          | \@ref(hyp:homoskedasticity)            | X                                                           | X                                            |                                                              |                                                |                                                                          | -->
<!-- | \rotatebox[origin=c]{90}{Uncorrelated residuals}     | \@ref(hyp:noncorrelResid)             | X                                                           | X                                            |                                                              |                                                |                                                                          | -->
<!-- | \rotatebox[origin=c]{90}{ Normality of disturbances} | \@ref(hyp:normality)                   | X                                                           |                                              |                                                              |                                                |                                                                          | -->
<!-- $^*$: see however Prop. \@ref(prp:XXX) and Prop. \@ref(prp:AsymptGRM) for additional hypotheses. Specifically $\bv{X}'\bv{X}/n$ and $\bv{X}'\boldsymbol{\Sigma}\bv{X}/n$ must converge in proba. to finite positive definite matrices ($\boldsymbol\Sigma$ is defined in Eq. \@ref(eq:assumGLS2)). -->
</div>
<div id="Clusters" class="section level3" number="1.5.5">
<h3>
<span class="header-section-number">1.5.5</span> Cluster-robust covariance matrices<a class="anchor" aria-label="anchor" href="#Clusters"><i class="fas fa-link"></i></a>
</h3>
<p>The present section is based on <span class="citation">J. G. MacKinnon, Nielsen, and Webb (<a href="references.html#ref-MACKINNON2022">2022</a>)</span>; another useful reference is <span class="citation">Cameron and Miller (<a href="references.html#ref-Cameron_Miller_2014">2014</a>)</span>. We will see how one can approximate <span class="math inline">\(Q_{x\boldsymbol\Sigma x}=\mbox{plim }(\mathbf{X}'\boldsymbol\Sigma\mathbf{X}/n)\)</span> (see Prop. <a href="ChapterLS.html#prp:AsymptGRM">1.14</a>) when the dataset can be decomposed into <em>clusters</em>. These clusters constitute a partition of the sample, and they are such that the error terms may be correlated within a given cluster, but not across different clusters. A cluster may, e.g., gathers entities from a given geographical area, from the same industry, or same age cohort.</p>
<p>The OLS estimator satisfies:
<span class="math display" id="eq:BBB">\[\begin{equation}
\mathbf{b} = \boldsymbol\beta + (\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\boldsymbol\varepsilon.\tag{1.38}
\end{equation}\]</span>
Consider a set <span class="math inline">\(\{n_1,n_2,\dots,n_G\}\)</span> s.t. <span class="math inline">\(n=\sum_g n_g\)</span>, on which is based the following decomposition of <span class="math inline">\(\mathbf{X}\)</span>:
<span class="math display">\[
\mathbf{X} = \left[
\begin{array}{c}
\mathbf{X}_1 \\
\mathbf{X}_2 \\
\vdots\\
\mathbf{X}_G
\end{array}
\right].
\]</span>
With these notations, Eq. <a href="ChapterLS.html#eq:BBB">(1.38)</a> rewrites:
<span class="math display" id="eq:cluster1">\[\begin{equation}
\mathbf{b} - \boldsymbol\beta = \left(\sum_{g=1}^G \mathbf{X}_g'\mathbf{X}_g\right)^{-1}\sum_{g=1}^G \mathbf{s}_g,\tag{1.39}
\end{equation}\]</span>
where
<span class="math display" id="eq:definiS">\[\begin{equation}
\mathbf{s}_g = \mathbf{X}_g'\boldsymbol\varepsilon_g \tag{1.40}
\end{equation}\]</span>
denotes the score vector (of dimension <span class="math inline">\(K \times 1\)</span>) associated with the <span class="math inline">\(g^{th}\)</span> cluster.</p>
<p>If the model is correctly specified then <span class="math inline">\(\mathbb{E}(\mathbf{s}_g)=0\)</span> for all clusters <span class="math inline">\(g\)</span>. Note that Eq. <a href="ChapterLS.html#eq:cluster1">(1.39)</a> is valid for any partition of <span class="math inline">\(\{1,\dots,n\}\)</span>. Dividing the sample into clusters becomes meaningful if we further assume that the following hypothesis holds:</p>
<div class="hypothesis">
<p><span id="hyp:cluster" class="hypothesis"><strong>Hypothesis 1.6  (Clusters) </strong></span>We have:
<span class="math display">\[
(i)\; \mathbb{E}(\mathbf{s}_g\mathbf{s}_g')=\Sigma_g,\quad (ii)\; \mathbb{E}(\mathbf{s}_g\mathbf{s}_q')=0,\;g \ne q,
\]</span>
where <span class="math inline">\(s_g\)</span> is defined in Eq. <a href="ChapterLS.html#eq:definiS">(1.40)</a>.</p>
</div>
<p>The real assumption here is <span class="math inline">\((ii)\)</span>; the first one simply gives a notation for the covariance matrix of the score associated with the <span class="math inline">\(g^{th}\)</span> cluster. Remark that these covariance matrices can differ across clusters. That is, <em>cluster-based inference is robust against both heteroskedasticity and intra-cluster dependence without imposing any restrictions on the (unknown) form of either of them</em>.</p>
<!-- While the choice of clustering structure ---i.e., which entity is in which cluster--- is sometimes debatable, the structure is generally assumed known in both theoretical and applied work.  -->
<p>Naturally, matrix <span class="math inline">\(\Sigma_g\)</span> depends on the covariance structure of the <span class="math inline">\(\varepsilon\)</span>’s. In particular, if <span class="math inline">\(\Omega_g = \mathbb{E}(\boldsymbol\varepsilon_g\boldsymbol\varepsilon_g'|\mathbf{X}_g)\)</span>, then we have <span class="math inline">\(\Sigma_g = \mathbb{E}(\mathbf{X}_g'\Omega_g\mathbf{X}_g)\)</span>.</p>
<p>Under Hypothesis <a href="ChapterLS.html#hyp:cluster">1.6</a>, it comes that the conditional covariance matrix of <span class="math inline">\(\mathbf{b}\)</span> is:
<span class="math display" id="eq:cluster2">\[\begin{equation}
\left(\mathbf{X}'\mathbf{X}\right)^{-1}\left(\sum_{g=1}^G \Sigma_g\right)\left(\mathbf{X}'\mathbf{X}\right)^{-1}\tag{1.41}
\end{equation}\]</span></p>
<p>Let us denote by <span class="math inline">\(\varepsilon_{g,i}\)</span> the error associated with the <span class="math inline">\(i^{th}\)</span> component of vector <span class="math inline">\(\boldsymbol\varepsilon_g\)</span>. Consider the special case where <span class="math inline">\(\mathbb{E}(\varepsilon_{g,i} \varepsilon_{g,j}|\mathbf{X}_g)=\sigma^2\mathbb{I}_{\{i=j\}}\)</span>, then Eq. <a href="ChapterLS.html#eq:cluster2">(1.41)</a> gives the standard expression <span class="math inline">\(\sigma^2\left(\mathbf{X}'\mathbf{X}\right)^{-1}\)</span> (see Eq. <a href="ChapterLS.html#eq:distriBcondi">(1.8)</a>).</p>
<p>If we have <span class="math inline">\(\mathbb{E}(\varepsilon_{gi} \varepsilon_{gj}|\mathbf{X}_g)=\sigma_{gi}^2\mathbb{I}_{\{i=j\}}\)</span>, then we fall in the case addressed by the White formula (see Example <a href="ChapterLS.html#exm:HCheteroskedasticity">1.8</a>). That is, in this case, the conditional covariance matrix of <span class="math inline">\(\mathbf{b}\)</span> is:
<span class="math display">\[
\left(\mathbf{X}'\mathbf{X}\right)^{-1}\left(\mathbf{X}'\left[  \begin{array}{cccc}
\sigma_1^2 &amp; 0 &amp; \dots &amp; 0 \\
0 &amp; \sigma_2^2 &amp;  &amp; 0 \\
\vdots &amp;&amp; \ddots&amp; \vdots \\
0 &amp; \dots &amp; 0 &amp; \sigma_n^2
\end{array} \right]\mathbf{X}\right)\left(\mathbf{X}'\mathbf{X}\right)^{-1}.
\]</span>
As in <span class="citation">White (<a href="references.html#ref-White_1980">1980</a>)</span>, the natural way to approach the conditional covariance given in Eq. <a href="ChapterLS.html#eq:cluster2">(1.41)</a> consists in replacing the <span class="math inline">\(\Sigma_g\)</span> matrices by their sample equivalent, i.e. <span class="math inline">\(\widehat{\Sigma}_g=\mathbf{X}_g'\mathbf{e}_g\mathbf{e}_g'\mathbf{X}_g\)</span>. Adding corrections for the number of degrees of freedom, this leads to the following estimate of the covariance matrix of <span class="math inline">\(\mathbf{b}\)</span>:
<span class="math display" id="eq:AsymptCL">\[\begin{equation}
\frac{G(n-1)}{(G-1)(n-K)}\left(\mathbf{X}'\mathbf{X}\right)^{-1}\left(\sum_{g=1}^G\widehat{\Sigma}_g\right) \left(\mathbf{X}'\mathbf{X}\right)^{-1}. \tag{1.42}
\end{equation}\]</span>
The previous estimate is CRCV1 in <span class="citation">J. G. MacKinnon, Nielsen, and Webb (<a href="references.html#ref-MACKINNON2022">2022</a>)</span>. Note that we indeed find the White-MacKinnon estimator (Eq. <a href="ChapterLS.html#eq:WhiteHC1">(1.34)</a>) when <span class="math inline">\(G=n\)</span>.</p>
<p>Remark that if there was only one cluster (<span class="math inline">\(G=1\)</span>), and neglecting the degree-of-freedom correction, we would have:
<span class="math display">\[
\left(\mathbf{X}'\mathbf{X}\right)^{-1}\left(\mathbf{X}'\mathbf{e}\mathbf{e}'\mathbf{X}\right) \left(\mathbf{X}'\mathbf{X}\right)^{-1} = 0
\]</span>
because <span class="math inline">\(\mathbf{X}'\mathbf{e}=0\)</span>. Hence, having large clusters does not necessarily increase variance.</p>
<p>Often, when working with panel data (see Chapter <a href="Panel.html#Panel">2</a>), we want to cluster in different dimensions. A typical case is when the data are indexed by both individuals (<span class="math inline">\(i\)</span>) and time (<span class="math inline">\(t\)</span>). In that case, we may indeed suspect that: (a) the residuals are correlated across clusters of dates (e.g., with monthly data, a cluster may be one year) and (b) the residuals are correlated across clusters of individuals (e.g., with data at the county level a cluster may be a state). In this case, one can employ <strong>two-way clustering</strong>.</p>
<p>Formally, consider two distinct partitions of the data: one through index <span class="math inline">\(g\)</span>, with <span class="math inline">\(g \in \{1,\dots,G\}\)</span>, and the other through index <span class="math inline">\(h\)</span>, with <span class="math inline">\(h \in \{1,\dots,H\}\)</span>. Accordingly, we denote by <span class="math inline">\(\mathbf{X}_{g,h}\)</span> the submatrix of <span class="math inline">\(\mathbf{X}\)</span> that contains the explanatory variables corresponding to clusters <span class="math inline">\(g\)</span> and <span class="math inline">\(h\)</span> (e.g., the firms of a given country <span class="math inline">\(g\)</span> at a given date <span class="math inline">\(h\)</span>). We also denote by <span class="math inline">\(\mathbf{X}_{g,\bullet}\)</span> (respectively <span class="math inline">\(\mathbf{X}_{\bullet,h}\)</span>) the submatrix of <span class="math inline">\(\mathbf{X}\)</span> containing all explanatory variables pertaining to cluster <span class="math inline">\(g\)</span>, for all possible values of <span class="math inline">\(h\)</span> (resp. to cluster <span class="math inline">\(h\)</span>, for all possible values of <span class="math inline">\(g\)</span>).</p>
<p>We will make the following assumption:</p>
<div class="hypothesis">
<p><span id="hyp:twowaycluster" class="hypothesis"><strong>Hypothesis 1.7  (Two-way clusters) </strong></span>We have:
<span class="math display">\[\begin{eqnarray*}
&amp;&amp;\mathbb{E}(\mathbf{s}_{g,\bullet}\mathbf{s}_{g,\bullet}')=\Sigma_g,\quad \mathbb{E}(\mathbf{s}_{\bullet,h}\mathbf{s}_{\bullet,h}')=\Sigma^*_h,\quad \mathbb{E}(\mathbf{s}_{g,h}\mathbf{s}_{g,h}')=\Sigma_{g,h},\\ &amp;&amp;\mathbb{E}(\mathbf{s}_{g,h}\mathbf{s}_{q,k}')=0\;\mbox{if }g\neq q\mbox{ and }h \ne k.
\end{eqnarray*}\]</span></p>
</div>
<div class="proposition">
<p><span id="prp:twoWayCov" class="proposition"><strong>Proposition 1.16  (Covariance of scores in the two-way-cluster setup) </strong></span>Under this assumption, the matrix of covariance of the scores is given by:
<span class="math display">\[
\Sigma = \sum_{g=1}^G \Sigma_{g} + \sum_{h=1}^H \Sigma^*_{h} - \sum_{g=1}^G\sum_{h=1}^H \Sigma_{g,h}.
\]</span>
(The last term on the right-hand side must be subtracted in order to avoid double counting.)</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-15" class="proof"><em>Proof</em>. </span>We have:
<span class="math display">\[\begin{eqnarray*}
\Sigma &amp;=&amp; \sum_{g=1}^G\sum_{q=1}^G\sum_{h=1}^H\sum_{k=1}^H \mathbf{s}_{g,h}\mathbf{s}_{q,k}'\\
&amp;=&amp; \sum_{g=1}^G\underbrace{\left(\sum_{h=1}^H\sum_{k=1}^H \mathbf{s}_{g,h}\mathbf{s}_{g,k}'\right)}_{=\Sigma_g}+\sum_{h=1}^H\underbrace{\left(\sum_{g=1}^G\sum_{q=1}^G \mathbf{s}_{g,h}\mathbf{s}_{q,h}'\right)}_{=\Sigma^*_h}-\sum_{g=1}^G\sum_{h=1}^H \mathbf{s}_{g,h}\mathbf{s}_{g,h}',
\end{eqnarray*}\]</span>
which gives the result.</p>
</div>
<p>The asymptotic theory can be based on two different approaches: (i) large number of clusters (common case), and (ii) fixed number of clusters but large number of observations in each cluster (see Subsections 4.1 and 4.2 in <span class="citation">J. G. MacKinnon, Nielsen, and Webb (<a href="references.html#ref-MACKINNON2022">2022</a>)</span>). The more variable the <span class="math inline">\(N_g\)</span>’s (clusters’ sizes are heterogeneous in terms of size), the less reliable asymptotic inference based on Eq. <a href="ChapterLS.html#eq:AsymptCL">(1.42)</a>, especially when a very few clusters are unusually large, or when the distribution of the data is heavy-tailed. These issues are somehow mitigated when the clusters have an approximate factor structure.</p>
<p>In practice, <span class="math inline">\(\Sigma\)</span> is estimated by:
<span class="math display">\[
\widehat{\Sigma} = \sum_{g=1}^G \widehat{\mathbf{s}}_{g,\bullet}\widehat{\mathbf{s}}_{g,\bullet}' + \sum_{h=1}^H \widehat{\mathbf{s}}_{\bullet,h}\widehat{\mathbf{s}}_{\bullet,h} - \sum_{g=1}^G\sum_{h=1}^H \widehat{\mathbf{s}}_{g,h}\widehat{\mathbf{s}}_{g,h}',
\]</span>
and we use:
<span class="math display">\[
\widehat{\mathbb{V}ar}(\mathbf{b}) = \left(\mathbf{X}'\mathbf{X}\right)^{-1}\widehat{\Sigma}\left(\mathbf{X}'\mathbf{X}\right)^{-1}.
\]</span></p>
<p>As an alternative to the asymptotic approximation to the distribution of a statistic of interest, one can resort to bootstrap approximation (see Section 5 of <span class="citation">J. G. MacKinnon, Nielsen, and Webb (<a href="references.html#ref-MACKINNON2022">2022</a>)</span>). In R, the package <code>fwildclusterboot</code> allows to implement such approaches.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;See, e.g., &lt;a href="https://cran.r-project.org/web/packages/fwildclusterboot/vignettes/fwildclusterboot.html"&gt;this tutorial by Alexander Fischer&lt;/a&gt;.&lt;/p&gt;'><sup>4</sup></a></p>
<p>Let us come back to the analysis of the effect of systemic financial crises on GDP growth. Clustering the data at the country level and, further, at both the country and time levels gives the following:</p>
<div class="sourceCode" id="cb31"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">eq</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">growth</span><span class="op">~</span><span class="va">crisisJST</span><span class="op">+</span><span class="va">iso</span><span class="op">+</span><span class="va">year</span>,data<span class="op">=</span><span class="va">JST.red</span><span class="op">)</span></span>
<span><span class="va">vcov1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://sandwich.R-Forge.R-project.org/reference/vcovHC.html">vcovHC</a></span><span class="op">(</span><span class="va">eq</span>, type <span class="op">=</span> <span class="st">"HC1"</span><span class="op">)</span></span>
<span><span class="va">vcov2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://sandwich.R-Forge.R-project.org/reference/vcovCL.html">vcovCL</a></span><span class="op">(</span><span class="va">eq</span>, cluster <span class="op">=</span> <span class="va">JST.red</span><span class="op">[</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"iso"</span><span class="op">)</span><span class="op">]</span><span class="op">)</span></span>
<span><span class="va">vcov3</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://sandwich.R-Forge.R-project.org/reference/vcovCL.html">vcovCL</a></span><span class="op">(</span><span class="va">eq</span>, cluster <span class="op">=</span> <span class="va">JST.red</span><span class="op">[</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"iso"</span>,<span class="st">"year"</span><span class="op">)</span><span class="op">]</span><span class="op">)</span></span>
<span><span class="fu">stargazer</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/stargazer/man/stargazer.html">stargazer</a></span><span class="op">(</span><span class="va">eq</span>, <span class="va">eq</span>, <span class="va">eq</span>, <span class="va">eq</span>,type <span class="op">=</span> <span class="st">"text"</span>,</span>
<span>                     column.labels <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"No HAC"</span>, <span class="st">"Heterosk."</span>,</span>
<span>                                       <span class="st">"1-way Clust."</span>,<span class="st">"1-way Clust."</span><span class="op">)</span>,</span>
<span>                     omit <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"iso"</span>,<span class="st">"year"</span><span class="op">)</span>,no.space <span class="op">=</span> <span class="cn">TRUE</span>,keep.stat <span class="op">=</span> <span class="st">"n"</span>,</span>
<span>                     se <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span><span class="cn">NULL</span>,<span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/diag.html">diag</a></span><span class="op">(</span><span class="va">vcov1</span><span class="op">)</span><span class="op">)</span>,</span>
<span>                               <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/diag.html">diag</a></span><span class="op">(</span><span class="va">vcov2</span><span class="op">)</span><span class="op">)</span>,<span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/diag.html">diag</a></span><span class="op">(</span><span class="va">vcov3</span><span class="op">)</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<pre><code>## 
## ==========================================================
##                           Dependent variable:             
##              ---------------------------------------------
##                                 growth                    
##               No HAC   Heterosk. 1-way Clust. 1-way Clust.
##                 (1)       (2)        (3)          (4)     
## ----------------------------------------------------------
## crisisJST    -0.015*** -0.015***  -0.015***     -0.015**  
##               (0.005)   (0.006)    (0.006)      (0.007)   
## Constant     0.042***  0.042***    0.042***     0.042***  
##               (0.005)   (0.007)    (0.007)      (0.002)   
## ----------------------------------------------------------
## Observations   1,258     1,258      1,258        1,258    
## ==========================================================
## Note:                          *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01</code></pre>
</div>
</div>
<div id="shrinkage-methods" class="section level2" number="1.6">
<h2>
<span class="header-section-number">1.6</span> Shrinkage methods<a class="anchor" aria-label="anchor" href="#shrinkage-methods"><i class="fas fa-link"></i></a>
</h2>
<p>Chosing the appropriate explanatory variables is often complicated, especially in the presence of many potentially relevant covariates. Keeping a large number of covariates results in large standard deviations for the estimated parameters (see Section <a href="ChapterLS.html#irrelevant">1.3.3</a>). In order to address this issue, shrinkage methods have been designed. The objective of these methods is to help select a limited number of variables by shrinking the regression coefficients of the less useful variables towards zero. The two best-known shrinkage techniques are <strong>ridge regression</strong> and the <strong>lasso</strong> approach.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;See &lt;span class="citation"&gt;Tibshirani (&lt;a href="references.html#ref-Tibshirani_2011"&gt;2011&lt;/a&gt;)&lt;/span&gt; for a review of the lasso approach. See also Section 6.2 of &lt;span class="citation"&gt;James et al. (&lt;a href="references.html#ref-James2013"&gt;2013&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;'><sup>5</sup></a> In both cases (ridge and lasso), the OLS minimization problem (see Section <a href="ChapterLS.html#LSquares">1.2</a>), i.e.,
<span class="math display">\[\begin{equation}
\mathbf{b} = \underset{\boldsymbol\beta}{\mbox{argmin}}\; \sum_{i=1}^n(y_i - \mathbf{x}_i'\boldsymbol\beta)^2
\end{equation}\]</span>
is replaced with the following:
<span class="math display" id="eq:minLasso">\[\begin{equation}
\mathbf{b}_\lambda = \underset{\boldsymbol\beta}{\mbox{argmin}}\; \sum_{i=1}^n(y_i - \mathbf{x}_i'\boldsymbol\beta)^2 + \lambda f(\boldsymbol\beta),\tag{1.43}
\end{equation}\]</span>
where <span class="math inline">\(\lambda f(\boldsymbol\beta)\)</span> is a penalty term that positively depends on the “size” of the components of <span class="math inline">\(\boldsymbol\beta\)</span>. This term is called the <em>shrinkage penalty</em> term.</p>
<p>Specifically, assuming that the vector <span class="math inline">\(\mathbf{x}_i\)</span> of potential covariates is of dimension <span class="math inline">\(K \times 1\)</span>, we have:
<span class="math display">\[\begin{eqnarray*}
f(\boldsymbol\beta) &amp; = &amp; \sum_{j=1}^K \beta_j^2 \quad \mbox{in the ridge case ($\ell_2$ norm)},\\
f(\boldsymbol\beta) &amp; = &amp; \sum_{j=1}^K |\beta_j| \quad \mbox{in the lasso case ($\ell_1$ norm)}.
\end{eqnarray*}\]</span></p>
<p>In most cases, we do not want to involve the intercept in the set of parameters to shrink, and the preceding equations are respectively replaced with:
<span class="math display">\[\begin{eqnarray*}
f(\boldsymbol\beta) &amp; = &amp; \sum_{j=2}^K \beta_j^2 \quad \mbox{(ridge)},\\
f(\boldsymbol\beta) &amp; = &amp; \sum_{j=2}^K |\beta_j| \quad \mbox{(lasso)}.
\end{eqnarray*}\]</span></p>
<p>The nature of the penalty —based on either the <span class="math inline">\(\ell_1\)</span> or the <span class="math inline">\(\ell_2\)</span> norm— implies a different behaviour of the parameter estimates when <span class="math inline">\(\lambda\)</span> —the <em>tuning parameter</em>— grows. In the ridge regression, coefficient estimates go to zero (shrinkage); in the lasso case, some coefficients reach zero when <span class="math inline">\(\lambda\)</span> reach some values. In other words, while ridge regression achieve shrinkage, lasso regressions achieve shrinkage <em>and</em> variable selection.</p>
<p>Parameter <span class="math inline">\(\lambda\)</span> has to be determined separately from the minimization problem of Eq. <a href="ChapterLS.html#eq:minLasso">(1.43)</a>. One can combine standard criteria (e.g., BIC or Akaike) for this purpose.</p>
<p>In R, one can use the <code>glmnet</code> package to run ridge and lasso regressions. In the following example, we employ this package to model interest rates proposed to debtors, using data extracted from the <a href="https://www.kaggle.com/datasets/ethon0426/lending-club-20072020q1">Lending Club</a> platform.</p>
<p>To begin with, let us define the variables we want to consider:</p>
<div class="sourceCode" id="cb33"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">AEC</span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://glmnet.stanford.edu">glmnet</a></span><span class="op">)</span></span>
<span><span class="va">credit</span><span class="op">$</span><span class="va">owner</span> <span class="op">&lt;-</span> <span class="fl">1</span><span class="op">*</span><span class="op">(</span><span class="va">credit</span><span class="op">$</span><span class="va">home_ownership</span><span class="op">==</span><span class="st">"OWN"</span><span class="op">)</span></span>
<span><span class="va">credit</span><span class="op">$</span><span class="va">renter</span> <span class="op">&lt;-</span> <span class="fl">1</span><span class="op">*</span><span class="op">(</span><span class="va">credit</span><span class="op">$</span><span class="va">home_ownership</span><span class="op">==</span><span class="st">"MORTGAGE"</span><span class="op">)</span></span>
<span><span class="va">credit</span><span class="op">$</span><span class="va">verification_status</span> <span class="op">&lt;-</span> <span class="fl">1</span><span class="op">*</span><span class="op">(</span><span class="va">credit</span><span class="op">$</span><span class="va">verification_status</span><span class="op">==</span><span class="st">"Not Verified"</span><span class="op">)</span></span>
<span><span class="va">credit</span><span class="op">$</span><span class="va">emp_length_10</span> <span class="op">&lt;-</span> <span class="fl">1</span><span class="op">*</span><span class="op">(</span><span class="va">credit</span><span class="op">$</span><span class="va">emp_length_10</span><span class="op">)</span></span>
<span><span class="va">credit</span><span class="op">$</span><span class="va">log_annual_inc</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">credit</span><span class="op">$</span><span class="va">annual_inc</span><span class="op">)</span></span>
<span><span class="va">credit</span><span class="op">$</span><span class="va">log_funded_amnt</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">credit</span><span class="op">$</span><span class="va">funded_amnt</span><span class="op">)</span></span>
<span><span class="va">credit</span><span class="op">$</span><span class="va">annual_inc2</span> <span class="op">&lt;-</span> <span class="op">(</span><span class="va">credit</span><span class="op">$</span><span class="va">annual_inc</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span></span>
<span><span class="va">credit</span><span class="op">$</span><span class="va">funded_amnt2</span> <span class="op">&lt;-</span> <span class="op">(</span><span class="va">credit</span><span class="op">$</span><span class="va">funded_amnt</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span></span>
<span><span class="va">x</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/subset.html">subset</a></span><span class="op">(</span><span class="va">credit</span>,</span>
<span>            select <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">delinq_2yrs</span>,<span class="va">annual_inc</span>,<span class="va">annual_inc2</span>,<span class="va">log_annual_inc</span>,</span>
<span>                       <span class="va">dti</span>,<span class="va">installment</span>,<span class="va">funded_amnt</span>,</span>
<span>                       <span class="va">funded_amnt2</span>,<span class="va">log_funded_amnt</span>,<span class="va">pub_rec</span>,<span class="va">emp_length_10</span>,</span>
<span>                       <span class="va">owner</span>,<span class="va">renter</span>,<span class="va">pub_rec_bankruptcies</span>,<span class="va">revol_util</span>,<span class="va">revol_bal</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<p>Let us standardize the data:</p>
<div class="sourceCode" id="cb34"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">y</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/scale.html">scale</a></span><span class="op">(</span><span class="va">credit</span><span class="op">$</span><span class="va">int_rate</span><span class="op">)</span></span>
<span><span class="va">x</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/scale.html">scale</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span></span>
<span><span class="va">indic_complete_cases</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/complete.cases.html">complete.cases</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span></span>
<span><span class="va">y</span> <span class="op">&lt;-</span> <span class="va">y</span><span class="op">[</span><span class="va">indic_complete_cases</span><span class="op">]</span>  <span class="co"># to remove observations with NaN's</span></span>
<span><span class="va">x</span> <span class="op">&lt;-</span> <span class="va">x</span><span class="op">[</span><span class="va">indic_complete_cases</span>,<span class="op">]</span> <span class="co"># to remove observations with NaN's</span></span></code></pre></div>
<p>Next, we define the set of <span class="math inline">\(\lambda\)</span> we will use, and run the ridge and lasso regressions:</p>
<div class="sourceCode" id="cb35"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">grid.lambda</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span><span class="op">(</span><span class="fl">0</span>,<span class="fl">.2</span>,by<span class="op">=</span><span class="fl">.005</span><span class="op">)</span></span>
<span><span class="va">result.ridge</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://glmnet.stanford.edu/reference/glmnet.html">glmnet</a></span><span class="op">(</span><span class="va">x</span>, <span class="va">y</span>, alpha <span class="op">=</span> <span class="fl">0</span>, lambda <span class="op">=</span> <span class="va">grid.lambda</span><span class="op">)</span></span>
<span><span class="va">result.lasso</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://glmnet.stanford.edu/reference/glmnet.html">glmnet</a></span><span class="op">(</span><span class="va">x</span>, <span class="va">y</span>, alpha <span class="op">=</span> <span class="fl">1</span>, lambda <span class="op">=</span> <span class="va">grid.lambda</span><span class="op">)</span></span></code></pre></div>
<p>The following figure shows how estimated parameters depend on <span class="math inline">\(\lambda\)</span>:</p>
<div class="sourceCode" id="cb36"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">variab</span> <span class="op">&lt;-</span> <span class="fl">6</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">result.ridge</span><span class="op">$</span><span class="va">lambda</span>,<span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="va">result.ridge</span><span class="op">)</span><span class="op">[</span><span class="va">variab</span>,<span class="op">]</span>,type<span class="op">=</span><span class="st">"l"</span>,</span>
<span>     ylim<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Extremes.html">min</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="va">result.ridge</span><span class="op">)</span><span class="op">[</span><span class="va">variab</span>,<span class="op">]</span>,<span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="va">result.lasso</span><span class="op">)</span><span class="op">[</span><span class="va">variab</span>,<span class="op">]</span><span class="op">)</span>,</span>
<span>            <span class="fu"><a href="https://rdrr.io/r/base/Extremes.html">max</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="va">result.ridge</span><span class="op">)</span><span class="op">[</span><span class="va">variab</span>,<span class="op">]</span>,<span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="va">result.lasso</span><span class="op">)</span><span class="op">[</span><span class="va">variab</span>,<span class="op">]</span><span class="op">)</span><span class="op">)</span>,</span>
<span>     xlab<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/expression.html">expression</a></span><span class="op">(</span><span class="va">lambda</span><span class="op">)</span>,ylab<span class="op">=</span><span class="st">"Estimated parameter"</span>,lwd<span class="op">=</span><span class="fl">2</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/lines.html">lines</a></span><span class="op">(</span><span class="va">result.lasso</span><span class="op">$</span><span class="va">lambda</span>,<span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="va">result.lasso</span><span class="op">)</span><span class="op">[</span><span class="va">variab</span>,<span class="op">]</span>,col<span class="op">=</span><span class="st">"red"</span>,lwd<span class="op">=</span><span class="fl">2</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="MicroEc_files/figure-html/lasso4-1.png" width="672"></div>
<p>Let us take two values of <span class="math inline">\(\lambda\)</span> and see the associated estimated parameters in the context of lasso regressions:</p>
<div class="sourceCode" id="cb37"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">i</span> <span class="op">&lt;-</span> <span class="fl">20</span>; <span class="va">j</span> <span class="op">&lt;-</span> <span class="fl">40</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cbind.html">cbind</a></span><span class="op">(</span><span class="va">result.lasso</span><span class="op">$</span><span class="va">lambda</span><span class="op">[</span><span class="va">i</span><span class="op">]</span>,<span class="va">result.lasso</span><span class="op">$</span><span class="va">lambda</span><span class="op">[</span><span class="va">j</span><span class="op">]</span><span class="op">)</span></span></code></pre></div>
<pre><code>##       [,1]  [,2]
## [1,] 0.105 0.005</code></pre>
<div class="sourceCode" id="cb39"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/cbind.html">cbind</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="va">result.lasso</span><span class="op">)</span><span class="op">[</span>,<span class="va">i</span><span class="op">]</span>,<span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="va">result.lasso</span><span class="op">)</span><span class="op">[</span>,<span class="va">j</span><span class="op">]</span><span class="op">)</span></span></code></pre></div>
<pre><code>##                              [,1]         [,2]
## (Intercept)          -0.003183346 -0.002290639
## delinq_2yrs           0.086457831  0.096538222
## annual_inc            0.000000000  0.009288932
## annual_inc2           0.000000000  0.000000000
## log_annual_inc        0.000000000 -0.052405808
## dti                   0.000000000 -0.021505827
## installment           0.100715553  7.017150265
## funded_amnt           0.000000000 -6.307245657
## funded_amnt2          0.000000000 -0.328837450
## log_funded_amnt       0.000000000 -0.175860616
## pub_rec               0.000000000  0.046974246
## emp_length_10         0.000000000 -0.020078967
## owner                 0.000000000 -0.007930234
## renter               -0.007220882 -0.043264994
## pub_rec_bankruptcies  0.000000000  0.000000000
## revol_util            0.379766899  0.249451915
## revol_bal             0.000000000 -0.024474673</code></pre>
<div class="sourceCode" id="cb41"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Compute values of y predicted by the model, for all lambdas:</span></span>
<span><span class="va">pred1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">result.lasso</span>,<span class="fu"><a href="https://rdrr.io/r/base/matrix.html">as.matrix</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co"># Compute values of y predicted by the model, for a specific value:</span></span>
<span><span class="va">pred2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">result.lasso</span>,<span class="fu"><a href="https://rdrr.io/r/base/matrix.html">as.matrix</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span>,s<span class="op">=</span><span class="fl">0.085</span><span class="op">)</span></span></code></pre></div>
<p>The <code>glmnet</code> package (see <a href="https://glmnet.stanford.edu/articles/glmnet.html">Hastie et al. (2021)</a>) also offers tools to implement cross-validation:</p>
<div class="sourceCode" id="cb42"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># cross validation (cv):</span></span>
<span><span class="va">cvglmnet</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://glmnet.stanford.edu/reference/cv.glmnet.html">cv.glmnet</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/matrix.html">as.matrix</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span>,<span class="va">y</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">cvglmnet</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="MicroEc_files/figure-html/lasso6-1.png" width="672"></div>
<div class="sourceCode" id="cb43"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># lambda.min: lambda that gives minimum mean cross-validated error</span></span>
<span><span class="va">cvglmnet</span><span class="op">$</span><span class="va">lambda.min</span></span></code></pre></div>
<pre><code>## [1] 0.004277307</code></pre>
<div class="sourceCode" id="cb45"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># lambda.1se: largest lambda s.t. cost within the one-std-dev cv-based band</span></span>
<span><span class="va">cvglmnet</span><span class="op">$</span><span class="va">lambda.1se</span></span></code></pre></div>
<pre><code>## [1] 0.004694339</code></pre>
<div class="sourceCode" id="cb47"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="va">cvglmnet</span>, s <span class="op">=</span> <span class="st">"lambda.min"</span><span class="op">)</span> <span class="co"># associated parameters</span></span></code></pre></div>
<pre><code>## 17 x 1 sparse Matrix of class "dgCMatrix"
##                                s1
## (Intercept)          -0.002262204
## delinq_2yrs           0.093465307
## annual_inc            0.010181514
## annual_inc2           .          
## log_annual_inc       -0.052917617
## dti                  -0.022498219
## installment           7.289164713
## funded_amnt          -6.542070431
## funded_amnt2         -0.359156943
## log_funded_amnt      -0.183939650
## pub_rec               0.045755394
## emp_length_10        -0.019706751
## owner                -0.008752887
## renter               -0.042208229
## pub_rec_bankruptcies  .          
## revol_util            0.241530260
## revol_bal            -0.024248623</code></pre>
<div class="sourceCode" id="cb49"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># predicted values of y for specific values of x:</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">cvglmnet</span>, newx <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">as.matrix</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">5</span>,<span class="op">]</span>, s <span class="op">=</span> <span class="st">"lambda.min"</span><span class="op">)</span></span></code></pre></div>
<pre><code>##       lambda.min
## 21529  0.3218125
## 21547 -0.1873492
## 21579  0.6810101
## 21583 -0.2687087
## 21608 -0.2072584</code></pre>

</div>
</div>

  <div class="chapter-nav">
<div class="prev"><a href="index.html">Micro-Econometrics</a></div>
<div class="next"><a href="Panel.html"><span class="header-section-number">2</span> Panel regressions</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#ChapterLS"><span class="header-section-number">1</span> Linear Regressions</a></li>
<li><a class="nav-link" href="#linearHyp"><span class="header-section-number">1.1</span> Hypotheses</a></li>
<li>
<a class="nav-link" href="#LSquares"><span class="header-section-number">1.2</span> Least square estimation</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#derivation-of-the-ols-formula"><span class="header-section-number">1.2.1</span> Derivation of the OLS formula</a></li>
<li><a class="nav-link" href="#properties-of-the-ols-estimate-small-sample"><span class="header-section-number">1.2.2</span> Properties of the OLS estimate (small sample)</a></li>
<li><a class="nav-link" href="#goodness-of-fit"><span class="header-section-number">1.2.3</span> Goodness of fit</a></li>
<li><a class="nav-link" href="#inference-and-confidence-intervals-in-small-sample"><span class="header-section-number">1.2.4</span> Inference and confidence intervals (in small sample)</a></li>
<li><a class="nav-link" href="#Ftest"><span class="header-section-number">1.2.5</span> Testing a set of linear restrictions</a></li>
<li><a class="nav-link" href="#largeSample"><span class="header-section-number">1.2.6</span> Large Sample Properties</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#CommonPitfalls"><span class="header-section-number">1.3</span> Common pitfalls in linear regressions</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#multicollinearity"><span class="header-section-number">1.3.1</span> Multicollinearity</a></li>
<li><a class="nav-link" href="#Omitted"><span class="header-section-number">1.3.2</span> Omitted variables</a></li>
<li><a class="nav-link" href="#irrelevant"><span class="header-section-number">1.3.3</span> Irrelevant variable</a></li>
</ul>
</li>
<li><a class="nav-link" href="#IV"><span class="header-section-number">1.4</span> Instrumental Variables</a></li>
<li>
<a class="nav-link" href="#general-regression-model-grm-and-robust-covariance-matrices"><span class="header-section-number">1.5</span> General Regression Model (GRM) and robust covariance matrices</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#presentation-of-the-general-regression-model-grm"><span class="header-section-number">1.5.1</span> Presentation of the General Regression Model (GRM)</a></li>
<li><a class="nav-link" href="#GLS"><span class="header-section-number">1.5.2</span> Generalized Least Squares</a></li>
<li><a class="nav-link" href="#asymptotic-properties-of-the-ols-estimator-in-the-grm-framework"><span class="header-section-number">1.5.3</span> Asymptotic properties of the OLS estimator in the GRM framework</a></li>
<li><a class="nav-link" href="#HAC"><span class="header-section-number">1.5.4</span> HAC-robust covariance matrices</a></li>
<li><a class="nav-link" href="#Clusters"><span class="header-section-number">1.5.5</span> Cluster-robust covariance matrices</a></li>
</ul>
</li>
<li><a class="nav-link" href="#shrinkage-methods"><span class="header-section-number">1.6</span> Shrinkage methods</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
          
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Micro-Econometrics</strong>" was written by Jean-Paul Renne. It was last built on 2025-02-13.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
