# Microeconometrics

In microeconometric models, the variables of interest often feature restricted distributions ---for instance with discontinuous support---, which necessitates specific models. Typical examples are discrete-choice models (binary, multinomial, ordered outcomes), sample selection models (censored or truncated outcomes), and count-data models (integer outcomes). This chapter describes the estimation and interpretation of these models. It also shows how the discrete-choice models can emerge from (structural) random-utility frameworks.

## Binary-choice models

In many instances, the variables to be explained (the $y_i$'s) have only two possible values ($0$ and $1$, say). That is, they are binary variables. The probability for them to be equal to either 0 or 1 may depend on independent variables, gathered in vectors $\bv{x}_i$ ($K \times 1$).

The spectrum of applications is wide:

* Binary decisions (e.g. in referendums, being owner or renter, living in the city or in the countryside, in/out of the labour force,...),
* Contamination (disease or default),
* Success/failure (exams).

Without loss of generality, the model reads:
\begin{equation}\label{eq:binaryBenroulli}
y_i | \bv{X} \sim \mathcal{B}(g(\bv{x}_i;\boldsymbol\theta)),
\end{equation}
where $g(\bv{x}_i;\boldsymbol\theta)$ is the parameter of the Bernoulli distribution. In other words, conditionally on $\bv{X}$:
\begin{equation}
y_i = \left\{
\begin{array}{cl}
1 & \mbox{ with probability } g(\bv{x}_i;\boldsymbol\theta)\\
0 & \mbox{ with probability } 1-g(\bv{x}_i;\boldsymbol\theta),
\end{array}
\right.(\#eq:genericBinary)
\end{equation}
where $\boldsymbol\theta$ is a vector of parameters to be estimated.

An estimation strategy is to assume that $g(\bv{x}_i;\boldsymbol\theta)$ can be proxied by $\tilde{\boldsymbol\theta}'\bv{x}_i$ and to run a linear regression to estimate $\tilde{\boldsymbol\theta}$ (a situation called **Linear Probability Model, LPM**):
$$
y_i = \tilde{\boldsymbol\theta}'\bv{x}_i + \varepsilon_i.
$$
Notwithstanding the fact that this specification does not exclude negative probabilities or probabilities greater than one, it could be compatible with the *assumption of zero conditional mean* (Hypothesis \@ref(hyp:exogeneity)) and with the *assumption of non-correlated residuals* (Hypothesis \@ref(hyp:noncorrelResid)), but more difficultly with the *homoskedasticity assumption* (Hypothesis \@ref(hyp:homoskedasticity)). Moreover, the $\varepsilon_i$'s cannot be Gaussian (because $y_i \in \{0,1\}$). Hence, using a linear regression to study the relationship between $\bv{x}_i$ and $y_i$ can be consistent but it is inefficient.



Figure \@ref(fig:LPM) illustrates the fit resulting from an application of the LPM model to binary (dependent) variables.

```{r LPM, echo=FALSE, fig.cap="Fitting a binary variable with a linear model (Linear Probability Model, LPM). The model is $\\mathbb{P}(y_i=1|x_i)=\\Phi(0.5+2x_i)$, where $\\Phi$ is the c.d.f. of the normal distribution and where $x_i \\sim \\,i.i.d.\\,\\mathcal{N}(0,1)$.", fig.asp = .6, out.width = "95%", fig.align = 'left-aligned'}
beta0 <- 0.5;beta1 <- 2
nb.sim <- 100
xx <- rnorm(nb.sim)
P.vec <- pnorm(beta0 + beta1*xx)
U <- runif(nb.sim)
yy <- (U<P.vec)
eqY <- lm(yy~xx)
par(mfrow=c(1,1));par(plt=c(.1,.95,.25,.95))
plot(xx,yy,pch=19,xlab="x",ylab="y",xlim=c(-5,3))
abline(eqY,col="blue",lwd=2)
legend("topleft",
       c("Observations","Linear regression line (LPM)"),
       lty=c(NaN,1), # gives the legend appropriate symbols (lines)       
       lwd=c(2,2), # line width
       col=c("black","blue"),
       pch=c(19,NaN),seg.len=2)
```

Except for its last row (LPM case), Table \@ref(tab:foo) provides examples of functions $g$ valued in $[0,1]$, and that can therefore used in models of the type: $\mathbb{P}(y_i=1|\bv{x}_i;\boldsymbol\theta) = g(\boldsymbol\theta'\bv{x}_i)$ (see Eq. \@ref(eq:genericBinary)). The "linear" case is given for comparison, but note that it does not satisfy $g(\boldsymbol\theta'\bv{x}_i) \in [0,1]$ for any value of $\boldsymbol\theta'\bv{x}_i$.


Table: (\#tab:foo) This table provides examples of function $g$, s.t. $\mathbb{P}(y_i=1|\bv{x}_i;\boldsymbol	heta) = g(\boldsymbol\theta'\bv{x}_i)$. The LPM case (last row) is given for comparison but, again, it does not satisfy $g(\boldsymbol\theta'\bv{x}_i) \in [0,1]$ for any value of $\boldsymbol\theta'\bv{x}_i$.

| Model   | Function $g$ | Derivative   |
|---------|---------------|------------------|
| Probit  | $\Phi$  | $\phi$      |
| Logit   | $\dfrac{\exp(x)}{1+\exp(x)}$    | $\dfrac{\exp(x)}{(1+\exp(x))^2}$ |
| log-log | $1 - \exp(-\exp(x))$ | $\exp(-\exp(x))\exp(x)$     |
| linear (LPM)  | $x$  | 1  |


Figure \@ref(fig:ProbLogit) displays the first three $g$ functions appearing in Table \@ref(tab:foo).

```{r ProbLogit, echo=FALSE, fig.cap="Probit, Logit, and Log-log functions.", fig.asp = .6, out.width = "95%", fig.align = 'left-aligned'}
x <- seq(-5,5,by=.01)
par(plt=c(.1,.95,.25,.95))
plot(x,pnorm(x),type="l",lwd=2,col="black",xlab="x",ylab="")
lines(x,1/(1+exp(-x)),col="blue",lwd=2)
lines(x,1- exp(-exp(x)),col="red",lwd=2)
legend("topleft",
       c("Probit","Logit","Log-log"),
       lty=c(1,1,1), # gives the legend appropriate symbols (lines)       
       lwd=c(2,2,2), # line width
       col=c("black","blue","red"),
       seg.len=2)
```


The **probit** and the **logit** models are popular binary-choice models. In the probit model, we have:
\begin{equation}
g(z) = \Phi(z),(\#eq:probit)
\end{equation}
where $\Phi$ is the c.d.f. of the normal distribution. And for the logit model:
\begin{equation}
g(z) = \frac{1}{1+\exp(-z)}.(\#eq:logit)
\end{equation}

Figure \@ref(fig:LPM2) shows the conditional probabilities associated with the (probit) model that had been used to generate the data of Figure \@ref(fig:LPM).

```{r LPM2, echo=FALSE, fig.cap="The model is $\\mathbb{P}(y_i=1|x_i)=\\Phi(0.5+2x_i)$, where $\\Phi$ is the c.d.f. of the normal distribution and where $x_i \\sim \\,i.i.d.\\,\\mathcal{N}(0,1)$. Crosses give the model-implied probabilities of having $y_i=1$ (conditional on $x_i$).", fig.asp = .6, out.width = "95%", fig.align = 'left-aligned'}
P.vec <- pnorm(beta0 + beta1*xx)
par(plt=c(.1,.95,.25,.95))
plot(xx,yy,pch=19,xlab="x",ylab="y",xlim=c(-5,3))
abline(eqY,col="blue",lwd=2)
points(xx,P.vec,pch=3,col="red",lwd=2)
legend("topleft",
       c("Observations","fitted linear regressions",expression(paste(P,"(",y[i],"=",1,"|",x[i],")",sep=""))),
       lty=c(NaN,1,NaN), # gives the legend appropriate symbols (lines)       
       lwd=c(2,2,2), # line width
       col=c("black","blue","red"),
       pch=c(19,NaN,3),
       seg.len=2)
```


### Interpretation in terms of latent variable, and utility-based models {#latent}

The probit model has an interpretation in terms of latent variables, which, in turn, is often exploited  in structural models, called **Random Utility Models (RUM)**. In such structural models, it is assumed that the agents that have to take the decision do so by selecting the outcome that provides them with the larger utility (for agent $i$, two possible outcomes: $y_i=0$ or $y_i=1$). Part of this utility is observed by the econometrician ---it depends on the covariates $\bv{x}_i$--- and part of it is latent.

In the probit model, we have:
$$
\mathbb{P}(y_i=1|\bv{x}_i;\boldsymbol\theta) = \Phi(\boldsymbol\theta'\bv{x}_i) = \mathbb{P}(-\varepsilon_{i}<\boldsymbol\theta'\bv{x}_i),
$$
where $\varepsilon_{i} \sim \mathcal{N}(0,1)$. That is:
$$
\mathbb{P}(y_i=1|\bv{x}_i;\boldsymbol\theta) = \mathbb{P}(0< y_i^*),
$$
where $y_i^* = \boldsymbol\theta'\bv{x}_i + \varepsilon_i$, with $\varepsilon_{i} \sim \mathcal{N}(0,1)$. Variable $y_i^*$ can be interpreted as a (latent) variable that determines $y_i$ (since $y_i = \mathbb{I}_{\{y_i^*>0\}}$).

Figure \@ref(fig:Latent) illustrates this situation.

```{r Latent, echo=FALSE, fig.cap="Distribution of $y_i^*$ conditional on $\\bv{x}_i$.", fig.asp = .6, out.width = "95%", fig.align = 'left-aligned'}
par(plt=c(.12,.95,.25,.95))
mu <- 1
x <- seq(-4,4,by=.01)
plot(x,dnorm(x-mu),type="l",lwd=2,
     xlab=expression(paste("Possible values of ",y[i],"*",sep="")),
     ylab="")
abline(v=mu,lty=2,col="grey")
abline(v=0)
abline(h=0)
text(mu,.02,expression(paste(theta,"'",x_i,sep="")),pos=4)
x.0 <- seq(-4,0,by=.01)
polygon(c(x.0,x.0[length(x.0):1]),c(dnorm(x.0-mu),0*x.0),col="grey")
abline(v=0)
arrows(x0=-2, y0=.3, x1 = -1, y1 = .01, length = 0.05, angle = 20,
       code = 2)
text(-2,.3,expression(paste(P,"(",y[i],"=0|",x[i],";",theta,")",sep="")),pos=2)
arrows(x0=2.5, y0=.3, x1 = 1.5, y1 = .2, length = 0.05, angle = 20,
       code = 2)
text(2.5,.3,expression(paste(P,"(",y[i],"=1|",x[i],";",theta,")",sep="")),pos=4)
```


Assume that agent ($i$) chooses $y_i=1$ if the utility associated with this choice ($U_{i,1}$) is higher than the one associated with $y_i=0$ (that is $U_{i,0}$). Assume further that the utility of agent $i$, if she chooses outcome $j$ ($\in \{0,1\}$), is given by
$$
U_{i,j} = V_{i,j} + \varepsilon_{i,j},
$$
where $V_{i,j}$ is the deterministic component of the utility associated with choice and where $\varepsilon_{i,j}$ is a random (agent-specific) component. Moreover, posit $V_{i,j} = \boldsymbol\theta_j'\bv{x}_i$. We then have:
\begin{eqnarray}
\mathbb{P}(y_i = 1|\bv{x}_i;\boldsymbol\theta) &=& \mathbb{P}(\boldsymbol\theta_1'\bv{x}_i+\varepsilon_{i,1}>\boldsymbol\theta_0'\bv{x}_i+\varepsilon_{i,0}) \nonumber\\
&=& F(\boldsymbol\theta_1'\bv{x}_i-\boldsymbol\theta_0'\bv{x}_i) = F([\boldsymbol\theta_1-\boldsymbol\theta_0]'\bv{x}_i),(\#eq:utility)
\end{eqnarray}
where $F$ is the c.d.f. of $\varepsilon_{i,0}-\varepsilon_{i,1}$.

Note that only the difference $\boldsymbol\theta_1-\boldsymbol\theta_0$ is identifiable (as opposed to $\boldsymbol\theta_1$ *and* $\boldsymbol\theta_0$). Indeed, replacing $U$ with $aU$ ($a>0$) gives the same model. This *scaling* issue can be solved by fixing the variance of $\varepsilon_{i,0}-\varepsilon_{i,1}$.

:::{.example #migration name="Migration and income"}

The RUM approach has been used by @Nakosteen_Zimmer_1980 to study migration choices. Their model is based on the comparison of marginal costs and benefits associated with migration. The main ingredients of their approach are as follows:

* Wage that can be earned at the present location: $y_p^* = \boldsymbol\theta_p'\bv{x}_p + \varepsilon_p$.
* Migration cost: $C^*= \boldsymbol\theta_c'\bv{x}_c + \varepsilon_c$.
* Wage earned elsewhere: $y_m^* = \boldsymbol\theta_m'\bv{x}_m + \varepsilon_m$.

In this context, agents decision to migrate if $y_m^* >  y_p^* + C^*$, i.e. if
$$
y^* = y_m^* -  y_p^* - C^* =  \boldsymbol\theta'\bv{x} + \underbrace{\varepsilon}_{=\varepsilon_m - \varepsilon_c - \varepsilon_p}>0,
$$
where $\bv{x}$ is the union of the $\bv{x}_i$s, for $i \in \{p,m,c\}$.
:::

### Alternative-Varying Regressors {#Avregressors}

In some cases, regressors may depend on the considered alternative ($0$ or $1$). For instance:

* When modeling the decision to participate in the labour force (or not), the wage depends on the alternative. Typically, it is zero if the considered agent has decided not to work (and strictly positive otherwise).
* In the context of the choice of transportation mode, "time cost" depends on the considered transportation mode.

In terms of utility, we then have:
$$
V_{i,j} = {\theta^{(u)}_{j}}'\bv{u}_{i,j} + {\theta^{(v)}_{j}}'\bv{v}_{i},
$$
where the $\bv{u}_{i,j}$'s are regressors associated with agent $i$, but taking different values for the different choices ($j=0$ or $j=1$). In that case, Eq. \@ref(eq:utility) becomes:
\begin{equation}
\mathbb{P}(y_i = 1|\bv{x}_i;\boldsymbol\theta)  = F\left({\theta^{(u)}_{1}}'\bv{u}_{i,1}-{\theta^{(u)}_{0}}'\bv{u}_{i,0}+[\boldsymbol\theta_1^{(v)}-\boldsymbol\theta_0^{(v)}]'\bv{v}_i\right),(\#eq:utility2)
\end{equation}
and, if $\theta^{(u)}_{1}=\theta^{(u)}_{0}=\theta^{(u)}$ ---as is customary--- we get:
\begin{equation}
\mathbb{P}(y_i = 1|\bv{x}_i;\boldsymbol\theta)  = F\left({\theta^{(u)}_{1}}'(\bv{u}_{i,1}-\bv{u}_{i,0})+[\boldsymbol\theta_1^{(v)}-\boldsymbol\theta_0^{(v)}]'\bv{v}_i\right).(\#eq:utility3)
\end{equation}

:::{.example #FishingTable name="Fishing-mode dataset"}

The fishing-mode dataset used in @Cameron_Trivedi_2005 (Chapters 14 and 15) contains alternative-specific variables. Specifically, for each individual, the price and catch rate depend on the fishing model. In the table reported below, lines `price` and `catch` correspond to the prices and catch rates associated with the chosen alternative.

```{r fishing1, warning=FALSE, message=FALSE}
library(mlogit)
data("Fishing",package="mlogit")
stargazer::stargazer(Fishing,type="text")
```
:::

### Estimation

These models can be estimated by Maximum Likelihood approaches (see Section \@ref(secMLE)).

To simplify the exposition, we consider the $\bv{x}_i$ vectors of covariates to be deterministic. Moreover, we assume that the r.v. are independent across entities $i$. How to write the likelihood in that case? It is easily checked that:
$$
f(y_i|\bv{x}_i;\boldsymbol\theta) =   g(\boldsymbol\theta'\bv{x}_i)^{y_i}(1-g(\boldsymbol\theta'\bv{x}_i))^{1-y_i}.
$$

Therefore, if the observations $(\bv{x}_i,y_i)$ are independent across entities $i$, we obtain:
$$
\log \mathcal{L}(\boldsymbol\theta;\bv{y},\bv{X}) = \sum_{i=1}^{n}y_i \log[g(\boldsymbol\theta'\bv{x}_i)] + (1-y_i)\log[1-g(\boldsymbol\theta'\bv{x}_i)].
$$

The likelihood equation reads (FOC of the optimization program, see Def. \@ref(def:likFunction)):
$$
\dfrac{\partial \log \mathcal{L}(\boldsymbol\theta;\bv{y},\bv{X})}{\partial \boldsymbol\theta} = \bv{0},
$$
that is:
$$
\sum_{i=1}^{n} y_i \bv{x}_i\frac{g'(\boldsymbol\theta'\bv{x}_i)}{g(\boldsymbol\theta'\bv{x}_i)} - (1-y_i) \bv{x}_i \frac{g'(\boldsymbol\theta'\bv{x}_i)}{1-g(\boldsymbol\theta'\bv{x}_i)} = \bv{0}.
$$

This is a nonlinear (multivariate) equation that can be solved numerically. Under regularity conditions (Hypotheses \@ref(hyp:MLEregularity)), we approximately have (Prop. \@ref(prp:MLEproperties)):
$$
\boldsymbol\theta_{MLE} \sim \mathcal{N}(\boldsymbol\theta_0,\bv{I}(\boldsymbol\theta_0)^{-1}),
$$
where
$$
\bv{I}(\boldsymbol\theta_0) = - \mathbb{E}_0 \left( \frac{\partial^2 \log \mathcal{L}(\theta;\bv{y},\bv{X})}{\partial \boldsymbol\theta \partial \boldsymbol\theta'}\right) = n \mathcal{I}_Y(\boldsymbol\theta_0).
$$

For finite samples, we can e.g. approximate $\bv{I}(\boldsymbol\theta_0)^{-1}$ by Eq. \@ref(eq:III1):
$$
\bv{I}(\boldsymbol\theta_0)^{-1} \approx -\left(\frac{\partial^2 \log \mathcal{L}(\boldsymbol\theta_{MLE};\bv{y},\bv{X})}{\partial \boldsymbol\theta \partial \boldsymbol\theta'}\right)^{-1}.
$$


In the Probit case (see Table \@ref(tab:foo)), it can be shown that we have:
\begin{eqnarray*}
&&\frac{\partial^2 \log \mathcal{L}(\boldsymbol\theta;\bv{y},\bv{X})}{\partial \boldsymbol\theta \partial \boldsymbol\theta'} = - \sum_{i=1}^{n} g'(\boldsymbol\theta'\bv{x}_i) [\bv{x}_i \bv{x}_i'] \times \\
&&\left[y_i \frac{g'(\boldsymbol\theta'\bv{x}_i) + \boldsymbol\theta'\bv{x}_ig(\boldsymbol\theta'\bv{x}_i)}{g(\boldsymbol\theta'\bv{x}_i)^2} + (1-y_i) \frac{g'(\boldsymbol\theta'\bv{x}_i) - \boldsymbol\theta'\bv{x}_i (1 - g(\boldsymbol\theta'\bv{x}_i))}{(1-g(\boldsymbol\theta'\bv{x}_i))^2}\right].
\end{eqnarray*}

In the Logit case (see Table \@ref(tab:foo)), it can be shown that we have:
$$
\frac{\partial^2 \log \mathcal{L}(\boldsymbol\theta;\bv{y},\bv{X})}{\partial \boldsymbol\theta \partial \boldsymbol\theta'} = - \sum_{i=1}^{n} g'(\boldsymbol\theta'\bv{x}_i) \bv{x}_i\bv{x}_i',
$$
where $g'(x)=\dfrac{\exp(-x)}{(1 + \exp(-x))^2}$.

Remark that, since $g'(x)>0$, $-\partial^2 \log \mathcal{L}(\boldsymbol\theta;\bv{y},\bv{X})/\partial \boldsymbol\theta \partial \boldsymbol\theta'$ is positive definite.

### Marginal effects {#marginalFX}

How to measure marginal effects, i.e. the effect on the probability that $y_i=1$ of a marginal increase of $x_{i,k}$? This  object is given by:
$$
\frac{\partial \mathbb{P}(y_i=1|\bv{x}_i;\boldsymbol\theta)}{\partial x_{i,k}} = \underbrace{g'(\boldsymbol\theta'\bv{x}_i)}_{>0}\theta_k,
$$
which is of the same sign as $\theta_k$ if function $g$ is monotonously increasing.

For agent $i$, this marginal effect is consistently estimated by $g'(\boldsymbol\theta_{MLE}'\bv{x}_i)\theta_{MLE,k}$. It is important to see that the marginal effect depends on $\bv{x}_i$: respective increases by 1 unit of $x_{i,k}$ (entity $i$) and of $x_{j,k}$ (entity $j$) do not necessarily have the same effect on $\mathbb{P}(y_i=1|\bv{x}_i;\boldsymbol\theta)$ as on $\mathbb{P}(y_j=1|\bv{x}_j;\boldsymbol\theta)$. To address this issue, one can compute some measures of "average" marginal effect. There are two main solutions. For each explanatory variable $k$:

i. Denoting by $\hat{\bv{x}}$ the sample average of the $\bv{x}_i$s, compute $g'(\boldsymbol\theta_{MLE}'\hat{\bv{x}})\theta_{MLE,k}$.
ii. Compute the average (across $i$) of $g'(\boldsymbol\theta_{MLE}'\bv{x}_i)\theta_{MLE,k}$.


### Goodness of fit

There is no obvious version of "$R^2$" for binary-choice models. Existing measures are called **pseudo-$R^2$ measures**.

Denoting by $\log \mathcal{L}_0(\bv{y})$ the (maximum) log-likelihood that would be obtained for a model containing only a constant term (i.e. with $\bv{x}_i = 1$ for all $i$), the McFadden's pseudo-$R^2$ is given by:
$$
R^2_{MF} = 1 - \frac{\log \mathcal{L}(\boldsymbol\theta;\bv{y})}{\log \mathcal{L}_0(\bv{y})}.
$$
Intuitively, $R^2_{MF}=0$ if the explanatory variables do not convey any information on the outcome $y$. Indeed, in this case, the model is not better than the reference model, that simply captures the fraction of $y_i$'s that are equal to 1.

:::{.example #creditProbit name="Credit and defaults (Lending-club dataset)"}

This example makes use of the `credit` data of package `AEC`. The objective is to model the default probabilities of borrowers.

Let us first represent the relationship between the fraction of households that have defaulted on their loan and their annual income:

```{r Probitlending}
library(AEC)
credit$Default <- 0
credit$Default[credit$loan_status == "Charged Off"] <- 1
credit$Default[credit$loan_status ==
                 "Does not meet the credit policy. Status:Charged Off"] <- 1
credit$amt2income <- credit$loan_amnt/credit$annual_inc
plot(as.factor(credit$Default)~log(credit$annual_inc),
     ylevels=2:1,ylab="Default status",xlab="log(annual income)")
```

The previous figure suggests that the effect of annual income on the probability of default is non-monotonous. We will therefore include a quadratic term in one of our specification (namely `eq1` below).

We consider three specifications. The first one (`eq0`), with no explanatory variables, is trivial. It will just be used to compute the pseudo-$R^2$. In the second (`eq1`), we consider a few covariates (loan amount, the ratio between the amount and annual income, The number of more-than-30 days past-due incidences of delinquency in the borrower's credit file for the past 2 years, and a quadratic function of annual income). In the third model (`eq2`), we add a credit rating.

```{r Probitlending2,warning=FALSE}
eq0 <- glm(Default ~ 1,data=credit,family=binomial(link="probit"))
eq1 <- glm(Default ~ log(loan_amnt) + amt2income + delinq_2yrs + 
             log(annual_inc)+ I(log(annual_inc)^2),
           data=credit,family=binomial(link="probit"))
eq2 <- glm(Default ~ grade + log(loan_amnt) + amt2income + delinq_2yrs + 
             log(annual_inc)+ I(log(annual_inc)^2),
           data=credit,family=binomial(link="probit"))
stargazer::stargazer(eq0,eq1,eq2,type="text",no.space = TRUE)
```

Let us compute the pseudo R2 for the last two models:

```{r Probitlending2bis}
logL0 <- logLik(eq0);logL1 <- logLik(eq1);logL2 <- logLik(eq2)
pseudoR2_eq1 <- 1 - logL1/logL0 # pseudo R2
pseudoR2_eq2 <- 1 - logL2/logL0 # pseudo R2
c(pseudoR2_eq1,pseudoR2_eq2)
```

Let us now compute the (average) marginal effects, using method ii of Section \@ref(marginalFX):

```{r Probitlending3}
mean(dnorm(predict(eq2)),na.rm=TRUE)*eq2$coefficients
```

There is an issue for the `annual_inc` variable. Indeed, the previous computation does not realize that this variable appears twice among the explanatory variables (through `log(annual_inc)` and `I(log(annual_inc)^2)`). To address this, one can proceed as follows: (1) we construct a new counterfactual dataset where annual incomes are increased by 1\%, (2) we use the model to compute model-implied probabilities of default on this new dataset and (3), we subtract the probabilities resulting from the original dataset from these counterfactual probabilities:

```{r Probitlending4}
new_credit <- credit
new_credit$annual_inc <- 1.01 * new_credit$annual_inc
bas_predict_eq2  <- predict(eq2, newdata = credit, type = "response")
# This is equivalent to pnorm(predict(eq2, newdata = credit))
new_predict_eq2  <- predict(eq2, newdata = new_credit, type = "response")
mean(new_predict_eq2 - bas_predict_eq2)
```
The negative sign means that, on average across the entities considered in the analysis, a 1\% increase in annual income results in a decrease in the default probability. This average effect is however pretty low. To get an economic sense of the size of this effect, let us compute the average effect associated with a unit increase in the number of delinquencies:

```{r Probitlending5}
new_credit <- credit
new_credit$delinq_2yrs <- credit$delinq_2yrs + 1
new_predict_eq2  <- predict(eq2, newdata = new_credit, type = "response")
mean(new_predict_eq2 - bas_predict_eq2)
```

We can employ a likelihood ratio test (see Def. \@ref(def:LR)) to see if the two variables associated with annual income are jointly statistically significant (in the context of `eq1`):
```{r Probitlending6}
eq1restr <- glm(Default ~ log(loan_amnt) + amt2income + delinq_2yrs,
                data=credit,family=binomial(link="probit"))
LRstat <- 2*(logL1 - logLik(eq1restr))
pvalue <- 1 - c(pchisq(LRstat,df=2))
```

The computation gives a p-value of `r round(pvalue,4)`.

:::



:::{.example #Fisch142 name="Replicating Table 14.2 of Cameron and Trivedi (2005)"}

The following lines of codes replicate Table 14.2 of @Cameron_Trivedi_2005 (see Example \@ref(exm:FishingTable)).

```{r fishing2, warning=FALSE}
data.reduced <- subset(Fishing,mode %in% c("charter","pier"))
data.reduced$lnrelp <- log(data.reduced$price.charter/data.reduced$price.pier)
data.reduced$y <- 1*(data.reduced$mode=="charter")
# check first line of Table 14.1:
price.charter.y0 <- mean(data.reduced$pcharter[data.reduced$y==0])
price.charter.y1 <- mean(data.reduced$pcharter[data.reduced$y==1])
price.charter    <- mean(data.reduced$pcharter)
# Run probit regression:
reg.probit <- glm(y ~ lnrelp,
                  data=data.reduced,
                  family=binomial(link="probit"))
# Run Logit regression:
reg.logit <- glm(y ~ lnrelp,
                 data=data.reduced,
                 family=binomial(link="logit"))
# Run OLS regression:
reg.OLS <- lm(y ~ lnrelp,
              data=data.reduced)
# Replicates Table 14.2 of Cameron and Trivedi:
stargazer::stargazer(reg.logit, reg.probit, reg.OLS,no.space = TRUE,
                     type="text")
```
:::


### Predictions and ROC curves

How to compute model-implied predicted outcomes? As is the case for $y_i$, predicted outcomes $\hat{y}_i$ need to be valued in $\{0,1\}$. A natural choice consists in considering that $\hat{y}_i=1$ if $\mathbb{P}(y_i=1|\bv{x}_i;\boldsymbol\theta) > 0.5$, i.e., in taking a cutoff of $c=0.5$. There exist, though, situations where doing so is not relevant. For instance, we may have some models where all predicted probabilities are small, but some less than others. In this context, a model-implied probability of 10\% (say) could characterize a "high-risk" entity. However, using a cutoff of 50\% would not identify this level of riskiness.

The **receiver operating characteristics (ROC)** curve consitutes a more general approach. The idea is to remain agnostic and to consider all possible values of the cutoff $c$. It works as follows. For each potential cutoff $c \in [0,1]$, compute (and plot):

*	The fraction of $y = 1$ values correctly classified (*True Positive Rate*) against
* The fraction of $y = 0$ values incorrectly specified (*False Positive Rate*).

Such a curve mechanically starts at (0,0) ---which corresponds to $c=1$--- and terminates at (1,1) --situation when $c=0$.

In the case of no predictive ability (worst situation), the ROC curve is a straight line between (0,0) and (1,1).

:::{.example #FishingROC name="ROC with the fishing-mode dataset"}
Figure \@ref(fig:fishing3) shows the ROC curve associated with the probit model estimated in Example \@ref(exm:Fisch142).

```{r fishing3, message=FALSE, warning=FALSE,results='hide',fig.keep='all', fig.cap="Application of the ROC methodology on the fishing-mode dataset.", fig.asp = .6, out.width = "95%", fig.align = 'left-aligned'}
library(pROC)
predict_model <- predict.glm(reg.probit,type = "response")
roc(data.reduced$y, predict_model, percent=T,
    boot.n=1000, ci.alpha=0.9, stratified=T, plot=TRUE, grid=TRUE,
    show.thres=TRUE, legacy.axes = TRUE, reuse.auc = TRUE,
    print.auc = TRUE, print.thres.col = "blue", ci=TRUE,
    ci.type="bars", print.thres.cex = 0.7, col = 'red',
    main = paste("ROC curve using","(N = ",nrow(data.reduced),")") )
```
:::

## Multiple Choice Models

We will now consider cases where the number of possible outcomes (or alternatives) is larger than two. Let us denote by $J$ this number. We have $y_j \in \{1,\dots,J\}$. This situation arise for instance when the outcome variable reflects:

* Opinions: strongly opposed / opposed / neutral / support (ranked choices),
* Occupational field: lawyer / farmer / engineer / doctor / ...,
* Alternative shopping areas,
* Transportation types.

In a few cases, the values associated with the choices will themselves be meaningful, for example, number of accidents per day: $y = 0, 1,2, \dots$ (count data). In most cases, the values are meaningless.

We assume the existence of covariates, gathered in vector $\bv{x}_i$ ($K \times 1$), that are suspected to influence for the probabilities of obtaining the different outcomes ($y_i=j$, $j \in \{1,\dots,J\}$).

In what follows, we will assume that the $y_i$'s are assumed to be independently distributed, with: 
\begin{equation}
y_i = \left\{
\begin{array}{cl}
1 & \mbox{ with probability } g_1(\bv{x}_i;\boldsymbol\theta)\\
\vdots \\
J & \mbox{ with probability } g_J(\bv{x}_i;\boldsymbol\theta).
\end{array}
\right.(\#eq:generalMultiNom)
\end{equation}

(Of course, for all entities ($i$), we must have $\sum_{j=1}^J g_j(\bv{x}_i;\boldsymbol\theta)=1$.) Our objective is to estimate the vector of population parameters $\boldsymbol\theta$ given functional forms for the $g_j$'s.


### Ordered case

Sometimes, there exists a natural order for the different alternatives. This is typically the case where respondents have to choose a level of agreement to a statement, e.g.: (1) Strongly disagree; (2) Disagree; (3) Neither agree nor disagree; (4) Agree; (5) Strongly agree. Another standard case is that of ratings (from A to F, say).

The ordered probit model consists in extending the binary case, considering the latent-variable view of the latter (see Section \@ref(latent)). Formally, the model is as follows:
\begin{equation}
\mathbb{P}(y_i = j | \bv{x}_i) = \mathbb{P}(\alpha_{j-1} <y^*_i < \alpha_{j} |\bv{x}_i), (\#eq:Pordered)
\end{equation}
where
$$
y_{i}^* = \boldsymbol\theta'\bv{x}_i + \varepsilon_i,
$$
with $\varepsilon_i \sim \,i.i.d.\,\mathcal{N}(0,1)$. The $\alpha_j$'s, $j \in \{1,\dots,J-1\}$, are  (new) parameters that have to be estimated, on top of $\boldsymbol\theta$. Naturally, we have $\alpha_1<\alpha_2<\dots<\alpha_{J-1}$. Moreover $\alpha_0$ is $- \infty$ and $\alpha_J$ is $+ \infty$, so that Eq. \@ref(eq:Pordered) is valid for any $j \in \{1,\dots,J\}$ (including $1$ and $J$).

We have:
\begin{eqnarray*}
g_j(\bv{x}_i;\boldsymbol\theta,\boldsymbol\alpha) = \mathbb{P}(y_i = j | \bv{x}_i) &=& \mathbb{P}(\alpha_{j-1} <y^*_i < \alpha_{j} |\bv{x}_i) \\
&=& \mathbb{P}(\alpha_{j-1} - \boldsymbol\theta'\bv{x}_i  <\varepsilon_i < \alpha_{j} - \boldsymbol\theta'\bv{x}_i) \\
&=& \Phi(\alpha_{j} - \boldsymbol\theta'\bv{x}_i) - \Phi(\alpha_{j-1} - \boldsymbol\theta'\bv{x}_i),
\end{eqnarray*}
where $\Phi$ is the c.d.f. of $\mathcal{N}(0,1)$.


If, for all $i$, one of the components of $\bv{x}_i$ is equal to 1 (which is what is done in linear regression to introduce an intercept in the specification), then one of the $\alpha_j$ ($j\in\{1,\dots,J-1\}$) is not identified. One can then arbitrarily set $\alpha_1=0$. This is what is done in the binary logit/probit cases.

This model can be estimated by maximizing the likelihood function (see Section \@ref(secMLE)). This function is given by:
\begin{equation}
\log \mathcal{L}(\boldsymbol\theta,\boldsymbol\alpha;\bv{y},\bv{X}) = \sum_{i=1}^n  \sum_{j=1}^J \mathbb{I}_{\{y_i=j\}} \log \left(g_j(\bv{x}_i;\boldsymbol\theta,\boldsymbol\alpha)\right). (\#eq:multipleLogLik)
\end{equation}

Let us stress that we have two types of parameters to estimate: those included in vector $\boldsymbol\theta$, and the $\alpha_j$'s, gathered in vector $\boldsymbol\alpha$.

The estimated values of the $\theta_j$'s are slightly more complicated to interpret (at least in term of sign) than in the binary case. Indeed, we have:
$$
\mathbb{P}(y_i \le j | \bv{x}_i) = \Phi(\alpha_{j} - \boldsymbol\theta'\bv{x}_i) \Rightarrow \frac{\partial \mathbb{P}(y_i \le j | \bv{x}_i)}{\bv{x}_i} =- \underbrace{\Phi'(\alpha_{j} - \boldsymbol\theta'\bv{x}_i)}_{>0}\boldsymbol\theta.
$$
Hence the sign of $\theta_k$ indicates whether $\mathbb{P}(y_i \le j | \bv{x}_i)$ increases or decreases w.r.t. $x_{i,k}$ (the $k^{th}$ component of $\bv{x}_i$). By contrast:
$$
\frac{\partial \mathbb{P}(y_i = j | \bv{x}_i)}{\bv{x}_i} = \underbrace{\left(-F'(\alpha_{j} + \boldsymbol\theta'\bv{x}_i)+F'(\alpha_{j-1} + \boldsymbol\theta'\bv{x}_i)\right)}_{A}\boldsymbol\theta.
$$
Therefore the signs of the components of $\boldsymbol\theta$ are not necessarily those of the marginal effects. (For the sign of $A$ is a priori unknown.)


:::{.example #orderedCredit name="Predicting credit ratings (Lending-club dataset)"}

Let us use credit dataset again (see Example \@ref(exm:creditProbit)), and let use try and model the ratings attributed by the lending-club:

```{r ordered1, warning=FALSE, message=FALSE}
library(AEC)
library(MASS)
credit$emp_length_low5y   <- credit$emp_length %in%
  c("< 1 year","1 year","2 years","3 years","4 years")
credit$emp_length_high10y <- credit$emp_length=="10+ years"
credit$annual_inc <- credit$annual_inc/1000
credit$loan_amnt  <- credit$loan_amnt/1000
credit$income2loan <- credit$annual_inc/credit$loan_amnt
training <- credit[1:20000,] # sample is reduced
training <- subset(training,grade!=c("E","F","G"))
training <- droplevels(training)
training$grade.ordered <- factor(training$grade,ordered=TRUE,
                                 levels = c("D","C","B","A"))
model1 <- polr(grade.ordered ~ log(loan_amnt) + log(income2loan) + delinq_2yrs,
               data=training, Hess=TRUE, method="probit")
model2 <- polr(grade.ordered ~ log(loan_amnt) + log(income2loan) + delinq_2yrs +
                 emp_length_low5y + emp_length_high10y,
               data=training, Hess=TRUE, method="probit")
stargazer::stargazer(model1,model2,ord.intercepts = TRUE,type="text",
                     no.space = TRUE)
```

Predicted ratings (and probabilties of being given a given rating) can be computed as follows:

```{r ordered1bis, warning=FALSE, message=FALSE}
pred.grade <- predict(model1,newdata = training)
# pred.grade = predicted grade, defined as the most likely according model
pred.proba <- predict(model1,newdata = training, type="probs")
```
:::

<!-- \begin{exerc}[Political information] -->
<!-- Using the data available \href{https://vincentarelbundock.github.io/Rdatasets/csv/pscl/politicalInformation.csv}{here} and documented \href{https://vincentarelbundock.github.io/Rdatasets/doc/pscl/politicalInformation.html}{here}, propose a model for the political information level. -->
<!-- \end{exerc} -->

<!-- \vspace{.5cm} -->

<!-- \begin{exerc}[Wine quality] -->
<!-- Using the data available \href{https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv}{here} and documented \href{https://archive.ics.uci.edu/ml/datasets/Wine+Quality}{here}, propose a model for wine quality. -->
<!-- \end{exerc} -->


### General multinomial logit model {#MNL}

This section introduces the general multinomial logit model, which is the natural extension of the binary logit model (see Table \@ref(tab:foo)). Its general formulation is as follows:
\begin{equation}
g_j(\bv{x}_i;\boldsymbol\theta) = \frac{\exp(\theta_j'\bv{x}_i)}{\sum_{k=1}^J \exp(\theta_k'\bv{x}_i)}.(\#eq:GeneralMNL)
\end{equation}

Note that, by construction, $g_j(\bv{x}_i;\boldsymbol\theta) \in [0,1]$ and $\sum_{j}g_j(\bv{x}_i;\boldsymbol\theta)=1$.

The components of $\bv{x}_i$ (regressors, or covariates) may be *alternative-specific* or *alternative invariant* (see also Section  \@ref(Avregressors)). We may, e.g., organize $\bv{x}_i$ as follows:
\begin{equation}
\bv{x}_i = [\bv{u}_{i,1}',\dots,\bv{u}_{i,J}',\bv{v}_{i}']',(\#eq:xorganiz)
\end{equation}
where the notations are as in Section \@ref(Avregressors), that is:

* $\bv{u}_{i,j}$ ($j \in \{1,\dots,J\}$): vector of variables associated with agent $i$ and alternative $j$ (alternative-specific regressors). Examples: Travel time per type of transportation (transportation choice), wage per type of work, cost per type of car.
* $\bv{v}_{i}$: vector of variables associated with agent $i$ but alternative-invariant. Examples: age or gender of agent $i$,

When $\bv{x}_i$ is as in Eq. \@ref(eq:xorganiz), with obvious notations, $\theta_j$ is of the form:
\begin{equation}
\theta_j = [{\theta^{(u)}_{1,j}}',\dots,{\theta^{(u)}_{J,j}}',{\theta_j^{(v)}}']',(\#eq:thetaOrganiz)
\end{equation}
and $\boldsymbol\theta=[\theta_1',\dots,\theta_J']'$.

The literature has considered different specific cases of the general multinomial logit model:[^FootnoteAdHocNames]

[^FootnoteAdHocNames]: The labelling "CL" and "MNL" ---used in the literature--- are relatively *ad hoc* (see 15.4.1 in @Cameron_Trivedi_2005).

* **Conditional logit (CL)** with alternative-varying regressors:
\begin{equation}
\theta_j = [\bv{0}',\dots,\bv{0}',\underbrace{\boldsymbol\beta'}_{\mbox{j$^{th}$ position}},\bv{0}',\dots]',(\#eq:thetaOrganizCL)
\end{equation}
i.e., we have $\boldsymbol\beta=\theta^{(u)}_{1,1}=\dots=\theta^{(u)}_{J,J}$ and $\theta^{(u)}_{i,j}=\bv{0}$ for $i \ne j$.
* **Multinomial logit (MNL)** with alternative-invariant regressors:
\begin{equation}
\theta_j = \left[\bv{0}',\dots,\bv{0}',{\theta_j^{(v)}}'\right]'.(\#eq:thetaOrganizML)
\end{equation}
* **Mixed logit:**
\begin{equation}
\theta_j = \left[\bv{0}',\dots,\bv{0}',\boldsymbol\beta',\bv{0}',\dots,\bv{0}',{\theta_j^{(v)}}'\right]'.(\#eq:thetaOrganizCL)
\end{equation}

:::{.example #FishingGeneralLogit name="CL and MNL with the fishing-mode dataset"}

The following lines replicate Table 15.2 in @Cameron_Trivedi_2005 (see also Examples \@ref(exm:FishingTable) and \@ref(exm:Fisch142)):

```{r fishingMulti,message=FALSE,warning=FALSE}
# Specify data organization:
library(mlogit)
library(stargazer)
data("Fishing",package="mlogit")
Fish <- mlogit.data(Fishing,
                    varying = c(2:9),
                    choice = "mode",
                    shape = "wide")
MNL1 <- mlogit(mode ~ price + catch, data = Fish)
MNL2 <- mlogit(mode ~ price + catch - 1, data = Fish)
MNL3 <- mlogit(mode ~ 0 | income, data = Fish)
MNL4 <- mlogit(mode ~ price + catch | income, data = Fish)
stargazer(MNL1,MNL2,MNL3,MNL4,type="text",no.space = TRUE,
          omit.stat = c("lr"))
```
:::


**ML estimation**

General multinomial logit models can be estimated by Maximum Likelihood techniques (see Section \@ref(secMLE)). Consider the general model described in Eq. \@ref(eq:generalMultiNom). It can be noted that:
$$
f(y_i|\bv{x}_i;\boldsymbol\theta) = \prod_{j=1}^J g_j(\bv{x}_i;\boldsymbol\theta)^{\mathbb{I}_{\{y_i=j\}}},
$$
which leads to
$$
\log f(y_i|\bv{x}_i;\boldsymbol\theta) = \sum_{j=1}^J \mathbb{I}_{\{y_i=j\}} \log \left(g_j(\bv{x}_i;\boldsymbol\theta)\right).
$$
The log-likelihood function is therefore given by:
\begin{equation}
\log \mathcal{L}(\boldsymbol\theta;\bv{y},\bv{X}) = \sum_{i=1}^n  \sum_{j=1}^J \mathbb{I}_{\{y_i=j\}} \log \left(g_j(\bv{x}_i;\boldsymbol\theta)\right).(\#eq:multipleLogLik)
\end{equation}
Numerical methods have to be employed in order to find the maximum-likelihood estimate of $\boldsymbol\theta$. (Standard packages contain fast algorithms.)


**Marginal Effects**

Let us consider the computation of marginal effects in the general multinomial logit model (Eq. \@ref(eq:GeneralMNL)). Using the notation $p_{i,j} \equiv \mathbb{P}(y_i=j|\bv{x}_i;\boldsymbol\theta)$, we have:
\begin{eqnarray*}
\frac{\partial p_{i,j}}{\partial x_{i,s}} &=& \frac{\theta_{j,s}\exp(\theta_j'\bv{x}_i)\sum_{k=1}^J \exp(\theta_k'\bv{x}_i)}{(\sum_{k=1}^J \exp(\theta_k'\bv{x}_i))^2} \\
&& - \frac{\exp(\theta_j'\bv{x}_i)\sum_{k=1}^J \theta_{k,s} \exp(\theta_k'\bv{x}_i)}{(\sum_{k=1}^J \exp(\theta_k'\bv{x}_i))^2}\\
&=& \theta_{j,s} p_{i,j} - \sum_{k=1}^J \theta_{k,s} p_{i,j}p_{i,k}\\
&=&  p_{i,j} \times \Big(\theta_{j,s} - \underbrace{\sum_{k=1}^J \theta_{k,s} p_{i,k}}_{=\overline{\boldsymbol{\theta}}^{(i)}_{s}}\Big),
\end{eqnarray*}
where $\overline{\boldsymbol\theta}^{(i)}_{s}$ does not depend on $j$. Note that the sign of the marginal effect is not necessarily that of $\theta_{j,s}$.


**Random Utility models**

The general multinomial logit model may arise as the natural specification arising in structural contexts where agents compare (random) utilities associated with $J$ potential outcomes (see Section \@ref(latent) for the binary situation).

Let's drop the $i$ subscript for simplicity and assume that the utility derived form choosing $j$ is given by $U_j = V_j + \varepsilon_j$, where $V_j$ is deterministic (may depend on observed covariates) and $\varepsilon_j$ is stochastic. We have (with obvious notations):
\begin{eqnarray*}
\mathbb{P}(y=j) &=& \mathbb{P}(U_j>U_k,\,\forall k \ne j)\\
\mathbb{P}(y=j) &=& \mathbb{P}(U_k-U_j<0,\,\forall k \ne j)\\
\mathbb{P}(y=j) &=& \mathbb{P}(\underbrace{\varepsilon_k-\varepsilon_j}_{=:\tilde\varepsilon_{k,j}}<\underbrace{V_j - V_k}_{=:-\tilde{V}_{k,j}},\,\forall k \ne j).
\end{eqnarray*}

The last expression is an $(J-1)$-variate integral. While it has, in general, no analytical solution, Prop. \@ref(prp:Weibull) shows that it is the case when employing Gumbel distributions (see Def. \@ref(def:Gumbel)).

:::{.definition #Gumbel name="Gumbel distribution"}
The c.d.f. of the Gumbel distribution  ($\mathcal{W}$) is:
$$
F(u) = \exp(-\exp(-u)), \qquad f(u)=\exp(-u-\exp(u)).
$$
:::

Remark: if $X\sim\mathcal{W}$, then $\mathbb{E}(X)=0.577$ (Euler constant)[^FootnoteEuler] and $\mathbb{V}ar(X)=\pi^2/6$.

[^FootnoteEuler]: The Euler constant $\gamma$ satisfies $\gamma = \lim_{n\rightarrow \infty} \left(- \ln(n) + \sum_{k=1}^n \frac{1}{k}\right)$.

```{r Gumbel, echo=FALSE, fig.cap="C.d.f. of the Gumbel distribution ($F(x)=\\exp(-\\exp(-x))$).", fig.asp = .6, out.width = "95%", fig.align = 'left-aligned'}
x <- seq(-6,6,by=.01)
y <- exp(-exp(-x))
par(plt=c(.15,.95,.25,.95))
plot(x,y,type="l",lwd=2, xlab="",ylab="")
abline(v=0,col="grey")
```


:::{.proposition #Weibull name="Weibull"}
In the context of the utility model described above, if $\varepsilon_j \sim \,i.i.d.\,\mathcal{W}$, then
$$
\mathbb{P}(y=j) = \frac{\exp(V_j)}{\sum_{k=1}^J \exp(V_k)}.
$$
:::
:::{.proof}
We have:
\begin{eqnarray*}
\mathbb{P}(y=j) &=& \mathbb{P}(\forall\,k \ne j,\,U_k < U_j) =  \mathbb{P}(\forall\,k \ne j,\,\varepsilon_k < V_j - V_k + \varepsilon_j) \\
&=& \int \prod_{k \ne j} F(V_j - V_k + \varepsilon) f(\varepsilon)d\varepsilon.
\end{eqnarray*}
After computation, it comes that
$$
\prod_{k \ne j} F(V_j - V_k + \varepsilon) f(\varepsilon) = \exp\left[-\varepsilon-\exp(-\varepsilon+\lambda_j)\right],
$$
where $\lambda_j = \log\left(1 + \frac{\sum_{k \ne j} \exp(V_k)}{\exp(V_j)}\right)$. We then have:
\begin{eqnarray*}
\mathbb{P}(y=j) &=& \int  \exp\left[-\varepsilon-\exp(-\varepsilon+\lambda_j)\right] d\varepsilon\\
&=& \int  \exp\left[-t - \lambda_j-\exp(-t)\right] d\varepsilon = \exp(- \lambda_j),
\end{eqnarray*}
which leads to the result.
:::


Some remarks on identification (see Def. \@ref(def:identif)) are in order.

1. We have:
\begin{eqnarray*}
\mathbb{P}(y=j) &=& \frac{\exp(V_j)}{\sum_{k=1}^J \exp(V_k)}= \frac{\exp(V^*_j)}{1 + \sum_{k=2}^J \exp(V^*_k)},
\end{eqnarray*}
where $V^*_j = V_j - V_1$. We can therefore always assume that $V_{1}=0$. In the case where $V_{i,j} = \theta_j'\bv{x}_i = \boldsymbol\beta'\bv{u}_{i,j}+{\theta_j^{(v)}}'\bv{v}_i$ (see Eqs. \@ref(eq:xorganiz) and \@ref(eq:thetaOrganizCL)), we can for instance assume that:
\begin{eqnarray*}
&(A)& \bv{u}_{i,1}=0,\\
&(B)& \theta_1^{(v)} = 0.
\end{eqnarray*}
If (A) does not hold, we can replace $\bv{u}_{i,j}$ with $\bv{u}_{i,j}-\bv{u}_{i,1}$.
2. If $J=2$ and $j \in \{0,1\}$ (shift by one unit), we have $\mathbb{P}(y=1|\bv{x})=\dfrac{\exp(\boldsymbol\theta'\bv{x})}{1+\exp(\boldsymbol\theta'\bv{x})}$, this is the logit model (Table \@ref(tab:foo)).

**Limitations of logit models**

In a Logit model, we have:
\begin{equation}
\mathbb{P}(y=j|y \in \{k,j\}) = \frac{\exp(\theta_j'\bv{x})}{\exp(\theta_j'\bv{x}) + \exp(\theta_k'\bv{x})}.(\#eq:condiProba)
\end{equation}
This conditional probability does not depend on other alternatives (i.e., it does not depend on $\theta_m$, $m \ne j,k$). In particular, if $\bv{x} = [\bv{u}_1',\dots,\bv{u}_J',\bv{v}']'$, then changes in $\bv{u}_m$ ($m \ne j,\,k$) have no impact on the object shown in Eq. \@ref(eq:condiProba).

That is, a Multinomial Logit can be seen as a series of pairwise comparisons that are unaffected by the characteristics of alternatives. Such a model is said to satisfy the **independence from irrelevant alternatives (IIA)** property. That is, in these models, for any individual, the ratio of probabilities of choosing two alternatives is independent of the availability or attributes of any other alternatives. While this may not sound alarming, there are situations where you would like it not to be the case, this is for instance the case when you want to extrapolate the results of your estimated model to a situation where there is a novel outcome that is highly susbstitutable to one of the previous ones. This can be illustrated with the famous "red-blue bus" example:


:::{.example #redbluebus name="Red-blue bus and IIA"}
Assume one has a logit model capturing the decision to travel using either a car ($y=1$) or a (red) bus ($y=2$). Assume you want to augment this model to allow for a third choice ($y=3$): travel with a blue bus. If a blue bus ($y=3$) is exactly as a red bus, except for the color, then one would expect to have:
$$
\mathbb{P}(y=3|y \in \{2,3\}) = 0.5,
$$
i.e. $\theta_2 = \theta_3$.

Assume we had $V_1=V_2$. We expect to have $V_2=V_3$ (hence $p_2=p_3$). A multinomial logit model would then imply $p_1=p_2=p_3=0.33$. It would however seem more reasonable to have $p_1 = p_2 + p_3 = 0.5$ and $p_2=p_3=0.25$.
:::


<!-- **Sequential models** -->


<!-- Naturally, other possible ad-hoc modelling strategy are possible, e.g. sequential modelling: -->
<!-- \begin{eqnarray*} -->
<!-- \mathbb{P}(y=1|\bv{x}) &=& F(\theta_1'\bv{x}) \\ -->
<!-- \mathbb{P}(y=2|\bv{x}) &=& (1-F(\theta_1'\bv{x})) \times F(\theta_2'\bv{x}) \\ -->
<!-- \mathbb{P}(y=3|\bv{x}) &=& (1-F(\theta_1'\bv{x})) \times (1 - F(\theta_2'\bv{x})). -->
<!-- \end{eqnarray*} -->

<!-- Easily checked that $\sum_j \mathbb{P}(y=j|\bv{x})=1$. -->
<!-- Example: car purchase; 1st choice = brand, 2nd choice = color. -->

<!-- Likelihood function: -->
<!-- \begin{eqnarray*} -->
<!-- \mathcal{L}(\theta_1,\theta_2;\bv{y},\bv{X}) &=& \prod_{j \in \{1,2,3\}} \left( \prod_{i\,s.t.\,y_i=j} \mathbb{P}(y_i=j|\bv{x}_i)\right). -->
<!-- \end{eqnarray*} -->

<!-- The maximization of the log-likelihood can be performed sequentially (w.r.t. to $\theta_1$ first, and next w.r.t. $\theta_2$). -->

<!-- Weaknesses: results may depend on the chosen sequence + no clear mapping with utility framework. -->

<!-- Alternative: nested logit model (see below) where utilities of alternatives 2 and 3 are affected by correlated shocks. -->


### Nested logits

Nested Logits are natural extensions of logit models when choices feature a nesting structure. This approach is relevant when it makes sense to group some choices into the same *nest*, also called *limbs*. Intuitively, this framework is consistent with the idea according to which, for each agent, there exist unobserved nest-specific variables.

<!-- %	\item Choice of selecting $j_1 \in \{1,\dots,J_1\}$ and then $j_2 \in \{1,\dots,J_2\}$ is given by: -->
<!-- %	$$ -->
<!-- %	\mathbb{P}(y_1 = j_1,y_2=j_2) = \underbrace{\mathbb{P}(y_2=j_2|y_1 = j_1)}_{\mbox{2$^{nd}$ logit}}\underbrace{\mathbb{P}(y_1 = j_1)}_{\mbox{1$^{st}$ logit}}. -->
<!-- %	$$ -->

The setup is as follows: we consider $J$ *limbs*. For each limb $j$, we have $K_j$ *branches*. Let us denotes by $y_1$ the limb choice (i.e., $y_1 \in \{1,\dots,J\}$) and by $y_2$ the branch choice (with $y_2 \in \{1,\dots,K_j\}$). The utility associated with the pair of choices $(j,k)$ is given by
$$
U_{j,k} = V_{j,k} + \varepsilon_{j,k}.
$$
We have:
$$
\mathbb{P}[(y_1,y_2) = (j,k)|\bv{x}] = \mathbb{P}(U_{j,k}>U_{l,m},\,(l,m) \ne (j,k)|\bv{x}).
$$

One usually make the following two assumptions:

i. The deterministic part of the utility is given by $V_{j,k} = \bv{u}_j'\boldsymbol\alpha + \bv{v}_{j,k}'\boldsymbol\beta_j$, where $\boldsymbol\alpha$ is common to all nests and the $\boldsymbol\beta_j$'s are nest-specific.

ii. The disturbances $\boldsymbol\varepsilon$ follow the Generalized Extreme Value (GEV) distribution (see Def. \@ref(def:GEVdistri)).

The following figure displays simulations of pairs $(\varepsilon_1,\varepsilon_2)$ drawn from GEV distributions for different values of $\rho$. The simulation approach is based on [Bhat](http://www.caee.utexas.edu/prof/bhat/ABSTRACTS/Supp_material.pdf). The code used to produce this chart is provided in Appendix \@ref(App:GEV).

```{r GEV, fig.align = 'left-aligned', out.width = "95%", fig.cap = "GEV simulations.", echo=FALSE}
knitr::include_graphics("images/Figure_GEV.png")
```


Under (i) and (ii), we have:
\begin{eqnarray}
\mathbb{P}[(y_1,y_2) = (j,k)|\bv{x}] &=& \underbrace{\frac{\exp(\bv{u}_j'\boldsymbol\alpha + \rho_j I_j)}{\sum_{m=1}^J \exp(\bv{u}_m'\boldsymbol\alpha + \rho_m I_m)}}_{= \mathbb{P}[y_1 = j|\bv{x}]} \times \nonumber\\
&& \underbrace{\frac{\exp(\bv{v}_{j,k}'\boldsymbol\beta_j/\rho_j)}{\sum_{l=1}^{K_j} \exp(\bv{v}_{j,l}'\boldsymbol\beta_j/\rho_j)}}_{= \mathbb{P}[y_2 = k|y_1=j,\bv{x}]}, (\#eq:Nested)
\end{eqnarray}
where $I_j$'s are called inclusive values (or log sums), given by:
$$
I_j = \log \left( \sum_{l=1}^{K_j} \exp(\bv{v}_{j,l}'\boldsymbol\beta_j/\rho_j)\right).
$$


Some remarks are in order:

a. It can be shown that $\rho_j = \sqrt{1 - \mathbb{C}or(\varepsilon_{j,k},\varepsilon_{j,l})}$, for $k \ne l$.
b. $\rho_j=1$ implies that $\varepsilon_{j,k}$ and $\varepsilon_{j,l}$ are uncorrelated (we are then back to the multinomial logit case).
c. When $J=1$:
$$
F([\varepsilon_1,\dots,\varepsilon_K]',\rho) = \exp\left(-\left(\sum_{k=1}^{K} \exp(-\varepsilon_k/\rho)\right)^{\rho}\right).
$$
d. We have:
\begin{eqnarray*}
I_j = \mathbb{E}(\max_k(U_{j,k})) &=& \mathbb{E}(\max_k(V_{j,k} + \varepsilon_{j,k})),
\end{eqnarray*}
The inclusive values can therefore be seen as measures of the relative attractiveness of a nest.


This approach allows for some level of correlation across the $\varepsilon_{j,k}$ (for a given $j$). This can be interpreted as the existence of an (unobserved) *common error component* for the alternatives of a same nest. This component contributes to making the alternatives of a given nest more similar. In other words, this approach can accommodate a higher sensitivity (cross-elasticity) between the alternatives of a given nest.

Note that if the common component is reduced to zero (i.e. $\rho_i=1$), the model boils down to the multinomial logit model with no covariance of error terms among the alternatives.

Contrary to the general multinmial model, nested logits can solve the Red-Blue problem described in Section \@ref(MNL) (see Example \@ref(exm:redbluebus)). Assume you have estimated a model specifying $U_{1} = V_{1} + \varepsilon_{1}$ (car choice) and $U_{2} = V_{2} + \varepsilon_{2}$ (red bus choice). You can then assume that the blue-bus utility is of the form $U_{3} = V_{2} + \varepsilon_{3}$ where $\varepsilon_{3}$ is perfectly correlated to $\varepsilon_{2}$. This is done by redefining the set of choices as follows:
\begin{eqnarray*}
j=1 &\Leftrightarrow& (j'=1,k=1) \\
j=2 &\Leftrightarrow& (j'=2,k=1) \\
j=3 &\Leftrightarrow& (j'=2,k=2),
\end{eqnarray*}
and by setting $\rho_2 \rightarrow 0$.

IIA holds within a nest, but not when considering alternatives in different nests. Indeed, using Eq. \@ref(eq:Nested):
$$
\frac{\mathbb{P}[y_1=j,y_2=k_A|\bv{x}] }{\mathbb{P}[y_1=j,y_2=k_B|\bv{x}]} = \frac{\exp(\bv{v}_{j,k_A}'\boldsymbol\beta_j/\rho_j)}{\exp(\bv{v}_{j,k_B}'\boldsymbol\beta_j/\rho_j)},
$$
i.e. we have IIA in nest $j$.

By contrast:
\begin{eqnarray*}
\frac{\mathbb{P}[y_1=j_A,y_2=k_A|\bv{x}] }{\mathbb{P}[y_1=j_B,y_2=k_B|\bv{x}]} &=& \frac{\exp(\bv{u}_{j_A}'\boldsymbol\alpha + \rho_{j_A} I_{j_A})\exp(\bv{v}_{{j_A},{k_A}}'\boldsymbol\beta_{j_A}/\rho_{j_A})}{\exp(\bv{u}_{j_B}'\boldsymbol\alpha + \rho_{j_B} I_{j_B})\exp(\bv{v}_{{j_B},{k_B}}'\boldsymbol\beta_{j_B}/\rho_{j_B})}\times\\
&& \frac{\sum_{l=1}^{K_{j_B}} \exp(\bv{v}_{{j_B},l}'\boldsymbol\beta_{j_B}/\rho_{j_B})}{\sum_{l=1}^{K_{j_A}} \exp(\bv{v}_{{j_A},l}'\boldsymbol\beta_{J_A}/\rho_{j_A})},
\end{eqnarray*}
which depends on the expected utilities of all alternatives in nest $j_A$ and $j_B$. So the IIA does not hold.

:::{.example #nestedTravel name="Travel-mode dataset"}

Let us illustrate nested logits on the travel-mode dataset used, e.g., by @Hensher_Greene_2002 (see also @Heiss_2002).

```{r #nested1, warning=FALSE, message=FALSE}
library(mlogit)
library(stargazer)
data("TravelMode", package = "AER")
Prepared.TravelMode <- mlogit.data(TravelMode,chid.var = "individual",
                                   alt.var = "mode",choice = "choice",
                                   shape = "long")
# Fit a multinomial model:
hl <- mlogit(choice ~ wait + travel + vcost, Prepared.TravelMode,
             method = "bfgs", heterosc = TRUE, tol = 10)
## Fit a nested logit model:
TravelMode$avincome <- with(TravelMode, income * (mode == "air"))
TravelMode$time <- with(TravelMode, travel + wait)/60
TravelMode$timeair <- with(TravelMode, time * I(mode == "air"))
TravelMode$income <- with(TravelMode, income / 10)
# Hensher and Greene (2002), table 1 p.8-9 model 5
TravelMode$incomeother <- with(TravelMode,
                               ifelse(mode %in% c('air', 'car'), income, 0))
nl1 <- mlogit(choice ~ gcost + wait + incomeother, TravelMode,
              shape='long', # Indicates how the dataset is organized
              alt.var='mode', # variable that defines the alternative choices.
              nests=list(public=c('train', 'bus'),
                         car='car',air='air'), # defines the "limbs".
              un.nest.el = TRUE)
nl2 <- mlogit(choice ~ gcost + wait + time, TravelMode,
              shape='long', # Inidcates how the dataset is organized
              alt.var='mode', # variable that defines the alternative choices.
              nests=list(public=c('train', 'bus'),
                         car='car',air='air'), # defines the "limbs".
              un.nest.el = TRUE)
stargazer(nl1,nl2,type="text",no.space = TRUE)
```
:::



## Tobit models {#tobit}

In some situations, the dependent variable is incompletely observed, which may result in a non-representative sample. Typically, in some cases, observations of the dependent variable can have a lower and/or an upper limit, while the "true", underlying, dependent variable has not. In this case, OLS regression may lead to inconsistent parameter estimates. 

Tobit models have been designed to address some of these situations. This approach has been named after James Tobin, who developed this model in the late 50s (see @Tobin_1956).

Figure \@ref(fig:tobit1) illustrates the situation. The dots (white and black) represent the "true" observations. Now, assume that only the black are observed. If ones uses these observations in an OLS regression to estimate the relatonship between $x$ and $y$, then one gets the red line. It is clear that the sensitivity of $y$ to $x$ is then underestimated. The blue line is the line one would obtain if white dots were also observed; the grey line represents the model used to genrate the data ($y_i=x_i+\varepsilon_i$).

```{r tobit1, echo=FALSE, fig.cap="Bias in the case of sample selection. The grey line represents the population regression line. The model is $y_i = x_i + \\varepsilon_i$, with $\\varepsilon_{i,t} \\sim \\mathcal{N}(0,1)$. The red line is the OLS regression line based on black dots only.", fig.align = 'left-aligned'}
beta <- 1
sigma <- 1
N <- 100
eps <- rnorm(N) * sigma
x <- rnorm(N)
y <- beta * x + eps
indic.y.positive <- which(y>0)
y.positive <- y[indic.y.positive]
x.positive <- x[indic.y.positive]
model1 <- lm(y~x)
model2 <- lm(y.positive~x.positive)
new <- data.frame(x = seq(-3, 3, by = .01))
conf_interval.1 <- predict(model1,
                           newdata=new,
                           interval="confidence",
                           level = 0.95)
new <- data.frame(x.positive = seq(-3, 3, by = .01))
conf_interval.2 <- predict(model2,
                           newdata=new,
                           interval="confidence",
                           level = 0.95)
par(plt=c(.15,.95,.2,.95))
plot(x,y,pch=1,las=1)
points(x.positive,y.positive,pch=19)
abline(h=0)
lines(c(-10,10),beta*c(-10,10),col="grey",lwd=2)
abline(model1,col="blue",lwd=2)
abline(model2,col="red",lwd=2)
lines(new$x, conf_interval.1[,2], col="blue", lty=2)
lines(new$x, conf_interval.1[,3], col="blue", lty=2)
lines(new$x, conf_interval.2[,2], col="red",  lty=2)
lines(new$x, conf_interval.2[,3], col="red",  lty=2)
```


Assume that the (partially) observed dependent variable follows:
$$
y^* = \boldsymbol\beta'\bv{x} + \varepsilon,
$$
with $\varepsilon$ is drawn from a distribution characterized by a p.d.f. denoted by $f_{\boldsymbol\gamma}^*$ and a c.d.f. denoted by $F_{\boldsymbol\gamma}^*$; these functions depend on a vector of parameters $\boldsymbol{\gamma}$.

The observed dependent variable is:
\begin{eqnarray*}
\mbox{Censored case:}&&y = \left\{
\begin{array}{ccc}
y^* &if& y^*>L \\
L &if& y^*\le L,
\end{array}
\right.\\
\mbox{Truncated case:}&&y = \left\{
\begin{array}{ccc}
y^* &if& y^*>L \\
- &if& y^*\le L,
\end{array}
\right.
\end{eqnarray*}
where "$-$" stands for missing observations.

This formulation is easily extended to censoring from above ($L \rightarrow U$), or censoring from both below and above.

The model parameters are gathered in vector $\theta = [\boldsymbol\beta',\boldsymbol\gamma']'$. Let us write the conditional p.d.f. of the observed variable:
\begin{eqnarray*}
\mbox{Censored case:}&& f(y|\bv{x};\theta) = \left\{
\begin{array}{ccc}
f_{\boldsymbol\gamma}^*(y -  \boldsymbol\beta'\bv{x}) &if& y>L \\
F_{\boldsymbol\gamma}^*(L-  \boldsymbol\beta'\bv{x}) &if& y = L,
\end{array}
\right.\\
\mbox{Truncated case:}&&  f(y|\bv{x};\theta) =
\dfrac{f_{\boldsymbol\gamma}^*(y -  \boldsymbol\beta'\bv{x})}{1 - F_{\boldsymbol\gamma}^*(L-  \boldsymbol\beta'\bv{x})} \quad \mbox{with} \quad y>L.
\end{eqnarray*}

The (conditional) log-likelihood function is then given by:
$$
\log \mathcal{L}(\theta;\bv{y},\bv{X}) = \sum_{i=1}^n \log f(y_i|\bv{x}_i;\theta).
$$
In the censored case, we have:
\begin{eqnarray*}
\log \mathcal{L}(\theta;\bv{y},\bv{X}) &=& \sum_{i=1}^n \left\{
\mathbb{I}_{\{y_i=L\}}\log\left[F_{\boldsymbol\gamma}^*(L-  \boldsymbol\beta'\bv{x}_i)\right] + \right.\\
&& \left. \mathbb{I}_{\{y_i>0\}} \log \left[f_{\boldsymbol\gamma}^*(y_i -  \boldsymbol\beta'\bv{x}_i)\right]\right\}.
\end{eqnarray*}


The Tobit, or censored/truncated normal regression model, corresponds to the case described above, but with Gaussian errors $\varepsilon$. Specifically:
$$
y^* = \boldsymbol\beta'\bv{x} + \varepsilon,
$$
with $\varepsilon \sim \,i.i.d.\,\mathcal{N}(0,\sigma^2)$  ($\Rightarrow$ $\boldsymbol\gamma = \sigma^2$).

Without loss of generality, we can assume that $L=0$. (One can shift observed data if necessary.)

* The censored density (with $L=0$) is given by:
$$
f(y) = \left[
\frac{1}{\sqrt{2 \pi \sigma^2}}\exp\left(-\frac{1}{2 \sigma^2}(y - \boldsymbol\beta'\bv{x})^2\right)
\right]^{\mathbb{I}_{\{y>0\}}}
\left[
1 - \Phi\left(\frac{\boldsymbol\beta'\bv{x}}{\sigma}\right)
\right]^{\mathbb{I}_{\{y=0\}}}.
$$

* The truncated density (with $L=0$) is given by:
$$
f(y) = \frac{1}{\Phi(\boldsymbol\beta'\bv{x})}
\frac{1}{\sqrt{2 \pi \sigma^2}}\exp\left(-\frac{1}{2 \sigma^2}(y - \boldsymbol\beta'\bv{x})^2\right).
$$


Results usually heavily rely on distributional assumptions (more than in uncensored/untruncated case). The framework is easy to extend to an heteroskedastic case, for instance by setting $\sigma_i^2=\exp(\alpha'\bv{x}_i)$. Such a situation is illustrated by Figure \@ref(fig:tobit2).


```{r tobit2, echo=FALSE, fig.cap="Censored dataset with heteroskedasticitiy. The model is $y_i = x_i + \\varepsilon_i$, with $\\varepsilon_{i,t} \\sim \\mathcal{N}(0,\\sigma_i^2)$ where $\\sigma_i = \\exp(-1 + x_i)$.", fig.align = 'left-aligned'}
alpha.0 <- -1
alpha.1 <- 1
x <- rnorm(N)
sigma <- exp(alpha.0+alpha.1*x)
eps <- rnorm(N) * sigma
y <- beta * x + eps
indic.y.positive <- which(y>0)
y.positive <- y[indic.y.positive]
x.positive <- x[indic.y.positive]
model1 <- lm(y~x)
model2 <- lm(y.positive~x.positive)
new <- data.frame(x = seq(-3, 3, by = .01))
conf_interval.1 <- predict(model1,
                           newdata=new,
                           interval="confidence",
                           level = 0.95)
new <- data.frame(x.positive = seq(-3, 3, by = .01))
conf_interval.2 <- predict(model2,
                           newdata=new,
                           interval="confidence",
                           level = 0.95)
par(plt=c(.15,.95,.2,.95))
plot(x,y,pch=1,las=1)
points(x.positive,y.positive,pch=19)
abline(h=0)
lines(c(-10,10),beta*c(-10,10),col="grey",lwd=2)
abline(model1,col="blue",lwd=2)
abline(model2,col="red",lwd=2)
lines(new$x, conf_interval.1[,2], col="blue", lty=2)
lines(new$x, conf_interval.1[,3], col="blue", lty=2)
lines(new$x, conf_interval.2[,2], col="red",  lty=2)
lines(new$x, conf_interval.2[,3], col="red",  lty=2)
```



Let us consider the conditional means of $y$ in the general case, i.e., for any $\varepsilon$ distribution. Assume $\bv{x}$ is observed, such that expectations are conditional on $\bv{x}$.

* For data that are left-truncated at 0, we have:
\begin{eqnarray*}
\mathbb{E}(y) &=& \mathbb{E}(y^*|y^*>0) = \underbrace{\boldsymbol\beta'\bv{x}}_{=\mathbb{E}(y^*)} + \underbrace{\mathbb{E}(\varepsilon|\varepsilon>-\boldsymbol\beta'\bv{x})}_{>0} > \mathbb{E}(y^*).
\end{eqnarray*}

* Consider data that are left-censored at 0. By Bayes, we have:
$$
f_{y^*|y^*>0}(u) = \frac{f_{y^*}(u)}{\mathbb{P}(y^*>0)}\mathbb{I}_{\{u>0\}}.
$$
Therefore:
\begin{eqnarray*}
\mathbb{E}(y^*|y^*>0) &=& \frac{1}{\mathbb{P}(y^*>0)} \int_{-\infty}^\infty u\, f_{y^*}(u)\mathbb{I}_{\{u>0\}} du \\
&=&  \frac{1}{\mathbb{P}(y^*>0)} \mathbb{E}(\underbrace{y^*\mathbb{I}_{\{y^*>0\}}}_{=y}),
\end{eqnarray*}
and, further:
\begin{eqnarray*}
\mathbb{E}(y) &=&  \mathbb{P}(y^*>0)\mathbb{E}(y^*|y^*>0)\\
&>&  \mathbb{E}(y^*) =  \mathbb{P}(y^*>0)\mathbb{E}(y^*|y^*>0) +  \mathbb{P}(y^*<0)\underbrace{\mathbb{E}(y^*|y^*<0)}_{<0}.
\end{eqnarray*}

Now, let us come back to the Tobit (i.e., Gaussian case) case.

* For data that are left-truncated at 0:
\begin{eqnarray}
\mathbb{E}(y) &=& \boldsymbol\beta'\bv{x} + \mathbb{E}(\varepsilon|\varepsilon>-\boldsymbol\beta'\bv{x}) \nonumber\\
&=&  \boldsymbol\beta'\bv{x} + \sigma \underbrace{\frac{\phi(\boldsymbol\beta'\bv{x}/\sigma)}{\Phi(\boldsymbol\beta'\bv{x}/\sigma)}}_{=: \lambda(\boldsymbol\beta'\bv{x}/\sigma)} = \sigma \left( \frac{\boldsymbol\beta'\bv{x}}{\sigma} + \lambda\left(\frac{\boldsymbol\beta'\bv{x}}{\sigma}\right)\right). (\#eq:Econdtruncated)
\end{eqnarray}
where the penultimate line is obtained by using Eq. \@ref(eq:Etrunc).

* For data that are left-censored at 0:
\begin{eqnarray*}
\mathbb{E}(y) &=&  \mathbb{P}(y^*>0)\mathbb{E}(y^*|y^*>0)\\
&=&  \Phi\left( \frac{\boldsymbol\beta'\bv{x}}{\sigma}\right) \sigma \left(
\frac{\boldsymbol\beta'\bv{x}}{\sigma} +   \lambda\left(\frac{\boldsymbol\beta'\bv{x}}{\sigma}\right)
\right).
\end{eqnarray*}

```{r tobit3, echo=FALSE, fig.cap="Conditional means of $y$ in Tobit models. The model is $y_i = x_i + \\varepsilon_i$, with $\\varepsilon_i \\sim \\mathcal{N}(0,1)$.", fig.align = 'left-aligned'}
x <- seq(-4,4,by=.01)
par(plt=c(.15,.95,.15,.95))
plot(x,x + dnorm(x)/pnorm(x),las=1,type="l",lwd=2,ylab="E(y|x)",ylim=c(0,4))
lines(x,pnorm(x)*(x + dnorm(x)/pnorm(x)),lwd=2,lty=3)
grid()
lines(c(-10,10),c(-10,10),col="grey")
legend("topleft", # places a legend at the appropriate place 
       c("Truncated","Censored"),
       lty=c(1,3), # gives the legend appropriate symbols (lines)       
       lwd=c(2,2), # line width
       col=c("black"), # gives the legend lines the correct color and width
       #pch = c(3,NaN,NaN),#symbols,
       bg="white",
       seg.len = 4,
       bty="n"
)
```



<!-- %\begin{frame}{Tobit Model} -->
<!-- %\begin{footnotesize} -->
<!-- %\begin{itemize} -->
<!-- %	\item It is easily seen that: -->
<!-- %	$$ -->
<!-- %	\mathbb{P}(y^* \le 0) =  \Phi\left(-\frac{\boldsymbol\beta'\bv{x}}{\sigma}\right) = 1 - \Phi\left(\frac{\boldsymbol\beta'\bv{x}}{\sigma}\right). -->
<!-- %	$$ -->
<!-- %	\item If $y=0$ when $y^*<0$, then the p.d.f. of $y$ therefore is: -->
<!-- %	$$ -->
<!-- %	f(y) = \left[1 - \Phi\left(\frac{\boldsymbol\beta'\bv{x}}{\sigma}\right)\right]^{\mathbb{I}_{\{y=0\}}} -->
<!-- %	\left[\frac{1}{\sqrt{2\pi \sigma^2}}\exp\left(-\frac{(y - \boldsymbol\beta'\bv{x})}{2\sigma^2}\right)\right]^{\mathbb{I}_{\{y>0\}}}. -->
<!-- %	$$ -->
<!-- %	\item The log-likelihood function is: -->
<!-- %	\begin{eqnarray*} -->
<!-- %	\log \mathcal{L}(\boldsymbol\beta,\sigma^2;\bv{y},\bv{X}) &=& \sum_{i=1}^n \left\{ -->
<!-- %	\mathbb{I}_{\{y_i=0\}}\log\left[1 - \Phi\left(\frac{\boldsymbol\beta'\bv{x}_i}{\sigma}\right)\right] + \right.\\ -->
<!-- %	&& \left. \mathbb{I}_{\{y_i>0\}} \log \left[\frac{1}{\sqrt{2\pi \sigma^2}}\exp\left(-\frac{(y_i - \boldsymbol\beta'\bv{x}_i)}{2\sigma^2}\right)\right]\right\}. -->
<!-- %	\end{eqnarray*} -->
<!-- %\end{itemize} -->
<!-- %\end{footnotesize} -->
<!-- %\end{frame} -->


**Heckit regression**

The previous formula (Eq. \@ref(eq:Econdtruncated)) can in particular be used in an alternative estimation approach, namely the Heckman two-step estimation. This approach is based on two steps:[^FootnoteHeckit1]

[^FootnoteHeckit1]: See 16.10.2 of @Cameron_Trivedi_2005 for the derivation of asymptotic standard errors of $\boldsymbol\beta$.

1. Using the complete sample, fit a Probit model of $\mathbb{I}_{\{y_i>0\}}$ on $\bv{x}$. This provides a consistent estimate of $\frac{\boldsymbol\beta}{\sigma}$, and therefore of $\lambda(\boldsymbol\beta'\bv{x}/\sigma)$. (Indeed, if $z_i \equiv \mathbb{I}_{\{y_i>0\}}$, then $\mathbb{P}(z_i=1|\bv{x}_i;\boldsymbol\beta/\sigma)=\Phi(\boldsymbol\beta'\bv{x}_i/\sigma)$.)

2. Using the truncated sample only: run an OLS regression of $\bv{y}$ on $\left\{\bv{x},\lambda(\boldsymbol\beta'\bv{x}/\sigma)\right\}$ (having Eq. \@ref(eq:Econdtruncated) in mind). This provides a consistent estimate of $(\boldsymbol\beta,\sigma)$.

The underlying specification is of the form:
$$
\mbox{Conditional mean} + \mbox{disturbance}.
$$
where "Conditional mean" comes from Eq. \@ref(eq:Econdtruncated) and "disturbance" is an error with zero conditional mean.

This approach is also applied to the case of **sample selection models** (Section \@ref(SSM)).


:::{.example #WageMroz1 name="Wage prediction"}

The present example is based on the dataset used in @Mroz_1987 (whicht is part of the `sampleSelection` package).

```{r tobit4, warning=FALSE, message=FALSE}
library(sampleSelection)
library(AER)
data("Mroz87")
Mroz87$lfp.yesno <- NaN
Mroz87$lfp.yesno[Mroz87$lfp==1] <- "yes"
Mroz87$lfp.yesno[Mroz87$lfp==0] <- "no"
Mroz87$lfp.yesno <- as.factor(Mroz87$lfp.yesno)
ols <- lm(wage ~ educ + exper + I( exper^2 ) + city,data=subset(Mroz87,lfp==1))
tobit <- tobit(wage ~ educ + exper + I( exper^2 ) + city,
               left = 0, right = Inf,
               data=Mroz87)
Heckit <- heckit(lfp ~ educ + exper + I( exper^2 ) + city, # selection equation
                 wage ~ educ + exper + I( exper^2 ) + city, # outcome equation
                 data=Mroz87 )

stargazer(ols,Heckit,tobit,no.space = TRUE,type="text",omit.stat = "f")
```

Figure \@ref(fig:tobit5) shows that, low wages, the OLS model tends to over-predict wages. The slope between observed and Tobit-predicted wages is closer to one (the adjustment line is closer to the 45-degree line.)

```{r tobit5, echo=FALSE, fig.cap="Predicted versus observed wages.", fig.align = 'left-aligned'}
predicted.wage.tobit <- predict(tobit,newdata=Mroz87)
predicted.wage.OLS <- predict(ols,newdata=Mroz87)
par(mfrow=c(1,1))
par(plt=c(.15,.95,.2,.95))
plot(Mroz87$wage,predicted.wage.tobit,pch=19,
     xlab="Observed wage",ylab="Predicted wage",col="light grey",
     ylim=c(min(predicted.wage.OLS,predicted.wage.tobit),
            max(predicted.wage.OLS,predicted.wage.tobit)))
grid()
points(Mroz87$wage,predicted.wage.OLS,col="grey")
lines(c(-100,100),c(-100,100),lwd=2,col="black")
abline(lm(predicted.wage.tobit~Mroz87$wage),col="red",lwd=2)
abline(lm(predicted.wage.OLS~Mroz87$wage),col="blue",lwd=2)
legend("bottomright",
       c("Tobit","OLS","pred~obs Tobit","pred~obs OLS"),
       lty=c(NaN,NaN,1,1), # gives the legend appropriate symbols (lines)
       lwd=c(2), # line width
       col=c("grey","light grey","red","blue"), # gives the legend lines the correct color and width
       pt.bg=c(1),
       pch = c(19,1,NaN,NaN),#symbols,
       pt.cex = c(1),
       bg="white",
       seg.len = 4
)
```

:::

<!-- \begin{frame}{Tobit Model} -->
<!-- \begin{footnotesize} -->
<!-- \begin{itemize} -->
<!-- 	\item Tobit models crucially depend on distributional assumptions. Specification tests, see m-Test (Remark\,\@ref(remark:mTest}). -->
<!-- 	\item Specific case of (suspected) heteroskedasticity: -->
<!-- 		$$ -->
<!-- 		H_0: \,\sigma_i^2=\sigma^2, \qquad \quad H_1:\, \sigma_i^2 = \exp(\bv{x}_i\alpha). -->
<!-- 		$$ -->

<!-- 	\href{http://cameron.econ.ucdavis.edu/mmabook/mma.html}{Cameron and Trivedi}  (16.3.7) propose an OPG LM test (see Slide\,\@ref(Slide:OPG}), based on the OLS regression: -->
<!-- 	$$ -->
<!-- 	1 = \gamma_1 \bv{s}_{i,1} + \gamma_2 \bv{s}_{i,2} + \varepsilon_i, -->
<!-- 	$$ -->
<!-- 	where $\bv{s}_{i,1} = \partial \log f(y_i,\boldsymbol\beta,\boldsymbol\alpha)/\partial \boldsymbol\beta$ and $\bv{s}_{i,2} = \partial \log f(y_i,\boldsymbol\beta,\boldsymbol\alpha)/\partial \boldsymbol\alpha$. -->

<!-- 	(The test statistics is $n \times R_u^2$, where $R_u^2$ is the uncentered $R^2$.) -->
<!-- \end{itemize} -->
<!-- \end{footnotesize} -->
<!-- \end{frame} -->


**Two-part model**


In the standard Tobit framework, the model determining censored ---or truncated--- data *censoring mechanism* is the same as the one determining non-censored ---or observed--- data *outcome mechanism*. A two-part model adds flexibility by permitting the zeros and non-zeros to be generated by different densities. The second model characterizes the outcome *conditional on* the outcome being observed.

In a seminal paper, @Duan_et_al_1983 employ this methodology to account for individual annual hospital expenses. The two models are then as follows:

* $1^{st}$ model: $\mathbb{P}(hosp=1|\bv{x}) =  \Phi(\bv{x}_1'\boldsymbol\beta_1)$,
* $2^{nd}$ model: $Expense = \exp(\bv{x}_2'\boldsymbol\beta_2 + \eta)$, with $\eta \sim\,i.i.d.\, \mathcal{N}(0,\sigma_2^2)$.

Specifically:
$$
\mathbb{E}(Expense|\bv{x}_1,\bv{x}_2) = \Phi(\bv{x}_1'\boldsymbol\beta_1)\exp\left(\bv{x}_2'\boldsymbol\beta_2+ \frac{\sigma_2^2}{2}\right).
$$

In sample-selection models, studied in the next section, one specifies the joint distribution for the censoring and outcome mechanisms  (while the two parts are independent here).




## Sample Selection Models {#SSM}


The situation tackled by sample-selection models is the following. The dependent variable of interest, denoted by $y_2$, depends on observed variables $\bv{x}_2$. Observing $y_2$, or not, depends on the value of a latent variable ($y_1^*$) that is correlated to observed variables $\bv{x}_1$. The difference w.r.t. the two-part model skethed above is that, even conditionally on $(\bv{x}_1,\bv{x}_2)$, $y_1^*$ and $y_2$ may be correlated.

As in the Tobit case, even in the simplest case of population conditional mean linear in regressors (i.e. $y_2 = \bv{x}_2'\boldsymbol\beta_2 + \varepsilon_2$), OLS regression leads to inconsistent parameter estimates because the sample is not representative of the population.

There are two latent variables: $y_1^*$ and $y_2^*$. We observe $y_1$ and, if the considered entity "participates", we also observe $y_2$. More specifically:
\begin{eqnarray*}
y_1 &=& \left\{
\begin{array}{ccc}
1 &\mbox{if}& y_1^* > 0 \\
0 &\mbox{if}& y_1^* \le 0
\end{array}
\right. \quad \mbox{(participation equation)}\\
y_2 &=& \left\{
\begin{array}{ccc}
y_2^* &\mbox{if}& y_1 = 1 \\
- &\mbox{if}& y_1 = 0
\end{array}
\right. \quad \mbox{(outcome equation).}
\end{eqnarray*}

Moreover:
\begin{eqnarray*}
y_1^* &=& \bv{x}_1'\boldsymbol\beta_1 + \varepsilon_1 \\
y_2^* &=& \bv{x}_2'\boldsymbol\beta_2 + \varepsilon_2.
\end{eqnarray*}

Note that the Tobit model (Section \@ref(tobit)) is the special case where $y_1^*=y_2^*$.

Usually:
$$
\left[\begin{array}{c}\varepsilon_1\\\varepsilon_2\end{array}\right] \sim \mathcal{N}\left(\bv{0},
\left[
\begin{array}{cc}
1 & \rho  \sigma_2 \\
\rho  \sigma_2 & \sigma_2^2
\end{array}
\right]
\right).
$$
Let us derive the likelihood associated with this model. We have:
\begin{eqnarray}
f(\underbrace{0}_{=y_1},\underbrace{-}_{=y_2}|\bv{x};\theta) &=& \mathbb{P}(y_1^*\le 0) = \Phi(-\bv{x}_1'\boldsymbol\beta_1) (\#eq:probaPP1)\\
f(1,y_2|\bv{x};\theta) &=& f(y_2^*|\bv{x};\theta) \mathbb{P}(y_1^*>0|y_2^*,\bv{x};\theta) \nonumber \\
&=& \frac{1}{\sigma}\phi\left(\frac{y_2 - \bv{x}_2'\boldsymbol\beta_2}{\sigma}\right)  \mathbb{P}(y_1^*>0|y_2,\bv{x};\theta).(\#eq:probaPP2)
\end{eqnarray}

Let us compute $\mathbb{P}(y_1^*>0|y_2,\bv{x};\theta)$. By Prop. \@ref(prp:update) (in Appendix \@ref(GaussianVar)), applied to ($\varepsilon_1,\varepsilon_2$), we have:
$$
y_1^*|y_2 \sim \mathcal{N}\left(\bv{x}_1'\boldsymbol\beta_1 + \frac{\rho}{\sigma_2}(y_2-\bv{x}_2'\boldsymbol\beta_2),1-\rho^2\right).
$$
which leads to
\begin{equation}
\mathbb{P}(y_1^*>0|y_2,\bv{x};\theta) = \Phi\left( \frac{\bv{x}_1'\boldsymbol\beta_1 + \dfrac{\rho}{\sigma_2}(y_2-\bv{x}_2'\boldsymbol\beta_2)}{\sqrt{1-\rho^2}}\right).(\#eq:probaPP3)
\end{equation}

Figure \@ref(fig:SampleSelec) displays $\mathbb{P}(y_1^*>0|y_2,\bv{x};\theta)$ for different values of $y_2$ and of $\rho$, in the case where $\boldsymbol\beta_1=\boldsymbol\beta_2=0$.

```{r SampleSelec, echo=FALSE, fig.cap="Probability of observing $y_2$ depending on its value, for different values of conditional correlation between $y_2$ and $y_1^*$.", fig.align = 'left-aligned'}

rho <- .7
sigma2 <- 1

par(plt=c(.15,.95,.18,.95))
all.rhos <- c(-.1,0,.5,.9)
count <- 0
for(rho in all.rhos){
  count <- count + 1
  x <- seq(-5,5,by=.05)
  P <- pnorm(rho/sigma2*x/sqrt(1 - rho^2))
  if(count==1){
    plot(x,P,type="l",lwd=2,ylim=c(0,1),
         xlab=expression(y[2]),
         ylab=expression(paste(P,"(",y[1]^"*",">0|",y[2],")",sep="")))
    rho.4.legend <- expression(paste(rho,"=",-.1))
  }else{
    lines(x,P,type="l",col=count,lwd=2)
    if(count==2){
      rho.4.legend <- c(rho.4.legend,expression(paste(rho,"=",0)))
    }
    if(count==3){
      rho.4.legend <- c(rho.4.legend,expression(paste(rho,"=",0.5)))
    }
    if(count==4){
      rho.4.legend <- c(rho.4.legend,expression(paste(rho,"=",0.9)))
    }
  }
  
  legend("topleft",
         rho.4.legend,
         lty=c(1,1), # gives the legend appropriate symbols (lines)
         lwd=c(2,2), # line width
         col=1:length(all.rhos), # gives the legend lines the correct color and width
         bg="white",
         seg.len = 4
  )
}
```


Using Eqs. \@ref(eq:probaPP1), \@ref(eq:probaPP2) and \@ref(eq:probaPP3), one gets the log-likelihood function:
\begin{eqnarray*}
\log \mathcal{L}(\theta;\bv{y},\bv{X}) &=& \sum_{i=1}^n  (1 - y_{1,i})\log \Phi(-\bv{x}_{1,i}'\boldsymbol\beta_1) + \\
&&  \sum_{i=1}^n y_{1,i} \log \left(  \frac{1}{\sigma}\phi\left(\frac{y_{2,i} - \bv{x}_{2,i}'\boldsymbol\beta_2}{\sigma}\right)\right) + \\
&&  \sum_{i=1}^n y_{1,i} \log \left(\Phi\left( \frac{\bv{x}_{1,i}'\boldsymbol\beta_1 + \dfrac{\rho}{\sigma_2}(y_{2,i}-\bv{x}_2'\boldsymbol\beta_2)}{\sqrt{1-\rho^2}}\right)\right).
\end{eqnarray*}


We can also compute conditional expectations:
\begin{eqnarray}
\mathbb{E}(y_2^*|y_1=1,\bv{x}) &=& \mathbb{E}(\mathbb{E}(y_2^*|y_1^*,\bv{x})|y_1=1,\bv{x})\nonumber\\
&=& \mathbb{E}(\bv{x}_2'\boldsymbol\beta_2 + \rho\sigma_2(y_1^*-\bv{x}_1'\boldsymbol\beta_1)|y_1=1,\bv{x})\nonumber\\
&=& \bv{x}_2'\boldsymbol\beta_2 + \rho\sigma_2\mathbb{E}( \underbrace{y_1^*-\bv{x}_1'\boldsymbol\beta_1}_{=\varepsilon_1 \sim\mathcal{N}(0,1)}|\varepsilon_1>-\bv{x}_1'\boldsymbol\beta_1,\bv{x})\nonumber\\
&=& \bv{x}_2'\boldsymbol\beta_2 + \rho\sigma_2\frac{\phi(-\bv{x}_1'\boldsymbol\beta_1)}{1 - \Phi(-\bv{x}_1'\boldsymbol\beta_1)}\nonumber\\
&=& \bv{x}_2'\boldsymbol\beta_2 + \rho\sigma_2\frac{\phi(\bv{x}_1'\boldsymbol\beta_1)}{\Phi(\bv{x}_1'\boldsymbol\beta_1)}=\bv{x}_2'\boldsymbol\beta_2 + \rho\sigma_2\lambda(\bv{x}_1'\boldsymbol\beta_1),(\#eq:y2y11)
\end{eqnarray}
and:
\begin{eqnarray*}
\mathbb{E}(y_2^*|y_1=0,\bv{x}) &=&  \bv{x}_2'\boldsymbol\beta_2 + \rho\sigma_2\mathbb{E}(y_1^*-\bv{x}_1'\boldsymbol\beta_1|\varepsilon_1\le-\bv{x}_1'\boldsymbol\beta_1,\bv{x})\\
&=& \bv{x}_2'\boldsymbol\beta_2 + \rho\sigma_2\frac{\phi(-\bv{x}_1'\boldsymbol\beta_1)}{1 - \Phi(-\bv{x}_1'\boldsymbol\beta_1)}\\
&=& \bv{x}_2'\boldsymbol\beta_2 - \rho\sigma_2\frac{\phi(-\bv{x}_1'\boldsymbol\beta_1)}{\Phi(-\bv{x}_1'\boldsymbol\beta_1)}=\bv{x}_2'\boldsymbol\beta_2 - \rho\sigma_2\lambda(-\bv{x}_1'\boldsymbol\beta_1).
\end{eqnarray*}

**Heckman procedure**

As for tobit models (Section \@ref(tobit)), we can use the Heckman procedure to estimate this model. Eq. \@ref(eq:y2y11) shows that $\mathbb{E}(y_2^*|y_1=1,\bv{x}) \ne  \bv{x}_2'\boldsymbol\beta_2$ when $\rho \ne 0$. Therefore, the OLS approach yields biased estimates based when it is employed only on the sub-sample where $y_1=1$.

The Heckman two-step procedure (or "Heckit") consists in replacing $\lambda(\bv{x}_1'\boldsymbol\beta_1)$ appearing in Eq. \@ref(eq:y2y11) with a consistent estimate of it. More precisely:

1.  Get an estimate $\widehat{\boldsymbol\beta_1}$ of $\boldsymbol\beta_1$ (probit regression of $y_1$ on $\bv{x}_1$).
2.  Run the OLS regression (using only data associated with $y_1=1$):
\begin{equation}(\#eq:OLSregHeckit)
y_2  = \bv{x}_2'\boldsymbol\beta_2 + \rho \sigma_2 \lambda(\bv{x}_1'\widehat{\boldsymbol\beta_1}) + \varepsilon_2,
\end{equation}
considering $\lambda(\bv{x}_1'\widehat{\boldsymbol\beta_1})$ as a regressor.

How to estimate $\sigma_2^2$? By Eq. \@ref(eq:Vtrunc), we have:
$$
\mathbb{V}ar(y_2|y_1^*>0,\bv{x}) = \mathbb{V}ar(\varepsilon_2|\varepsilon_1>-\bv{x}_1'\boldsymbol\beta_1,\bv{x}).
$$
Using that $\varepsilon_2$ can be decomposed as $\rho\sigma_2\varepsilon_1 + \xi$, where $\xi \sim \mathcal{N}(0,\sigma_2^2(1-\rho^2))$ is independent from $\varepsilon_1$, we get:
$$
\mathbb{V}ar(y_2|y_1^*>0,\bv{x}) = \sigma_2^2(1-\rho^2) + \rho^2\sigma_2^2 \mathbb{V}ar(\varepsilon_1|\varepsilon_1>-\bv{x}_1'\boldsymbol\beta_1,\bv{x}).
$$
Using Eq. \@ref(eq:Vtrunc2), we get:
$$
\mathbb{V}ar(\varepsilon_1|\varepsilon_1>-\bv{x}_1'\boldsymbol\beta_1,\bv{x}) = 1 - \bv{x}_1'\boldsymbol\beta_1 \lambda(\bv{x}_1'\boldsymbol\beta_1) - \lambda(\bv{x}_1'\boldsymbol\beta_1)^2,
$$
which gives
\begin{eqnarray*}
\mathbb{V}ar(y_2|y_1^*>0,\bv{x}) &=& \sigma_2^2(1-\rho^2) + \rho^2\sigma_2^2 (1 - \bv{x}_1'\boldsymbol\beta_1 \lambda(\bv{x}_1'\boldsymbol\beta_1) - \lambda(\bv{x}_1'\boldsymbol\beta_1)^2)\\
&=& \sigma_2^2 - \rho^2\sigma_2^2 \left(\bv{x}_1'\boldsymbol\beta_1 \lambda(\bv{x}_1'\boldsymbol\beta_1) + \lambda(\bv{x}_1'\boldsymbol\beta_1)^2\right),
\end{eqnarray*}
and, finally:
$$
\sigma_2^2 \approx \widehat{\mathbb{V}ar}(y_2|y_1^*>0,\bv{x}) + \widehat{\rho \sigma_2}^2 \left(\bv{x}_1'\widehat{\boldsymbol\beta_1} \lambda(\bv{x}_1'\widehat{\boldsymbol\beta_1}) + \lambda(\bv{x}_1'\widehat{\boldsymbol\beta_1})^2\right).
$$

The Heckman procedure is computationally simple. Although computational costs are no longer an issue, the two-step solution allows certain generalisations more easily than ML, and is more robust in certain circumstances. The computation of parameter standard errors is fairly complicated because of the two steps (see @Cameron_Trivedi_2005, Subsection 16.10.2). Bootstrap can be resorted to.

:::{.example #WageSample name="Wage prediction"}

As in Example \@ref(exm:WageMroz1), let us use the @Mroz_1987 dataset again, with the objective of explaining wage setting.

```{r #SampleSelec, warning=FALSE, message=FALSE}
library(sampleSelection)
library(AER)
data("Mroz87")
Mroz87$lfp.yesno <- NaN
Mroz87$lfp.yesno[Mroz87$lfp==1] <- "yes"
Mroz87$lfp.yesno[Mroz87$lfp==0] <- "no"
Mroz87$lfp.yesno <- as.factor(Mroz87$lfp.yesno)
#Logit & Probit (selection equation)
logitW <- glm(lfp ~ age + I( age^2 ) + kids5 + huswage + educ,
              family = binomial(link = "logit"), data = Mroz87) 
probitW <- glm(lfp ~ age + I( age^2 ) + kids5 + huswage + educ,
               family = binomial(link = "probit"), data = Mroz87) 
# OLS for outcome:
ols1 <- lm(log(wage) ~ educ+exper+I( exper^2 )+city,data=subset(Mroz87,lfp==1))
# Two-step Heckman estimation
heckvan <- 
  heckit( lfp ~ age + I( age^2 ) + kids5 + huswage + educ, # selection equation
          log(wage) ~ educ + exper + I( exper^2 ) + city, # outcome equation
          data=Mroz87 )
# Maximun likelihood estimation of selection model:
ml <- selection(lfp~age+I(age^2)+kids5+huswage+educ, 
                log(wage)~educ+exper+I(exper^2)+city, data = Mroz87)
# Print selection-equation estimates:
stargazer(logitW,probitW,heckvan,ml,type = "text",no.space = TRUE,
          selection.equation = TRUE)
# Print outcome-equation estimates:
stargazer(ols1,heckvan,ml,type = "text",no.space = TRUE,omit.stat = "f")
```
:::

## Models of Count Data

Count-data models aim at explaining dependent variables $y_i$ that take integer values. Typically, one may want to account for the number of doctor visits, of customers, of hospital stays, of borrowers' defaults, of recreational trips, of accidents.  Quite often, these data feature large proportion of zeros (see, e.g., Table 20.1 in @Cameron_Trivedi_2005), and/or are skewed to the right.


### Poisson model

The most basic count-data model is the Poisson model. In this model, we have $y \sim \mathcal{P}(\mu)$, i.e.
$$
\mathbb{P}(y=k) = \frac{\mu^k e^{-\mu}}{k!},
$$
implying $\mathbb{E}(y) = \mathbb{V}ar(y) = \mu$.

the Poisson parameter, $\mu$, is then assumed to depend on some observed variables, gathered in vector $\bv{x}_i$ for entity $i$. To ensure that $\mu_i \ge 0$, it is common to take $\mu_i = \exp(\boldsymbol\beta'\bv{x}_i)$, which gives:
$$
y_i \sim \mathcal{P}(\exp[\boldsymbol\beta'\bv{x}_i]).
$$

The Poisson regression is intrinsically heteroskedastic (since $\mathbb{V}ar(y_i) = \mu_i = \exp(\boldsymbol\beta'\bv{x}_i)$).

Under the assumption of independence across entities, the log-likelihood is given by: 
$$
\log \mathcal{L}(\boldsymbol\beta;\bv{y},\bv{X}) = \sum_{i=1}^n (y_i \boldsymbol\beta'\bv{x}_i - \exp[\boldsymbol\beta'\bv{x}_i] - \ln[y_i!]).
$$
The first-order condition to get the MLE is:
\begin{equation}
\sum_{i=1}^n (y_i - \exp[\boldsymbol\beta'\bv{x}_i])\bv{x}_i = \underbrace{\bv{0}}_{K \times 1}. (\#eq:FOCPoisson)
\end{equation}

Eq. \@ref(eq:FOCPoisson) is equivalent to what would define the **Pseudo Maximum-Likelihood** estimator of $\boldsymbol\beta$ in the (misspecified) model 
$$
y_i \sim i.i.d.\,\mathcal{N}(\exp[\boldsymbol\beta'\bv{x}_i],\sigma^2).
$$
That is, Eq. \@ref(eq:FOCPoisson) also characterizes the (true) ML estimator of $\boldsymbol\beta$ in the previous model.

Since $\mathbb{E}(y_i|\bv{x}_i) = \exp(\boldsymbol\beta'\bv{x}_i)$, we have:
$$
y_i = \exp(\boldsymbol\beta'\bv{x}_i) + \varepsilon_i,
$$
with $\mathbb{E}(\varepsilon_i|\bv{x}_i) = 0$. This notably implies that the (N)LS estimator of $\boldsymbol\beta$ is consistent.

How to interpret regression coefficients (the components of $\boldsymbol\beta$)? We have:
$$
\frac{\partial \mathbb{E}(y_i|\bv{x}_i)}{\partial x_{i,j}} = \beta_j \exp(\boldsymbol\beta'\bv{x}_i),
$$
which depends on the considered individual.

The average estimated response is:
$$
\widehat{\beta}_j \frac{1}{n}\sum_{i=1}^n  \exp(\widehat{\boldsymbol\beta}'\bv{x}_i),
$$
which is equal to $\widehat{\beta}_j \overline{y}$ if the model includes a constant (e.g., if $x_{1,i}=1$ for all entities $i$).

The limitation of the standard Poisson model is that the distribution of $y_i$ conditional on $\bv{x}_i$ depends on a single parameter ($\mu_i$). Besides, there is often a tension between fitting the fraction of zeros, i.e. $\mathbb{P}(y_i=0|\bv{x}_i)=\exp[-\exp(\boldsymbol\beta'\bv{x}_i)]$, and the distribution of $y_i|\bv{x}_i,y_i>0$. The following models (negative binomial, or NB model, the Hurdle model, and the Zero-Inflated model) have been designed to address these points.


### Negative binomial model

In the negative binomial model, we have:
$$
y_i|\lambda_i \sim \mathcal{P}(\lambda_i),
$$
but $\lambda_i$ is now random. Specifically, it takes the form:
$$
\lambda_i = \nu_i \times \exp(\boldsymbol\beta'\bv{x}_i),
$$
where $\nu_i \sim \,i.i.d.\,\Gamma(\underbrace{\delta}_{\mbox{shape}},\underbrace{1/\delta}_{\mbox{scale}})$. That is, the p.d.f. of $\nu_i$ is:
$$
g(\nu) = \frac{\nu^{\delta - 1}e^{-\nu\delta}\delta^\delta}{\Gamma(\delta)},
$$
where $\Gamma:\,z \mapsto \int_0^{+\infty}t^{z-1}e^{-t}dt$ (and $\Gamma(k+1)=k!$).

This notably implies that:
$$
\mathbb{E}(\nu_i) = 1 \quad \mbox{and} \quad \mathbb{V}ar(\nu) = \frac{1}{\delta}.
$$


Hence, the p.d.f. of $y_i$ conditional on $\mu$ and $\delta$ (with $\mu=\exp(\boldsymbol\beta'\bv{x}_i)$) is obtained  as a mixture of densities:
$$
\mathbb{P}(y_i=k|\exp(\boldsymbol\beta'\bv{x}_i)=\mu;\delta)=\int_0^\infty \frac{e^{-\mu \nu}(\mu \nu)^k}{k!} \frac{\nu^{\delta - 1}e^{-\nu\delta}\delta^\delta}{\Gamma(\delta)} d \nu.
$$

It can be shown that:
$$
\mathbb{E}(y|\bv{x}) = \mu \quad \mbox{and}\quad \mathbb{V}ar(y|\bv{x}) = \mu\left(1+\alpha \mu\right),
$$
where $\exp(\boldsymbol\beta'\bv{x}_i)=\mu$ and $\alpha = 1/\delta$.

We have one additional degree of freedom w.r.t. the Poisson model  ($\alpha$).

Note that $\mathbb{V}ar(y|\bv{x}) > \mathbb{E}(y|\bv{x})$ (which is often called for by the data). Moreover, the conditional variance is quadratic in the mean:
$$
\mathbb{V}ar(y|\bv{x}) = \mu+\alpha \mu^2.
$$
The previous expression is the basis of the so-called **NB2** specification. If $\delta$ is replaced with $\mu/\gamma$, then we get the **NB1** model:
$$
\mathbb{V}ar(y|\bv{x}) = \mu(1+\gamma).
$$

:::{.example #Doctorvisits name="Number of doctor visits"}

The following example compares different specifications, namely a linear regression model, a Poisson model, and a NB model, to account for the number of doctor visits. The dataset (``randdata`) is the one used in Chapter 20 of @Cameron_Trivedi_2005 (available on [that page](http://cameron.econ.ucdavis.edu/mmabook/mmadata.html)).

```{r countdata1, warning=FALSE, message=FALSE, fig.cap="Distribution of the number of doctor visits.", fig.asp = .6, out.width = "95%", fig.align = 'left-aligned'}
library(AEC)
library(COUNT)
library(pscl) # for predprob function and hurdle model
par(plt=c(.15,.95,.1,.95))
plot(table(randdata$mdvis))
```

```{r countdata2, warning=FALSE, message=FALSE}
randdata$LC <- log(1 + randdata$coins)
model.OLS <- lm(mdvis ~ LC + idp + lpi + fmde + physlm + disea + hlthg + 
                  hlthf + hlthp - 1,data=randdata)
model.poisson <- glm(mdvis ~ LC + idp + lpi + fmde + physlm + disea + 
                       hlthg + hlthf + hlthp - 1,data=randdata,family = poisson)
model.neg.bin <- glm.nb(mdvis ~ LC + idp + lpi + fmde + physlm + disea +
                          hlthg + hlthf + hlthp - 1,data=randdata)
model.neg.bin.with.intercept <- 
  glm.nb(mdvis ~ LC + idp + lpi + fmde + physlm + disea + hlthg + 
           hlthf + hlthp,data=randdata)
stargazer::stargazer(model.OLS,model.poisson,model.neg.bin,
                     model.neg.bin.with.intercept,type="text",
                     no.space = TRUE,omit.stat = c("f","ser"))
```

Models' predictions can be obtained as follows:

```{r countdata3, warning=FALSE, message=FALSE}
# prediction of beta'x, equivalent to "model.poisson$fitted.values":
predict_poisson.beta.x <- predict(model.poisson)
# prediction of the number of events (exp(beta'x)):
predict_poisson <- predict(model.poisson,type="response")
predict_NB <- model.neg.bin$fitted.values
```

Let us now compute the model-implied probabilities, and let's compare them with the frequencies observed in the data.

```{r countdata4, warning=FALSE, message=FALSE}
prop.table.data  <- prop.table(table(randdata$mdvis))
predprob.poisson <- predprob(model.poisson) # part of pscl package
predprob.nb      <- predprob(model.neg.bin)
print(rbind(prop.table.data[1:6],
            apply(predprob.poisson[,1:6],2,mean),
            apply(predprob.nb[,1:6],2,mean)))
```

It appears that the NB model is better at capturing the relatively large number of zeros than the Poisson model. This will also be the case for the Hurdle and Zero-Inflation models:
:::

### Hurdle model

The main objective of this model, w.r.t. the Poisson model, is to generate more zeros in the data than predicted by the previous count models. The idea is to separate the modeling of the number of zeros and that of the number of non-zero counts. Specifically, the frequency of zeros is determined by $f_1$, the (relative) frequencies of non-zero counts are determined by $f_2$:
$$
f(y) = \left\{
\begin{array}{lll}
f_1(0) & \mbox{if $y=0$},\\
\dfrac{1-f_1(0)}{1-f_2(0)}f_2(y) & \mbox{if $y>0$}.
\end{array}
\right.
$$

Note that we are back to the standard Poisson model if $f_1 \equiv f_2$. This model is straightforwardly estimated by ML.


### Zero-inflated model

The objective is the same as for the Hurdle model, the modeling is slightly different. It is based on a mixture of a binary process $B$ (p.d.f. $f_1$) and a process $Z$ (p.d.f. $f_2$). $B$ and $Z$ are independent. Formally:
$$
y = B Z,
$$
implying:
$$
f(y) = \left\{
\begin{array}{lll}
f_1(0) + (1-f_1(0))f_2(0) & \mbox{if $y=0$},\\
(1-f_1(0))f_2(y) & \mbox{if $y>0$}.
\end{array}
\right.
$$
Typically, $f_1$ corresponds to a logit model and $f_2$ is Poisson or negative binomial density. This model is easily estimated by ML techniques.

:::{.example #Doctorvisits2 name="Number of doctor visits"}

Let us come back to the data used in Example \@ref(exm:Doctorvisits), and estimate Hurdle and a zero-inflation models:

<!-- # See Subsection 2.2 of https://cran.r-project.org/web/packages/pscl/vignettes/countreg.pdf -->

```{r countdata6}
model.hurdle <- 
  hurdle(mdvis ~ LC + idp + lpi + fmde + physlm + disea + hlthg + hlthf + 
           hlthp, data=randdata,
         dist = "poisson", zero.dist = "binomial", link = "logit")
model.zeroinfl <- zeroinfl(mdvis ~ LC + idp + lpi + fmde + physlm +
                             disea + hlthg + hlthf + hlthp, data=randdata,
                           dist = "poisson", link = "logit")
stargazer(model.hurdle,model.zeroinfl,zero.component=FALSE,
          no.space=TRUE,type="text")
stargazer(model.hurdle,model.zeroinfl,zero.component=TRUE,
          no.space=TRUE,type="text")
```

Let us test the importance of `LC` in the model using a Wald test:
```{r countdata7}
# Test whether LC is important in the model:
model.hurdle.reduced <- update(model.hurdle,.~.-LC)
lmtest::waldtest(model.hurdle, model.hurdle.reduced)
```

Finally, we compare average model-implied probabilities with the frequencies observed in the data:

```{r countdata8}
predprob.hurdle   <- predprob(model.hurdle)
predprob.zeroinfl <- predprob(model.zeroinfl)
print(rbind(prop.table.data[1:6],
  apply(predprob.poisson[,1:6],2,mean),
  apply(predprob.nb[,1:6],2,mean),
  apply(predprob.hurdle[,1:6],2,mean),
  apply(predprob.zeroinfl[,1:6],2,mean)))
```
:::

