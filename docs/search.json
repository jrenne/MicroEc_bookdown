[{"path":"index.html","id":"intro","chapter":"Micro-Econometrics","heading":"Micro-Econometrics","text":"microeconometric models, variables interest often feature restricted distributions—instance discontinuous support—, necessitates specific models. Typical examples discrete-choice models (binary, multinomial, ordered outcomes), sample selection models (censored truncated outcomes), count-data models (integer outcomes). course describes estimation interpretation models. also shows discrete-choice models can emerge (structural) random-utility frameworks.course developed Jean-Paul Renne. illustrated R codes using various packages can obtained CRAN. AEC package available GitHub. install , one need employ devtools library:Useful (R) links:Download R:\nR software: https://cran.r-project.org (basic R software)\nRStudio: https://www.rstudio.com (convenient R editor)\nDownload R:R software: https://cran.r-project.org (basic R software)RStudio: https://www.rstudio.com (convenient R editor)Tutorials:\nRstudio: https://dss.princeton.edu/training/RStudio101.pdf (Oscar Torres-Reyna)\nR: https://cran.r-project.org/doc/contrib/Paradis-rdebuts_en.pdf (Emmanuel Paradis)\ntutorial: https://jrenne.shinyapps.io/Rtuto_publiShiny/\nTutorials:Rstudio: https://dss.princeton.edu/training/RStudio101.pdf (Oscar Torres-Reyna)R: https://cran.r-project.org/doc/contrib/Paradis-rdebuts_en.pdf (Emmanuel Paradis)tutorial: https://jrenne.shinyapps.io/Rtuto_publiShiny/","code":"\ninstall.packages(\"devtools\") # in case this library has not been loaded yet\nlibrary(devtools)\ninstall_github(\"jrenne/AEC\")\nlibrary(AEC)"},{"path":"ChapterLS.html","id":"ChapterLS","chapter":"1 Linear Regressions","heading":"1 Linear Regressions","text":"Definition 1.1  linear regression model form:\n\\[\\begin{equation}\ny_i = \\boldsymbol\\beta'\\mathbf{x}_{} + \\varepsilon_i,\\tag{1.1}\n\\end{equation}\\]\n\\(\\mathbf{x}_{}=[x_{,1},\\dots,x_{,K}]'\\) vector dimension \\(K \\times 1\\).entity \\(\\), \\(x_{,k}\\)’s, \\(k \\\\{1,\\dots,K\\}\\), explanatory variables, regressors, covariates. variable interest, \\(y_i\\), often called dependent variable, regressand. last term specification, namely \\(\\varepsilon_i\\), called error, disturbance.researcher usually interested components vector \\(\\boldsymbol\\beta\\), denoted \\(\\beta_k\\), \\(k \\\\{1,\\dots,K\\}\\). usually aims estimating coefficients based observations \\(\\{y_i,\\mathbf{x}_{}\\}\\), \\(\\\\{1,\\dots,n\\}\\), constitutes sample. following, denote sample length \\(n\\).intercept specification (1.1), one set \\(x_{,1}=1\\) \\(\\); \\(\\beta_1\\) corresponds intercept.","code":""},{"path":"ChapterLS.html","id":"linearHyp","chapter":"1 Linear Regressions","heading":"1.1 Hypotheses","text":"section, introduce different assumptions regarding covariates /errors. properties estimators used researcher depend assumptions satisfied.Hypothesis 1.1  (Full rank) exact linear relationship among independent variables (\\(x_{,k}\\)’s, given \\(\\\\{1,\\dots,n\\}\\)).Intuitively, Hypothesis 1.1 satisfied, estimation model parameters unfeasible since, value \\(\\boldsymbol\\beta\\), changes explanatory variables exactly compensated changes another set explanatory variables, preventing identification effects.Let us denote \\(\\mathbf{X}\\) matrix containing explanatory variables, dimension \\(n \\times K\\). (, row \\(\\) \\(\\mathbf{X}\\) \\(\\mathbf{x}_i'\\).) following hypothesis concerns relationship errors (gathered \\(\\boldsymbol\\varepsilon\\), \\(n\\)-dimensional vector) explanatory variables \\(\\mathbf{X}\\):Hypothesis 1.2  (Conditional mean-zero assumption) \\[\\begin{equation}\n\\mathbb{E}(\\boldsymbol\\varepsilon|\\mathbf{X}) = 0.\n\\end{equation}\\]Hypothesis 1.2 important implications:Proposition 1.1  Hypothesis 1.2:\\(\\mathbb{E}(\\varepsilon_{})=0\\);\\(x_{ij}\\)’s \\(\\varepsilon_{}\\)’s uncorrelated, .e. \\(\\forall ,\\,j \\quad \\mathbb{C}orr(x_{ij},\\varepsilon_{})=0\\).Proof. Let us prove () (ii):law iterated expectations:\n\\[\n\\mathbb{E}(\\boldsymbol\\varepsilon)=\\mathbb{E}(\\mathbb{E}(\\boldsymbol\\varepsilon|\\mathbf{X}))=\\mathbb{E}(0)=0.\n\\]\\(\\mathbb{E}(x_{ij}\\varepsilon_i)=\\mathbb{E}(\\mathbb{E}(x_{ij}\\varepsilon_i|\\mathbf{X}))=\\mathbb{E}(x_{ij}\\underbrace{\\mathbb{E}(\\varepsilon_i|\\mathbf{X})}_{=0})=0\\).next two hypotheses (1.3 1.4) concern stochastic properties errors \\(\\varepsilon_i\\):Hypothesis 1.3  (Homoskedasticity) \\[\n\\forall , \\quad \\mathbb{V}ar(\\varepsilon_i|\\mathbf{X}) = \\sigma^2.\n\\]following lines code generate figure comparing two situations: Panel () Figure 1.1 corresponds situation homoskedasticity, Panel (b) corresponds situation heteroskedasticity. Let us specific. two plots, \\(X_i \\sim \\mathcal{N}(0,1)\\) \\(\\varepsilon^*_i \\sim \\mathcal{N}(0,1)\\). Panel () (homoskedasticity):\n\\[\nY_i = 2 + 2X_i + \\varepsilon^*_i.\n\\]\nPanel (b) (heteroskedasticity):\n\\[\nY_i = 2 + 2X_i + \\left(2\\mathbb{}_{\\{X_i<0\\}}+0.2\\mathbb{}_{\\{X_i\\ge0\\}}\\right)\\varepsilon^*_i\\].\nFigure 1.1: Homoskedasticity vs heteroskedasticity. See text exact specifications.\nFigure 1.2 shows real-data situation heteroskedasticity, based data taken Swiss Household Panel. sample restricted persons () younger 35 year 2019, (ii) completed least 19 years study. figure shows dispersion yearly income increases age.\nFigure 1.2: Income versus age. Data Swiss Household Panel. sample restricted persons completed least 19 years study. figure shows dispersion yearly income increases age.\nnext assumption concerns correlation errors across entities.Hypothesis 1.4  (Uncorrelated errors) \\[\n\\forall \\ne j, \\quad \\mathbb{C}ov(\\varepsilon_i,\\varepsilon_j|\\mathbf{X})=0.\n\\]often need work covariance matrix errors. Proposition 1.2 give specific form covariance matrix errors —conditional \\(\\mathbf{X}\\)— Hypotheses 1.3 1.4 satisfied:Proposition 1.2  Hypotheses 1.3 1.4 hold, :\n\\[\n\\mathbb{V}ar(\\boldsymbol\\varepsilon|\\mathbf{X})= \\sigma^2 Id,\n\\]\n\\(Id\\) \\(n \\times n\\) identity matrix.sometimes assume errors Gaussian—normal. invoke Hypothesis 1.5:Hypothesis 1.5  (Normal distribution) \\[\n\\forall , \\quad \\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2).\n\\]","code":"\nN <- 200\nX <- rnorm(N);eps <- rnorm(N)\npar(mfrow=c(1,2),plt=c(.2,.95,.2,.8))\nY <- 2 + 2*X + eps\nplot(X,Y,pch=19,main=\"(a) Homoskedasticity\",\n     las=1,cex.lab=.8,cex.axis=.8,cex.main=.8,)\nY <- 2 + 2*X + eps*( (X<0)*2 + (X>=0)*.2 )\nplot(X,Y,pch=19,main=\"(b) Heteroskedasticity\",\n     las=1,cex.lab=.8,cex.axis=.8,cex.main=.8,)\nlibrary(AEC)\ntable(shp$edyear19)## \n##    8    9   10   12   13   14   16   19   21 \n##   70  325  350 1985  454  117  990 1263  168\nshp_higherEd <- subset(shp,(edyear19>18)&age19<35)\nplot(i19wyg/1000~age19,data=shp_higherEd,pch=19,las=1,\n     xlab=\"Age\",ylab=\"Yearly work income\")\nabline(lm(i19wyg/1000~age19,data=shp_higherEd),col=\"red\",lwd=2)"},{"path":"ChapterLS.html","id":"LSquares","chapter":"1 Linear Regressions","heading":"1.2 Least square estimation","text":"","code":""},{"path":"ChapterLS.html","id":"derivation-of-the-ols-formula","chapter":"1 Linear Regressions","heading":"1.2.1 Derivation of the OLS formula","text":"section, present study properties popular estimation approach, namely Ordinary Least Squares (OLS) approach. suggested name, OLS estimator \\(\\boldsymbol\\beta\\) defined vector \\(\\mathbf{b}\\) minimizes sum squared residuals. (residuals estimates errors \\(\\varepsilon_i\\).)given vector coefficients \\(\\mathbf{b}=[b_1,\\dots,b_K]'\\), sum squared residuals :\n\\[\nf(\\mathbf{b}) =\\sum_{=1}^n \\left(y_i - \\sum_{j=1}^K x_{,j} b_j \\right)^2 = \\sum_{=1}^n (y_i - \\mathbf{x}_i' \\mathbf{b})^2.\n\\]\nMinimizing sum amounts minimizing:\n\\[\nf(\\mathbf{b}) = (\\mathbf{y} - \\mathbf{X}\\mathbf{b})'(\\mathbf{y} - \\mathbf{X}\\mathbf{b}).\n\\]Since (see Def. ?? Proposition 5.7):\n\\[\n\\frac{\\partial f}{\\partial \\mathbf{b}}(\\mathbf{b}) = - 2 \\mathbf{X}'\\mathbf{y} + 2 \\mathbf{X}'\\mathbf{X}\\mathbf{b},\n\\]\ncomes necessary first-order condition (FOC) :\n\\[\\begin{equation}\n\\mathbf{X}'\\mathbf{X}\\mathbf{b} = \\mathbf{X}'\\mathbf{y}.\\tag{1.2}\n\\end{equation}\\]\nAssumption 1.1, \\(\\mathbf{X}'\\mathbf{X}\\) invertible. Hence:\n\\[\n\\boxed{\\mathbf{b} = (\\mathbf{X}'\\mathbf{X})^{-1} \\mathbf{X}'\\mathbf{y}.}\n\\]\nVector \\(\\mathbf{b}\\) minimizes sum squared residuals. (\\(f\\) non-negative quadratic function, therefore admits minimum.):\n\\[\n\\mathbf{y} = \\underbrace{\\mathbf{X}\\mathbf{b}}_{\\mbox{fitted values } (\\hat{\\mathbf{y}})} + \\underbrace{\\mathbf{e}}_{\\mbox{residuals}}\n\\]estimated residuals :\n\\[\\begin{equation}\n\\mathbf{e} = \\mathbf{y} - \\mathbf{X} (\\mathbf{X}'\\mathbf{X})^{-1} \\mathbf{X}' \\mathbf{y} = \\mathbf{M} \\mathbf{y},\\tag{1.3}\n\\end{equation}\\]\n\\(\\mathbf{M} := \\mathbf{} - \\mathbf{X} (\\mathbf{X}'\\mathbf{X})^{-1} \\mathbf{X}'\\) called residual maker matrix.Moreover, fitted values \\(\\hat{\\mathbf{y}}\\) given :\n\\[\\begin{equation}\n\\hat{\\mathbf{y}}=\\mathbf{X} (\\mathbf{X}'\\mathbf{X})^{-1} \\mathbf{X}' \\mathbf{y} = \\mathbf{P} \\mathbf{y},\\tag{1.4}\n\\end{equation}\\]\n\\(\\mathbf{P}=\\mathbf{X} (\\mathbf{X}'\\mathbf{X})^{-1} \\mathbf{X}'\\) projection matrix.matrices \\(\\mathbf{M}\\) \\(\\mathbf{P}\\) :\\(\\mathbf{M} \\mathbf{X} = \\mathbf{0}\\): one regresses one explanatory variables \\(\\mathbf{X}\\), residuals null.\\(\\mathbf{M}\\mathbf{y}=\\mathbf{M}\\boldsymbol\\varepsilon\\) (\\(\\mathbf{y} = \\mathbf{X}\\boldsymbol\\beta + \\boldsymbol\\varepsilon\\) \\(\\mathbf{M} \\mathbf{X} = \\mathbf{0}\\)).additional properties \\(\\mathbf{M}\\) \\(\\mathbf{P}\\):\\(\\mathbf{M}\\) symmetric (\\(\\mathbf{M} = \\mathbf{M}'\\)) idempotent (\\(\\mathbf{M} = \\mathbf{M}^2 = \\mathbf{M}^k\\) \\(k>0\\)).\\(\\mathbf{P}\\) symmetric idempotent.\\(\\mathbf{P}\\mathbf{X} = \\mathbf{X}\\).\\(\\mathbf{P} \\mathbf{M} = \\mathbf{M} \\mathbf{P} = 0\\).\\(\\mathbf{y} = \\mathbf{P}\\mathbf{y} + \\mathbf{M}\\mathbf{y}\\) (decomposition \\(\\mathbf{y}\\) two orthogonal parts).easily checked \\(\\mathbf{X}'\\mathbf{e}=0\\). column \\(\\mathbf{X}\\) therefore orthogonal \\(\\mathbf{e}\\). particular, intercept included regression (\\(x_{,1} \\equiv 1\\) \\(\\)’s, .e., first column \\(\\mathbf{X}\\) filled ones), average residuals null.Example 1.1  (Bivariate case) Consider bivariate situation, regress \\(y_i\\) constant explanatory variable \\(w_i\\). \\(K=2\\), \\(\\mathbf{X}\\) \\(n \\times 2\\) matrix whose \\(^{th}\\) row \\([x_{,1},x_{,2}]\\), \\(x_{,1}=1\\) (account intercept) \\(w_i = x_{,2}\\) (say).:\n\\[\\begin{eqnarray*}\n\\mathbf{X}'\\mathbf{X} &=&\n\\left[\\begin{array}{cc}\nn & \\sum_i w_i \\\\\n\\sum_i w_i & \\sum_i w_i^2\n\\end{array}\n\\right],\\\\\n(\\mathbf{X}'\\mathbf{X})^{-1} &=&\n\\frac{1}{n\\sum_i w_i^2-(\\sum_i w_i)^2}\n\\left[\\begin{array}{cc}\n\\sum_i w_i^2 & -\\sum_i w_i \\\\\n-\\sum_i w_i & n\n\\end{array}\n\\right],\\\\\n(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\mathbf{y} &=&\n\\frac{1}{n\\sum_i w_i^2-(\\sum_i w_i)^2}\n\\left[\\begin{array}{c}\n\\sum_i w_i^2\\sum_i y_i -\\sum_i w_i \\sum_i w_iy_i \\\\\n-\\sum_i w_i \\sum_i y_i + n \\sum_i w_i y_i\n\\end{array}\n\\right]\\\\\n&=& \\frac{1}{\\frac{1}{n}\\sum_i(w_i - \\bar{w})^2}\n\\left[\\begin{array}{c}\n\\frac{\\bar{y}}{n}\\sum_i w_i^2 -\\frac{\\bar{w}}{n}\\sum_i w_iy_i \\\\\n\\frac{1}{n}\\sum_i (w_i-\\bar{w})(y_i-\\bar{y})\n\\end{array}\n\\right].\n\\end{eqnarray*}\\]can seen second element \\(\\mathbf{b}=(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\mathbf{y}\\) :\n\\[\nb_2 = \\frac{\\overline{\\mathbb{C}ov(W,Y)}}{\\overline{\\mathbb{V}ar(W)}},\n\\]\n\\(\\overline{\\mathbb{C}ov(W,Y)}\\) \\(\\overline{\\mathbb{V}ar(W)}\\) sample estimates.Since constant regression, \\(b_1 = \\bar{y} - b_2 \\bar{w}\\).","code":""},{"path":"ChapterLS.html","id":"properties-of-the-ols-estimate-small-sample","chapter":"1 Linear Regressions","heading":"1.2.2 Properties of the OLS estimate (small sample)","text":"OLS properties stated Proposition 1.3 valid sample size \\(n\\):Proposition 1.3  (Properties OLS estimator) :Assumptions 1.1 1.2, OLS estimator linear unbiased.Assumptions 1.1 1.2, OLS estimator linear unbiased.Hypotheses 1.1 1.4, conditional covariance matrix \\(\\mathbf{b}\\) : \\(\\mathbb{V}ar(\\mathbf{b}|\\mathbf{X}) = \\sigma^2 (\\mathbf{X}'\\mathbf{X})^{-1}\\).Hypotheses 1.1 1.4, conditional covariance matrix \\(\\mathbf{b}\\) : \\(\\mathbb{V}ar(\\mathbf{b}|\\mathbf{X}) = \\sigma^2 (\\mathbf{X}'\\mathbf{X})^{-1}\\).Proof. Hypothesis 1.1, \\(\\mathbf{X}'\\mathbf{X}\\) can inverted. :\n\\[\n\\mathbf{b} = (\\mathbf{X}'\\mathbf{X})^{-1} \\mathbf{X}'\\mathbf{y} = \\boldsymbol\\beta + (\\mathbf{X}'\\mathbf{X})^{-1} \\mathbf{X}' \\mathbf{\\varepsilon}.\n\\]Let us consider expectation last term, .e. \\(\\mathbb{E}((\\mathbf{X}'\\mathbf{X})^{-1} \\mathbf{X}' \\mathbf{\\varepsilon})\\). Using law iterated expectations, obtain:\n\\[\n\\mathbb{E}((\\mathbf{X}'\\mathbf{X})^{-1} \\mathbf{X}' \\mathbf{\\varepsilon}) = \\mathbb{E}(\\mathbb{E}[(\\mathbf{X}'\\mathbf{X})^{-1} \\mathbf{X}' \\mathbf{\\varepsilon}|\\mathbf{X}]) = \\mathbb{E}((\\mathbf{X}'\\mathbf{X})^{-1} \\mathbf{X}'\\mathbb{E}[\\mathbf{\\varepsilon}|\\mathbf{X}]).\n\\]\nHypothesis 1.2, \\(\\mathbb{E}[\\mathbf{\\varepsilon}|\\mathbf{X}]=0\\). Hence \\(\\mathbb{E}((\\mathbf{X}'\\mathbf{X})^{-1} \\mathbf{X}' \\mathbf{\\varepsilon}) =0\\) result () follows.\\(\\mathbb{V}ar(\\mathbf{b}|\\mathbf{X}) = (\\mathbf{X}'\\mathbf{X})^{-1} \\mathbf{X}' \\mathbb{E}(\\boldsymbol\\varepsilon\\boldsymbol\\varepsilon'|\\mathbf{X}) \\mathbf{X} (\\mathbf{X}'\\mathbf{X})^{-1}\\).\nProp. 1.2, 1.3 1.4 hold, \\(\\mathbb{E}(\\boldsymbol\\varepsilon\\boldsymbol\\varepsilon'|\\mathbf{X})=\\mathbb{V}ar(\\boldsymbol\\varepsilon|\\mathbf{X})=\\sigma^2 Id\\).Together, Hypotheses 1.1 1.4 form -called Gauss-Markov set assumptions. assumptions, OLS estimator feature lowest possible variance within family linear unbiased estimates \\(\\boldsymbol\\beta\\):Theorem 1.1  (Gauss-Markov Theorem) Assumptions 1.1 1.4, vector \\(w\\), minimum-variance linear unbiased estimator \\(w' \\boldsymbol\\beta\\) \\(w' \\mathbf{b}\\), \\(\\mathbf{b}\\) least squares estimator. (BLUE: Best Linear Unbiased Estimator.)Proof. Consider \\(\\mathbf{b}^* = C \\mathbf{y}\\), another linear unbiased estimator \\(\\boldsymbol\\beta\\). Since unbiased, must \\(\\mathbb{E}(C\\mathbf{y}|\\mathbf{X}) = \\mathbb{E}(C\\mathbf{X}\\boldsymbol\\beta + C\\boldsymbol\\varepsilon|\\mathbf{X}) = \\boldsymbol\\beta\\). \\(\\mathbb{E}(C\\boldsymbol\\varepsilon|\\mathbf{X})=C\\mathbb{E}(\\boldsymbol\\varepsilon|\\mathbf{X})=0\\) (1.2). Therefore \\(\\mathbf{b}^*\\) unbiased \\(\\mathbb{E}(C\\mathbf{X})\\boldsymbol\\beta=\\boldsymbol\\beta\\). case \\(\\boldsymbol\\beta\\), implies must \\(C\\mathbf{X}=\\mathbf{}\\). Let us compute \\(\\mathbb{V}ar(\\mathbf{b^*}|\\mathbf{X})\\). , introduce \\(D = C - (\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\), \\(D\\mathbf{y}=\\mathbf{b}^*-\\mathbf{b}\\). fact \\(C\\mathbf{X}=\\mathbf{}\\) implies \\(D\\mathbf{X} = \\mathbf{0}\\). \\(\\mathbb{V}ar(\\mathbf{b^*}|\\mathbf{X}) = \\mathbb{V}ar(C \\mathbf{y}|\\mathbf{X}) =\\mathbb{V}ar(C \\boldsymbol\\varepsilon|\\mathbf{X}) = \\sigma^2CC'\\) (Assumptions 1.3 1.4, see Prop. 1.2). Using \\(C=D+(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\) exploiting fact \\(D\\mathbf{X} = \\mathbf{0}\\) leads :\n\\[\n\\mathbb{V}ar(\\mathbf{b^*}|\\mathbf{X}) =\\sigma^2\\left[(D+(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}')(D+(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}')'\\right] = \\mathbb{V}ar(\\mathbf{b}|\\mathbf{X}) + \\sigma^2 \\mathbf{D}\\mathbf{D}'.\n\\]\nTherefore, \n\\[\\begin{eqnarray*}\n&&\\mathbb{V}ar(w'\\mathbf{b^*}|\\mathbf{X})=w'\\mathbb{V}ar(\\mathbf{b}|\\mathbf{X})w + \\sigma^2 w'\\mathbf{D}\\mathbf{D}'w\\\\\n&\\ge& w'\\mathbb{V}ar(\\mathbf{b}|\\mathbf{X})w=\\mathbb{V}ar(w'\\mathbf{b}|\\mathbf{X}).\n\\end{eqnarray*}\\]Frish-Waugh theorem (Theorem 1.2) reveals relationship OLS estimator notion partial correlation coefficient. Consider linear least square regression \\(\\mathbf{y}\\) \\(\\mathbf{X}\\). introduce notations:\\(\\mathbf{b}^{\\mathbf{y}/\\mathbf{X}}\\): OLS estimates \\(\\boldsymbol\\beta\\),\\(\\mathbf{M}^{\\mathbf{X}}\\): residual-maker matrix regression \\(\\mathbf{X}\\) (given \\(\\mathbf{} - \\mathbf{X} (\\mathbf{X}'\\mathbf{X})^{-1} \\mathbf{X}'\\)),\\(\\mathbf{P}^{\\mathbf{X}}\\): projection matrix regression \\(\\mathbf{X}\\) (given \\(\\mathbf{X} (\\mathbf{X}'\\mathbf{X})^{-1} \\mathbf{X}'\\)).Let us split set explanatory variables two: \\(\\mathbf{X} = [\\mathbf{X}_1,\\mathbf{X}_2]\\). obvious notations: \\(\\mathbf{b}^{\\mathbf{y}/\\mathbf{X}}=[\\mathbf{b}_1',\\mathbf{b}_2']'\\).Theorem 1.2  (Frisch-Waugh Theorem) :\n\\[\n\\mathbf{b}_2 = \\mathbf{b}^{\\mathbf{M^{\\mathbf{X}_1}y}/\\mathbf{M^{\\mathbf{X}_1}\\mathbf{X}_2}}.\n\\]Proof. minimization least squares leads (first-order conditions, see Eq. (1.2)):\n\\[\n\\left[ \\begin{array}{cc} \\mathbf{X}_1'\\mathbf{X}_1 & \\mathbf{X}_1'\\mathbf{X}_2 \\\\ \\mathbf{X}_2'\\mathbf{X}_1 & \\mathbf{X}_2'\\mathbf{X}_2\\end{array}\\right]\n\\left[ \\begin{array}{c} \\mathbf{b}_1 \\\\ \\mathbf{b}_2\\end{array}\\right] =\n\\left[ \\begin{array}{c} \\mathbf{X}_1' \\mathbf{y} \\\\ \\mathbf{X}_2' \\mathbf{y} \\end{array}\\right].\n\\]\nUse first-row block equations solve \\(\\mathbf{b}_1\\) first; comes function \\(\\mathbf{b}_2\\). use second set equations solve \\(\\mathbf{b}_2\\), leads :\n\\[\\begin{eqnarray*}\n\\mathbf{b}_2 &=& [\\mathbf{X}_2'\\mathbf{X}_2 - \\mathbf{X}_2'\\mathbf{X}_1(\\mathbf{X}_1'\\mathbf{X}_1)\\mathbf{X}_1'\\mathbf{X}_2]^{-1}\\mathbf{X}_2'(Id - \\mathbf{X}_1(\\mathbf{X}_1'\\mathbf{X}_1)\\mathbf{X}_1')\\mathbf{y}\\\\\n&=& [\\mathbf{X}_2' \\mathbf{M}^{\\mathbf{X}_1}\\mathbf{X}_2]^{-1}\\mathbf{X}_2'\\mathbf{M}^{\\mathbf{X}_1}\\mathbf{y}.\n\\end{eqnarray*}\\]\nUsing fact \\(\\mathbf{M}^{\\mathbf{X}_1}\\) idempotent symmetric leads result.suggests second way estimating \\(\\mathbf{b}_2\\):Regress \\(Y\\) \\(X_1\\), regress \\(X_2\\) \\(X_1\\).Regress residuals associated former regression associated latter regressions.illustrated following code, run different regressions involving number Google searches “parapluie” (umbrella French). broad specification, regress French precipitations month dummies. Next, deseasonalize dependent variable precipitations regressing month dummies. stated Theorem 1.2, regressing deseasonalized Google searches deseasonalized precipitations give coefficient baseline regression.\\(b_2\\) scalar (\\(\\mathbf{X}_2\\) dimension \\(n \\times 1\\)), Theorem 1.2 gives expression partial regression coefficient \\(b_2\\):\n\\[\nb_2 = \\frac{\\mathbf{X}_2'M^{\\mathbf{X}_1}\\mathbf{y}}{\\mathbf{X}_2'M^{\\mathbf{X}_1}\\mathbf{X}_2}.\n\\]","code":"\nlibrary(AEC)\ndummies <- as.matrix(parapluie[,4:14])\neq_all <- lm(parapluie~dummies+precip,data=parapluie)\ndeseas_parapluie <- lm(parapluie~dummies,data=parapluie)$residuals\ndeseas_precip    <- lm(precip~dummies,data=parapluie)$residuals\neq_frac <- lm(deseas_parapluie~deseas_precip-1)\nstargazer::stargazer(eq_all,eq_frac,omit=c(1:11,\"Constant\"),type=\"text\",\n                     omit.stat = c(\"f\",\"ser\"),digits=5,\n                     add.lines=list(c('Monthly dummy','Yes','No')))## \n## ==========================================\n##                   Dependent variable:     \n##               ----------------------------\n##                parapluie  deseas_parapluie\n##                   (1)           (2)       \n## ------------------------------------------\n## precip        0.13001***                  \n##                (0.03594)                  \n##                                           \n## deseas_precip                0.13001***   \n##                              (0.03277)    \n##                                           \n## ------------------------------------------\n## Monthly dummy     Yes            No       \n## Observations      72             72       \n## R2              0.51793       0.18148     \n## Adjusted R2     0.41988       0.16995     \n## ==========================================\n## Note:          *p<0.1; **p<0.05; ***p<0.01"},{"path":"ChapterLS.html","id":"goodness-of-fit","chapter":"1 Linear Regressions","heading":"1.2.3 Goodness of fit","text":"Define total variation \\(y\\) sum squared deviations (sample mean):\n\\[\nTSS = \\sum_{=1}^{n} (y_i - \\bar{y})^2.\n\\]\n:\n\\[\n\\mathbf{y} = \\mathbf{X}\\mathbf{b} + \\mathbf{e} = \\hat{\\mathbf{y}} + \\mathbf{e}\n\\]\nfollowing, assume regression includes constant (.e. \\(\\), \\(x_{,1}=1\\)). Denote \\(\\mathbf{M}^0\\) matrix transforms observations deviations sample means. Using \\(\\mathbf{M}^0 \\mathbf{e} = \\mathbf{e}\\) \\(\\mathbf{X}' \\mathbf{e}=0\\), :\n\\[\\begin{eqnarray*}\n\\underbrace{\\mathbf{y}'\\mathbf{M}^0\\mathbf{y}}_{\\mbox{Total sum sq.}} &=& (\\mathbf{X}\\mathbf{b} + \\mathbf{e})' \\mathbf{M}^0 (\\mathbf{X}\\mathbf{b} + \\mathbf{e})\\\\\n&=& \\underbrace{\\mathbf{b}' \\mathbf{X}' \\mathbf{M}^0 \\mathbf{X}\\mathbf{b}}_{\\mbox{\"Explained\" sum sq.}} + \\underbrace{\\mathbf{e}'\\mathbf{e}}_{\\mbox{Sum sq. residuals}}\\\\\nTSS &=& Expl.SS + SSR.\n\\end{eqnarray*}\\]can now define coefficient determination:\n\\[\\begin{equation}\n\\boxed{\\mbox{Coefficient determination} = \\frac{Expl.SS}{TSS} = 1 - \\frac{SSR}{TSS} = 1 - \\frac{\\mathbf{e}'\\mathbf{e}}{\\mathbf{y}'\\mathbf{M}^0\\mathbf{y}}.}\\tag{1.5}\n\\end{equation}\\]can shown (Greene (2003), Section 3.5) :\n\\[\n\\mbox{Coefficient determination} = \\frac{[\\sum_{=1}^n(y_i - \\bar{y})(\\hat{y_i} - \\bar{y})]^2}{\\sum_{=1}^n(y_i - \\bar{y})^2 \\sum_{=1}^n(\\hat{y_i} - \\bar{y})^2}.\n\\]\n, \\(R^2\\) sample squared correlation \\(y\\) (regression-implied) \\(y\\)’s predictions.hgher \\(R^2\\), higher goodness fit model. One however cautious \\(R^2\\). Indeed, easy increase : suffices add explanatory variables. stated Proposition 1.5, adding explanatory variable (even truly relate dependent variable) mechanically results increase \\(R^2\\). limit, taking set \\(n\\) non-linearly-dependent explanatory variables (.e., variables satisfying Hypothesis 1.1) results \\(R^2\\) equal one.Proposition 1.4  (Change SSR variable added) :\n\\[\\begin{equation}\n\\mathbf{u}'\\mathbf{u} = \\mathbf{e}'\\mathbf{e} - c^2(\\mathbf{z^*}'\\mathbf{z^*}) \\qquad (\\le \\mathbf{e}'\\mathbf{e}) \\tag{1.6}\n\\end{equation}\\]\n() \\(\\mathbf{u}\\) \\(\\mathbf{e}\\) residuals regressions \\(\\mathbf{y}\\) \\([\\mathbf{X},\\mathbf{z}]\\) \\(\\mathbf{y}\\) \\(\\mathbf{X}\\), respectively, (ii) \\(c\\) regression coefficient \\(\\mathbf{z}\\) former regression \\(\\mathbf{z}^*\\) residuals regression \\(\\mathbf{z}\\) \\(\\mathbf{X}\\).Proof. OLS estimates \\([\\mathbf{d}',\\mathbf{c}]'\\) regression \\(\\mathbf{y}\\) \\([\\mathbf{X},\\mathbf{z}]\\) satisfies (first-order cond., Eq. (1.2)):\n\\[\n\\left[ \\begin{array}{cc} \\mathbf{X}'\\mathbf{X} & \\mathbf{X}'\\mathbf{z} \\\\ \\mathbf{z}'\\mathbf{X} & \\mathbf{z}'\\mathbf{z}\\end{array}\\right]\n\\left[ \\begin{array}{c} \\mathbf{d} \\\\ c\\end{array}\\right] =\n\\left[ \\begin{array}{c} \\mathbf{X}' \\mathbf{y} \\\\ \\mathbf{z}' \\mathbf{y} \\end{array}\\right].\n\\]\nHence, particular \\(\\mathbf{d} = \\mathbf{b} - (\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\mathbf{z}c\\), \\(\\mathbf{b}\\) OLS \\(\\mathbf{y}\\) \\(\\mathbf{X}\\). Substituting \\(\\mathbf{u} = \\mathbf{y} - \\mathbf{X}\\mathbf{d} - \\mathbf{z}c\\), get \\(\\mathbf{u} = \\mathbf{e} - \\mathbf{z}^*c\\). therefore :\n\\[\\begin{equation}\n\\mathbf{u}'\\mathbf{u} = (\\mathbf{e} - \\mathbf{z}^*c)(\\mathbf{e} - \\mathbf{z}^*c)= \\mathbf{e}'\\mathbf{e} + c^2(\\mathbf{z^*}'\\mathbf{z^*}) - 2 c\\mathbf{z^*}'\\mathbf{e}.\\tag{1.7}\n\\end{equation}\\]\nNow \\(\\mathbf{z^*}'\\mathbf{e} = \\mathbf{z^*}'(\\mathbf{y} - \\mathbf{X}\\mathbf{b}) = \\mathbf{z^*}'\\mathbf{y}\\) \\(\\mathbf{z}^*\\) residuals OLS regression \\(\\mathbf{X}\\). Since \\(c = (\\mathbf{z^*}'\\mathbf{z^*})^{-1}\\mathbf{z^*}'\\mathbf{y^*}\\) (application Theorem 1.2), \\((\\mathbf{z^*}'\\mathbf{z^*})c = \\mathbf{z^*}'\\mathbf{y^*}\\) , therefore, \\(\\mathbf{z^*}'\\mathbf{e} = (\\mathbf{z^*}'\\mathbf{z^*})c\\). Inserting Eq. (1.7) leads results.Proposition 1.5  (Change coefficient determination variable added) Denoting \\(R_W^2\\) coefficient determination regression \\(\\mathbf{y}\\) variable \\(\\mathbf{W}\\), :\n\\[\nR_{\\mathbf{X},\\mathbf{z}}^2 = R_{\\mathbf{X}}^2 + (1-R_{\\mathbf{X}}^2)(r_{yz}^\\mathbf{X})^2,\n\\]\n\\(r_{yz}^\\mathbf{X}\\) coefficient partial correlation (see Definition 5.5).Proof. Let’s use notations Prop. 1.4. Theorem 1.2 implies \\(c = (\\mathbf{z^*}'\\mathbf{z^*})^{-1}\\mathbf{z^*}'\\mathbf{y^*}\\). Using Eq. (1.6) gives \\(\\mathbf{u}'\\mathbf{u} = \\mathbf{e}'\\mathbf{e} - (\\mathbf{z^*}'\\mathbf{y^*})^2/(\\mathbf{z^*}'\\mathbf{z^*})\\). Using definition partial correlation (Eq. (5.2)), get \\(\\mathbf{u}'\\mathbf{u} = \\mathbf{e}'\\mathbf{e}\\left(1 - (r_{yz}^\\mathbf{X})^2\\right)\\). results obtained dividing sides previous equation \\(\\mathbf{y}'\\mathbf{M}_0\\mathbf{y}\\).Figure 1.3, , illustrates fact one can obtain \\(R^2\\) one regressing sample length \\(n\\) set \\(n\\) linearly-independent variables.\nFigure 1.3: figure illustrates monotonous increase \\(R^2\\) function number explanatory variables. true model, explanatory variables, .e., \\(y_i = \\varepsilon_i\\). take (independent) regressors regress \\(y\\) latter, progressively increasing set regressors.\norder address risk adding irrelevant explanatory variables, measures adjusted \\(R^2\\) proposed. Compared standard \\(R^2\\), measures add penalties depend number covariates employed regression. common adjusted \\(R^2\\) measure, denoted \\(\\bar{R}^2\\), following:\n\\[\\begin{equation*}\n\\boxed{\\bar{R}^2 = 1 - \\frac{\\mathbf{e}'\\mathbf{e}/(n-K)}{\\mathbf{y}'\\mathbf{M}^0\\mathbf{y}/(n-1)} = 1 - \\frac{n-1}{n-K}(1-R^2).}\n\\end{equation*}\\]","code":"\nn <- 30;Y <- rnorm(n);X <- matrix(rnorm(n^2),n,n)\nall_R2 <- NULL;all_adjR2 <- NULL\nfor(j in 0:(n-1)){\n  if(j==0){eq <- lm(Y~1)}else{eq <- lm(Y~X[,1:j])}\n  all_R2 <- c(all_R2,summary(eq)$r.squared)\n  all_adjR2 <- c(all_adjR2,summary(eq)$adj.r.squared)\n}\npar(plt=c(.15,.95,.25,.95))\nplot(all_R2,pch=19,ylim=c(min(all_adjR2,na.rm = TRUE),1),\n     xlab=\"number of regressors\",ylab=\"R2\")\npoints(all_adjR2,pch=3);abline(h=0,col=\"light grey\",lwd=2)\nlegend(\"topleft\",c(\"R2\",\"Adjusted R2\"),\n       lty=NaN,col=c(\"black\"),pch=c(19,3),lwd=2)"},{"path":"ChapterLS.html","id":"inference-and-confidence-intervals-in-small-sample","chapter":"1 Linear Regressions","heading":"1.2.4 Inference and confidence intervals (in small sample)","text":"normality assumption (Assumption 1.5), know distribution \\(\\mathbf{b}\\) (conditional \\(\\mathbf{X}\\)). Indeed, \\(\\mathbf{b} = \\boldsymbol\\beta + (\\mathbf{X}'\\mathbf{X})^{-1} \\mathbf{X}'\\boldsymbol\\varepsilon\\). Therefore, conditional \\(\\mathbf{X}\\), vector \\(\\mathbf{b}\\) affine combination Gaussian variables—components \\(\\boldsymbol\\varepsilon\\). result, also Gaussian. distribution therefore completely characterized mean variance, :\n\\[\\begin{equation}\n\\mathbf{b}|\\mathbf{X} \\sim \\mathcal{N}(\\boldsymbol\\beta,\\sigma^2(\\mathbf{X}'\\mathbf{X})^{-1}).\\tag{1.8}\n\\end{equation}\\]Eq. (1.8) can used conduct inference tests. However, practice, know \\(\\sigma^2\\) (population parameter). following proposition gives unbiased estimate \\(\\sigma^2\\).Proposition 1.6  1.1 1.4, unbiased estimate \\(\\sigma^2\\) given :\n\\[\\begin{equation}\ns^2 = \\frac{\\mathbf{e}'\\mathbf{e}}{n-K}.\\tag{1.9}\n\\end{equation}\\]\n(sometimes denoted \\(\\sigma^2_{OLS}\\).)Proof. :\n\\[\\begin{eqnarray*}\n\\mathbb{E}(\\mathbf{e}'\\mathbf{e}|\\mathbf{X})&=&\\mathbb{E}(\\boldsymbol{\\varepsilon}'\\mathbf{M}\\boldsymbol{\\varepsilon}|\\mathbf{X})=\\mathbb{E}(\\mbox{Tr}(\\boldsymbol{\\varepsilon}'\\mathbf{M}\\boldsymbol{\\varepsilon})|\\mathbf{X}))\\\\\n&=&\\mbox{Tr}(\\mathbf{M}\\mathbb{E}(\\boldsymbol{\\varepsilon}\\boldsymbol{\\varepsilon}'|\\mathbf{X}))=\\sigma^2 \\mbox{Tr}(\\mathbf{M}).\n\\end{eqnarray*}\\]\n(Note \\(\\mathbb{E}(\\boldsymbol{\\varepsilon}\\boldsymbol{\\varepsilon}'|\\mathbf{X})=\\sigma^2Id\\) Assumptions 1.3 1.4, see Prop. 1.2.) Moreover:\n\\[\\begin{eqnarray*}\n\\mbox{Tr}(\\mathbf{M})&=&n-\\mbox{Tr}(\\mathbf{X}(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}')\\\\\n&=&n-\\mbox{Tr}((\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\mathbf{X})=n-\\mbox{Tr}(Id_{K \\times K}),\n\\end{eqnarray*}\\]\nleads result.Two results prove important produce inference:know conditional distribution \\(s^2\\) (Prop. 1.7).\\(s^2\\) \\(\\mathbf{b}\\) independent random variables (Prop. 1.8).Proposition 1.7  1.1 1.5, : \\(\\dfrac{s^2}{\\sigma^2} | \\mathbf{X} \\sim \\frac{1}{n-K}\\chi^2(n-K)\\).Proof. \\(\\mathbf{e}'\\mathbf{e}=\\boldsymbol\\varepsilon'\\mathbf{M}\\boldsymbol\\varepsilon\\). \\(\\mathbf{M}\\) idempotent symmetric matrix. Therefore can decomposed \\(PDP'\\) \\(D\\) diagonal matrix \\(P\\) orthogonal matrix. result \\(\\mathbf{e}'\\mathbf{e} = (P'\\boldsymbol\\varepsilon)'D(P'\\boldsymbol\\varepsilon)\\), .e. \\(\\mathbf{e}'\\mathbf{e}\\) weighted sum independent squared Gaussian variables (entries \\(P'\\boldsymbol\\varepsilon\\) independent Gaussian —1.5— uncorrelated). variance ..d. Gaussian variable \\(\\sigma^2\\). \\(\\mathbf{M}\\) idempotent symmetric matrix, eigenvalues either 0 1, rank equals trace (see Propositions 5.3 5.4). , trace equal \\(n-K\\) (see proof Eq. (1.9)). Therefore \\(D\\) \\(n-K\\) entries equal 1 \\(K\\) equal 0. Hence, \\(\\mathbf{e}'\\mathbf{e} = (P'\\boldsymbol\\varepsilon)'D(P'\\boldsymbol\\varepsilon)\\) sum \\(n-K\\) squared independent Gaussian variables variance \\(\\sigma^2\\). Therefore \\(\\frac{\\mathbf{e}'\\mathbf{e}}{\\sigma^2} = (n-K)\\frac{s^2}{\\sigma^2}\\) sum \\(n-k\\) squared ..d. standard normal variables. result follows definition chi-square distribution (see Def. 5.13).Proposition 1.8  Hypotheses 1.1 1.5, \\(\\mathbf{b}\\) \\(s^2\\) independent.Proof. \\(\\mathbf{b}=\\boldsymbol\\beta + [\\mathbf{X}'{\\mathbf{X}}]^{-1}\\mathbf{X}\\boldsymbol\\varepsilon\\) \\(s^2 = \\boldsymbol\\varepsilon' \\mathbf{M} \\boldsymbol\\varepsilon/(n-K)\\). Hence \\(\\mathbf{b}\\) affine combination \\(\\boldsymbol\\varepsilon\\) \\(s^2\\) quadratic combination Gaussian shocks. One can write \\(s^2\\) \\(s^2 = (\\mathbf{M}\\boldsymbol\\varepsilon)' \\mathbf{M} \\boldsymbol\\varepsilon/(n-K)\\) \\(\\mathbf{b}\\) \\(\\boldsymbol\\beta + \\mathbf{T}\\boldsymbol\\varepsilon\\). Since \\(\\mathbf{T}\\mathbf{M}=0\\), \\(\\mathbf{T}\\boldsymbol\\varepsilon\\) \\(\\mathbf{M}\\boldsymbol\\varepsilon\\) independent (two uncorrelated Gaussian variables independent), therefore \\(\\mathbf{b}\\) \\(s^2\\), functions two sets independent variables, independent.Consistently Eq. (1.8), Hypotheses 1.1 1.5, \\(k^{th}\\) entry \\(\\mathbf{b}\\) satisfies:\n\\[\nb_k | \\mathbf{X} \\sim \\mathcal{N}(\\beta_k,\\sigma^2 v_k),\n\\]\n\\(v_k\\) k\\(^{th}\\) component diagonal \\((\\mathbf{X}'\\mathbf{X})^{-1}\\).Moreover, (Prop. 1.7):\n\\[\n\\frac{(n-K)s^2}{\\sigma^2} | \\mathbf{X} \\sim \\chi ^2 (n-K).\n\\]result (using Propositions 1.7 1.8), :\n\\[\\begin{equation}\n\\boxed{t_k = \\frac{\\frac{b_k - \\beta_k}{\\sqrt{\\sigma^2 v_k}}}{\\sqrt{\\frac{(n-K)s^2}{\\sigma^2(n-K)}}} = \\frac{b_k - \\beta_k}{\\sqrt{s^2v_k}} \\sim t(n-K),}\\tag{1.10}\n\\end{equation}\\]\n\\(t(n-K)\\) denotes Student \\(t\\) distribution \\(n-K\\) degrees freedom (see Def. 5.12).1Note \\(s^2 v_k\\) exactly conditional variance \\(b_k\\): variance \\(b_k\\) conditional \\(\\mathbf{X}\\) \\(\\sigma^2 v_k\\). However \\(s^2 v_k\\) unbiased estimate \\(\\sigma^2 v_k\\) (Prop. 1.6).previous result (Eq. (1.10)) can extended linear combinations elements \\(\\mathbf{b}\\). (Eq. (1.10) \\(k^{th}\\) component .) Let us consider \\(\\boldsymbol\\alpha'\\mathbf{b}\\), OLS estimate \\(\\boldsymbol\\alpha'\\boldsymbol\\beta\\). Eq. (1.8), :\n\\[\n\\boldsymbol\\alpha'\\mathbf{b} | \\mathbf{X} \\sim \\mathcal{N}(\\boldsymbol\\alpha'\\boldsymbol\\beta,\\sigma^2 \\boldsymbol\\alpha'(\\mathbf{X}'\\mathbf{X})^{-1}\\boldsymbol\\alpha).\n\\]\nTherefore:\n\\[\n\\frac{\\boldsymbol\\alpha'\\mathbf{b} - \\boldsymbol\\alpha'\\boldsymbol\\beta}{\\sqrt{\\sigma^2 \\boldsymbol\\alpha'(\\mathbf{X}'\\mathbf{X})^{-1}\\boldsymbol\\alpha}} | \\mathbf{X} \\sim \\mathcal{N}(0,1).\n\\]\nUsing approach one used derive Eq. (1.10), one can show Props. 1.7 1.8 also imply :\n\\[\\begin{equation}\n\\boxed{\\frac{\\boldsymbol\\alpha'\\mathbf{b} - \\boldsymbol\\alpha'\\boldsymbol\\beta}{\\sqrt{s^2\\boldsymbol\\alpha'(\\mathbf{X}'\\mathbf{X})^{-1}\\boldsymbol\\alpha}} \\sim t(n-K).}\\tag{1.11}\n\\end{equation}\\]\nFigure 1.4: higher degree freedom, closer distribution \\(t(\\nu)\\) gets normal distribution. (Convergence distribution.)\nprecedes widely exploited statistical inference context linear regressions. Indeed, Eq. (1.10) gives sense distances \\(b_k\\) \\(\\beta_k\\) can deemed “likely” (, conversely, “unlikely”). instance, implies , \\(\\sqrt{v_k s^2}\\) equal 1 (say), probability obtain \\(b_k\\) smaller \\(\\beta_k-\\) 4.587 \\(\\times \\sqrt{v_k s^2}\\) larger \\(\\beta_k+\\) 4.587 \\(\\times \\sqrt{v_k s^2}\\) equal 0.1% \\(n-K=10\\).means instance , assumption \\(\\beta_k=0\\), extremely unlikely obtained \\(b_k/\\sqrt{v_k s^2}\\) smaller -4.587 larger 4.587. generally, shows t-statistic, .e., ratio \\(b_k/\\sqrt{v_k s^2}\\), test statistic associated null hypothesis:\n\\[\nH_0: \\beta_k=0.\n\\]\nnull hypothesis, test statistic follows Student-t distribution \\(n-K\\) degrees freedom. t-statistic therefore particular importance, , result, routinely reported regression outputs (see Example 1.2).Example 1.2  (Education income) Consider regression aims determining covariates households’ income. example makes use data Swiss Household Panel (SHP); edyear19 number years education age19 age respondent, 2019.last two columns previous table give t-statistic p-values associated t-tests, whose size-\\(\\alpha\\) critical region :\n\\[\n\\left]-\\infty,-\\Phi^{-1}_{t(n-K)}\\left(1-\\frac{\\alpha}{2}\\right)\\right] \\cup \\left[\\Phi^{-1}_{t(n-K)}\\left(1-\\frac{\\alpha}{2}\\right),+\\infty\\right[.\n\\]recall p-value defined probability \\(|Z| > |t|\\), \\(t\\) (computed) t-statistics \\(Z \\sim t(n-K)\\). , context t-test, p-value given \\(2(1 - \\Phi_{t(n-K)}(|t_k|))\\). See webpage details regarding link critical regions, p-value, test outcomes.Now, suppose want compute (symmetrical) confidence interval \\([I_{d,1-\\alpha},I_{u,1-\\alpha}]\\) \\(\\mathbb{P}(\\beta_k \\[I_{d,1-\\alpha},I_{u,1-\\alpha}])=1-\\alpha\\). , want : \\(\\mathbb{P}(\\beta_k < I_{d,1-\\alpha})=\\frac{\\alpha}{2}\\) \\(\\mathbb{P}(\\beta_k > I_{u,1-\\alpha})=\\frac{\\alpha}{2}\\). Let us focus \\(I_{d,1-\\alpha}\\) start . Using Eq. (1.10), .e., \\(t_k = \\frac{b_k - \\beta_k}{\\sqrt{s^2v_k}} \\sim t(n-K)\\), :\n\\[\\begin{eqnarray*}\n\\mathbb{P}(\\beta_k < I_{d,1-\\alpha})=\\frac{\\alpha}{2} &\\Leftrightarrow& \\\\\n\\mathbb{P}\\left(\\frac{b_k - \\beta_k}{\\sqrt{s^2v_k}} > \\frac{b_k - I_{d,1-\\alpha}}{\\sqrt{s^2v_k}}\\right)=\\frac{\\alpha}{2} &\\Leftrightarrow& \\mathbb{P}\\left(t_k > \\frac{b_k - I_{d,1-\\alpha}}{\\sqrt{s^2v_k}}\\right)=\\frac{\\alpha}{2} \\Leftrightarrow\\\\\n1 - \\mathbb{P}\\left(t_k \\le \\frac{b_k - I_{d,1-\\alpha}}{\\sqrt{s^2v_k}}\\right)=\\frac{\\alpha}{2} &\\Leftrightarrow& \\frac{b_k - I_{d,1-\\alpha}}{\\sqrt{s^2v_k}} = \\Phi^{-1}_{t(n-K)}\\left(1-\\frac{\\alpha}{2}\\right),\n\\end{eqnarray*}\\]\n\\(\\Phi_{t(n-K)}(\\alpha)\\) c.d.f. \\(t(n-K)\\) distribution (Table 5.2).\\(I_{u,1-\\alpha}\\), obtain:\n\\[\\begin{eqnarray*}\n&&[I_{d,1-\\alpha},I_{u,1-\\alpha}] =\\\\\n&&\\left[b_k - \\Phi^{-1}_{t(n-K)}\\left(1-\\frac{\\alpha}{2}\\right)\\sqrt{s^2v_k},b_k + \\Phi^{-1}_{t(n-K)}\\left(1-\\frac{\\alpha}{2}\\right)\\sqrt{s^2v_k}\\right].\n\\end{eqnarray*}\\]Using results presented Example 1.2, can compute lower upper bounds 95% confidence intervals estimated parameters follows:","code":"\nlibrary(AEC)\nlibrary(sandwich)\nshp$income <- shp$i19ptotn/1000\nshp$female <- 1*(shp$sex19==2)\neq <- lm(income ~ edyear19 + age19 + I(age19^2) + female,data=shp)\nlmtest::coeftest(eq)## \n## t test of coefficients:\n## \n##                Estimate  Std. Error t value  Pr(>|t|)    \n## (Intercept) -71.9738073   5.7082456 -12.609 < 2.2e-16 ***\n## edyear19      4.8442661   0.2172320  22.300 < 2.2e-16 ***\n## age19         3.2386215   0.2183812  14.830 < 2.2e-16 ***\n## I(age19^2)   -0.0289498   0.0020915 -13.842 < 2.2e-16 ***\n## female      -31.8089006   1.4578004 -21.820 < 2.2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nn <- length(eq$residuals); K <- length(eq$coefficients)\nlower.b <- eq$coefficients + qt(.025,df=n-K)*sqrt(diag(vcov(eq)))\nupper.b <- eq$coefficients + qt(.975,df=n-K)*sqrt(diag(vcov(eq)))\ncbind(lower.b,upper.b)##                  lower.b      upper.b\n## (Intercept) -83.16413225 -60.78348237\n## edyear19      4.41840914   5.27012310\n## age19         2.81051152   3.66673148\n## I(age19^2)   -0.03304986  -0.02484977\n## female      -34.66674188 -28.95105932"},{"path":"ChapterLS.html","id":"Ftest","chapter":"1 Linear Regressions","heading":"1.2.5 Testing a set of linear restrictions","text":"sometimes want test set restrictions jointly consistent data hand. Let us formalize set (\\(J\\)) linear restrictions:\n\\[\\begin{equation}\\label{eq:restrictions}\n\\begin{array}{ccc}\nr_{1,1} \\beta_1 + \\dots + r_{1,K} \\beta_K &=& q_1\\\\\n\\vdots && \\vdots\\\\\nr_{J,1} \\beta_1 + \\dots + r_{J,K} \\beta_K &=& q_J.\n\\end{array}\n\\end{equation}\\]\nmatrix form, get:\n\\[\\begin{equation}\n\\mathbf{R}\\boldsymbol\\beta = \\mathbf{q}.\n\\end{equation}\\]Define discrepancy vector \\(\\mathbf{m} = \\mathbf{R}\\mathbf{b} - \\mathbf{q}\\). null hypothesis:\n\\[\\begin{eqnarray*}\n\\mathbb{E}(\\mathbf{m}|\\mathbf{X}) &=& \\mathbf{R}\\boldsymbol\\beta - \\mathbf{q} = 0 \\quad \\mbox{} \\\\\n\\mathbb{V}ar(\\mathbf{m}|\\mathbf{X}) &=& \\mathbf{R} \\mathbb{V}ar(\\mathbf{b}|\\mathbf{X}) \\mathbf{R}'.\n\\end{eqnarray*}\\]notations, assumption test :\n\\[\\begin{equation}\n\\boxed{H_0: \\mathbf{R}\\boldsymbol\\beta - \\mathbf{q} = 0 \\mbox{ } H_1: \\mathbf{R}\\boldsymbol\\beta - \\mathbf{q} \\ne 0.}\\tag{1.12}\n\\end{equation}\\]Hypotheses 1.1 1.4, \\(\\mathbb{V}ar(\\mathbf{m}|\\mathbf{X}) = \\sigma^2 \\mathbf{R} (\\mathbf{X}'\\mathbf{X})^{-1} \\mathbf{R}'\\) (see Prop. 1.3). add normality assumption (Hypothesis 1.5), :\n\\[\\begin{equation}\nW = \\mathbf{m}'\\mathbb{V}ar(\\mathbf{m}|\\mathbf{X})^{-1}\\mathbf{m} \\sim \\chi^2(J). \\tag{1.13}\n\\end{equation}\\]\\(\\sigma^2\\) known, conduct Wald test (directly exploiting Eq. (1.13)). case practice compute \\(W\\). can, however, approximate replacing \\(\\sigma^2\\) \\(s^2\\) (given Eq. (1.9)). distribution new statistic \\(\\chi^2(J)\\) ; \\(\\mathcal{F}\\) distribution (whose quantiles shown Table 5.4), test called \\(F\\) test.Proposition 1.9  Hypotheses 1.1 1.5 Eq. (1.12) holds, :\n\\[\\begin{equation}\nF = \\frac{W}{J}\\frac{\\sigma^2}{s^2} = \\frac{\\mathbf{m}'(\\mathbf{R}(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{R}')^{-1}\\mathbf{m}}{s^2J} \\sim \\mathcal{F}(J,n-K),\\tag{1.14}\n\\end{equation}\\]\n\\(\\mathcal{F}\\) distribution F-statistic (see Def. 5.11).Proof. According Eq. (1.13), \\(W/J \\sim \\chi^2(J)/J\\). Moreover, denominator (\\(s^2/\\sigma^2\\)) \\(\\sim \\chi^2(n-K)\\). Therefore, \\(F\\) ratio r.v. distributed \\(\\chi^2(J)/J\\) another distributed \\(\\chi^2(n-K)/(n-K)\\). remains verify r.v. independent. \\(H_0\\), \\(\\mathbf{m} = \\mathbf{R}(\\mathbf{b}-\\boldsymbol\\beta) = \\mathbf{R}(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\boldsymbol\\varepsilon\\).\nTherefore \\(\\mathbf{m}'(\\mathbf{R}(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{R}')^{-1}\\mathbf{m}\\) form \\(\\boldsymbol\\varepsilon'\\mathbf{T}\\boldsymbol\\varepsilon\\) \\(\\mathbf{T}=\\mathbf{D}'\\mathbf{C}\\mathbf{D}\\) \\(\\mathbf{D}=\\mathbf{R}(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\) \\(\\mathbf{C}=(\\mathbf{R}(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{R}')^{-1}\\). Hypotheses 1.1 1.4, covariance \\(\\mathbf{T}\\boldsymbol\\varepsilon\\) \\(\\mathbf{M}\\boldsymbol\\varepsilon\\) \\(\\sigma^2\\mathbf{T}\\mathbf{M} = \\mathbf{0}\\). Therefore, 1.5, variables Gaussian variables 0 covariance. Hence independent.large \\(n-K\\), \\(\\mathcal{F}_{J,n-K}\\) distribution converges \\(\\mathcal{F}_{J,\\infty}=\\chi^2(J)/J\\). implies , large samples, F-statistic approximately \\(\\chi^2\\) distribution. words, one can approximately employ Eq. (1.13) perform Wald test (one just replace \\(\\sigma^2\\) \\(s^2\\) computing \\(\\mathbb{V}ar(\\mathbf{m}|\\mathbf{X})\\)).following proposition proposes another equivalent computation F-statistic, based \\(R^2\\) restricted unrestricted linear models.Proposition 1.10  F-statistic defined Eq. (1.14) also equal :\n\\[\\begin{equation}\nF = \\frac{(R^2-R_*^2)/J}{(1-R^2)/(n-K)} =  \\frac{(SSR_{restr}-SSR_{unrestr})/J}{SSR_{unrestr}/(n-K)},\\tag{1.15}\n\\end{equation}\\]\n\\(R_*^2\\) coef. determination (Eq. (1.5)) “restricted regression” (SSR: sum squared residuals.)Proof. Let’s denote \\(\\mathbf{e}_*=\\mathbf{y}-\\mathbf{X}\\mathbf{b}_*\\) vector residuals associated restricted regression (.e. \\(\\mathbf{R}\\mathbf{b}_*=\\mathbf{q}\\)).\n\\(\\mathbf{e}_*=\\mathbf{e} - \\mathbf{X}(\\mathbf{b}_*-\\mathbf{b})\\). Using \\(\\mathbf{e}'\\mathbf{X}=0\\), get \\(\\mathbf{e}_*'\\mathbf{e}_*=\\mathbf{e}'\\mathbf{e} + (\\mathbf{b}_*-\\mathbf{b})'\\mathbf{X}'\\mathbf{X}(\\mathbf{b}_*-\\mathbf{b}) \\ge \\mathbf{e}'\\mathbf{e}\\).Proposition 5.5 (Appendix 5.2), : \\(\\mathbf{b}_*-\\mathbf{b}=-(\\mathbf{X}'\\mathbf{X})^{-1} \\mathbf{R}'\\{\\mathbf{R}(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{R}'\\}^{-1}(\\mathbf{R}\\mathbf{b} - \\mathbf{q})\\). Therefore:\n\\[\n\\mathbf{e}_*'\\mathbf{e}_* - \\mathbf{e}'\\mathbf{e} = (\\mathbf{R}\\mathbf{b} - \\mathbf{q})'[\\mathbf{R}(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{R}']^{-1}(\\mathbf{R}\\mathbf{b} - \\mathbf{q}).\n\\]\nimplies F statistic defined Prop. 1.9 also equal :\n\\[\n\\frac{(\\mathbf{e}_*'\\mathbf{e}_* - \\mathbf{e}'\\mathbf{e})/J}{\\mathbf{e}'\\mathbf{e}/(n-K)},\n\\]\nleads result.null hypothesis \\(H_0\\) (Eq. (1.12)) F-test rejected \\(F\\) —defined Eq. (1.14) (1.15)— higher \\(\\mathcal{F}_{1-\\alpha}(J,n-K)\\). (Hence, test one-sided test.)","code":""},{"path":"ChapterLS.html","id":"largeSample","chapter":"1 Linear Regressions","heading":"1.2.6 Large Sample Properties","text":"Even relax normality assumption (Hypothesis 1.5), can approximate finite-sample behavior estimators using large-sample asymptotic properties.begin , proceed Hypothesis 1.1 1.4. (see, later , deal —partial— relaxations Hypothesis 1.3 1.4.)regularity assumptions, Hypotheses 1.1 1.4, even residuals normally-distributed, least square estimators can asymptotically normal inference can performed way small samples Hypotheses 1.1 1.5 hold. derives Prop. 1.11 (). F-test (Prop. 1.10) t-test (Eq. (1.10)) can performed.Proposition 1.11  Assumptions 1.1 1.4, assuming :\n\\[\\begin{equation}\nQ = \\mbox{plim}_{n \\rightarrow \\infty} \\frac{\\mathbf{X}'\\mathbf{X}}{n},\\tag{1.16}\n\\end{equation}\\]\n\\((\\mathbf{x}_i,\\varepsilon_i)\\)’s independent (across entities \\(\\)), :\n\\[\\begin{equation}\n\\sqrt{n}(\\mathbf{b} - \\boldsymbol\\beta)\\overset{d} {\\rightarrow} \\mathcal{N}\\left(0,\\sigma^2Q^{-1}\\right).\\tag{1.17}\n\\end{equation}\\]Proof. Since \\(\\mathbf{b} = \\boldsymbol\\beta + \\left( \\frac{\\mathbf{X}'\\mathbf{X}}{n}\\right)^{-1}\\left(\\frac{\\mathbf{X}'\\boldsymbol\\varepsilon}{n}\\right)\\), : \\(\\sqrt{n}(\\mathbf{b} - \\boldsymbol\\beta) = \\left( \\frac{\\mathbf{X}'\\mathbf{X}}{n}\\right)^{-1} \\left(\\frac{1}{\\sqrt{n}}\\right)\\mathbf{X}'\\boldsymbol\\varepsilon\\). Since \\(f:\\rightarrow ^{-1}\\) continuous function (\\(\\ne \\mathbf{0}\\)), \\(\\mbox{plim}_{n \\rightarrow \\infty} \\left(\\frac{\\mathbf{X}'\\mathbf{X}}{n}\\right)^{-1} = \\mathbf{Q}^{-1}\\) (see Prop. 5.12). Let us denote \\(V_i\\) vector \\(\\mathbf{x}_i \\varepsilon_i\\). \\((\\mathbf{x}_i,\\varepsilon_i)\\)’s independent, \\(V_i\\)’s independent well. covariance matrix \\(\\sigma^2\\mathbb{E}(\\mathbf{x}_i \\mathbf{x}_i')=\\sigma^2Q\\). Applying multivariate central limit theorem vectors \\(V_i\\) gives \\(\\sqrt{n}\\left(\\frac{1}{n}\\sum_{=1}^n \\mathbf{x}_i \\varepsilon_i\\right) = \\left(\\frac{1}{\\sqrt{n}}\\right)\\mathbf{X}'\\boldsymbol\\varepsilon \\overset{d}{\\rightarrow} \\mathcal{N}(0,\\sigma^2Q)\\). application Slutsky’s theorem (Prop. 5.12) leads results.practice, \\(\\sigma^2\\) approximated \\(s^2=\\frac{\\mathbf{e}'\\mathbf{e}}{n-K}\\) (Eq. (1.9)) \\(\\mathbf{Q}^{-1}\\) \\(\\left(\\frac{\\mathbf{X}'\\mathbf{X}}{n}\\right)^{-1}\\). , covariance matrix estimator approximated :\n\\[\\begin{equation}\n\\boxed{\\widehat{\\mathbb{V}ar}(\\mathbf{b}) = s^2 (\\mathbf{X}'\\mathbf{X})^{-1}.}\\tag{1.18}\n\\end{equation}\\]Eqs. (1.16) (1.17) respectively correspond convergences probability distribution (see Definitions 5.16 5.19, respectively).","code":""},{"path":"ChapterLS.html","id":"CommonPitfalls","chapter":"1 Linear Regressions","heading":"1.3 Common pitfalls in linear regressions","text":"","code":""},{"path":"ChapterLS.html","id":"multicollinearity","chapter":"1 Linear Regressions","heading":"1.3.1 Multicollinearity","text":"Consider model: \\(y_i = \\beta_1 x_{,1} + \\beta_2 x_{,2} + \\varepsilon_i\\), variables zero-mean \\(\\mathbb{V}ar(\\varepsilon_i)=\\sigma^2\\). :\n\\[\n\\mathbf{X}'\\mathbf{X} = \\left[ \\begin{array}{cc}\n\\sum_i x_{,1}^2 & \\sum_i x_{,1} x_{,2} \\\\\n\\sum_i x_{,1} x_{,2} & \\sum_i x_{,2}^2\n\\end{array}\\right],\n\\]\ntherefore:\n\\[\\begin{eqnarray*}\n(\\mathbf{X}'\\mathbf{X})^{-1} &=& \\frac{1}{\\sum_i x_{,1}^2\\sum_i x_{,2}^2 - (\\sum_i x_{,1} x_{,2})^2} \\left[ \\begin{array}{cc}\n\\sum_i x_{,2}^2 & -\\sum_i x_{,1} x_{,2} \\\\\n-\\sum_i x_{,1} x_{,2} & \\sum_i x_{,1}^2\n\\end{array}\\right].\n\\end{eqnarray*}\\]\ninverse upper-left parameter \\((\\mathbf{X}'\\mathbf{X})^{-1}\\) :\n\\[\\begin{equation}\n\\sum_i x_{,1}^2 - \\frac{(\\sum_i x_{,1} x_{,2})^2}{\\sum_i x_{,2}^2} = \\sum_i x_{,1}^2(1 - correl_{1,2}^2),\\tag{1.19}\n\\end{equation}\\]\n\\(correl_{1,2}\\) sample correlation \\(\\mathbf{x}_{1}\\) \\(\\mathbf{x}_{2}\\).Hence, closer one \\(correl_{1,2}\\), higher variance \\(b_1\\) (recall variance \\(b_1\\) upper-left component \\(\\sigma^2(\\mathbf{X}'\\mathbf{X})^{-1}\\)). , regressors close linear conbination ones, confidence intervals tend wide, typically reduces power t-test (tend fail reject null hypothesis coefficients different zero).","code":""},{"path":"ChapterLS.html","id":"Omitted","chapter":"1 Linear Regressions","heading":"1.3.2 Omitted variables","text":"Consider following model (“True model”):\n\\[\n\\mathbf{y} = \\underbrace{\\mathbf{X}_1}_{n \\times K_1}\\underbrace{\\boldsymbol\\beta_1}_{K_1 \\times 1} + \\underbrace{\\mathbf{X}_2}_{n\\times K_2}\\underbrace{\\boldsymbol\\beta_2}_{K_2 \\times 1} + \\boldsymbol\\varepsilon\n\\]\none computes \\(\\mathbf{b}_1\\) regressing \\(\\mathbf{y}\\) \\(\\mathbf{X}_1\\) , one gets:\n\\[\n\\mathbf{b}_1 = (\\mathbf{X}_1'\\mathbf{X}_1)^{-1}\\mathbf{X}_1'\\mathbf{y} = \\boldsymbol\\beta_1 + (\\mathbf{X}_1'\\mathbf{X}_1)^{-1}\\mathbf{X}_1'\\mathbf{X}_2\\boldsymbol\\beta_2 +\n(\\mathbf{X}_1'\\mathbf{X}_1)^{-1}\\mathbf{X}_1'\\boldsymbol\\varepsilon.\n\\]results omitted-variable formula:\n\\[\n\\mathbb{E}(\\mathbf{b}_1|\\mathbf{X}) = \\boldsymbol\\beta_1 + \\underbrace{(\\mathbf{X}_1'\\mathbf{X}_1)^{-1}(\\mathbf{X}_1'\\mathbf{X}_2)}_{K_1 \\times K_2}\\boldsymbol\\beta_2.\n\\]\n(column \\((\\mathbf{X}_1'\\mathbf{X}_1)^{-1}(\\mathbf{X}_1'\\mathbf{X}_2)\\) OLS regressors obtained regressing columns \\(\\mathbf{X}_2\\) \\(\\mathbf{X}_1\\).) Unless variables included \\(\\mathbf{X}_1\\) orthogonal \\(\\mathbf{X}_2\\), obtain bias. way address potential pitfall introduce “controls” specification.Example 1.3  Let us use California Test Score dataset (package AER). Assume want measure effect students--teacher ratio (str) student test scores (testscr). following regressions show effect lower controls added.","code":"\nlibrary(AER); data(\"CASchools\")\nCASchools$str <- CASchools$students/CASchools$teachers\nCASchools$testscr <- .5 * (CASchools$math + CASchools$read)\neq1 <- lm(testscr~str,data=CASchools)\neq2 <- lm(testscr~str+lunch,data=CASchools)\neq3 <- lm(testscr~str+lunch+english,data=CASchools)\nstargazer::stargazer(eq1,eq2,eq3,type=\"text\",\n                     no.space = TRUE,omit.stat=c(\"f\",\"ser\"))## \n## =============================================\n##                    Dependent variable:       \n##              --------------------------------\n##                          testscr             \n##                 (1)        (2)        (3)    \n## ---------------------------------------------\n## str          -2.280***  -1.117***  -0.998*** \n##               (0.480)    (0.240)    (0.239)  \n## lunch                   -0.600***  -0.547*** \n##                          (0.017)    (0.022)  \n## english                            -0.122*** \n##                                     (0.032)  \n## Constant     698.933*** 702.911*** 700.150***\n##               (9.467)    (4.700)    (4.686)  \n## ---------------------------------------------\n## Observations    420        420        420    \n## R2             0.051      0.767      0.775   \n## Adjusted R2    0.049      0.766      0.773   \n## =============================================\n## Note:             *p<0.1; **p<0.05; ***p<0.01"},{"path":"ChapterLS.html","id":"irrelevant","chapter":"1 Linear Regressions","heading":"1.3.3 Irrelevant variable","text":"Consider true model:\n\\[\n\\mathbf{y} = \\mathbf{X}_1\\boldsymbol\\beta_1 + \\boldsymbol\\varepsilon,\n\\]\nestimated model :\n\\[\n\\mathbf{y} = \\mathbf{X}_1\\boldsymbol\\beta_1 + \\mathbf{X}_2\\boldsymbol\\beta_2 + \\boldsymbol\\varepsilon\n\\]estimates unbiased. However, adding irrelevant explanatory variables increases variance estimate \\(\\boldsymbol\\beta_1\\) (compared case one uses correct explanatory variables). case unless correlation \\(\\mathbf{X}_1\\) \\(\\mathbf{X}_2\\) null, see Eq. (1.19).words, estimator inefficient, .e., exists alternative consistent estimator whose variance lower. inefficiency problem can serious consequences testing hypotheses \\(H_0: \\beta_1 = 0\\). Due loss power, might wrongly infer \\(\\mathbf{X}_1\\) variables “relevant” (Type-II error, False Negative).","code":""},{"path":"ChapterLS.html","id":"IV","chapter":"1 Linear Regressions","heading":"1.4 Instrumental Variables","text":"conditional mean zero assumption (Hypothesis 1.2), according \\(\\mathbb{E}(\\boldsymbol\\varepsilon|\\mathbf{X})=0\\) —implies particular \\(\\mathbf{x}_i\\) \\(\\varepsilon_i\\) uncorrelated— sometimes consistent considered economic framework. case, parameters interest may still estimated consistently resorting instrumental variable techniques.Consider following model:\n\\[\\begin{equation}\ny_i = \\mathbf{x_i}'\\boldsymbol\\beta + \\varepsilon_i, \\quad \\mbox{} \\mathbb{E}(\\varepsilon_i)=0  \\mbox{ } \\mathbf{x_i}\\\\perp \\varepsilon_i.\\tag{1.20}\n\\end{equation}\\]Let us illustrate situation may result biased OLS estimate. Consider instance situation :\n\\[\\begin{equation}\n\\mathbb{E}(\\varepsilon_i)=0 \\quad \\mbox{} \\quad \\mathbb{E}(\\varepsilon_i \\mathbf{x_i})=\\boldsymbol\\gamma,\\tag{1.21}\n\\end{equation}\\]\ncase \\(\\mathbf{x}_i\\\\perp \\varepsilon_i\\) (consistently Eq. (1.20)).law large numbers, \\(\\mbox{plim}_{n \\rightarrow \\infty} \\mathbf{X}'\\boldsymbol\\varepsilon / n = \\boldsymbol\\gamma\\). \\(\\mathbf{Q}_{xx} := \\mbox{plim } \\mathbf{X}'\\mathbf{X}/n\\), OLS estimator consistent \n\\[\n\\mathbf{b} = \\boldsymbol\\beta + (\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\boldsymbol\\varepsilon \\overset{p}{\\rightarrow} \\boldsymbol\\beta + \\mathbf{Q}_{xx}^{-1}\\boldsymbol\\gamma \\ne \\boldsymbol\\beta.\n\\]Let us now introduce notion instruments.Definition 1.2  (Instrumental variables) \\(L\\)-dimensional random variable \\(\\mathbf{z}_i\\) valid set instruments :\\(\\mathbf{z}_i\\) correlated \\(\\mathbf{x}_i\\);\\(\\mathbb{E}(\\boldsymbol\\varepsilon|\\mathbf{Z})=0\\) andthe orthogonal projections \\(\\mathbf{x}_i\\)’s \\(\\mathbf{z}_i\\)’s multicollinear.Point c implies particular dimension \\(\\mathbf{z}_i\\) least large \\(\\mathbf{x}_i\\). \\(\\mathbf{z}_i\\) valid set instruments, :\n\\[\n\\mbox{plim}\\left( \\frac{\\mathbf{Z}'\\mathbf{y}}{n} \\right) =\\mbox{plim}\\left( \\frac{\\mathbf{Z}'(\\mathbf{X}\\boldsymbol\\beta + \\boldsymbol\\varepsilon)}{n} \\right) = \\mbox{plim}\\left( \\frac{\\mathbf{Z}'\\mathbf{X}}{n} \\right)\\boldsymbol\\beta.\n\\]\nIndeed, law large numbers, \\(\\frac{\\mathbf{Z}'\\boldsymbol\\varepsilon}{n} \\overset{p}{\\rightarrow}\\mathbb{E}(\\mathbf{z}_i\\varepsilon_i)=0\\).\\(L = K\\), matrix \\(\\frac{\\mathbf{Z}'\\mathbf{X}}{n}\\) dimension \\(K \\times K\\) :\n\\[\n\\left[\\mbox{plim }\\left( \\frac{\\mathbf{Z}'\\mathbf{X}}{n} \\right)\\right]^{-1}\\mbox{plim }\\left( \\frac{\\mathbf{Z}'\\mathbf{y}}{n} \\right) = \\boldsymbol\\beta.\n\\]\ncontinuity inverse function (everywhere 0): \\(\\left[\\mbox{plim }\\left( \\frac{\\mathbf{Z}'\\mathbf{X}}{n} \\right)\\right]^{-1}=\\mbox{plim }\\left( \\frac{\\mathbf{Z}'\\mathbf{X}}{n} \\right)^{-1}\\).\nSlutsky Theorem (Prop. 5.12) implies :\n\\[\n\\mbox{plim }\\left( \\frac{\\mathbf{Z}'\\mathbf{X}}{n} \\right)^{-1} \\mbox{plim }\\left( \\frac{\\mathbf{Z}'\\mathbf{y}}{n} \\right)  = \\mbox{plim }\\left( \\left( \\frac{\\mathbf{Z}'\\mathbf{X}}{n} \\right)^{-1} \\frac{\\mathbf{Z}'\\mathbf{y}}{n} \\right).\n\\]\nHence \\(\\mathbf{b}_{iv}\\) consistent defined :\n\\[\n\\boxed{\\mathbf{b}_{iv} = (\\mathbf{Z}'\\mathbf{X})^{-1}\\mathbf{Z}'\\mathbf{y}.}\n\\]Proposition 1.12  (Asymptotic distribution IV estimator) \\(\\mathbf{z}_i\\) \\(L\\)-dimensional random variable constitutes valid set instruments (see Def. 1.2) \\(L=K\\), asymptotic distribution \\(\\mathbf{b}_{iv}\\) :\n\\[\n\\mathbf{b}_{iv} \\overset{d}{\\rightarrow} \\mathcal{N}\\left(\\boldsymbol\\beta,\\frac{\\sigma^2}{n}\\left[Q_{xz}Q_{zz}^{-1}Q_{zx}\\right]^{-1}\\right)\n\\]\n\\(\\mbox{plim } \\mathbf{Z}'\\mathbf{Z}/n =: \\mathbf{Q}_{zz}\\), \\(\\mbox{plim } \\mathbf{Z}'\\mathbf{X}/n =: \\mathbf{Q}_{zx}\\), \\(\\mbox{plim } \\mathbf{X}'\\mathbf{Z}/n =: \\mathbf{Q}_{xz}\\).Proof. proof similar Prop. 1.11, starting point \\(\\mathbf{b}_{iv} = \\boldsymbol\\beta + (\\mathbf{Z}'\\mathbf{X})^{-1}\\mathbf{Z}'\\boldsymbol\\varepsilon\\).\\(L=K\\), :\n\\[\n\\left[Q_{xz}Q_{zz}^{-1}Q_{zx}\\right]^{-1}=Q_{zx}^{-1}Q_{zz}Q_{xz}^{-1}.\n\\]\npractice, estimate \\(\\mathbb{V}ar(\\mathbf{b}_{iv}) = \\frac{\\sigma^2}{n}Q_{zx}^{-1}Q_{zz}Q_{xz}^{-1}\\), replace \\(\\sigma^2\\) :\n\\[\ns_{iv}^2 = \\frac{1}{n}\\sum_{=1}^{n} (y_i - \\mathbf{x}_i'\\mathbf{b}_{iv})^2.\n\\]\\(L > K\\)? case, proceed follows:Regress \\(\\mathbf{X}\\) space spanned \\(\\mathbf{Z}\\) andRegress \\(\\mathbf{y}\\) fitted values \\(\\hat{\\mathbf{X}}:=\\mathbf{Z}(\\mathbf{Z}'\\mathbf{Z})^{-1}\\mathbf{Z}'\\mathbf{X}\\).two-step approach called Two-Stage Least Squares (2SLS). results :\n\\[\\begin{equation}\n\\boxed{\\mathbf{b}_{iv} = [\\mathbf{X}'\\mathbf{Z}(\\mathbf{Z}'\\mathbf{Z})^{-1}\\mathbf{Z}'\\mathbf{X}]^{-1}\\mathbf{X}'\\mathbf{Z}(\\mathbf{Z}'\\mathbf{Z})^{-1}\\mathbf{Z}'\\mathbf{Y}.} \\tag{1.22}\n\\end{equation}\\]case, Prop. 1.12 still holds, \\(\\mathbf{b}_{iv}\\) given Eq. (1.22).instruments properly satisfy Condition () Def. 1.2 (.e. \\(\\mathbf{x}_i\\) \\(\\mathbf{z}_i\\) loosely related), instruments said weak (see, e.g., J. H. Stock Yogo (2005), available Andrews, Stock, Sun (2019)). simple standard way test weak instruments consist looking F-statistic associated first stage estimation. easier reject null hypothesis (large test statistic), less weak —stronger— instruments.Durbin-Wu-Hausman test (Durbin (1954), Wu (1973), Hausman (1978)) can used test IV necessary. (IV techniques required \\(\\mbox{plim}_{n \\rightarrow \\infty} \\mathbf{X}'\\boldsymbol\\varepsilon / n \\ne 0\\).) Hausman (1978) proposes test efficiency estimators. null hypothesis two estimators, \\(\\mathbf{b}_0\\) \\(\\mathbf{b}_1\\), consistent \\(\\mathbf{b}_0\\) (asymptotically) efficient relative \\(\\mathbf{b}_1\\). alternative hypothesis, \\(\\mathbf{b}_1\\) (IV present case) remains consistent \\(\\mathbf{b}_0\\) (OLS present case). , reject null hypothesis, means OLS estimator consistent, potentially due endogeneity issue.test statistic :\n\\[\nH = (\\mathbf{b}_1 - \\mathbf{b}_0)' MPI(\\mathbb{V}ar(\\mathbf{b}_1) - \\mathbb{V}ar(\\mathbf{b}_0))(\\mathbf{b}_1 - \\mathbf{b}_0),\n\\]\n\\(MPI\\) Moore-Penrose pseudo-inverse. null hypothesis, \\(H \\sim \\chi^2(q)\\), \\(q\\) rank \\(\\mathbb{V}ar(\\mathbf{b}_1) - \\mathbb{V}ar(\\mathbf{b}_0)\\).Example 1.4  (Estimation price elasticity) See e.g. estimation tobacco price elasticity demand.want estimate effect demand exogenous increase prices cigarettes (say).model :\n\\[\\begin{eqnarray*}\n\\underbrace{q^d_t}_{\\mbox{log(demand)}} &=& \\alpha_0 + \\alpha_1 \\underbrace{\\times p_t}_{\\mbox{log(price)}} + \\alpha_2 \\underbrace{\\times w_t}_{\\mbox{income}} + \\varepsilon_t^d\\\\\n\\underbrace{q^s_t}_{\\mbox{log(supply)}} &=& \\gamma_0 + \\gamma_1 \\times p_t + \\gamma_2 \\underbrace{\\times \\mathbf{y}_t}_{\\mbox{cost factors}} + \\varepsilon_t^s,\n\\end{eqnarray*}\\]\n\\(\\mathbf{y}_t\\), \\(w_t\\), \\(\\varepsilon_t^s \\sim \\mathcal{N}(0,\\sigma^2_s)\\) \\(\\varepsilon_t^d \\sim \\mathcal{N}(0,\\sigma^2_d)\\) independent.Equilibrium: \\(q^d_t = q^s_t\\). implies prices endogenous:\n\\[\np_t = \\frac{\\alpha_0 + \\alpha_2 w_t + \\varepsilon_t^d - \\gamma_0 - \\gamma_2 \\mathbf{y}_t - \\varepsilon_t^s}{\\gamma_1 - \\alpha_1}.\n\\]\nparticular \\(\\mathbb{E}(p_t \\varepsilon_t^d) = \\frac{\\sigma^2_d}{\\gamma_1 - \\alpha_1} \\ne 0\\) \\(\\Rightarrow\\) Regressing OLS \\(q_t^d\\) \\(p_t\\) gives biased estimates (see Eq. (1.21)).\nFigure 1.5: figure illustrates situation prevailing estimating price-elasticity (price endogenous).\nLet us use IV regressions estimate price elasticity cigarette demand. purpose, use CigarettesSW dataset package AER (data used J. Stock Watson (2003)). panel dataset documents cigarette consumption 48 continental US States 1985–1995. instrument real tax cigarettes arising state’s general sales tax. rationale larger general sales tax drives cigarette prices , general tax determined forces affecting \\(\\varepsilon_t^d\\).last three tests interpreted follows:Since p-value first test small, reject null hypothesis according instrument weak.small p-value Wu-Hausman test implies reject null hypothesis according OLS estimates consistent (10% level , though).-identification (misspecification) detected Sargan test (large p-value).Example 1.5  (Education wage) example, make use another dataset proposed J. Stock Watson (2003), namely CollegeDistance dataset.2 objective estimate effect education wages. Education choice suspected endogenous variable, calls IV strategy. instrumental variable distance college (see, e.g., Dee (2004)).","code":"\ndata(\"CigarettesSW\", package = \"AER\")\nCigarettesSW$rprice  <- with(CigarettesSW, price/cpi)\nCigarettesSW$rincome <- with(CigarettesSW, income/population/cpi)\nCigarettesSW$tdiff   <- with(CigarettesSW, (taxs - tax)/cpi)\n\n## model \neq.IV1 <- ivreg(log(packs) ~ log(rprice) + log(rincome) |\n                  log(rincome) + tdiff + I(tax/cpi),\n                data = CigarettesSW, subset = year == \"1995\")\neq.IV2 <- ivreg(log(packs) ~ log(rprice) | tdiff,\n                data = CigarettesSW, subset = year == \"1995\")\neq.no.IV <- lm(log(packs) ~ log(rprice) + log(rincome),\n               data = CigarettesSW, subset = year == \"1995\")\nstargazer::stargazer(eq.no.IV,eq.IV1,eq.IV2,type=\"text\",no.space = TRUE,\n                     omit.stat=c(\"f\",\"ser\"))## \n## ==========================================\n##                   Dependent variable:     \n##              -----------------------------\n##                       log(packs)          \n##                 OLS       instrumental    \n##                             variable      \n##                 (1)       (2)       (3)   \n## ------------------------------------------\n## log(rprice)  -1.407*** -1.277*** -1.084***\n##               (0.251)   (0.263)   (0.317) \n## log(rincome)   0.344     0.280            \n##               (0.235)   (0.239)           \n## Constant     10.342*** 9.895***  9.720*** \n##               (1.023)   (1.059)   (1.514) \n## ------------------------------------------\n## Observations    48        48        48    \n## R2             0.433     0.429     0.401  \n## Adjusted R2    0.408     0.404     0.388  \n## ==========================================\n## Note:          *p<0.1; **p<0.05; ***p<0.01\nsummary(eq.IV1,diagnostics = TRUE)$diagnostics##                  df1 df2   statistic      p-value\n## Weak instruments   2  44 244.7337536 1.444054e-24\n## Wu-Hausman         1  44   3.0678163 8.682505e-02\n## Sargan             1  NA   0.3326221 5.641191e-01\nlibrary(sem)\ndata(\"CollegeDistance\", package = \"AER\")\neq.1st.stage <- lm(education ~ urban + gender + ethnicity + unemp + distance,\n                   data = CollegeDistance)\nCollegeDistance$ed.pred<- predict(eq.1st.stage)\neq.2nd.stage <- lm(wage ~ urban + gender + ethnicity + unemp + ed.pred,\n                   data = CollegeDistance)\neqOLS <- lm(wage ~ urban + gender + ethnicity + unemp + education,\n            data=CollegeDistance)\neq2SLS <- ivreg(wage ~ urban + gender + ethnicity + unemp + education|\n                  urban + gender + ethnicity + unemp + distance,\n                data=CollegeDistance)\nstargazer::stargazer(eq.1st.stage,eq.2nd.stage,eq2SLS,eqOLS,\n                     type=\"text\",no.space = TRUE,omit.stat = c(\"f\",\"ser\"))## \n## ============================================================\n##                              Dependent variable:            \n##                   ------------------------------------------\n##                   education               wage              \n##                      OLS       OLS    instrumental    OLS   \n##                                         variable            \n##                      (1)       (2)        (3)         (4)   \n## ------------------------------------------------------------\n## urbanyes           -0.092     0.046      0.046       0.070  \n##                    (0.065)   (0.045)    (0.060)     (0.045) \n## genderfemale       -0.025    -0.071*     -0.071    -0.085** \n##                    (0.052)   (0.037)    (0.050)     (0.037) \n## ethnicityafam     -0.524*** -0.227***   -0.227**   -0.556***\n##                    (0.072)   (0.073)    (0.099)     (0.052) \n## ethnicityhispanic -0.275*** -0.351***  -0.351***   -0.544***\n##                    (0.068)   (0.057)    (0.077)     (0.049) \n## unemp               0.010   0.139***    0.139***   0.133*** \n##                    (0.010)   (0.007)    (0.009)     (0.007) \n## distance          -0.087***                                 \n##                    (0.012)                                  \n## ed.pred                     0.647***                        \n##                              (0.101)                        \n## education                               0.647***     0.005  \n##                                         (0.136)     (0.010) \n## Constant          14.061***  -0.359      -0.359    8.641*** \n##                    (0.083)   (1.412)    (1.908)     (0.157) \n## ------------------------------------------------------------\n## Observations        4,739     4,739      4,739       4,739  \n## R2                  0.023     0.117      -0.612      0.110  \n## Adjusted R2         0.022     0.116      -0.614      0.109  \n## ============================================================\n## Note:                            *p<0.1; **p<0.05; ***p<0.01"},{"path":"ChapterLS.html","id":"general-regression-model-grm-and-robust-covariance-matrices","chapter":"1 Linear Regressions","heading":"1.5 General Regression Model (GRM) and robust covariance matrices","text":"statistical inference presented relies strong assumptions regarding stochastic properties errors. Namely, assumed mutually uncorrelated (Hypothesis 1.4) homoskedastic (Hypothesis 1.3).objective section present approaches aimed adjusting estimate covariance matrix OLS estimator (\\((\\mathbf{X}'\\mathbf{X})^{-1}s^2\\), see Eq. (1.18)), previous hypotheses hold.","code":""},{"path":"ChapterLS.html","id":"presentation-of-the-general-regression-model-grm","chapter":"1 Linear Regressions","heading":"1.5.1 Presentation of the General Regression Model (GRM)","text":"prove useful introduce following notation:\n\\[\\begin{eqnarray}\n\\mathbb{V}ar(\\boldsymbol\\varepsilon | \\mathbf{X}) = \\mathbb{E}(\\boldsymbol\\varepsilon \\boldsymbol\\varepsilon'| \\mathbf{X}) &=& \\boldsymbol\\Sigma. \\tag{1.23}\n\\end{eqnarray}\\]Note Eq. (1.23) general Hypothesis 1.3 1.4 diagonal entries \\(\\boldsymbol\\Sigma\\) may different (opposed Hypothesis 1.3), non-diagonal entries \\(\\boldsymbol\\Sigma\\) can non-null (opposed Hypothesis 1.4).Definition 1.3  (General Regression Model (GRM)) Hypothesis 1.1 1.2, together Eq. (1.23), form General Regression Model (GRM) framework.Naturally, regression model Hypotheses 1.1 1.4 hold specific case GRM framework.GRM context notably encompasses situations heteroskedasticity autocorrelation:Heteroskedasticity:\n\\[\\begin{equation}\n\\boldsymbol\\Sigma = \\left[  \\begin{array}{cccc}\n\\sigma_1^2 & 0 & \\dots & 0 \\\\\n0 & \\sigma_2^2 &  & 0 \\\\\n\\vdots && \\ddots& \\vdots \\\\\n0 & \\dots & 0 & \\sigma_n^2\n\\end{array} \\right]. \\tag{1.24}\n\\end{equation}\\]Heteroskedasticity:\n\\[\\begin{equation}\n\\boldsymbol\\Sigma = \\left[  \\begin{array}{cccc}\n\\sigma_1^2 & 0 & \\dots & 0 \\\\\n0 & \\sigma_2^2 &  & 0 \\\\\n\\vdots && \\ddots& \\vdots \\\\\n0 & \\dots & 0 & \\sigma_n^2\n\\end{array} \\right]. \\tag{1.24}\n\\end{equation}\\]Autocorrelation:\n\\[\\begin{equation}\n\\boldsymbol\\Sigma = \\sigma^2 \\left[ \\begin{array}{cccc}\n1 & \\rho_{2,1} & \\dots & \\rho_{n,1} \\\\\n\\rho_{2,1} & 1 &  & \\vdots \\\\\n\\vdots && \\ddots& \\rho_{n,n-1} \\\\\n\\rho_{n,1} & \\rho_{n,2} & \\dots & 1\n\\end{array} \\right]. \\tag{1.25}\n\\end{equation}\\]Autocorrelation:\n\\[\\begin{equation}\n\\boldsymbol\\Sigma = \\sigma^2 \\left[ \\begin{array}{cccc}\n1 & \\rho_{2,1} & \\dots & \\rho_{n,1} \\\\\n\\rho_{2,1} & 1 &  & \\vdots \\\\\n\\vdots && \\ddots& \\rho_{n,n-1} \\\\\n\\rho_{n,1} & \\rho_{n,2} & \\dots & 1\n\\end{array} \\right]. \\tag{1.25}\n\\end{equation}\\]Example 1.6  (Auto-regressive processes) Autocorrelation common time-series contexts (see Section ??). time-series context, subscript \\(\\) refers date.Assume instance :\n\\[\\begin{equation}\ny_i = \\mathbf{x}_i' \\boldsymbol\\beta + \\varepsilon_i \\tag{1.26}\n\\end{equation}\\]\n\n\\[\\begin{equation}\n\\varepsilon_i = \\rho \\varepsilon_{-1} + v_i, \\quad v_i \\sim \\mathcal{N}(0,\\sigma_v^2).\\tag{1.27}\n\\end{equation}\\]\ncase, GRM context, :\n\\[\\begin{equation}\n\\boldsymbol\\Sigma =\\frac{ \\sigma_v^2}{1 - \\rho^2} \\left[    \\begin{array}{cccc}\n1 & \\rho & \\dots & \\rho^{n-1} \\\\\n\\rho & 1 &  & \\vdots \\\\\n\\vdots && \\ddots& \\rho \\\\\n\\rho^{n-1} & \\rho^{n-2} & \\dots & 1\n\\end{array} \\right].\\tag{1.28}\n\\end{equation}\\]cases —particular one assumes parametric formulation \\(\\boldsymbol\\Sigma\\)— one can determine better (accurate) estimator OLS one. approach called Generalized Least Squares (GLS), present .","code":""},{"path":"ChapterLS.html","id":"GLS","chapter":"1 Linear Regressions","heading":"1.5.2 Generalized Least Squares","text":"Assume \\(\\boldsymbol\\Sigma\\) known (“feasible GLS”). \\(\\boldsymbol\\Sigma\\) symmetric positive, admits spectral decomposition form \\(\\boldsymbol\\Sigma = \\mathbf{C} \\boldsymbol\\Lambda \\mathbf{C}'\\), \\(\\mathbf{C}\\) orthogonal matrix (.e. \\(\\mathbf{C}\\mathbf{C}'=Id\\)) \\(\\boldsymbol\\Lambda\\) diagonal matrix (diagonal entries eigenvalues \\(\\boldsymbol\\Sigma\\)).\\(\\boldsymbol\\Sigma = (\\mathbf{P}\\mathbf{P}')^{-1}\\) \\(\\mathbf{P} = \\mathbf{C}\\boldsymbol\\Lambda^{-1/2}\\). Consider transformed model:\n\\[\n\\mathbf{P}'\\mathbf{y} = \\mathbf{P}'\\mathbf{X}\\boldsymbol\\beta + \\mathbf{P}'\\boldsymbol\\varepsilon \\quad \\mbox{} \\quad \\mathbf{y}^* = \\mathbf{X}^*\\boldsymbol\\beta + \\boldsymbol\\varepsilon^*.\n\\]\nvariance \\(\\boldsymbol\\varepsilon^*\\) identity matrix \\(Id\\). transformed model, OLS BLUE (Gauss-Markow Theorem 1.1).Generalized least squares estimator \\(\\boldsymbol\\beta\\) :\n\\[\\begin{equation}\n\\boxed{\\mathbf{b}_{GLS} = (\\mathbf{X}'\\boldsymbol\\Sigma^{-1}\\mathbf{X})^{-1}\\mathbf{X}'\\boldsymbol\\Sigma^{-1}\\mathbf{y}.}\\tag{1.29}\n\\end{equation}\\]\n:\n\\[\n\\mathbb{V}ar(\\mathbf{b}_{GLS}|\\mathbf{X}) = (\\mathbf{X}'\\boldsymbol\\Sigma^{-1}\\mathbf{X})^{-1}.\n\\]However, general, \\(\\boldsymbol\\Sigma\\) unknown. GLS estimator said infeasible. structure required. Assume \\(\\boldsymbol\\Sigma\\) admits parametric form \\(\\boldsymbol\\Sigma(\\theta)\\). estimation becomes feasible (FGLS) one replaces \\(\\boldsymbol\\Sigma(\\theta)\\) \\(\\boldsymbol\\Sigma(\\hat\\theta)\\), \\(\\hat\\theta\\) consistent estimator \\(\\theta\\). case, FGLS asymptotically efficient (see Example 1.7).\\(\\boldsymbol\\Sigma\\) obvious structure: OLS (IV) estimator available. regularity assumptions, remains unbiased, consistent, asymptotically normally distributed, efficient. Standard inference procedures longer appropriate.Example 1.7  (GLS auto-correlation case) Consider case presented Example 1.6. OLS estimate \\(\\mathbf{b}\\) \\(\\boldsymbol\\beta\\) consistent, estimates \\(e_i\\) \\(\\varepsilon_i\\)’s also . Consistent estimators \\(\\rho\\) \\(\\sigma_v\\) obtained regressing \\(e_i\\)’s \\(e_{-1}\\)’s. Using estimates Eq. (1.28) provides consistent estimate \\(\\boldsymbol\\Sigma\\). Applying steps recursively gives efficient estimator \\(\\boldsymbol\\beta\\) (Cochrane Orcutt (1949)).","code":""},{"path":"ChapterLS.html","id":"asymptotic-properties-of-the-ols-estimator-in-the-grm-framework","chapter":"1 Linear Regressions","heading":"1.5.3 Asymptotic properties of the OLS estimator in the GRM framework","text":"Since \\(\\mathbf{b} = \\boldsymbol\\beta + \\left(\\mathbf{X}'\\mathbf{X}\\right)^{-1} \\mathbf{X}'\\boldsymbol\\varepsilon\\) \\(\\mathbb{V}ar(\\boldsymbol\\varepsilon|\\mathbf{X})=\\boldsymbol\\Sigma\\), :\n\\[\\begin{equation}\n\\mathbb{V}ar(\\mathbf{b}|\\mathbf{X}) = \\frac{1}{n}\\left(\\frac{1}{n}\\mathbf{X}'\\mathbf{X}\\right)^{-1}\\left(\\frac{1}{n}\\mathbf{X}'\\boldsymbol\\Sigma\\mathbf{X}\\right)\\left(\\frac{1}{n}\\mathbf{X}'\\mathbf{X}\\right)^{-1}.\\tag{1.30}\n\\end{equation}\\]Therefore, conditional covariance matrix OLS estimator \\(\\sigma^2 (\\mathbf{X}'\\mathbf{X})^{-1}\\) longer, using \\(s^2 (\\mathbf{X}'\\mathbf{X})^{-1}\\) inference may misleading. , see construct appropriate estimates covariance matrix \\(\\mathbf{b}\\). , let us prove OLS estimator remains consistent GRM framework.Proposition 1.13  (Consistency OLS estimator GRM framework) \\(\\mbox{plim }(\\mathbf{X}'\\mathbf{X}/n)\\) \\(\\mbox{plim }(\\mathbf{X}'\\boldsymbol\\Sigma\\mathbf{X}/n)\\) finite positive definite matrices, \\(\\mbox{plim }(\\mathbf{b})=\\boldsymbol\\beta\\).Proof. \\(\\mathbb{V}ar(\\mathbf{b})=\\mathbb{E}[\\mathbb{V}ar(\\mathbf{b}|\\mathbf{X})]+\\mathbb{V}ar[\\mathbb{E}(\\mathbf{b}|\\mathbf{X})]\\). Since \\(\\mathbb{E}(\\mathbf{b}|\\mathbf{X})=\\boldsymbol\\beta\\), \\(\\mathbb{V}ar[\\mathbb{E}(\\mathbf{b}|\\mathbf{X})]=0\\). Eq. (1.30) implies \\(\\mathbb{V}ar(\\mathbf{b}|\\mathbf{X}) \\rightarrow 0\\). Hence \\(\\mathbf{b}\\) converges mean square, therefore probability (see Prop. 5.13).Prop. 1.14 gives asymptotic distribution OLS estimator GRM framework.Proposition 1.14  (Asymptotic distribution OLS estimator GRM framework) \\(Q_{xx}=\\mbox{plim }(\\mathbf{X}'\\mathbf{X}/n)\\) \\(Q_{x\\boldsymbol\\Sigma x}=\\mbox{plim }(\\mathbf{X}'\\boldsymbol\\Sigma\\mathbf{X}/n)\\) finite positive definite matrices, :\n\\[\n\\sqrt{n}(\\mathbf{b}-\\boldsymbol\\beta) \\overset{d}{\\rightarrow} \\mathcal{N}(0,Q_{xx}^{-1}Q_{x\\boldsymbol\\Sigma x}Q_{xx}^{-1}).\n\\]IV estimator also features normal asymptotic distribution:Proposition 1.15  (Asymptotic distribution IV estimator GRM framework) regressors IV variables “well-behaved”, :\n\\[\n\\mathbf{b}_{iv} \\overset{}{\\sim} \\mathcal{N}(\\boldsymbol\\beta,\\mathbf{V}_{iv}),\n\\]\n\n\\[\n\\mathbf{V}_{iv} = \\frac{1}{n}(\\mathbf{Q}^*)\\mbox{ plim }\\left(\\frac{1}{n} \\mathbf{Z}'\\boldsymbol\\Sigma \\mathbf{Z}\\right)(\\mathbf{Q}^*)',\n\\]\n\n\\[\n\\mathbf{Q}^* = [\\mathbf{Q}_{xz}\\mathbf{Q}_{zz}^{-1}\\mathbf{Q}_{zx}]^{-1}\\mathbf{Q}_{xz}\\mathbf{Q}_{zz}^{-1}.\n\\]practical purposes, one needs estimates \\(\\boldsymbol\\Sigma\\) Props. 1.14 1.15. complication comes fact \\(\\boldsymbol\\Sigma\\) dimension \\(n \\times n\\), estimation —based sample length \\(n\\)— therefore infeasible general case. Notwithstanding, looking Eq. (1.30), appears one can focus estimation \\(Q_{x\\boldsymbol\\Sigma x}=\\mbox{plim }(\\mathbf{X}'\\boldsymbol\\Sigma\\mathbf{X}/n)\\) (\\(\\mbox{plim }\\left(\\frac{1}{n} \\mathbf{Z}'\\boldsymbol\\Sigma \\mathbf{Z}\\right)\\) IV case). matrix dimension \\(K \\times K\\), estimation easier.:\n\\[\\begin{equation}\n\\frac{1}{n}\\mathbf{X}'\\boldsymbol\\Sigma\\mathbf{X} = \\frac{1}{n}\\sum_{=1}^{n}\\sum_{j=1}^{n}\\sigma_{,j}\\mathbf{x}_i\\mathbf{x}'_j. \\tag{1.31}\n\\end{equation}\\]-called robust covariance matrices estimates previous matrix. computation based fact \\(\\mathbf{b}\\) consistent, \\(e_i\\)’s consistent estimators \\(\\varepsilon_i\\)’s.following sections (1.5.4 1.5.5), present two types robust covariance matrices.","code":""},{"path":"ChapterLS.html","id":"HAC","chapter":"1 Linear Regressions","heading":"1.5.4 HAC-robust covariance matrices","text":"heteroskedasticity prevails, .e., matrix \\(\\boldsymbol\\Sigma\\) Eq. (1.24), one can use formula proposed White (1980) estimate \\(\\frac{1}{n}\\mathbf{X}'\\boldsymbol\\Sigma\\mathbf{X}\\) (see Example 1.8). residuals feature heteroskedasticity auto-correlation, one can use Newey West (1987) approach (see Example 1.9).Example 1.8  (Heteroskedasticity) case Eq. (1.24). \\(\\sigma_{,j}=0\\) \\(\\ne j\\). Hence, case, need estimate \\(\\frac{1}{n}\\sum_{=1}^{n}\\sigma_{}^2\\mathbf{x}_i\\mathbf{x}'_i\\). White (1980) shown , general conditions:\n\\[\\begin{equation}\n\\mbox{plim}\\left( \\frac{1}{n}\\sum_{=1}^{n}\\sigma_{}^2\\mathbf{x}_i\\mathbf{x}'_i \\right) =\n\\mbox{plim}\\left( \\frac{1}{n}\\sum_{=1}^{n}e_{}^2\\mathbf{x}_i\\mathbf{x}'_i \\right). \\tag{1.32}\n\\end{equation}\\]\nestimator \\(\\frac{1}{n}\\mathbf{X}'\\boldsymbol\\Sigma\\mathbf{X}\\) therefore :\n\\[\\begin{equation}\nM_{HC0} = \\frac{1}{n}\\mathbf{X}'\n\\left[\n\\begin{array}{cccc}\ne_1^2 & 0 & \\dots & 0 \\\\\n0 & e_2^2 &  \\\\\n\\vdots & & \\ddots&0 \\\\\n0 & \\dots & 0 & e_n^2\n\\end{array}\n\\right]\n\\mathbf{X}.\\tag{1.33}\n\\end{equation}\\]\n\\(e_i\\) OLS residuals regression. previous estimator often called HC0. HC1 estimator, due J. MacKinnon White (1985), obtained applying adjustment factor \\(n/(n-K)\\) number degrees freedom (Prop. 1.6). :\n\\[\\begin{equation}\nM_{HC1} = \\frac{n}{n-K}M_{HC0}.\\tag{1.34}\n\\end{equation}\\]can illustrate influence heteroskedasticity using simulations. Consider following model:\n\\[\ny_i = x_i + \\varepsilon_i, \\quad \\varepsilon_i \\sim \\mathcal{N}(0,x_i^2),\n\\]\n\\(x_i\\)’s ..d. \\(t(6)\\).simulated sample (\\(n=200\\)) model:\nFigure 1.6: Situation heteroskedasticity. model \\(y_i = x_i + \\varepsilon_i, \\quad \\varepsilon_i \\sim \\mathcal{N}(0,x_i^2)\\), \\(x_i\\)’s ..d. \\(t(6)\\).\nsimulate 1000 samples model \\(n=200\\). sample, compute OLS estimate \\(\\beta\\) (\\(=1\\)). 1000 OLS estimations, employ () standard OLS variance formula (\\(s^2 (\\mathbf{X}'\\mathbf{X})^{-1}\\)) (b) White formula estimate variance \\(b\\). formula, compute average 1000 resulting standard deviations compare standard deviation 1000 OLS estimate \\(\\beta\\).results show White formula yields, average, estimated standard deviation much closer “true” value standard OLS formula. latter underestimate standard deviation \\(b\\).following example, regress GDP growth rates Jordà, Schularick, Taylor (2017) database systemic financial crisis dummy. compute HC0- HC1-based standard deviations parameter estimate, compare one based standard OLS formula. adjusted standard deviations close one provided non-adjusted OLS formula.Example 1.9  (Heteroskedasticity Autocorrelation (HAC)) Newey West (1987) proposed formula address heteroskedasticity auto-correlation residuals (Eqs. (1.24) (1.25)). show , correlation terms \\(\\) \\(j\\) gets sufficiently small \\(|-j|\\) increases:\n\\[\\begin{eqnarray}\n&&\\mbox{plim} \\left( \\frac{1}{n}\\sum_{=1}^{n}\\sum_{j=1}^{n}\\sigma_{,j}\\mathbf{x}_i\\mathbf{x}'_j \\right) \\approx  \\\\\n&&\\mbox{plim} \\left( \\frac{1}{n}\\sum_{t=1}^{n}e_{t}^2\\mathbf{x}_t\\mathbf{x}'_t +\n\\frac{1}{n}\\sum_{\\ell=1}^{L}\\sum_{t=\\ell+1}^{n}w_\\ell e_{t}e_{t-\\ell}(\\mathbf{x}_t\\mathbf{x}'_{t-\\ell} + \\mathbf{x}_{t-\\ell}\\mathbf{x}'_{t})\n\\right), \\nonumber \\tag{1.35}\n\\end{eqnarray}\\]\n\\(w_\\ell = 1 - \\ell/(L+1)\\) (\\(L\\) large).Let us illustrate influence autocorrelation using simulations. consider following model:\n\\[\\begin{equation}\ny_i = x_i + \\varepsilon_i, \\quad \\varepsilon_i \\sim \\mathcal{N}(0,x_i^2),\\tag{1.36}\n\\end{equation}\\]\n\\(x_i\\)’s \\(\\varepsilon_i\\)’s :\n\\[\\begin{equation}\nx_i = 0.8 x_{-1} + u_i \\quad \\quad \\varepsilon_i = 0.8 \\varepsilon_{-1} + v_i, \\tag{1.37}\n\\end{equation}\\]\n\\(u_i\\)’s \\(v_i\\)’s ..d. \\(\\mathcal{N}(0,1)\\).simulate 500 samples model \\(n=200\\). sample, compute OLS estimate \\(\\beta\\) (=1). 1000 OLS estimations, employ () standard OLS variance formula (\\(s^2 (\\mathbf{X}'\\mathbf{X})^{-1}\\)), (b) White formula, (c) Newey-West formula estimate variance \\(b\\). formula, compute average 500 resulting standard deviations compare standard deviation 500 OLS estimate \\(\\beta\\).results show Newey-West formula yields, average, estimated standard deviation closer “true” value standard OLS formula. latter underestimate standard deviation \\(b\\).precedes suggest , residuals feature autocorrelation, important use appropriately adjusted covariance matrices make statistical inference. detect autocorrelation residuals? popular test proposed Durbin Watson (1950) Durbin Watson (1951). Durbin-Watson test statistic :\n\\[\nDW = \\frac{\\sum_{=2}^{n}(e_i - e_{-1})^2}{\\sum_{=1}^{n}e_i^2}= 2(1 - r) - \\underbrace{\\frac{e_1^2 + e_n^2}{\\sum_{=1}^{n}e_i^2}}_{\\overset{p}{\\rightarrow} 0},\n\\]\n\\(r\\) slope regression \\(e_i\\)’s \\(e_{-1}\\)’s, .e.:\n\\[\nr = \\frac{\\sum_{=2}^{n}e_i e_{-1}}{\\sum_{=1}^{n-1}e_i^2}.\n\\]\n(\\(r\\) consistent estimator \\(\\mathbb{C}(\\varepsilon_i,\\varepsilon_{-1})\\), .e. \\(\\rho\\) Eq. (1.27).)one-sided test \\(H_0\\): \\(\\rho=0\\) \\(H_1\\): \\(\\rho>0\\) carried comparing \\(DW\\) values \\(d_L(T, K)\\) \\(d_U(T, K)\\):\n\\[\n\\left\\{\n\\begin{array}{ll}\n\\mbox{$DW < d_L$,}&\\mbox{ null hypothesis rejected;}\\\\\n\\mbox{$DW > d_U$,}&\\mbox{ hypothesis rejected;}\\\\\n\\mbox{$d_L \\le DW \\le d_U$,} &\\mbox{ conclusion drawn.}\n\\end{array}\n\\right.\n\\]Example 1.10  (Durbin-Watson test) regress short-term nominal US interest rate inflation. employ Durbin-Watson test see whether residuals auto-correlated (quite obviously case).","code":"\nn <- 200\nx <- rt(n,df=6)\ny <- x + x*rnorm(n)\npar(plt=c(.1,.95,.1,.95))\nplot(x,y,pch=19)\nn <- 200 # sample size\nN <- 1000 # number of simulated samples\nXX <- matrix(rt(n*N,df=6),n,N)\nYY <- matrix(XX + XX*rnorm(n),n,N)\nall_b       <- NULL;all_V_OLS   <- NULL;all_V_White <- NULL\nfor(j in 1:N){\n  Y <- matrix(YY[,j],ncol=1)\n  X <- matrix(XX[,j],ncol=1)\n  b <- solve(t(X)%*%X) %*% t(X)%*%Y\n  e <- Y - X %*% b\n  S <- 1/n * t(X) %*% diag(c(e^2)) %*% X\n  V_OLS   <- solve(t(X)%*%X) * var(e)\n  V_White <- 1/n * (solve(1/n*t(X)%*%X)) %*% S %*% (solve(1/n*t(X)%*%X))\n  all_b       <- c(all_b,b)\n  all_V_OLS   <- c(all_V_OLS,V_OLS)\n  all_V_White <- c(all_V_White,V_White)\n}\nc(sd(all_b),mean(sqrt(all_V_OLS)),mean(sqrt(all_V_White)))## [1] 0.14024423 0.06748804 0.13973431\nlibrary(lmtest)\nlibrary(sandwich)\nnT <- dim(JST)[1]\nJST$growth <- NaN\nJST$growth[2:nT] <- log(JST$rgdpbarro[2:nT]/JST$rgdpbarro[1:(nT-1)])\nJST.red <- subset(JST,year>1950)\nJST.red$iso <- as.factor(JST.red$iso)\nJST.red$year <- as.factor(JST.red$year)\neq <- lm(growth~crisisJST+iso,data=JST.red)\nrbind(coeftest(eq)[2,],\n      coeftest(eq, vcov = vcovHC(eq, type = \"HC0\"))[2,],\n      coeftest(eq, vcov = vcovHC(eq, type = \"HC1\"))[2,])##         Estimate  Std. Error   t value     Pr(>|t|)\n## [1,] -0.02481424 0.005490411 -4.519560 6.789284e-06\n## [2,] -0.02481424 0.005605452 -4.426804 1.040602e-05\n## [3,] -0.02481424 0.005648268 -4.393247 1.212105e-05\nlibrary(AEC)\nn <- 100 # sample length\nnb.sim <- 500 # number of simulated samples\nall.b <- NULL;all.OLS.stdv.b <- NULL\nall.Whi.stdv.b <- NULL;all.NW.stdv.b <- NULL\nfor(i in 1:nb.sim){\n  eps <- rnorm(n);x <- rnorm(n)\n  for(i in 2:n){\n    eps[i] <- eps[i] + .8*eps[i-1]\n    x[i]   <- x[i]   + .8*x[i-1]\n  }\n  y <- x + eps\n  eq <- lm(y~x)\n  all.b <- c(all.b,eq$coefficients[2])\n  all.OLS.stdv.b <- c(all.OLS.stdv.b,summary(eq)$coefficients[2,2])\n  X <- cbind(rep(1,n),x)\n  XX <- t(X) %*% X;XX_1 <- solve(XX)\n  E2 <- diag(eq$residuals^2)\n  White.V <- XX_1 %*% (t(X) %*% E2 %*% X) %*% XX_1\n  all.Whi.stdv.b <- c(all.Whi.stdv.b,sqrt(White.V[2,2]))\n  # HAC:\n  U <- X * cbind(eq$residuals,eq$residuals)\n  XSigmaX <- NW.LongRunVariance(U,5)\n  NW.V <- 1/n * (n*XX_1) %*% XSigmaX %*% (n*XX_1)\n  all.NW.stdv.b <- c(all.NW.stdv.b,sqrt(NW.V[2,2]))\n}\ncbind(sd(all.b),mean(all.OLS.stdv.b),\n      mean(all.Whi.stdv.b),mean(all.NW.stdv.b))##          [,1]      [,2]      [,3]     [,4]\n## [1,] 0.201172 0.1013048 0.0962689 0.146974\nlibrary(car)\ndata <- subset(JST,iso==\"USA\");T <- dim(data)[1]\ndata$infl <- c(NaN,100*log(data$cpi[2:T]/data$cpi[1:(T-1)]))\ndata$infl[(data$infl< -5)|(data$infl>10)] <- NaN\npar(mfrow=c(1,2))\nplot(data$year,data$stir,ylim=c(-10,20),type=\"l\",lwd=2,xlab=\"\",\n     ylab=\"\",main=\"Nominal rate and inflation\")\nlines(data$year,data$infl,col=\"red\",lwd=2)\neq <- lm(stir~infl,data=data)\nplot(eq$residuals,type=\"l\",col=\"blue\",main=\"Residuals\",xlab=\"\",ylab=\"\")\ndurbinWatsonTest(eq)##  lag Autocorrelation D-W Statistic p-value\n##    1       0.7321902     0.4984178       0\n##  Alternative hypothesis: rho != 0"},{"path":"ChapterLS.html","id":"Clusters","chapter":"1 Linear Regressions","heading":"1.5.5 Cluster-robust covariance matrices","text":"present section based J. G. MacKinnon, Nielsen, Webb (2022); another useful reference Cameron Miller (2014). see one can approximate \\(Q_{x\\boldsymbol\\Sigma x}=\\mbox{plim }(\\mathbf{X}'\\boldsymbol\\Sigma\\mathbf{X}/n)\\) (see Prop. 1.14) dataset can decomposed clusters. clusters constitute partition sample, error terms may correlated within given cluster, across different clusters. cluster may, e.g., gathers entities given geographical area, industry, age cohort.OLS estimator satisfies:\n\\[\\begin{equation}\n\\mathbf{b} = \\boldsymbol\\beta + (\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\boldsymbol\\varepsilon.\\tag{1.38}\n\\end{equation}\\]\nConsider set \\(\\{n_1,n_2,\\dots,n_G\\}\\) s.t. \\(n=\\sum_g n_g\\), based following decomposition \\(\\mathbf{X}\\):\n\\[\n\\mathbf{X} = \\left[\n\\begin{array}{c}\n\\mathbf{X}_1 \\\\\n\\mathbf{X}_1 \\\\\n\\vdots\\\\\n\\mathbf{X}_G\n\\end{array}\n\\right].\n\\]\nnotations, Eq. (1.38) rewrites:\n\\[\\begin{equation}\n\\mathbf{b} - \\boldsymbol\\beta = \\left(\\sum_{g=1}^G \\mathbf{X}_g'\\mathbf{X}_g\\right)^{-1}\\sum_{g=1}^G \\mathbf{s}_g,\\tag{1.39}\n\\end{equation}\\]\n\n\\[\\begin{equation}\n\\mathbf{s}_g = \\mathbf{X}_g'\\boldsymbol\\varepsilon_g \\tag{1.40}\n\\end{equation}\\]\ndenotes score vector (dimension \\(K \\times 1\\)) associated \\(g^{th}\\) cluster.model correctly specified \\(\\mathbb{E}(\\mathbf{s}_g)=0\\) clusters \\(g\\). Note Eq. (1.39) valid partition \\(\\{1,\\dots,n\\}\\). Dividing sample clusters becomes meaningful assume following hypothesis holds:Hypothesis 1.6  (Clusters) :\n\\[\n()\\; \\mathbb{E}(\\mathbf{s}_g\\mathbf{s}_g')=\\Sigma_g,\\quad (ii)\\; \\mathbb{E}(\\mathbf{s}_g\\mathbf{s}_q')=0,\\;g \\ne q,\n\\]\n\\(s_g\\) defined Eq. (1.40).real assumption \\((ii)\\); first one simply gives notation covariance matrix score associated \\(g^{th}\\) cluster. Remark covariance matrices can differ across clusters. , cluster-based inference robust heteroskedasticity intra-cluster dependence without imposing restrictions (unknown) form either .Naturally, matrix \\(\\Sigma_g\\) depends covariance structure \\(\\varepsilon\\)’s. particular, \\(\\Omega_g = \\mathbb{E}(\\boldsymbol\\varepsilon_g\\boldsymbol\\varepsilon_g'|\\mathbf{X}_g)\\), \\(\\Sigma_g = \\mathbb{E}(\\mathbf{X}_g'\\Omega_g\\mathbf{X}_g)\\).Hypothesis 1.6, comes conditional covariance matrix \\(\\mathbf{b}\\) :\n\\[\\begin{equation}\n\\left(\\mathbf{X}'\\mathbf{X}\\right)^{-1}\\left(\\sum_{g=1}^G \\Sigma_g\\right)\\left(\\mathbf{X}'\\mathbf{X}\\right)^{-1}\\tag{1.41}\n\\end{equation}\\]Let us denote \\(\\varepsilon_{g,}\\) error associated \\(^{th}\\) component vector \\(\\boldsymbol\\varepsilon_g\\). Consider special case \\(\\mathbb{E}(\\varepsilon_{g,} \\varepsilon_{g,j}|\\mathbf{X}_g)=\\sigma^2\\mathbb{}_{\\{=j\\}}\\), Eq. (1.41) gives standard expression \\(\\sigma^2\\left(\\mathbf{X}'\\mathbf{X}\\right)^{-1}\\) (see Eq. (1.8)).\\(\\mathbb{E}(\\varepsilon_{gi} \\varepsilon_{gj}|\\mathbf{X}_g)=\\sigma_{gi}^2\\mathbb{}_{\\{=j\\}}\\), fall case addressed White formula (see Example 1.8). , case, conditional covariance matrix \\(\\mathbf{b}\\) :\n\\[\n\\left(\\mathbf{X}'\\mathbf{X}\\right)^{-1}\\left(\\mathbf{X}'\\left[  \\begin{array}{cccc}\n\\sigma_1^2 & 0 & \\dots & 0 \\\\\n0 & \\sigma_2^2 &  & 0 \\\\\n\\vdots && \\ddots& \\vdots \\\\\n0 & \\dots & 0 & \\sigma_n^2\n\\end{array} \\right]\\mathbf{X}\\right)\\left(\\mathbf{X}'\\mathbf{X}\\right)^{-1}.\n\\]\nWhite (1980), natural way approach conditional covariance given Eq. (1.41) consists replacing \\(\\Sigma_g\\) matrices sample equivalent, .e. \\(\\widehat{\\Sigma}_g=\\mathbf{X}_g'\\mathbf{e}_g\\mathbf{e}_g'\\mathbf{X}_g\\). Adding corrections number degrees freedom, leads following estimate covariance matrix \\(\\mathbf{b}\\):\n\\[\\begin{equation}\n\\frac{G(n-1)}{(G-1)(n-K)}\\left(\\mathbf{X}'\\mathbf{X}\\right)^{-1}\\left(\\sum_{g=1}^G\\widehat{\\Sigma}_g\\right) \\left(\\mathbf{X}'\\mathbf{X}\\right)^{-1}. \\tag{1.42}\n\\end{equation}\\]\nprevious estimate CRCV1 J. G. MacKinnon, Nielsen, Webb (2022). Note indeed find White-MacKinnon estimator (Eq. (1.34)) \\(G=n\\).Remark one cluster (\\(G=1\\)), neglecting degree--freedom correction, :\n\\[\n\\left(\\mathbf{X}'\\mathbf{X}\\right)^{-1}\\left(\\mathbf{X}'\\mathbf{e}\\mathbf{e}'\\mathbf{X}\\right) \\left(\\mathbf{X}'\\mathbf{X}\\right)^{-1} = 0\n\\]\n\\(\\mathbf{X}'\\mathbf{e}=0\\). Hence, large clusters necessarily increase variance.Often, working panel data (see Chapter 2), want cluster different dimensions. typical case data indexed individuals (\\(\\)) time (\\(t\\)). case, may indeed suspect : () residuals correlated across clusters dates (e.g., monthly data, cluster may one year) (b) residuals correlated across clusters individuals (e.g., data county level cluster may state). case, one can employ two-way clustering.Formally, consider two distinct partitions data: one index \\(g\\), \\(g \\\\{1,\\dots,G\\}\\), index \\(h\\), \\(h \\\\{1,\\dots,H\\}\\). Accordingly, denote \\(\\mathbf{X}_{g,h}\\) submatrix \\(\\mathbf{X}\\) contains explanatory variables corresponding clusters \\(g\\) \\(h\\) (e.g., firms given country \\(g\\) given date \\(h\\)). also denote \\(\\mathbf{X}_{g,\\bullet}\\) (respectively \\(\\mathbf{X}_{\\bullet,h}\\)) submatrix \\(\\mathbf{X}\\) containing explanatory variables pertaining cluster \\(g\\), possible values \\(h\\) (resp. cluster \\(h\\), possible values \\(g\\)).make following assumption:Hypothesis 1.7  (Two-way clusters) :\n\\[\\begin{eqnarray*}\n&&\\mathbb{E}(\\mathbf{s}_{g,\\bullet}\\mathbf{s}_{g,\\bullet}')=\\Sigma_g,\\quad \\mathbb{E}(\\mathbf{s}_{\\bullet,h}\\mathbf{s}_{\\bullet,h}')=\\Sigma^*_h,\\quad \\mathbb{E}(\\mathbf{s}_{g,h}\\mathbf{s}_{g,h}')=\\Sigma_{g,h},\\\\ &&\\mathbb{E}(\\mathbf{s}_{g,h}\\mathbf{s}_{q,k}')=0\\;\\mbox{}g\\neq q\\mbox{ }h \\ne k.\n\\end{eqnarray*}\\]Proposition 1.16  (Covariance scores two-way-cluster setup) assumption, matrix covariance scores given :\n\\[\n\\Sigma = \\sum_{g=1}^G \\Sigma_{g} + \\sum_{h=1}^H \\Sigma^*_{h} - \\sum_{g=1}^G\\sum_{h=1}^H \\Sigma_{g,h}.\n\\]\n(last term right-hand side must subtracted order avoid double counting.)Proof. :\n\\[\\begin{eqnarray*}\n\\Sigma &=& \\sum_{g=1}^G\\sum_{q=1}^G\\sum_{h=1}^H\\sum_{k=1}^H \\mathbf{s}_{g,h}\\mathbf{s}_{q,k}'\\\\\n&=& \\sum_{g=1}^G\\underbrace{\\left(\\sum_{h=1}^H\\sum_{k=1}^H \\mathbf{s}_{g,h}\\mathbf{s}_{g,k}'\\right)}_{=\\Sigma_g}+\\sum_{h=1}^H\\underbrace{\\left(\\sum_{g=1}^G\\sum_{q=1}^G \\mathbf{s}_{g,h}\\mathbf{s}_{q,h}'\\right)}_{=\\Sigma^*_h}-\\sum_{g=1}^G\\sum_{h=1}^H \\mathbf{s}_{g,h}\\mathbf{s}_{g,h}',\n\\end{eqnarray*}\\]\ngives result.asymptotic theory can based two different approaches: () large number clusters (common case), (ii) fixed number clusters large number observations cluster (see Subsections 4.1 4.2 J. G. MacKinnon, Nielsen, Webb (2022)). variable \\(N_g\\)’s (clusters’ sizes heterogeneous terms size), less reliable asymptotic inference based Eq. (1.42), especially clusters unusually large, distribution data heavy-tailed. issues somehow mitigated clusters approximate factor structure.practice, \\(\\Sigma\\) estimated :\n\\[\n\\widehat{\\Sigma} = \\sum_{g=1}^G \\widehat{\\mathbf{s}}_{g,\\bullet}\\widehat{\\mathbf{s}}_{g,\\bullet}' + \\sum_{h=1}^H \\widehat{\\mathbf{s}}_{\\bullet,h}\\widehat{\\mathbf{s}}_{\\bullet,h} - \\sum_{g=1}^G\\sum_{h=1}^H \\widehat{\\mathbf{s}}_{g,h}\\widehat{\\mathbf{s}}_{g,h}',\n\\]\nuse:\n\\[\n\\widehat{\\mathbb{V}ar}(\\mathbf{b}) = \\left(\\mathbf{X}'\\mathbf{X}\\right)^{-1}\\widehat{\\Sigma}\\left(\\mathbf{X}'\\mathbf{X}\\right)^{-1}.\n\\]alternative asymptotic approximation distribution statistic interest, one can resort bootstrap approximation (see Section 5 J. G. MacKinnon, Nielsen, Webb (2022)). R, package fwildclusterboot allows implement approaches.3Let us come back analysis effect systemic financial crises GDP growth. Clustering data country level , , country time levels gives following:","code":"\neq <- lm(growth~crisisJST+iso,data=JST.red)\nrbind(coeftest(eq)[2,],\n      coeftest(eq, vcov = vcovHC(eq, type = \"HC1\"))[2,],\n      coeftest(eq, vcov = vcovCL(eq, cluster = JST.red[, c(\"iso\")]))[2,],\n      coeftest(eq, vcov = vcovCL(eq, cluster = JST.red[, c(\"iso\",\"year\")]))[2,])##         Estimate  Std. Error   t value     Pr(>|t|)\n## [1,] -0.02481424 0.005490411 -4.519560 6.789284e-06\n## [2,] -0.02481424 0.005648268 -4.393247 1.212105e-05\n## [3,] -0.02481424 0.005847708 -4.243413 2.365508e-05\n## [4,] -0.02481424 0.006546931 -3.790209 1.577572e-04"},{"path":"ChapterLS.html","id":"shrinkage-methods","chapter":"1 Linear Regressions","heading":"1.6 Shrinkage methods","text":"Chosing appropriate explanatory variables often complicated, especially presence many potentially relevant covariates. Keeping large number covariates results large standard deviations estimated parameters (see Section 1.3.3). order address issue, shrinkage methods designed. objective methods help select limited number variables shrinking regression coefficients less useful variables towards zero. two best-known shrinkage techniques ridge regression lasso approach.4 cases (ridge lasso), OLS minimization problem (see Section 1.2), .e.,\n\\[\\begin{equation}\n\\mathbf{b} = \\underset{\\boldsymbol\\beta}{\\mbox{argmin}}\\; \\sum_{=1}^n(y_i - \\mathbf{x}_i'\\boldsymbol\\beta)^2\n\\end{equation}\\]\nreplaced following:\n\\[\\begin{equation}\n\\mathbf{b}_\\lambda = \\underset{\\boldsymbol\\beta}{\\mbox{argmin}}\\; \\sum_{=1}^n(y_i - \\mathbf{x}_i'\\boldsymbol\\beta)^2 + \\lambda f(\\boldsymbol\\beta),\\tag{1.43}\n\\end{equation}\\]\n\\(\\lambda f(\\boldsymbol\\beta)\\) penalty term positively depends “size” components \\(\\boldsymbol\\beta\\). term called shrinkage penalty term.Specifically, assuming vector \\(\\mathbf{x}_i\\) potential covariates dimension \\(K \\times 1\\), :\n\\[\\begin{eqnarray*}\nf(\\boldsymbol\\beta) & = & \\sum_{j=1}^K \\beta_j^2 \\quad \\mbox{ridge case ($\\ell_2$ norm)},\\\\\nf(\\boldsymbol\\beta) & = & \\sum_{j=1}^K |\\beta_j| \\quad \\mbox{lasso case ($\\ell_1$ norm)}.\n\\end{eqnarray*}\\]cases, want involve intercept set parameters shrink, preceding equations respectively replaced :\n\\[\\begin{eqnarray*}\nf(\\boldsymbol\\beta) & = & \\sum_{j=2}^K \\beta_j^2 \\quad \\mbox{(ridge)},\\\\\nf(\\boldsymbol\\beta) & = & \\sum_{j=2}^K |\\beta_j| \\quad \\mbox{(lasso)}.\n\\end{eqnarray*}\\]nature penalty —based either \\(\\ell_1\\) \\(\\ell_2\\) norm— implies different behaviour parameter estimates \\(\\lambda\\) —tuning parameter— grows. ridge regression, coefficient estimates go zero (shrinkage); lasso case, coefficients reach zero \\(\\lambda\\) reach values. words, ridge regression achieve shrinkage, lasso regressions achieve shrinkage variable selection.Parameter \\(\\lambda\\) determined separately minimization problem Eq. (1.43). One can combine standard criteria (e.g., BIC Akaike) purpose.R, one can use glmnet package run ridge lasso regressions. following example, employ package model interest rates proposed debtors, using data extracted Lending Club platform.begin , let us define variables want consider:Let us standardize data:Next, define set \\(\\lambda\\) use, run ridge lasso regressions:following figure shows estimated parameters depend \\(\\lambda\\):Let us take two values \\(\\lambda\\) see associated estimated parameters context lasso regressions:glmnet package (see Hastie et al. (2021)) also offers tools implement cross-validation:","code":"\nlibrary(AEC)\nlibrary(glmnet)\ncredit$owner <- 1*(credit$home_ownership==\"OWN\")\ncredit$renter <- 1*(credit$home_ownership==\"MORTGAGE\")\ncredit$verification_status <- 1*(credit$verification_status==\"Not Verified\")\ncredit$emp_length_10 <- 1*(credit$emp_length_10)\ncredit$log_annual_inc <- log(credit$annual_inc)\ncredit$log_funded_amnt <- log(credit$funded_amnt)\ncredit$annual_inc2 <- (credit$annual_inc)^2\ncredit$funded_amnt2 <- (credit$funded_amnt)^2\nx <- subset(credit,\n            select = c(delinq_2yrs,annual_inc,annual_inc2,log_annual_inc,\n                       dti,installment,verification_status,funded_amnt,\n                       funded_amnt2,log_funded_amnt,pub_rec,emp_length_10,\n                       owner,renter,pub_rec_bankruptcies,revol_util,revol_bal))\ny <- scale(credit$int_rate)\nx <- scale(x)\ngrid.lambda <- seq(0,.2,by=.005)\nresult.ridge <- glmnet(x, y, alpha = 0, lambda = grid.lambda)\nresult.lasso <- glmnet(x, y, alpha = 1, lambda = grid.lambda)\nvariab <- 6\nplot(result.ridge$lambda,coef(result.ridge)[variab,],type=\"l\",\n     ylim=c(min(coef(result.ridge)[variab,],coef(result.lasso)[variab,]),\n            max(coef(result.ridge)[variab,],coef(result.lasso)[variab,])),\n     xlab=expression(lambda),ylab=\"Estimated parameter\",lwd=2)\nlines(result.lasso$lambda,coef(result.lasso)[variab,],col=\"red\",lwd=2)\ni <- 20; j <- 40\ncbind(result.lasso$lambda[i],result.lasso$lambda[j])##       [,1]  [,2]\n## [1,] 0.105 0.005\ncbind(coef(result.lasso)[,i],coef(result.lasso)[,j])##                               [,1]          [,2]\n## (Intercept)          -1.044971e-15  1.088731e-14\n## delinq_2yrs           6.308870e-02  6.893527e-02\n## annual_inc            0.000000e+00  4.595653e-03\n## annual_inc2           0.000000e+00  0.000000e+00\n## log_annual_inc        0.000000e+00 -3.612382e-02\n## dti                   0.000000e+00  2.242246e-02\n## installment           1.476796e-01  8.228729e+00\n## verification_status   0.000000e+00 -9.750047e-04\n## funded_amnt           0.000000e+00 -7.309169e+00\n## funded_amnt2          0.000000e+00 -4.711846e-01\n## log_funded_amnt       0.000000e+00 -2.460932e-01\n## pub_rec               3.390816e-02  5.997252e-02\n## emp_length_10         0.000000e+00 -1.924941e-02\n## owner                 0.000000e+00 -2.444599e-02\n## renter               -3.882640e-02 -6.243087e-02\n## pub_rec_bankruptcies  0.000000e+00  0.000000e+00\n## revol_util            0.000000e+00  0.000000e+00\n## revol_bal             0.000000e+00  2.402268e-03\n# Compute values of y predicted by the model, for all lambdas:\npred1 <- predict(result.lasso,as.matrix(x))\n# Compute values of y predicted by the model, for a specific value:\npred2 <- predict(result.lasso,as.matrix(x),s=0.085)\n# cross validation (cv):\ncvglmnet <- cv.glmnet(as.matrix(x),y)\nplot(cvglmnet)\n# lambda.min: lambda that gives minimum mean cross-validated error\ncvglmnet$lambda.min ## [1] 0.004091039\n# lambda.1se: largest lambda s.t. cost within the one-std-dev cv-based band\ncvglmnet$lambda.1se## [1] 0.00448991\ncoef(cvglmnet, s = \"lambda.min\") # associated parameters## 18 x 1 sparse Matrix of class \"dgCMatrix\"\n##                                 s1\n## (Intercept)           1.128990e-14\n## delinq_2yrs           6.625860e-02\n## annual_inc            6.054923e-03\n## annual_inc2           .           \n## log_annual_inc       -3.795151e-02\n## dti                   2.065272e-02\n## installment           8.518961e+00\n## verification_status  -2.894435e-03\n## funded_amnt          -7.560853e+00\n## funded_amnt2         -5.041923e-01\n## log_funded_amnt      -2.537567e-01\n## pub_rec               5.804333e-02\n## emp_length_10        -1.894666e-02\n## owner                -2.484192e-02\n## renter               -6.039831e-02\n## pub_rec_bankruptcies  .           \n## revol_util            .           \n## revol_bal             3.093752e-03\n# predicted values of y for specific values of x:\npredict(cvglmnet, newx = as.matrix(x)[1:5,], s = \"lambda.min\") ##        lambda.min\n## 21529  0.34496384\n## 21547 -0.04553753\n## 21579  0.56455499\n## 21583 -0.20696954\n## 21608 -0.12165356"},{"path":"Panel.html","id":"Panel","chapter":"2 Panel regressions","heading":"2 Panel regressions","text":"","code":""},{"path":"Panel.html","id":"specification-and-notations","chapter":"2 Panel regressions","heading":"2.1 Specification and notations","text":"standard panel situation follows: sample covers lot “entities”, indexed \\(\\\\{1,\\dots,n\\}\\), \\(n\\) large, , entity, observe different variables small number periods \\(t \\\\{1,\\dots,T\\}\\). longitudinal dataset.linear panel regression model :\n\\[\\begin{equation}\ny_{,t} = \\mathbf{x}'_{,t}\\underbrace{\\boldsymbol\\beta}_{K \\times 1} + \\underbrace{\\mathbf{z}'_{}\\boldsymbol\\alpha}_{\\mbox{Individual effects}} + \\varepsilon_{,t}.\\tag{2.1}\n\\end{equation}\\]running panel regressions, usual objective estimate \\(\\boldsymbol\\beta\\).Figure 2.1 illustrates panel-data situation. model \\(y_i = \\alpha_i + \\beta x_{,t} + \\varepsilon_{,t}\\), \\(t \\\\{1,2\\}\\). Panel (b), blue dots \\(t=1\\), red dots \\(t=2\\). lines relate dots associated entity \\(\\). remarkable simulated model , unconditional correlation \\(y\\) \\(x\\) negative, conditional correlation (conditional \\(\\alpha_i\\)) positive. Indeed, sign conditional correlation sign \\(\\beta\\), positive th simulated example (\\(\\beta=5\\)). words, one know panel nature data, tempting say \\(\\beta<0\\), case, due fixed effects (\\(\\alpha_i\\)’s) negatively correlated \\(x_i\\)’s.\nFigure 2.1: data panels. Panel (b), blue dots \\(t=1\\), red dots \\(t=2\\). lines relate dots associated entity \\(\\).\nFigure 2.2 presents type plot based Cigarette Consumption Panel dataset (CigarettesSW dataset, used J. Stock Watson (2003)). dataset documents average consumption cigarettes 48 continental US states two dates (1985 1995).\nFigure 2.2: Cigarette consumption versus real price CigarettesSW panel dataset.\nmake use following notations:\n\\[\n\\mathbf{y}_i =\n\\underbrace{\\left[\n\\begin{array}{c}\ny_{,1}\\\\\n\\vdots\\\\\ny_{,T}\n\\end{array}\\right]}_{T \\times 1}, \\quad\n\\boldsymbol\\varepsilon_i =\n\\underbrace{\\left[\n\\begin{array}{c}\n\\varepsilon_{,1}\\\\\n\\vdots\\\\\n\\varepsilon_{,T}\n\\end{array}\\right]}_{T \\times 1}, \\quad\n\\mathbf{x}_i =\n\\underbrace{\\left[\n\\begin{array}{c}\n\\mathbf{x}_{,1}'\\\\\n\\vdots\\\\\n\\mathbf{x}_{,T}'\n\\end{array}\\right]}_{T \\times K}, \\quad\n\\mathbf{X} =\n\\underbrace{\\left[\n\\begin{array}{c}\n\\mathbf{x}_{1}\\\\\n\\vdots\\\\\n\\mathbf{x}_{n}\n\\end{array}\\right]}_{(nT) \\times K}.\n\\]\n\\[\n\\tilde{\\mathbf{y}}_i =\n\\left[\n\\begin{array}{c}\ny_{,1} - \\bar{y}_i\\\\\n\\vdots\\\\\ny_{,T} - \\bar{y}_i\n\\end{array}\\right], \\quad\n\\tilde{\\boldsymbol\\varepsilon}_i =\n\\left[\n\\begin{array}{c}\n\\varepsilon_{,1} - \\bar{\\varepsilon}_i\\\\\n\\vdots\\\\\n\\varepsilon_{,T} - \\bar{\\varepsilon}_i\n\\end{array}\\right],\n\\]\n\\[\n\\tilde{\\mathbf{x}}_i =\n\\left[\n\\begin{array}{c}\n\\mathbf{x}_{,1}' - \\bar{\\mathbf{x}}_i'\\\\\n\\vdots\\\\\n\\mathbf{x}_{,T}' - \\bar{\\mathbf{x}}_i'\n\\end{array}\\right], \\quad\n\\tilde{\\mathbf{X}} =\n\\left[\n\\begin{array}{c}\n\\tilde{\\mathbf{x}}_{1}\\\\\n\\vdots\\\\\n\\tilde{\\mathbf{x}}_{n}\n\\end{array}\\right], \\quad\n\\tilde{\\mathbf{Y}} =\n\\left[\n\\begin{array}{c}\n\\tilde{\\mathbf{y}}_{1}\\\\\n\\vdots\\\\\n\\tilde{\\mathbf{y}}_{n}\n\\end{array}\\right],\n\\]\n\n\\[\n\\bar{y}_i = \\frac{1}{T} \\sum_{t=1}^T y_{,t}, \\quad \\bar{\\varepsilon}_i = \\frac{1}{T}\\sum_{t=1}^T \\varepsilon_{,t} \\quad \\mbox{} \\quad \\bar{\\mathbf{x}}_i = \\frac{1}{T}\\sum_{t=1}^T \\mathbf{x}_{,t}.\n\\]","code":"\nT <- 2; n <- 12 # 2 periods and 12 entities\nalpha <- 5*rnorm(n) # draw fixed effects\nx.1 <- rnorm(n) - .5*alpha # note: x_i's correlate to alpha_i's\nx.2 <- rnorm(n) - .5*alpha\nbeta <- 5; sigma <- .3\ny.1 <- alpha + x.1 + sigma*rnorm(n);y.2 <- alpha + x.2 + sigma*rnorm(n)\nx <- c(x.1,x.2) # pooled x\ny <- c(y.1,y.2) # pooled y\npar(mfrow=c(1,2))\nplot(x,y,col=\"black\",pch=19,xlab=\"x\",ylab=\"y\",main=\"(a)\")\nplot(x,y,col=\"black\",pch=19,xlab=\"x\",ylab=\"y\",main=\"(b)\")\npoints(x.1,y.1,col=\"blue\",pch=19);points(x.2,y.2,col=\"red\",pch=19)\nfor(i in 1:n){lines(c(x.1[i],x.2[i]),c(y.1[i],y.2[i]))}"},{"path":"Panel.html","id":"three-standard-cases","chapter":"2 Panel regressions","heading":"2.2 Three standard cases","text":"three typical situations:Pooled regression: \\(\\mathbf{z}_i \\equiv 1\\). case amounts case studied Chapter 1.Fixed Effects (Section 2.3): \\(\\mathbf{z}_i\\) unobserved, correlates \\(\\mathbf{x}_i\\) \\(\\Rightarrow\\) \\(\\mathbf{b}\\) biased inconsistent OLS regression \\(\\mathbf{y}\\) \\(\\mathbf{X}\\) (omitted variable, see Section 1.3.2).Random Effects (Section 2.4): \\(\\mathbf{z}_i\\) unobserved, uncorrelated \\(\\mathbf{x}_i\\). model writes:\n\\[\ny_{,t} = \\mathbf{x}'_{,t}\\boldsymbol\\beta + \\alpha +  \\underbrace{{\\color{blue}u_i + \\varepsilon_{,t}}}_{\\mbox{compound error}},\n\\]\n\\(\\alpha = \\mathbb{E}(\\mathbf{z}'_{}\\boldsymbol\\alpha)\\) \\(u_i = \\mathbf{z}'_{}\\boldsymbol\\alpha - \\mathbb{E}(\\mathbf{z}'_{}\\boldsymbol\\alpha) \\perp \\mathbf{x}_i\\). case, OLS consistent, efficient. GLS can used gain efficiencies OLS (see Section 1.5.2 presentation GLS approach).","code":""},{"path":"Panel.html","id":"FixedEffect","chapter":"2 Panel regressions","heading":"2.3 Estimation of Fixed-Effects Models","text":"Hypothesis 2.1  (Fixed-effect model) assume :perfect multicollinearity among regressors.\\(\\mathbb{E}(\\varepsilon_{,t}|\\mathbf{X})=0\\), \\(,t\\).:\n\\[\n\\mathbb{E}(\\varepsilon_{,t}\\varepsilon_{j,s}|\\mathbf{X}) =\n\\left\\{\n\\begin{array}{cl}\n\\sigma^2 & \\mbox{$=j$ $s=t$},\\\\\n0 & \\mbox{otherwise.}\n\\end{array}\\right.\n\\]assumptions analogous introduced standard linear regression:\\(\\leftrightarrow\\) Hyp. 1.1, (ii) \\(\\leftrightarrow\\) Hyp. 1.2, (iii) \\(\\leftrightarrow\\) Hyp. 1.3 + 1.4.matrix form, given \\(\\), model writes:\n\\[\n\\mathbf{y}_i = \\mathbf{x}_i \\boldsymbol\\beta + \\mathbf{1}\\alpha_i + \\boldsymbol\\varepsilon_i,\n\\]\n\\(\\mathbf{1}\\) \\(T\\)-dimensional vector ones.Least Square Dummy Variable (LSDV) model:\n\\[\\begin{equation}\n\\mathbf{y} = [\\mathbf{X} \\quad \\mathbf{D}]\n\\left[\n\\begin{array}{c}\n\\boldsymbol\\beta\\\\\n\\boldsymbol\\alpha\n\\end{array}\n\\right]\n+ \\boldsymbol\\varepsilon, \\tag{2.2}\n\\end{equation}\\]\n:\n\\[\n\\mathbf{D} = \\underbrace{ \\left[\\begin{array}{cccc}\n\\mathbf{1}&\\mathbf{0}&\\dots&\\mathbf{0}\\\\\n\\mathbf{0}&\\mathbf{1}&\\dots&\\mathbf{0}\\\\\n&&\\vdots&\\\\\n\\mathbf{0}&\\mathbf{0}&\\dots&\\mathbf{1}\\\\\n\\end{array}\\right]}_{(nT \\times n)}.\n\\]linear regression (Eq. (2.2)) —dummy variables— satisfies Gauss-Markov conditions (Theorem 1.1). Hence, context, OLS estimator best linear unbiased estimator (BLUE).Denoting \\(\\mathbf{Z}\\) matrix \\([\\mathbf{X} \\quad \\mathbf{D}]\\), \\(\\mathbf{b}\\) \\(\\mathbf{}\\) respective OLS estimates \\(\\boldsymbol\\beta\\) \\(\\boldsymbol\\alpha\\), :\n\\[\\begin{equation}\n\\boxed{\n\\left[\n\\begin{array}{c}\n\\mathbf{b}\\\\\n\\mathbf{}\n\\end{array}\n\\right]\n= [\\mathbf{Z}'\\mathbf{Z}]^{-1}\\mathbf{Z}'\\mathbf{y}.} \\tag{2.3}\n\\end{equation}\\]asymptotical distribution \\([\\mathbf{b}',\\mathbf{}']'\\) derives standard OLS context: Prop. 1.11 can used replaced \\(\\mathbf{X}\\) \\(\\mathbf{Z}=[\\mathbf{X} \\quad \\mathbf{D}]\\).:\n\\[\\begin{equation}\n\\boxed{\\left[\n\\begin{array}{c}\n\\mathbf{b}\\\\\n\\mathbf{}\n\\end{array}\n\\right] \\overset{d}{\\rightarrow}\n\\mathcal{N}\\left(\n\\left[\n\\begin{array}{c}\n\\boldsymbol\\beta\\\\\n\\boldsymbol\\alpha\n\\end{array}\n\\right],\n\\sigma^2 \\frac{Q^{-1}}{nT}\n\\right),}\n\\end{equation}\\]\n\n\\[\nQ = \\mbox{plim}_{nT \\rightarrow \\infty} \\frac{1}{nT} \\mathbf{Z}'\\mathbf{Z},\n\\]\nassuming previous limit exists.practice, estimator covariance matrix \\([\\mathbf{b}',\\mathbf{}']'\\) :\n\\[\ns^2 \\left( \\mathbf{Z}'\\mathbf{Z}\\right)^{-1} \\quad \\quad s^2 = \\frac{\\mathbf{e}'\\mathbf{e}}{nT - K - n},\n\\]\n\\(\\mathbf{e}\\) \\((nT) \\times 1\\) vector OLS residuals.alternative way expressing LSDV estimators. involves residual-maker matrix matrix \\(\\mathbf{M_D}=\\mathbf{} - \\mathbf{D}(\\mathbf{D}'\\mathbf{D})^{-1}\\mathbf{D}'\\) (see Eq. (1.3)), acts operator removes entity-specific means, .e.:\n\\[\n\\tilde{\\mathbf{Y}} = \\mathbf{M_D}\\mathbf{Y}, \\quad \\tilde{\\mathbf{X}} = \\mathbf{M_D}\\mathbf{X} \\quad \\quad \\tilde{\\boldsymbol\\varepsilon} = \\mathbf{M_D}\\boldsymbol\\varepsilon.\n\\]notations, using Frisch-Waugh theorem (Theorem 1.2), get another expression estimator \\(\\mathbf{b}\\) appearing Eq. (2.3):\n\\[\\begin{equation}\n\\boxed{\\mathbf{b} = [\\mathbf{X}'\\mathbf{M_D}\\mathbf{X}]^{-1}\\mathbf{X}'\\mathbf{M_D}\\mathbf{y}.}\\tag{2.4}\n\\end{equation}\\]amounts regressing \\(\\tilde{y}_{,t}\\)’s (\\(= y_{,t} - \\bar{y}_i\\)) \\(\\tilde{\\mathbf{x}}_{,t}\\)’s (\\(=\\mathbf{x}_{,t} - \\bar{\\mathbf{x}}_i\\)).estimate \\(\\boldsymbol\\alpha\\) given :\n\\[\\begin{equation}\n\\boxed{\\mathbf{} = (\\mathbf{D}'\\mathbf{D})^{-1}\\mathbf{D}'(\\mathbf{y} - \\mathbf{X}\\mathbf{b}),} \\tag{2.5}\n\\end{equation}\\]\nobtained developing second row \n\\[\n\\left[\n\\begin{array}{cc}\n\\mathbf{X}'\\mathbf{X} & \\mathbf{X}'\\mathbf{D}\\\\\n\\mathbf{D}'\\mathbf{X} & \\mathbf{D}'\\mathbf{D}\n\\end{array}\\right]\n\\left[\n\\begin{array}{c}\n\\mathbf{b}\\\\\n\\mathbf{}\n\\end{array}\\right] =\n\\left[\n\\begin{array}{c}\n\\mathbf{X}'\\mathbf{Y}\\\\\n\\mathbf{D}'\\mathbf{Y}\n\\end{array}\\right],\n\\]\nfirst-order conditions resulting least squares problem (see Eq. (1.2)).One can use different types fixed effects regression. Typically, one can time entity fixed effects. case, model writes:\n\\[\ny_{,t} = \\mathbf{x}_i'\\boldsymbol\\beta + \\alpha_i + \\gamma_t + \\varepsilon_{,t}.\n\\]LSDV approach (Eq. (2.2)) can still resorted . suffices extend \\(\\mathbf{Z}\\) matrix additional columns (called time dummies):\n\\[\\begin{equation}\n\\mathbf{y} = [\\mathbf{X} \\quad \\mathbf{D} \\quad \\mathbf{C}]\n\\left[\n\\begin{array}{c}\n\\boldsymbol\\beta\\\\\n\\boldsymbol\\alpha\\\\\n\\boldsymbol\\gamma\n\\end{array}\n\\right]\n+ \\boldsymbol\\varepsilon, \\tag{2.6}\n\\end{equation}\\]\n:\n\\[\n\\mathbf{C} = \\left[\\begin{array}{cccc}\n\\boldsymbol{\\delta}_1&\\boldsymbol{\\delta}_2&\\dots&\\boldsymbol{\\delta}_{T-1}\\\\\n\\vdots&\\vdots&&\\vdots\\\\\n\\boldsymbol{\\delta}_1&\\boldsymbol{\\delta}_2&\\dots&\\boldsymbol{\\delta}_{T-1}\\\\\n\\end{array}\\right],\n\\]\n\\(T\\)-dimensional vector \\(\\boldsymbol\\delta_t\\) (time dummy) \n\\[\n[0,\\dots,0,\\underbrace{1}_{\\mbox{t$^{th}$ entry}},0,\\dots,0]'.\n\\]Using state year fixed effects CigarettesSW panel dataset yields following results:Example 2.1  (Housing prices interest rates) example, want estimate effect short long-term interest rate housing prices. data come Jordà, Schularick, Taylor (2017) dataset (see website).","code":"\nCigarettesSW$rincome <- with(CigarettesSW, income/population/cpi)\neq.pooled <- lm(log(packs)~log(rprice)+log(rincome),data=CigarettesSW)\neq.LSDV <- lm(log(packs)~log(rprice)+log(rincome)+state,\n              data=CigarettesSW)\nCigarettesSW$year <- as.factor(CigarettesSW$year)\neq.LSDV2 <- lm(log(packs)~log(rprice)+log(rincome)+state+year,\n               data=CigarettesSW)\nstargazer::stargazer(eq.pooled,eq.LSDV,eq.LSDV2,type=\"text\",no.space = TRUE,\n                     omit=c(\"state\",\"year\"),\n                     add.lines=list(c('State FE','No','Yes','Yes'),\n                                    c('Year FE','No','No','Yes')),\n                     omit.stat=c(\"f\",\"ser\"))## \n## ==========================================\n##                   Dependent variable:     \n##              -----------------------------\n##                       log(packs)          \n##                 (1)       (2)       (3)   \n## ------------------------------------------\n## log(rprice)  -1.334*** -1.210*** -1.056***\n##               (0.135)   (0.114)   (0.149) \n## log(rincome)  0.318**    0.121     0.497  \n##               (0.136)   (0.190)   (0.304) \n## Constant     10.067*** 9.954***  8.360*** \n##               (0.516)   (0.264)   (1.049) \n## ------------------------------------------\n## State FE        No        Yes       Yes   \n## Year FE         No        No        Yes   \n## Observations    96        96        96    \n## R2             0.552     0.966     0.967  \n## Adjusted R2    0.542     0.929     0.931  \n## ==========================================\n## Note:          *p<0.1; **p<0.05; ***p<0.01\nlibrary(AEC);library(sandwich)\ndata(JST); JST <- subset(JST,year>1950);N <- dim(JST)[1]\nJST$hpreal <- JST$hpnom/JST$cpi # real house price index\nJST$dhpreal <- 100*log(JST$hpreal/c(NaN,JST$hpreal[1:(N-1)]))\n# Put NA's when change in country:\nJST$dhpreal[c(0,JST$iso[2:N]!=JST$iso[1:(N-1)])] <- NaN\nJST$dhpreal[abs(JST$dhpreal)>30] <- NaN # remove extreme price change\nJST$YEAR <- as.factor(JST$year) # to have time fixed effects\neq1_noFE <- lm(dhpreal ~ stir + ltrate,data=JST)\neq1_FE   <- lm(dhpreal ~ stir + ltrate + iso + YEAR,data=JST)\neq2_noFE <- lm(dhpreal ~ I(ltrate-stir),data=JST)\neq2_FE <- lm(dhpreal ~ I(ltrate-stir) + iso + YEAR,data=JST)\nvcov_cluster1_noFE <- vcovHC(eq1_noFE, cluster = JST[, c(\"iso\",\"YEAR\")])\nvcov_cluster1_FE   <- vcovHC(eq1_FE, cluster = JST[, c(\"iso\",\"YEAR\")])\nvcov_cluster2_noFE <- vcovHC(eq2_noFE, cluster = JST[, c(\"iso\",\"YEAR\")])\nvcov_cluster2_FE   <- vcovHC(eq2_FE, cluster = JST[, c(\"iso\",\"YEAR\")])\nrobust_se_FE1_noFE <- sqrt(diag(vcov_cluster1_noFE))\nrobust_se_FE1_FE   <- sqrt(diag(vcov_cluster1_FE))\nrobust_se_FE2_noFE <- sqrt(diag(vcov_cluster2_noFE))\nrobust_se_FE2_FE   <- sqrt(diag(vcov_cluster2_FE))\nstargazer::stargazer(eq1_noFE, eq1_FE, eq2_noFE, eq2_FE, type = \"text\",\n                     column.labels = c(\"no FE\", \"with FE\", \"no FE\",\"with FE\"),\n                     omit = c(\"iso\",\"YEAR\",\"Constant\"),keep.stat = \"n\",\n                     add.lines=list(c('Country FE','No','Yes','No','Yes'),\n                                    c('Year FE','No','Yes','No','Yes')),\n                     se = list(robust_se_FE1_noFE,robust_se_FE1_FE,\n                               robust_se_FE2_noFE,robust_se_FE2_FE))## \n## =======================================================\n##                           Dependent variable:          \n##                  --------------------------------------\n##                                 dhpreal                \n##                    no FE   with FE    no FE    with FE \n##                     (1)      (2)       (3)       (4)   \n## -------------------------------------------------------\n## stir             0.485***  0.532***                    \n##                   (0.149)  (0.170)                     \n##                                                        \n## ltrate           -0.690*** -0.384**                    \n##                   (0.164)  (0.182)                     \n##                                                        \n## I(ltrate - stir)                    -0.476*** -0.475***\n##                                      (0.145)   (0.159) \n##                                                        \n## -------------------------------------------------------\n## Country FE          No       Yes       No        Yes   \n## Year FE             No       Yes       No        Yes   \n## Observations       1,141    1,141     1,141     1,141  \n## =======================================================\n## Note:                       *p<0.1; **p<0.05; ***p<0.01"},{"path":"Panel.html","id":"RandomEffect","chapter":"2 Panel regressions","heading":"2.4 Estimation of random effects models","text":", individual effects assumed correlated variables (\\(\\mathbf{x}_i\\)’s). context, OLS estimator consistent. However, efficient. GLS approach can employed gain efficiency.Random-effect models write:\n\\[\ny_{,t}=\\mathbf{x}'_{}\\boldsymbol\\beta + (\\alpha + \\underbrace{u_i}_{\\substack{\\text{Random}\\\\\\text{heterogeneity}}}) + \\varepsilon_{,t},\n\\]\n\n\\[\\begin{eqnarray*}\n\\mathbb{E}(\\varepsilon_{,t}|\\mathbf{X})&=&\\mathbb{E}(u_{}|\\mathbf{X}) =0,\\\\\n\\mathbb{E}(\\varepsilon_{,t}\\varepsilon_{j,s}|\\mathbf{X}) &=&\n\\left\\{\n\\begin{array}{cl}\n\\sigma_\\varepsilon^2 & \\mbox{ $=j$ $s=t$},\\\\\n0 & \\mbox{ otherwise.}\n\\end{array}\n\\right.\\\\\n\\mathbb{E}(u_{}u_{j}|\\mathbf{X}) &=&\n\\left\\{\n\\begin{array}{cl}\n\\sigma_u^2 & \\mbox{ $=j$},\\\\\n0 & \\mbox{otherwise.}\n\\end{array}\n\\right.\\\\\n\\mathbb{E}(\\varepsilon_{,t}u_{j}|\\mathbf{X})&=&0 \\quad \\text{$$, $j$ $t$}.\n\\end{eqnarray*}\\]Introducing notations \\(\\eta_{,t} = u_i + \\varepsilon_{,t}\\) \\(\\boldsymbol\\eta_i = [\\eta_{,1},\\dots,\\eta_{,T}]'\\), \\(\\mathbb{E}(\\boldsymbol\\eta_i |\\mathbf{X}) = \\mathbf{0}\\) \\(\\mathbb{V}ar(\\boldsymbol\\eta_i | \\mathbf{X}) = \\boldsymbol\\Gamma\\), \n\\[\n\\boldsymbol\\Gamma = \\left[  \\begin{array}{ccccc}\n\\sigma_\\varepsilon^2+\\sigma_u^2 & \\sigma_u^2 & \\sigma_u^2 & \\dots & \\sigma_u^2\\\\\n\\sigma_u^2 & \\sigma_\\varepsilon^2+\\sigma_u^2 & \\sigma_u^2 & \\dots & \\sigma_u^2\\\\\n\\vdots && \\ddots && \\vdots \\\\\n\\sigma_u^2 & \\sigma_u^2 & \\sigma_u^2 & \\dots & \\sigma_\\varepsilon^2+\\sigma_u^2\\\\\n\\end{array}\n\\right] = \\sigma_\\varepsilon^2\\mathbf{} + \\sigma_u^2\\mathbf{1}\\mathbf{1}'.\n\\]Denoting \\(\\boldsymbol\\Sigma\\) covariance matrix \\(\\boldsymbol\\eta = [\\boldsymbol\\eta_1',\\dots,\\boldsymbol\\eta_n']'\\), :\n\\[\n\\boldsymbol\\Sigma = \\mathbf{} \\otimes \\boldsymbol\\Gamma.\n\\]knew \\(\\boldsymbol\\Sigma\\), apply (feasible) GLS (Eq. (1.29), Section 1.5.2):\n\\[\n\\boldsymbol\\beta = (\\mathbf{X}'\\boldsymbol\\Sigma^{-1}\\mathbf{X})^{-1}\\mathbf{X}'\\boldsymbol\\Sigma^{-1}\\mathbf{y}.\n\\]\n(explained Section 1.5.2, amounts regressing \\({\\boldsymbol\\Sigma^{-1/2}}'\\mathbf{y}\\) \\({\\boldsymbol\\Sigma^{-1/2}}'\\mathbf{X}\\).)can checked \\(\\boldsymbol\\Sigma^{-1/2} = \\mathbf{} \\otimes (\\boldsymbol\\Gamma^{-1/2})\\) \n\\[\n\\boldsymbol\\Gamma^{-1/2} = \\frac{1}{\\sigma_\\varepsilon}\\left( \\mathbf{} - \\frac{\\theta}{T}\\mathbf{1}\\mathbf{1}'\\right),\\quad \\mbox{}\\quad\\theta = 1 - \\frac{\\sigma_\\varepsilon}{\\sqrt{\\sigma_\\varepsilon^2+T\\sigma_u^2}}.\n\\]Hence, knew \\(\\boldsymbol\\Sigma\\), transform data follows:\n\\[\n\\boldsymbol\\Gamma^{-1/2}\\mathbf{y}_i = \\frac{1}{\\sigma_\\varepsilon}\\left[\\begin{array}{c}y_{,1} - \\theta\\bar{y}_i\\\\y_{,2} - \\theta\\bar{y}_i\\\\\\vdots\\\\y_{,T} - \\theta\\bar{y}_i\\\\\\end{array}\\right].\n\\]\\(\\boldsymbol\\Sigma\\) unknown? One can take deviations group means remove heterogeneity:\n\\[\\begin{equation}\ny_{,t} - \\bar{y}_i = [\\mathbf{x}_{,t} - \\bar{\\mathbf{x}}_i]'\\boldsymbol\\beta + (\\varepsilon_{,t} - \\bar{\\varepsilon}_i).\\tag{2.7}\n\\end{equation}\\]\nprevious equation can consistently estimated OLS. (Although residuals correlated across \\(t\\)’s observations pertaining given entity, OLS remain consistent; see Prop. 1.13.)\\(\\mathbb{E}\\left[\\sum_{=1}^{T}(\\varepsilon_{,t}-\\bar{\\varepsilon}_i)^2\\right] = (T-1)\\sigma_{\\varepsilon}^2\\).\\(\\varepsilon_{,t}\\)’s observed \\(\\mathbf{b}\\), OLS estimator \\(\\boldsymbol\\beta\\) Eq. (2.7), consistent estimator \\(\\boldsymbol\\beta\\). Using adjustment degrees freedom, can approximate variance :\n\\[\n\\hat{\\sigma}_e^2 = \\frac{1}{nT-n-K}\\sum_{=1}^{n}\\sum_{t=1}^{T}(e_{,t} - \\bar{e}_i)^2.\n\\]\\(\\sigma_u^2\\)? can exploit fact OLS consistent pooled regression:\n\\[\n\\mbox{plim }s^2_{pooled} = \\mbox{plim }\\frac{\\mathbf{e}'\\mathbf{e}}{nT-K-1} = \\sigma_u^2 + \\sigma_\\varepsilon^2,\n\\]\ntherefore use \\(s^2_{pooled} - \\hat{\\sigma}_e^2\\) approximation \\(\\sigma_u^2\\).Let us come back Example 2.1 (relationship changes housing prices interest rates). following, use random effect specification; compare results obtained pooled regression fixed-effect model. , use function plm package name. (Note eq.FE similar eq1 Example 2.1.)One can run Hausman (1978) test order check whether fixed-effect model needed. Indeed, case (.e., covariates correlated disturbances), preferable use random-effect estimation latter efficient.p-value high, reject null hypothesis according covariates errors uncorrelated. therefore prefer random-effect model.Example 2.2  (Spatial data) example makes use Airbnb prices (Zürich, 22 June 2017), collected Tom Slee’s website. covariates number bedrooms number people can accommodated. consider use district fixed effects. Figure 2.3 shows price explain (size circles proportional prices). white lines delineate 12 districts city.\nFigure 2.3: Airbnb prices Zurich area, 22 June 2017. size circles proportional prices. White lines delineate 12 districts city.\nLet us regress prices covariates well district dummies:Figure 2.4 compares residuals without fixed effects. sizes circles proportional absolute values residuals, color indicates sign (blue positive).\nFigure 2.4: Regression residuals. sizes circles proportional absolute values residuals, color indicates sign (blue negative).\nfixed effects, colors better balanced within district.","code":"\nlibrary(plm);library(stargazer)\neq.RE <- plm(dhpreal ~ stir + ltrate,data=JST,index=c(\"iso\",\"YEAR\"),\n             model=\"random\",effect=\"twoways\")\neq.FE <- plm(dhpreal ~ stir + ltrate,data=JST,index=c(\"iso\",\"YEAR\"),\n             model=\"within\",effect=\"twoways\")\neq0   <- plm(dhpreal ~ stir + ltrate,data=JST,index=c(\"iso\",\"YEAR\"),\n             model=\"pooling\") \nstargazer(eq0, eq.RE, eq.FE, type = \"text\",no.space = TRUE,\n                     column.labels=c(\"Pooled\",\"Random Effect\",\"Fixed Effects\"),\n                     add.lines=list(c('State FE','No','Yes','Yes'),\n                                    c('Year FE','No','Yes','Yes')),\n                     omit.stat=c(\"f\",\"ser\"))## \n## ==================================================\n##                       Dependent variable:         \n##              -------------------------------------\n##                             dhpreal               \n##               Pooled   Random Effect Fixed Effects\n##                 (1)         (2)           (3)     \n## --------------------------------------------------\n## stir         0.485***    0.456***      0.532***   \n##               (0.114)     (0.019)       (0.134)   \n## ltrate       -0.690***   -0.541***     -0.384***  \n##               (0.127)     (0.020)       (0.145)   \n## Constant     4.103***    3.341***                 \n##               (0.421)     (0.096)                 \n## --------------------------------------------------\n## State FE        No          Yes           Yes     \n## Year FE         No          Yes           Yes     \n## Observations   1,141       1,141         1,141    \n## R2             0.027       0.024         0.015    \n## Adjusted R2    0.025       0.022        -0.067    \n## ==================================================\n## Note:                  *p<0.1; **p<0.05; ***p<0.01\nphtest(eq.FE,eq.RE)## \n##  Hausman Test\n## \n## data:  dhpreal ~ stir + ltrate\n## chisq = 3.8386, df = 2, p-value = 0.1467\n## alternative hypothesis: one model is inconsistent\neq_noFE <- lm(price~bedrooms+accommodates,data=airbnb)\neq_FE   <- lm(price~bedrooms+accommodates+neighborhood,data=airbnb)\n# Adjust standard errors:\ncov_FE          <- vcovHC(eq_FE, cluster = airbnb[, c(\"neighborhood\")])\nrobust_se_FE    <- sqrt(diag(cov_FE))\ncov_noFE        <- vcovHC(eq_noFE, cluster = airbnb[, c(\"neighborhood\")])\nrobust_se_noFE  <- sqrt(diag(cov_noFE))\nstargazer::stargazer(eq_FE, eq_noFE, eq_FE, eq_noFE, type = \"text\",\n                     column.labels = c(\"FE (no HAC)\", \"No FE (no HAC)\",\n                                       \"FE (with HAC)\", \"No FE (with HAC)\"),\n                     omit = c(\"neighborhood\"),no.space = TRUE,\n                     omit.labels = c(\"District FE\"),keep.stat = \"n\",\n                     se = list(NULL, NULL, robust_se_FE, robust_se_noFE))## \n## ======================================================================\n##                                 Dependent variable:                   \n##              ---------------------------------------------------------\n##                                        price                          \n##              FE (no HAC) No FE (no HAC) FE (with HAC) No FE (with HAC)\n##                  (1)          (2)            (3)            (4)       \n## ----------------------------------------------------------------------\n## bedrooms      7.229***      5.629**       7.229***        5.629***    \n##                (2.135)      (2.194)        (2.052)        (2.073)     \n## accommodates  16.426***    17.449***      16.426***      17.449***    \n##                (1.284)      (1.323)        (1.431)        (1.428)     \n## Constant      95.118***    68.417***      95.118***      68.417***    \n##                (5.323)      (3.223)        (5.664)        (3.527)     \n## ----------------------------------------------------------------------\n## District FE      Yes           No            Yes             No       \n## ----------------------------------------------------------------------\n## Observations    1,321        1,321          1,321          1,321      \n## ======================================================================\n## Note:                                      *p<0.1; **p<0.05; ***p<0.01"},{"path":"Panel.html","id":"DynPanel","chapter":"2 Panel regressions","heading":"2.5 Dynamic Panel Regressions","text":"precedes, assumed correlation observations indexed \\((,t)\\) indexed \\((j,s)\\) long \\(j \\ne \\) \\(t \\ne s\\). one suspects errors \\(\\varepsilon_{,t}\\) correlated (across entities \\(\\) given date \\(t\\), across dates given entity, ), one employ robust covariance matrix (see Section 1.5.5).several cases, auto-correlation variable interest may stem auto-regressive specification. , Eq. (2.1) replaced :\n\\[\\begin{equation}\ny_{,t} = \\rho y_{,t-1} + \\mathbf{x}'_{,t}\\underbrace{\\boldsymbol\\beta}_{K \\times 1} + \\underbrace{\\alpha_i}_{\\mbox{Individual effects}} + \\varepsilon_{,t}.\\tag{2.8}\n\\end{equation}\\]case, even explanatory variables \\(\\mathbf{x}_{,t}\\) uncorrelated errors \\(\\varepsilon_{,t}\\), additional explanatory variable \\(y_{,t-1}\\) correlates errors \\(\\varepsilon_{,t-1},\\varepsilon_{,t-2},\\dots,\\varepsilon_{,1}\\). result, LSDV estimate model parameters \\(\\{\\rho,\\boldsymbol\\beta\\}\\) may biased, even \\(n\\) large. see , notice LSDV regression amounts regressing \\(\\widetilde{\\mathbf{y}}\\) \\(\\widetilde{\\mathbf{X}}\\) (see Eq. (2.4)), elements \\(\\widetilde{\\mathbf{X}}\\) explanatory variables subtract within-sample means. particular, :\n\\[\n\\tilde{y}_{,t-1} = y_{,t-1} - \\frac{1}{T} \\sum_{s=1}^{T} y_{,s-1},\n\\]\ncorrelates corresponding error, :\n\\[\n\\tilde{\\varepsilon}_{,t} = \\varepsilon_{,t} - \\frac{1}{T} \\sum_{s=1}^{T} \\varepsilon_{,s}.\n\\]previous equation shows within-group estimator (LSDV) introduces realizations \\(\\varepsilon_{,t}\\) errors transformed error term (\\(\\tilde{\\varepsilon}_{,t}\\)). result, large-\\(n\\) fixed-\\(T\\) panels, consistent right-hand-side variables regression strictly exogenous (.e., correlate past, present, future errors \\(\\varepsilon_{,t}\\)).5 case lags \\(y_{,t}\\) right-hand side regression formula.following simulation illustrate bias. \\(x\\)-coordinates dots fixed effects \\(\\alpha_i\\)’s, \\(y\\)-coordinates LSDV estimates. blue line 45-degree line.\nFigure 2.5: illustration bias pertianing LSDV estimation approach presence auto-correlation depend variable.\nprevious example, estimate \\(\\rho\\) (whose true value 0.8) 0.531.address , one can resort instrumental-variable regressions. Anderson Hsiao (1982) , particular, proposed first-differenced Two Stage Least Squares (2SLS) estimator (see Eq. (1.22) Section 1.4). estimation based following transformation model:\n\\[\\begin{equation}\n\\Delta y_{,t} = \\rho \\Delta y_{,t-1} + (\\Delta \\mathbf{x}_{,t})'\\boldsymbol\\beta + \\Delta\\varepsilon_{,t}.\\tag{2.9}\n\\end{equation}\\]\nOLS estimates parameters biased \\(\\varepsilon_{,t-1}\\) —part error \\(\\Delta\\varepsilon_{,t}\\)— correlated \\(y_{,t-1}\\) —part “explanatory variable”, namely \\(\\Delta y_{,t-1}\\). consistent estimates can obtained using 2SLS instrumental variables correlated \\(\\Delta y_{,t}\\) orthogonal \\(\\Delta\\varepsilon_{,t}\\). One can instance use \\(\\{y_{,t-2},\\mathbf{x}_{,t-2}\\}\\) instruments. Note approach can implemented 3 time observations per entity \\(\\).explanatory variables \\(\\mathbf{x}_{,t}\\) assumed predetermined (.e., contemporaneous correlate errors \\(\\varepsilon_{,t}\\)), \\(\\mathbf{x}_{,t-1}\\) can added instruments associated \\(\\Delta y_{,t}\\). , variables (\\(\\mathbf{x}_{,t}\\)’s) exogenous (.e., contemporaneous correlate errors \\(\\varepsilon_{,s}\\), \\(\\forall s\\)), \\(\\mathbf{x}_{,t}\\) also constitute valid instrument.Using previous (simulated) example, approach consists following steps:OLS estimate \\(\\rho\\) (whose true value 0.8) 0.531, obtain rho_2SLS \\(=\\) 0.89.Let us come back general case (covariates \\(\\mathbf{x}_{,k}\\)’s). \\(t=3\\), \\(y_{,1}\\) (\\(\\mathbf{x}_{,1}\\)) possible instrument. However, \\(t=4\\), one use \\(y_{,2}\\) \\(y_{,1}\\) (well \\(\\mathbf{x}_{,2}\\) \\(\\mathbf{x}_{,1}\\)). generally, defining matrix \\(Z_i\\) follows:\n\\[\nZ_i = \\left[\n\\begin{array}{ccccccccccccccccc}\n\\mathbf{z}_{,1}' & 0 & \\dots \\\\\n0 & \\mathbf{z}_{,1}' & \\mathbf{z}_{,2}' & 0 & \\dots \\\\\n0 &0 &0 & \\mathbf{z}_{,1} & \\mathbf{z}_{,2}' & \\mathbf{z}_{,3}' & 0 & \\dots \\\\\n\\vdots \\\\\n0 & \\dots &&&&&& 0 & \\mathbf{z}_{,1}' &  \\dots &   \\mathbf{z}_{,T-2}'\n\\end{array}\n\\right],\n\\]\n\\(\\mathbf{z}_{,t} = [y_{,t},\\mathbf{x}_{,t}']'\\), moment conditions:6\n\\[\n\\mathbb{E}(Z_i'\\Delta  {\\boldsymbol\\varepsilon}_i)=0,\n\\]\\(\\Delta{\\boldsymbol\\varepsilon}_i = [ \\Delta \\varepsilon_{,3},\\dots,\\Delta \\varepsilon_{,T}]'\\).restrictions used GMM approach employed Arellano Bond (1991). Specifically, GMM estimator model parameters given :\n\\[\n\\mbox{argmin}\\;\\left(\\frac{1}{n} \\sum_{=1}^n Z_i' \\Delta \\boldsymbol\\varepsilon_i\\right)'W_n\\left(\\frac{1}{n} \\sum_{=1}^n Z_i' \\Delta \\boldsymbol\\varepsilon_i\\right),\n\\]\nusing weighting matrix\n\\[\nW_n = \\left(\\frac{1}{n}\\sum_{=1}^n Z_i'\\widehat{\\Delta\\boldsymbol\\varepsilon_i}\\widehat{\\Delta\\boldsymbol\\varepsilon_i}'Z_i\\right)^{-1},\n\\]\n\\(\\widehat{\\Delta\\boldsymbol\\varepsilon_i}\\)’s consistent estimates \\(\\Delta\\boldsymbol\\varepsilon_i\\)’s result preliminary estimation. sense, estimator two-step GMM one.disturbances homoskedastic, can shown asymptotically equivalent (efficient) GMM estimator can obtained using:\n\\[\nW_{1,n} = \\left(\\frac{1}{n}Z_i'HZ_i\\right)^{-1},\n\\]\n\\(H\\) \\((T-2) \\times (T-2)\\) matrix form:\n\\[\nH = \\left[\\begin{array}{ccccccc}\n2 & -1 & 0 & \\dots &0 \\\\\n-1 & 2 & -1 &  & \\vdots \\\\\n0 & \\ddots& \\ddots & \\ddots & 0 \\\\\n\\vdots &  & -1 & 2&-1\\\\\n0&\\dots & 0 & -1 & 2\n\\end{array}\\right].\n\\]straightforward extend GMM methods cases one lag dependent variable right-hand side equation cases disturbances feature limited moving-average serial correlation.pdynmc package allows run GMM approaches (see Fritsch, Pua, Schnurbus (2019)). following lines code allow replicate results Arellano Bond (1991):generate novel results (m2) replacing “onestep” “twostep” (estimation field). resulting estimated coefficients :Arellano Bond (1991) proposed specification test. model correctly specified, errors Eq. (2.9) —first-difference equation— feature non-zero first-order auto-correlations, zero higher-order autocorrelations.Function mtest.fct package pdynmc implements test. result present case:One can also implement Hansen J-test -identifying restrictions (see Section 3.1.3):","code":"\nn <- 400;T <- 10\nrho <- 0.8;sigma <- .5\nalpha <- rnorm(n)\ny <- alpha /(1-rho) + sigma^2/(1 - rho^2) * rnorm(n)\nall_y <- y\nfor(t in 2:T){\n  y <- rho * y + alpha + sigma * rnorm(n)\n  all_y <- rbind(all_y,y)\n}\ny   <- c(all_y[2:T,]);y_1 <- c(all_y[1:(T-1),])\nD <- diag(n) %x% rep(1,T-1)\nZ <- cbind(c(y_1),D)\nb <- solve(t(Z) %*% Z) %*% t(Z) %*% y\na <- b[2:(n+1)]\nplot(alpha,a)\nlines(c(-10,10),c(-10,10),col=\"blue\")\nDy   <- c(all_y[3:T,]) - c(all_y[2:(T-1),])\nDy_1 <- c(all_y[2:(T-1),]) - c(all_y[1:(T-2),])\ny_2  <- c(all_y[1:(T-2),])\nZ <- matrix(y_2,ncol=1)\nPz <- Z %*% solve(t(Z) %*% Z) %*% t(Z)\nDy_1hat <- Pz %*% Dy_1\nrho_2SLS <- solve(t(Dy_1hat) %*% Dy_1hat) %*% t(Dy_1hat) %*% Dy\nlibrary(pdynmc)\ndata(EmplUK, package = \"plm\")\ndat <- EmplUK\ndat[,c(4:7)]         <- log(dat[,c(4:7)])\nm1 <- pdynmc(dat = dat, # name of the dataset\n             varname.i = \"firm\", # name of the cross-section identifier\n             varname.t = \"year\", # name of the time-series identifiers\n             use.mc.diff = TRUE, # use moment conditions from equations in differences? (i.e. instruments in levels) \n             use.mc.lev = FALSE, # use moment conditions from equations in levels? (i.e. instruments in differences)\n             use.mc.nonlin = FALSE, # use nonlinear (quadratic) moment conditions?\n             include.y = TRUE, # instruments should be derived from the lags of the dependent variable?\n             varname.y = \"emp\", # name of the dependent variable in the dataset\n             lagTerms.y = 2, # number of lags of the dependent variable\n             fur.con = TRUE, # further control variables (covariates) are included?\n             fur.con.diff = TRUE, # include further control variables in equations from differences ?\n             fur.con.lev = FALSE, # include further control variables in equations from level?\n             varname.reg.fur = c(\"wage\", \"capital\", \"output\"), # covariate(s) -in the dataset- to treat as further controls\n             lagTerms.reg.fur = c(1,2,2), # number of lags of the further controls\n             include.dum = TRUE, # A logical variable indicating whether dummy variables for the time periods are included (defaults to 'FALSE').\n             dum.diff = TRUE, # A logical variable indicating whether dummy variables are included in the equations in first differences (defaults to 'NULL').\n             dum.lev = FALSE, # A logical variable indicating whether dummy variables are included in the equations in levels (defaults to 'NULL').\n             varname.dum = \"year\",\n             w.mat = \"iid.err\", # One of the character strings c('\"iid.err\"', '\"identity\"', '\"zero.cov\"') indicating the type of weighting matrix to use (defaults to '\"iid.err\"')\n             std.err = \"corrected\",\n             estimation = \"onestep\", # One of the character strings c('\"onestep\"', '\"twostep\"', '\"iterative\"'). Denotes the number of iterations of the parameter procedure (defaults to '\"twostep\"').\n             opt.meth = \"none\" # numerical optimization procedure. When no nonlinear moment conditions are employed in estimation, closed form estimates can be computed by setting the argument to '\"none\"\n)\nsummary(m1,digits=3)## \n## Dynamic linear panel estimation (onestep)\n## Estimation steps: 1\n## \n## Coefficients:\n##             Estimate Std.Err.rob z-value.rob Pr(>|z.rob|)    \n## L1.emp      0.686226    0.144594       4.746      < 2e-16 ***\n## L2.emp     -0.085358    0.056016      -1.524      0.12751    \n## L0.wage    -0.607821    0.178205      -3.411      0.00065 ***\n## L1.wage     0.392623    0.167993       2.337      0.01944 *  \n## L0.capital  0.356846    0.059020       6.046      < 2e-16 ***\n## L1.capital -0.058001    0.073180      -0.793      0.42778    \n## L2.capital -0.019948    0.032713      -0.610      0.54186    \n## L0.output   0.608506    0.172531       3.527      0.00042 ***\n## L1.output  -0.711164    0.231716      -3.069      0.00215 ** \n## L2.output   0.105798    0.141202       0.749      0.45386    \n## 1979        0.009554    0.010290       0.929      0.35289    \n## 1980        0.022015    0.017710       1.243      0.21387    \n## 1981       -0.011775    0.029508      -0.399      0.68989    \n## 1982       -0.027059    0.029275      -0.924      0.35549    \n## 1983       -0.021321    0.030460      -0.700      0.48393    \n## 1976       -0.007703    0.031411      -0.245      0.80646    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n##  41 total instruments are employed to estimate 16 parameters\n##  27 linear (DIF) \n##  8 further controls (DIF) \n##  6 time dummies (DIF) \n##  \n## J-Test (overid restrictions):  70.82 with 25 DF, pvalue: <0.001\n## F-Statistic (slope coeff):  528.06 with 10 DF, pvalue: <0.001\n## F-Statistic (time dummies):  14.98 with 6 DF, pvalue: 0.0204##      L1.emp      L2.emp     L0.wage     L1.wage  L0.capital  L1.capital \n##  0.62870890 -0.06518800 -0.52575951  0.31128961  0.27836190  0.01409950 \n##  L2.capital   L0.output   L1.output   L2.output        1979        1980 \n## -0.04024847  0.59192286 -0.56598515  0.10054264  0.01121551  0.02306871 \n##        1981        1982        1983        1976 \n## -0.02135806 -0.03111604 -0.01799335 -0.02336762\nmtest.fct(m1,order=3)## \n##  Arellano and Bond (1991) serial correlation test of degree 3\n## \n## data:  1step GMM Estimation\n## normal = 0.045945, p-value = 0.9634\n## alternative hypothesis: serial correlation of order 3 in the error terms\njtest.fct(m1)## \n##  J-Test of Hansen\n## \n## data:  1step GMM Estimation\n## chisq = 70.82, df = 25, p-value = 2.905e-06\n## alternative hypothesis: overidentifying restrictions invalid\njtest.fct(m2)## \n##  J-Test of Hansen\n## \n## data:  2step GMM Estimation\n## chisq = 31.381, df = 25, p-value = 0.1767\n## alternative hypothesis: overidentifying restrictions invalid"},{"path":"Panel.html","id":"introduction-to-program-evaluation","chapter":"2 Panel regressions","heading":"2.6 Introduction to program evaluation","text":"section brielfy introduces econometrics program evaluation. Program evaluation refer analysis causal effects “treatments” broad sense. treatment can, e.g., correspond implementation (announcement) policy measures. comprehensive review proposed Abadie Cattaneo (2018). seminal book subject Angrist Pischke (2008).","code":""},{"path":"Panel.html","id":"presentation-of-the-problem","chapter":"2 Panel regressions","heading":"2.6.1 Presentation of the problem","text":"begin , let us consider single entity. simplify notations, drop entity index (\\(\\)). Let us denote \\(Y\\) outcome variable (variable interest), \\(W\\) binary variable indicating whether considered entity received treatment (\\(W=1\\)) (\\(W=0\\)), \\(X\\) vector covariates, assumed predetermined relative treatment. , \\(W\\) \\(X\\) correlated, values \\(X\\) determined \\(W\\) (way realization \\(W\\) affect \\(X\\)). Typcally, \\(X\\) contains characteristics considered entity.interested effect treatment, :\n\\[\nY_1 - Y_0,\n\\]\n\\(Y_1\\) correspond outcome obtained treatment, \\(Y_0\\) outcome obtained without . Notice :\n\\[\nY = (1-W) Y_0 + W Y_1.\n\\]problem observing \\((Y,W,X)\\) sufficient observe treatment effect \\(Y_1 - Y_0\\). Additional assumptions needed estimate , , precisely, expectations (average treatment effect):\n\\[\nATE = \\mathbb{E}(Y_1 - Y_0).\n\\]Importantly, \\(ATE\\) different following quantity:\n\\[\n\\alpha = \\underbrace{\\mathbb{E}(Y|W=1)}_{=\\mathbb{E}(Y_1|W=1)} - \\underbrace{\\mathbb{E}(Y|W=0)}_{=\\mathbb{E}(Y_0|W=0)},\n\\]\neasier estimate. Indeed, consistent estimate \\(\\alpha\\) difference means outcome variables two sub-samples: one containing treated entities (gives estimate \\(\\mathbb{E}(Y_1|W=1)\\)) containing non-treated entities (gives estimate \\(\\mathbb{E}(Y_0|W=0)\\)). Coming back \\(ATE\\), problem won’t direct information regarding \\(\\mathbb{E}(Y_0|W=1)\\) \\(\\mathbb{E}(Y_1|W=0)\\). However, two conditional expectations part \\(ATE\\). Indeed, \\(ATE = \\mathbb{E}(Y_1) - \\mathbb{E}(Y_0)\\), :\n\\[\\begin{eqnarray}\n\\mathbb{E}(Y_1) &=& \\mathbb{E}(Y_1|W=0)\\mathbb{P}(W=0)+\\mathbb{E}(Y_1|W=1)\\mathbb{P}(W=1) \\tag{2.10} \\\\\n\\mathbb{E}(Y_0) &=& \\mathbb{E}(Y_0|W=0)\\mathbb{P}(W=0)+\\mathbb{E}(Y_0|W=1)\\mathbb{P}(W=1). \\tag{2.11}\n\\end{eqnarray}\\]","code":""},{"path":"Panel.html","id":"randomized-controlled-trials-rcts","chapter":"2 Panel regressions","heading":"2.6.2 Randomized controlled trials (RCTs)","text":"context Randomized controlled trials (RCTs), entities randomly assigned receive treatment. result, \\(\\mathbb{E}(Y_1) = \\mathbb{E}(Y_1|W=0) = \\mathbb{E}(Y_1|W=1)\\) \\(\\mathbb{E}(Y_0) = \\mathbb{E}(Y_0|W=0) = \\mathbb{E}(Y_0|W=1)\\). Using Eqs. (2.10) (2.11) yields \\(ATE = \\alpha\\).Therefore, context, estimating \\(\\mathbb{E}(Y_1-Y_0)\\) amounts computing difference two sample means, namely () sample mean subset \\(Y_i\\)’s corresponding entities \\(W_i=1\\), (b) one \\(W_i=0\\).accurate estimates can obtained regressions. Assume model reads:\n\\[\nY_{} = W_{} \\beta_{1} + X_i'\\boldsymbol\\beta_z + \\varepsilon_i,\n\\]\n\\(\\mathbb{E}(\\varepsilon_i|X_i) = 0\\) (\\(W_i\\) independent \\(X_i\\) \\(\\varepsilon_i\\)). case, obtain consistent estimate \\(\\beta_1\\) regressing \\(\\mathbf{y}\\) \\(\\mathbf{Z} = [\\mathbf{w},\\mathbf{X}]\\).","code":""},{"path":"Panel.html","id":"difference-in-difference-did-approach","chapter":"2 Panel regressions","heading":"2.6.3 Difference-in-Difference (DiD) approach","text":"approach popular methodology implemented cases \\(W\\) considered independent variable. exploits two dimensions: entities (\\(\\)), time (\\(t\\)). simplify exposition, consider two periods (\\(t=0\\) \\(t=1\\)).Consider following model:\n\\[\\begin{equation}\nY_{,t} = W_{,t} \\beta_1 + \\mu_i + \\delta_t + \\varepsilon_{,t}\\tag{2.12}\n\\end{equation}\\]parameter interest \\(\\beta_{1}\\), treatment effect (recall \\(W_{,t} \\\\{0,1\\}\\)). Usually, entities \\(\\), \\(W_{,t=0}=0\\). treated date 1, .e., \\(W_{,1} \\\\{0,1\\}\\).disturbance \\(\\varepsilon_{,t}\\) affects outcome, assume relate selection treatment; therefore, \\(\\mathbb{E}(\\varepsilon_{,t}|W_{,t})=0\\). contrast, exclude correlation \\(W_{,t}\\) (\\(t=1\\)) \\(\\mu_i\\); hence, \\(\\mu_i\\) may constitute confounder. Finally, suppose micro-variables \\(W_i\\) affect time fixed effects \\(\\delta_t\\), \\(\\mathbb{E}(\\delta_t|W_{,t})=\\mathbb{E}(\\delta_t)\\).:\\[\n\\begin{array}{cccccccccc}\n\\mathbb{E}(Y_{,1}|W_{,1}=1) &=& \\beta_1 &+& \\mathbb{E}(\\mu_i|W_{,1}=1) &+&\\mathbb{E}(\\delta_1|W_{,1}=1) &+& \\mathbb{E}(\\varepsilon_{,1}) \\\\\n\\mathbb{E}(Y_{,0}|W_{,1}=1) &=&  && \\mathbb{E}(\\mu_i|W_{,1}=1) &+&\\mathbb{E}(\\delta_0|W_{,1}=1) &+& \\mathbb{E}(\\varepsilon_{,0}) \\\\\n\\mathbb{E}(Y_{,1}|W_{,1}=0) &=& && \\mathbb{E}(\\mu_i|W_{,1}=0) &+&\\mathbb{E}(\\delta_1|W_{,1}=0) &+& \\mathbb{E}(\\varepsilon_{,1}) \\\\\n\\mathbb{E}(Y_{,0}|W_{,1}=0) &=&  && \\mathbb{E}(\\mu_i|W_{,1}=0) &+&\\mathbb{E}(\\delta_0|W_{,1}=0) &+& \\mathbb{E}(\\varepsilon_{,0}).\n\\end{array}\n\\]\n, assumptions, can checked :\\[\n\\beta_1 = \\mathbb{E}(\\Delta Y_{,1}|W_{,1}=1) - \\mathbb{E}(\\Delta Y_{,1}|W_{,1}=0),\n\\]\n\\(\\Delta Y_{,1}=Y_{,1}-Y_{,0}\\). Therefore, context, treatment effect appears difference (two conditionnal expectations) difference (outcome variable, time).illustrated Figure 2.6, represents generic framework.\nFigure 2.6: Source: Abadie et al., (1998).\npractice, implementing approach consists running linear regression type Eq. (2.12). regressions also usually involve controls top fixed effects \\(\\mu_i\\). illustrated next subsection, parameter interest (\\(\\beta_1\\)) often associated interaction term.","code":""},{"path":"Panel.html","id":"application-of-the-did-approach","chapter":"2 Panel regressions","heading":"2.6.4 Application of the DiD approach","text":"example based data used Meyer, Viscusi, Durbin (1995). dataset part wooldridge package. paper examines effect workers’ compensation injury time work. exploits natural experiment approach comparing individuals injured increases maximum weekly benefit amount. Specifically, 1980, cap weekly earnings covered worker’s compensation increased Kentucky Michigan. Let us check whether new policy followed increase amount time workers spent unemployed (example, higher compensation may reduce workers’ incentives avoid injury).shown Figure 2.7, measure affected high-earning workers. idea exploited Meyer, Viscusi, Durbin (1995) compare increase time work -1980 higher-earnings workers one hand (entities received treatment) low-earnings workers hand (control group).\nFigure 2.7: Source: Meyer et al., (1995).\nnext lines codes replicate results. dependent variable logarithm duration benefits. information use ?injury, loaded wooldridge library.table results , parameter interest one associated interaction term afchnge:highearn. Columns 2 3 correspond first two column Table 6 Meyer, Viscusi, Durbin (1995).","code":"\nlibrary(wooldridge)\ndata(injury)\ninjury <- subset(injury,ky==1)\ninjury$indust <- as.factor(injury$indust)\ninjury$injtype <- as.factor(injury$injtype)\n#names(injury)\neq1 <- lm(log(durat) ~ afchnge + highearn + afchnge*highearn,data=injury)\neq2 <- lm(log(durat) ~ afchnge + highearn + afchnge*highearn +\n            lprewage*highearn + male + married + lage + ltotmed + hosp +\n            indust + injtype,data=injury)\neq3 <- lm(log(durat) ~ afchnge + highearn + afchnge*highearn +\n            lprewage*highearn + male + married + lage + indust +\n            injtype,data=injury)\nstargazer::stargazer(eq1,eq2,eq3,type=\"text\",\n                     omit=c(\"indust\",\"injtype\",\"Constant\"),no.space = TRUE,\n                     add.lines = list(c(\"industry dummy\",\"no\",\"yes\",\"yes\"),\n                                      c(\"injury dummy\",\"no\",\"yes\",\"yes\")),\n                     order = c(1,2,18,3:17,19,20),omit.stat = c(\"f\",\"ser\"))## \n## ===============================================\n##                        Dependent variable:     \n##                   -----------------------------\n##                            log(durat)          \n##                      (1)       (2)       (3)   \n## -----------------------------------------------\n## afchnge             0.008    -0.004     0.016  \n##                    (0.045)   (0.038)   (0.045) \n## highearn          0.256***   -0.595    -1.522  \n##                    (0.047)   (0.930)   (1.099) \n## afchnge:highearn  0.191***  0.162***  0.215*** \n##                    (0.069)   (0.059)   (0.069) \n## lprewage                     0.207**   0.258** \n##                              (0.088)   (0.104) \n## male                         -0.070*   -0.072  \n##                              (0.039)   (0.046) \n## married                       0.055     0.051  \n##                              (0.035)   (0.041) \n## lage                        0.244***  0.252*** \n##                              (0.044)   (0.052) \n## ltotmed                     0.361***           \n##                              (0.011)           \n## hosp                        0.252***           \n##                              (0.044)           \n## highearn:lprewage             0.065     0.232  \n##                              (0.158)   (0.187) \n## -----------------------------------------------\n## industry dummy       no        yes       yes   \n## injury dummy         no        yes       yes   \n## Observations        5,626     5,347     5,347  \n## R2                  0.021     0.319     0.049  \n## Adjusted R2         0.020     0.316     0.046  \n## ===============================================\n## Note:               *p<0.1; **p<0.05; ***p<0.01"},{"path":"estimation-methods.html","id":"estimation-methods","chapter":"3 Estimation Methods","heading":"3 Estimation Methods","text":"chapter presents three approaches estimate parametric models: General Method Moments (GMM), Maximum Likelihood approach (ML), Bayesian approach. general context following: observe sample \\(\\mathbf{y}=\\{y_1,\\dots,y_n\\}\\), assume data generated model parameterized \\({\\boldsymbol\\theta} \\\\mathbb{R}^K\\), want estimate vector \\({\\boldsymbol\\theta}_0\\).","code":""},{"path":"estimation-methods.html","id":"secGMM","chapter":"3 Estimation Methods","heading":"3.1 Generalized Method of Moments (GMM)","text":"","code":""},{"path":"estimation-methods.html","id":"definition-of-the-gmm-estimator","chapter":"3 Estimation Methods","heading":"3.1.1 Definition of the GMM estimator","text":"denote \\(y_i\\) \\(p \\times 1\\) vector variables; \\(\\boldsymbol\\theta\\) \\(K \\times 1\\) vector parameters, \\(h(y_i;\\boldsymbol\\theta)\\) continuous \\(r \\times 1\\) vector-valued function.denote \\(\\boldsymbol\\theta_0\\) true value \\(\\boldsymbol\\theta\\) assume \\(\\boldsymbol\\theta_0\\) satisfies:\n\\[\n\\mathbb{E}[h(y_i;\\boldsymbol\\theta_0)] = \\mathbf{0}.\n\\]denote \\(\\underline{y_i}\\) information contained current past observations \\(y_i\\), : \\(\\underline{y_i} = \\{y_i,y_{-1},\\dots,y_1\\}\\). denote \\(g(\\underline{y_n};\\boldsymbol\\theta)\\) sample average \\(h(y_i;\\boldsymbol\\theta)\\) vectors, .e.:\n\\[\ng(\\underline{y_n};\\boldsymbol\\theta) = \\frac{1}{n} \\sum_{=1}^{n} h(y_i;\\boldsymbol\\theta).\n\\]intuition behind GMM estimator following: choose \\(\\boldsymbol\\theta\\) make sample moment close possible population values, 0.Definition 3.1  GMM estimator \\(\\boldsymbol\\theta_0\\) given :\n\\[\n\\hat{\\boldsymbol\\theta}_n = \\mbox{argmin}_{\\boldsymbol\\theta} \\quad g(\\underline{y_n};\\boldsymbol\\theta)'\\, W_n \\, g(\\underline{y_n};\\boldsymbol\\theta),\n\\]\n\\(W_n\\) positive definite matrix (may depend \\(\\underline{y_n}\\)).specific case \\(K = r\\) (dimension \\(\\boldsymbol\\theta\\) \\(h(y_i;\\boldsymbol\\theta)\\) —\\(g(\\underline{y_n};\\boldsymbol\\theta)\\)— \\(\\hat{\\boldsymbol\\theta}_n\\) satisfies:\n\\[\ng(\\underline{y_n};\\hat{\\boldsymbol\\theta}_n) = \\mathbf{0}.\n\\]\nregularity identification conditions, estimator consistent, \\(\\hat{\\boldsymbol\\theta}_{n}\\) converges towards \\(\\boldsymbol\\theta_0\\) probability, denote :\n\\[\\begin{equation}\n\\mbox{plim}_n\\;\\hat{\\boldsymbol\\theta}_{n}= \\boldsymbol\\theta_0,\\quad \\mbox{} \\quad\\hat{\\boldsymbol\\theta}_{n} \\overset{p}{\\rightarrow} \\boldsymbol\\theta_0,\\tag{3.1}\n\\end{equation}\\]\n.e. \\(\\forall \\varepsilon>0\\), \\(\\lim_{n \\rightarrow \\infty} \\mathbb{P}(|\\hat{\\boldsymbol\\theta}_{n} - \\boldsymbol\\theta_0|>\\varepsilon) = 0\\) (Definition 5.16).Definition 3.1 involves positive definite matrix \\(W_n\\). one can take positive definite matrix consistency (Eq. (3.1)), can shown GMM estimator achieves minimum asymptotic variance \\(W_n\\) inverse matrix \\(S\\), latter defined :\n\\[\nS = Asy.\\mathbb{V}ar\\left(\\sqrt{n}g(\\underline{y_n};\\hat{\\boldsymbol\\theta}_n)\\right).\n\\]\ncase, \\(W_n\\) said optimal weighting matrix.intuition behind result underlies Generalized Least Squares (see Section 1.5.2), : beneficial use criterion weights inversely proportional variances moments.\\(h(x_i;\\boldsymbol\\theta_0)\\) correlated \\(h(x_j;\\boldsymbol\\theta_0)\\), \\(\\ne j\\), :\n\\[\nS = \\mathbb{V}ar(h(x_i;\\boldsymbol\\theta_0)),\n\\]\ncan approximated \n\\[\n\\hat{\\Gamma}_{0,n}=\\frac{1}{n}\\sum_{=1}^{n} h(x_i;\\hat{\\boldsymbol\\theta}_n)h(x_{};\\hat{\\boldsymbol\\theta}_n)'.\n\\]time series context, often correlation \\(x_i\\) \\(x_{+k}\\), especially small \\(k\\)’s. case, time series \\(\\{y_i\\}\\) covariance stationary (see Def. ??), :\n\\[\nS := \\sum_{\\nu = -\\infty}^{\\infty} \\Gamma_\\nu,\n\\]\n\\(\\Gamma_\\nu := \\mathbb{E}[h(x_i;\\boldsymbol\\theta_0) h(x_{-\\nu};\\boldsymbol\\theta_0)']\\). Matrix \\(S\\) called long-run variance process \\(\\{y_i\\}\\) (see Def. ??).\\(\\nu \\ge 0\\), let us define \\(\\hat{\\Gamma}_{\\nu,n}\\) :\n\\[\n\\hat{\\Gamma}_{\\nu,n} = \\frac{1}{n} \\sum_{=\\nu + 1}^{n} h(x_i;\\hat{\\boldsymbol\\theta}_n)h(x_{-\\nu};\\hat{\\boldsymbol\\theta}_n)',\n\\]\n\\(S\\) can approximated Newey West (1987) formula (similar Eq. (??)):\n\\[\\begin{equation}\n\\hat{\\Gamma}_{0,n} + \\sum_{\\nu=1}^{q}\\left[1-\\frac{\\nu}{q+1}\\right](\\hat{\\Gamma}_{\\nu,n}+\\hat{\\Gamma}_{\\nu,n}').    \\tag{3.2}\n\\end{equation}\\]","code":""},{"path":"estimation-methods.html","id":"asymptotic-distribution-of-the-gmm-estimator","chapter":"3 Estimation Methods","heading":"3.1.2 Asymptotic distribution of the GMM estimator","text":":\n\\[\\begin{equation}\n\\boxed{\\sqrt{n}(\\hat{\\boldsymbol\\theta}_n - \\boldsymbol\\theta_0) \\overset{d}{\\rightarrow} \\mathcal{N}(0,V),}\\tag{3.3}\n\\end{equation}\\]\n\\(V = (DS^{-1}D')^{-1}\\), \n\\[\nD := \\left.\\mathbb{E}\\left(\\frac{\\partial h(x_i;\\boldsymbol\\theta)}{\\partial \\boldsymbol\\theta'}\\right)\\right|_{\\boldsymbol\\theta = \\boldsymbol\\theta_0}.\n\\]Matrix \\(V\\) can approximated \n\\[\\begin{equation}\n\\hat{V}_n = (\\hat{D}_n\\hat{S}_n^{-1}\\hat{D}_n')^{-1},\\tag{3.4}\n\\end{equation}\\]\n\\(\\hat{S}_n\\) given Eq. (3.2) \n\\[\n\\hat{D}'_n := \\left.\\frac{\\partial g(\\underline{y_n};\\boldsymbol\\theta)}{\\partial \\boldsymbol\\theta'}\\right|_{\\boldsymbol\\theta = \\hat{\\boldsymbol\\theta}_n}.\n\\]\npractice, previous matrix computed numerically.","code":""},{"path":"estimation-methods.html","id":"overidentif","chapter":"3 Estimation Methods","heading":"3.1.3 Testing hypotheses in the GMM framework","text":"first important test one concerning validity moment restrictions (Sargan-Hansen test; Sargan (1958) Hansen (1982)). Assume number restrictions imposed larger number parameters estimate (\\(r>K\\)). case, restrictions said -identifiying.correct specification, asymptotically :\n\\[\n\\sqrt{n}g(\\underline{y_n};{\\boldsymbol\\theta}_0)  \\sim \\mathcal{N}(0,S).\n\\]\nresult, comes :\n\\[\\begin{equation}\nJ_n = \\left(\\sqrt{n}g(\\underline{y_n};{\\boldsymbol\\theta}_0)\\right)'S^{-1}\\left(\\sqrt{n}g(\\underline{y_n};{\\boldsymbol\\theta}_0)\\right) \\tag{3.5}\n\\end{equation}\\]\nasymptotically follows \\(\\chi^2\\) distribution. number degrees freedom equal \\(r-K\\). (Note , \\(r=K\\), , expected, \\(J=0\\).) , asymptotically:\n\\[\nJ_n \\sim \\chi^2(r-K).\n\\]\nGMM framework also allows easily test linear restrictions parameters. First, given Eq. (3.3), Wald tests (see Eq. (1.13) Section 1.2.5) readily available. Second, one can also resort test equivalent likelihood ratio tests (see Definition 3.8). precisely, consider unconstrained model constrained version model, number restrictions equal \\(k\\). two models estimated considering moment constraints, weighting matrix —using Eq. (3.4), based unrestricted model—, :\n\\[\nn \\left[(g(\\underline{y_n};\\hat{{\\boldsymbol\\theta}}^*_n)-g(\\underline{y_n};\\hat{{\\boldsymbol\\theta}}_n)\\right] \\sim \\chi^2(k),\n\\]\n\\(\\hat{{\\boldsymbol\\theta}}^*_n\\) constrained estimate \\({\\boldsymbol\\theta}_0\\).","code":""},{"path":"estimation-methods.html","id":"example-estimation-of-the-stochastic-discount-factor-s.d.f.","chapter":"3 Estimation Methods","heading":"3.1.4 Example: Estimation of the Stochastic Discount Factor (s.d.f.)","text":"-arbitrage assumption, exists random variable \\(\\mathcal{M}_{t,t+1}\\) (s.d.f.) \n\\[\n\\mathbb{E}_t(\\mathcal{M}_{t,t+1}R_{t+1})=1\n\\]\n(gross) asset return \\(R_t\\). following, \\(R_t\\) denotes \\(n_r\\)-dimensional vector gross returns.consider following specification s.d.f.:\n\\[\\begin{equation}\n\\mathcal{M}_{t,t+1} = 1 - \\textbf{b}_M'(F_{t+1} - \\mathbb{E}_t(F_{t+1})), \\tag{3.6}\n\\end{equation}\\]\n\\(F_t\\) vector factors. Eq. (3.6) reads:\n\\[\n\\mathbb{E}_t([1 - \\textbf{b}_M'(F_{t+1} - \\mathbb{E}_t(F_{t+1}))]R_{t+1})=1.\n\\]Assume date-\\(t\\) information set \\(\\mathcal{}_t=\\{\\textbf{z}_t,\\mathcal{}_{t-1}\\}\\), \\(\\textbf{z}_t\\) vector variables observed date \\(t\\). (\\(\\mathbb{E}_t(\\bullet) \\equiv \\mathbb{E}(\\bullet|\\mathcal{}_t)\\).)can use \\(\\textbf{z}_t\\) instrument. Indeed, :\n\\[\\begin{eqnarray}\n&&\\mathbb{E}(z_{,t} [\\textbf{b}_M'\\{F_{t+1} - \\mathbb{E}_t(F_{t+1})\\}R_{t+1}-R_{t+1}+1]) \\nonumber \\\\\n&=&\\mathbb{E}(\\mathbb{E}_t\\{z_{,t} [\\textbf{b}_M'\\{F_{t+1} - \\mathbb{E}_t(F_{t+1})\\}R_{t+1}-R_{t+1}+1]\\})\\nonumber\\\\\n&=&\\mathbb{E}(z_{,t} \\underbrace{\\mathbb{E}_t\\{\\textbf{b}_M'\\{F_{t+1} - \\mathbb{E}_t(F_{t+1})\\}R_{t+1}-R_{t+1}+1\\}}_{1 - \\mathbb{E}_t(\\mathcal{M}_{t,t+1}R_{t+1})=0})=0.\\tag{3.7}\n\\end{eqnarray}\\]\nconverted conditional moment condition unconditional one (need implement GMM approach described ). However, stage, can still directly use GMM formulas conditional expectation \\(\\mathbb{E}_t(F_{t+1})\\) appears \\(\\mathbb{E}(z_{,t} [\\textbf{b}_M'\\{F_{t+1} - \\mathbb{E}_t(F_{t+1})\\}R_{t+1}-R_{t+1}+1])=0\\).go , let us assume :\n\\[\n\\mathbb{E}_t(F_{t+1}) = \\textbf{b}_F \\textbf{z}_t.\n\\]\ncan easily estimate matrix \\(\\textbf{b}_F\\) (dimension \\(n_F \\times n_z\\)) OLS. Note OLS can seen special GMM case. Indeed, done Eq. (3.7), can show , \\(j^{th}\\) component \\(F_t\\), :\n\\[\n\\mathbb{E}( [F_{j,t+1} - \\textbf{b}_{F,j} \\textbf{z}_t]\\textbf{z}_{t})=0,\n\\]\n\\(\\textbf{b}_{F,j}\\) denotes \\(j^{th}\\) row \\(\\textbf{b}_{F}\\). yields OLS formula.Equipped \\(\\textbf{b}_F\\), rely following moment restrictions estimate \\(\\textbf{b}_M\\):\n\\[\n\\mathbb{E}(z_{,t} [\\textbf{b}_M'\\{F_{t+1} - \\textbf{b}_F \\textbf{z}_t\\}R_{t+1}-R_{t+1}+1])=0.\n\\]\nSpecifically, number restrictions \\(n_R \\times n_z\\). Let us implement approach U.S. context, using data extracted FRED database. factor \\(F_t\\), use changes VIX personal consumption expenditures. returns (\\(R_t\\)) based Wilshire 5000 Price Index (stock price index) ICE BofA BBB US Corporate Index Total Return Index (bond return index).Define matrices containing \\(F_{t+1}\\), \\(\\textbf{z}_t\\), \\(R_{t+1}\\) vectors:Function f_aux compute \\(h(x_t;{\\boldsymbol\\theta})\\) \\(g(\\underline{y_T};{\\boldsymbol\\theta})\\); function f2beMin function minimized.Now, let’s minimize function, using use BFGS numerical algorithm (part optim wrapper). run 5 iterations (\\(W\\) updated).Finally, let’s compute standard deviation parameter estimates, using Eq. (3.4):Hansen statistic can used test model (see Eq. (3.5)). model correct, :\n\\[\nT g(\\underline{y_T};{\\boldsymbol\\theta})'\\, S^{-1} \\, g(\\underline{y_T};{\\boldsymbol\\theta}) \\sim \\,..d.\\,\\chi^2(J - K),\n\\]\n\\(J\\) number moment constraints (\\(n_z \\times n_r\\) ) \\(K\\) number estimated parameters (\\(=n_F\\) ).","code":"\nlibrary(fredr)\nfredr_set_key(\"df65e14c054697a52b4511e77fcfa1f3\")\nstart_date <- as.Date(\"1990-01-01\"); end_date <- as.Date(\"2022-01-01\")\nf <- function(ticker){\n  fredr(series_id = ticker,\n        observation_start = start_date,observation_end = end_date,\n        frequency = \"m\",aggregation_method = \"avg\")\n}\nvix <- f(\"VIXCLS\") # VIX\npce <- f(\"PCE\") # Personal consumption expenditures\nsto <- f(\"WILL5000PRFC\") # Wilshire 5000 Full Cap Price Index\nbdr <- f(\"BAMLCC0A4BBBTRIV\") # ICE BofA BBB US Corp. Index Tot. Return\nT <- dim(vix)[1]\ndvix <- c(vix$value[3:T]/vix$value[2:(T-1)]) # change in VIX t+1\ndpce <- c(pce$value[3:T]/pce$value[2:(T-1)]) # change in PCE t+1\ndsto <- c(sto$value[3:T]/sto$value[2:(T-1)]) # return t+1\ndbdr <- c(bdr$value[3:T]/bdr$value[2:(T-1)]) # return t+1\ndvix_1 <- c(vix$value[2:(T-1)]/vix$value[1:(T-2)]) # change in VIX t\ndpce_1 <- c(pce$value[2:(T-1)]/pce$value[1:(T-2)]) # change in PCE t\ndsto_1 <- c(sto$value[2:(T-1)]/sto$value[1:(T-2)]) # return t\ndbdr_1 <- c(bdr$value[2:(T-1)]/bdr$value[1:(T-2)]) # return t\nF_tp1 <- cbind(dvix,dpce)\nZ     <- cbind(1,dvix_1,dpce_1,dsto_1,dbdr_1)\nb_F <- t(solve(t(Z) %*% Z) %*% t(Z) %*% F_tp1)\nF_innov <- F_tp1 - Z %*% t(b_F)\nR_tp1 <- cbind(dsto,dbdr)\nn_F <- dim(F_tp1)[2]; n_R <- dim(R_tp1)[2]; n_z <- dim(Z)[2]\nf_aux <- function(theta){\n  b_M <- matrix(theta[1:n_F],ncol=1)\n  R_aux <- matrix(F_innov %*% b_M,T-2,n_R) * R_tp1 - R_tp1 + 1\n  H <- (R_aux %x% matrix(1,1,n_z)) * (matrix(1,1,n_R) %x% Z)\n  g <- matrix(apply(H,2,mean),ncol=1)\n  return(list(g=g,H=H))\n}\nf2beMin <- function(theta,W){# function to be minimized\n  res <- f_aux(theta)\n  return(t(res$g) %*% W %*% res$g)\n}\nlibrary(AEC)\ntheta <- c(rep(0,n_F)) # inital value\nfor(i in 1:10){# recursion on W\n  res <- f_aux(theta)\n  W <-  solve(NW.LongRunVariance(res$H,q=6))\n  res.optim <- optim(theta,f2beMin,W=W,\n                     method=\"BFGS\", # could be \"Nelder-Mead\"\n                     control=list(trace=FALSE,maxit=200),hessian=TRUE)\n  theta <- res.optim$par\n}\neps <- .0001\ng0 <- f_aux(theta)$g\nD <- NULL\nfor(i in 1:length(theta)){\n  theta.i <- theta\n  theta.i[i] <- theta.i[i] + eps\n  gi <- f_aux(theta.i)$g\n  D <- cbind(D,(gi-g0)/eps)\n}\nV <- 1/T * solve(t(D) %*% W %*% D)\nstd.dev <- sqrt(diag(V));t.stud <- theta/std.dev\ncbind(theta,std.dev,t.stud)##            theta    std.dev     t.stud\n## [1,]  -0.7180716  0.4646617 -1.5453642\n## [2,] -11.2042452 17.1039449 -0.6550679\ng <- f_aux(theta)$g\nHansen_stat <- T * t(g) %*% W %*% g\npvalue <- pchisq(q = Hansen_stat,df = n_R*n_z - n_F)\npvalue##           [,1]\n## [1,] 0.8789782"},{"path":"estimation-methods.html","id":"secMLE","chapter":"3 Estimation Methods","heading":"3.2 Maximum Likelihood Estimation","text":"","code":""},{"path":"estimation-methods.html","id":"intuition","chapter":"3 Estimation Methods","heading":"3.2.1 Intuition","text":"Intuitively, Maximum Likelihood Estimation (MLE) consists looking value \\({\\boldsymbol\\theta}\\) probability observed \\(\\mathbf{y}\\) (sample hand) highest possible.set example, assume time periods arrivals two customers shop, denoted \\(y_i\\), ..d. follow exponential distribution, .e. \\(y_i \\sim \\,..d.\\, \\mathcal{E}(\\lambda)\\). observed arrivals time, thereby constituting sample \\(\\mathbf{y}=\\{y_1,\\dots,y_n\\}\\). want estimate \\(\\lambda\\) (.e. case, vector parameters simply \\({\\boldsymbol\\theta} = \\lambda\\)).density \\(Y\\) (one observation) \\(f(y;\\lambda) = \\dfrac{1}{\\lambda}\\exp(-y/\\lambda)\\). Fig. 3.1 represents density functions different values \\(\\lambda\\).200 observations reported bottom Fig. 3.1 (red bars). build histogram display chart.\nFigure 3.1: red ticks, bottom, indicate observations (200 ). historgram based 200 observations\nestimate \\(\\lambda\\)? Intuitively, one led take \\(\\lambda\\) (theoretical) distribution closest histogram (can seen “empirical distribution”). approach consistent idea picking \\(\\lambda\\) probability observing values included \\(\\mathbf{y}\\) highest.Let us formal. Assume four observations: \\(y_1=1.1\\), \\(y_2=2.2\\), \\(y_3=0.7\\) \\(y_4=5.0\\). probability jointly observing:\\(1.1-\\varepsilon \\le Y_1 < 1.1+\\varepsilon\\),\\(2.2-\\varepsilon \\le Y_2 < 2.2+\\varepsilon\\),\\(0.7-\\varepsilon \\le Y_3 < 0.7+\\varepsilon\\), \\(5.0-\\varepsilon \\le Y_4 < 5.0+\\varepsilon\\)?\\(y_i\\)’s ..d., probability \\(\\prod_{=1}^4(2\\varepsilon f(y_i,\\lambda))\\).\nnext plot shows probability (divided \\(16\\varepsilon^4\\), depend \\(\\lambda\\)) function \\(\\lambda\\).\nFigure 3.2: Proba. \\(y_i-\\varepsilon \\le Y_i < y_i+\\varepsilon\\), \\(\\\\{1,2,3,4\\}\\). vertical red line indicates maximum function.\nvalue \\(\\lambda\\) maximizes probability 2.26.Let us come back example 200 observations:\nFigure 3.3: Log-likelihood function associated 200 ..d. observations. vertical red line indicates maximum function.\ncase, value \\(\\lambda\\) maxmimizes probability 3.42.","code":""},{"path":"estimation-methods.html","id":"definition-and-properties","chapter":"3 Estimation Methods","heading":"3.2.2 Definition and properties","text":"\\(f(y;\\boldsymbol\\theta)\\) denotes probability density function (p.d.f.) random variable \\(Y\\) depends set parameters \\(\\boldsymbol\\theta\\). density \\(n\\) independent identically distributed (..d.) observations \\(Y\\) given :\n\\[\nf(\\mathbf{y};\\boldsymbol\\theta) = \\prod_{=1}^n f(y_i;\\boldsymbol\\theta),\n\\]\n\\(\\mathbf{y}\\) denotes vector observations; \\(\\mathbf{y} = \\{y_1,\\dots,y_n\\}\\).Definition 3.2  (Likelihood function) likelihood function :\n\\[\n\\mathcal{L}: \\boldsymbol\\theta \\rightarrow  \\mathcal{L}(\\boldsymbol\\theta;\\mathbf{y})=f(\\mathbf{y};\\boldsymbol\\theta)=f(y_1,\\dots,y_n;\\boldsymbol\\theta).\n\\]often work \\(\\log \\mathcal{L}\\), log-likelihood function.Example 3.1  (Gaussian distribution) \\(y_i \\sim \\mathcal{N}(\\mu,\\sigma^2)\\), \n\\[\n\\log \\mathcal{L}(\\boldsymbol\\theta;\\mathbf{y}) = - \\frac{1}{2}\\sum_{=1}^n\\left( \\log \\sigma^2 + \\log 2\\pi + \\frac{(y_i-\\mu)^2}{\\sigma^2} \\right).\n\\]Definition 3.3  (Score) score \\(S(y;\\boldsymbol\\theta)\\) given \\(\\frac{\\partial \\log f(y;\\boldsymbol\\theta)}{\\partial \\boldsymbol\\theta}\\).\\(y_i \\sim \\mathcal{N}(\\mu,\\sigma^2)\\) (Example 3.1), \n\\[\n\\frac{\\partial \\log f(y;\\boldsymbol\\theta)}{\\partial \\boldsymbol\\theta} =\n\\left[\\begin{array}{c}\n\\dfrac{\\partial \\log f(y;\\boldsymbol\\theta)}{\\partial \\mu}\\\\\n\\dfrac{\\partial \\log f(y;\\boldsymbol\\theta)}{\\partial \\sigma^2}\n\\end{array}\\right] =\n\\left[\\begin{array}{c}\n\\dfrac{y-\\mu}{\\sigma^2}\\\\\n\\frac{1}{2\\sigma^2}\\left(\\frac{(y-\\mu)^2}{\\sigma^2}-1\\right)\n\\end{array}\\right].\n\\]Proposition 3.1  (Score expectation) expectation score zero.Proof. :\n\\[\\begin{eqnarray*}\n\\mathbb{E}\\left(\\frac{\\partial \\log f(Y;\\boldsymbol\\theta)}{\\partial \\boldsymbol\\theta}\\right) &=&\n\\int \\frac{\\partial \\log f(y;\\boldsymbol\\theta)}{\\partial \\boldsymbol\\theta} f(y;\\boldsymbol\\theta) dy \\\\\n&=& \\int \\frac{\\partial f(y;\\boldsymbol\\theta)/\\partial \\boldsymbol\\theta}{f(y;\\boldsymbol\\theta)} f(y;\\boldsymbol\\theta) dy =\n\\frac{\\partial}{\\partial \\boldsymbol\\theta} \\int f(y;\\boldsymbol\\theta) dy\\\\\n&=&\\partial 1 /\\partial \\boldsymbol\\theta = 0,\n\\end{eqnarray*}\\]\ngives result.Definition 3.4  (Fisher information matrix) information matrix (minus) expectation second derivatives log-likelihood function:\n\\[\n\\mathcal{}_Y(\\boldsymbol\\theta) = - \\mathbb{E} \\left( \\frac{\\partial^2 \\log f(Y;\\boldsymbol\\theta)}{\\partial \\boldsymbol\\theta \\partial \\boldsymbol\\theta'} \\right).\n\\]Proposition 3.2  \n\\[\n\\mathcal{}_Y(\\boldsymbol\\theta) = \\mathbb{E} \\left[ \\left( \\frac{\\partial \\log f(Y;\\boldsymbol\\theta)}{\\partial \\boldsymbol\\theta} \\right)\n\\left( \\frac{\\partial \\log f(Y;\\boldsymbol\\theta)}{\\partial \\boldsymbol\\theta} \\right)' \\right] = \\mathbb{V}ar[S(Y;\\boldsymbol\\theta)].\n\\]Proof. \\(\\frac{\\partial^2 \\log f(Y;\\boldsymbol\\theta)}{\\partial \\boldsymbol\\theta \\partial \\boldsymbol\\theta'} = \\frac{\\partial^2 f(Y;\\boldsymbol\\theta)}{\\partial \\boldsymbol\\theta \\partial \\boldsymbol\\theta'}\\frac{1}{f(Y;\\boldsymbol\\theta)} - \\frac{\\partial \\log f(Y;\\boldsymbol\\theta)}{\\partial \\boldsymbol\\theta}\\frac{\\partial \\log f(Y;\\boldsymbol\\theta)}{\\partial \\boldsymbol\\theta'}\\). expectation first right-hand side term \\(\\partial^2 1 /(\\partial \\boldsymbol\\theta \\partial \\boldsymbol\\theta') = \\mathbf{0}\\), gives result.Example 3.2  \\(y_i \\sim\\,..d.\\, \\mathcal{N}(\\mu,\\sigma^2)\\), let \\(\\boldsymbol\\theta = [\\mu,\\sigma^2]'\\) \n\\[\n\\frac{\\partial \\log f(y;\\boldsymbol\\theta)}{\\partial \\boldsymbol\\theta} = \\left[\\frac{y-\\mu}{\\sigma^2} \\quad \\frac{1}{2\\sigma^2}\\left(\\frac{(y-\\mu)^2}{\\sigma^2}-1\\right) \\right]',\n\\]\n\n\\[\n\\mathcal{}_Y(\\boldsymbol\\theta) = \\mathbb{E}\\left( \\frac{1}{\\sigma^4}\n\\left[\n\\begin{array}{cc}\n\\sigma^2&y-\\mu\\\\\ny-\\mu & \\frac{(y-\\mu)^2}{\\sigma^2}-\\frac{1}{2}\n\\end{array}\\right]\n\\right)=\n\\left[\n\\begin{array}{cc}\n1/\\sigma^2&0\\\\\n0 & 1/(2\\sigma^4)\n\\end{array}\\right].\n\\]Proposition 3.3  (Additive property Information matrix) information matrix resulting two independent experiments sum information matrices:\n\\[\n\\mathcal{}_{X,Y}(\\boldsymbol\\theta) = \\mathcal{}_X(\\boldsymbol\\theta) + \\mathcal{}_Y(\\boldsymbol\\theta).\n\\]Proof. Directly deduced definition information matrix (Def. 3.4), using epxectation product independent variables product expectations.Theorem 3.1  (Frechet-Darmois-Cramer-Rao bound) Consider unbiased estimator \\(\\boldsymbol\\theta\\) denoted \\(\\hat{\\boldsymbol\\theta}(Y)\\). variance random variable \\(\\boldsymbol\\omega'\\hat{\\boldsymbol\\theta}\\) (linear combination components \\(\\hat{\\boldsymbol\\theta}\\)) larger :\n\\[\n(\\boldsymbol\\omega'\\boldsymbol\\omega)^2/(\\boldsymbol\\omega' \\mathcal{}_Y(\\boldsymbol\\theta) \\boldsymbol\\omega).\n\\]Proof. Cauchy-Schwarz inequality implies \\(\\sqrt{\\mathbb{V}ar(\\boldsymbol\\omega'\\hat{\\boldsymbol\\theta}(Y))\\mathbb{V}ar(\\boldsymbol\\omega'S(Y;\\boldsymbol\\theta))} \\ge |\\boldsymbol\\omega'\\mathbb{C}ov[\\hat{\\boldsymbol\\theta}(Y),S(Y;\\boldsymbol\\theta)]\\boldsymbol\\omega |\\). Now, \\(\\mathbb{C}ov[\\hat{\\boldsymbol\\theta}(Y),S(Y;\\boldsymbol\\theta)] = \\int_y \\hat{\\boldsymbol\\theta}(y) \\frac{\\partial \\log f(y;\\boldsymbol\\theta)}{\\partial \\boldsymbol\\theta} f(y;\\boldsymbol\\theta)dy = \\frac{\\partial}{\\partial \\boldsymbol\\theta}\\int_y \\hat{\\boldsymbol\\theta}(y) f(y;\\boldsymbol\\theta)dy = \\mathbf{}\\) \\(\\hat{\\boldsymbol\\theta}\\) unbiased. Therefore \\(\\mathbb{V}ar(\\boldsymbol\\omega'\\hat{\\boldsymbol\\theta}(Y)) \\ge \\mathbb{V}ar(\\boldsymbol\\omega'S(Y;\\boldsymbol\\theta))^{-1} (\\boldsymbol\\omega'\\boldsymbol\\omega)^2\\). Prop. 3.2 leads result.Definition 3.5  (Identifiability) vector parameters \\(\\boldsymbol\\theta\\) identifiable , vector \\(\\boldsymbol\\theta^*\\):\n\\[\n\\boldsymbol\\theta^* \\ne \\boldsymbol\\theta \\Rightarrow \\mathcal{L}(\\boldsymbol\\theta^*;\\mathbf{y}) \\ne \\mathcal{L}(\\boldsymbol\\theta;\\mathbf{y}).\n\\]Definition 3.6  (Maximum Likelihood Estimator (MLE)) maximum likelihood estimator (MLE) vector \\(\\boldsymbol\\theta\\) maximizes likelihood function. Formally:\n\\[\\begin{equation}\n\\boldsymbol\\theta_{MLE} = \\arg \\max_{\\boldsymbol\\theta} \\mathcal{L}(\\boldsymbol\\theta;\\mathbf{y})  = \\arg \\max_{\\boldsymbol\\theta} \\log \\mathcal{L}(\\boldsymbol\\theta;\\mathbf{y}).\\tag{3.8}\n\\end{equation}\\]Definition 3.7  (Likelihood equation) necessary condition maximizing likelihood function (regularity assumption, see Hypotheses 3.1) :\n\\[\\begin{equation}\n\\dfrac{\\partial \\log \\mathcal{L}(\\boldsymbol\\theta;\\mathbf{y})}{\\partial \\boldsymbol\\theta} = \\mathbf{0}.\n\\end{equation}\\]Hypothesis 3.1  (Regularity assumptions) :\\(\\boldsymbol\\theta \\\\Theta\\) \\(\\Theta\\) compact.\\(\\boldsymbol\\theta_0\\) identified.log-likelihood function continuous \\(\\boldsymbol\\theta\\).\\(\\mathbb{E}_{\\boldsymbol\\theta_0}(\\log f(Y;\\boldsymbol\\theta))\\) exists.log-likelihood function \\((1/n)\\log\\mathcal{L}(\\boldsymbol\\theta;\\mathbf{y})\\) converges almost surely \\(\\mathbb{E}_{\\boldsymbol\\theta_0}(\\log f(Y;\\boldsymbol\\theta))\\), uniformly \\(\\boldsymbol\\theta \\\\Theta\\).log-likelihood function twice continuously differentiable open neighborood \\(\\boldsymbol\\theta_0\\).matrix \\(\\mathbf{}(\\boldsymbol\\theta_0) = - \\mathbb{E}_0 \\left( \\frac{\\partial^2 \\log \\mathcal{L}(\\boldsymbol\\theta;\\mathbf{y})}{\\partial \\boldsymbol\\theta \\partial \\boldsymbol\\theta'}\\right)\\) —Fisher Information matrix— exists nonsingular.Proposition 3.4  (Properties MLE) regularity conditions (Assumptions 3.1), MLE :Consistent: \\(\\mbox{plim}\\; \\boldsymbol\\theta_{MLE} = {\\boldsymbol\\theta}_0\\) (\\({\\boldsymbol\\theta}_0\\) true vector parameters).Asymptotically normal:\n\\[\\begin{equation}\n\\boxed{\\sqrt{n}(\\boldsymbol\\theta_{MLE} - \\boldsymbol\\theta_{0}) \\overset{d}{\\rightarrow} \\mathcal{N}(0,\\mathcal{}_Y(\\boldsymbol\\theta_0)^{-1}).} \\tag{3.9}\n\\end{equation}\\]Asymptotically efficient: \\(\\boldsymbol\\theta_{MLE}\\) asymptotically efficient achieves Freechet-Darmois-Cramer-Rao lower bound consistent estimators.Invariant: MLE \\(g(\\boldsymbol\\theta_0)\\) \\(g(\\boldsymbol\\theta_{MLE})\\) \\(g\\) continuous continuously differentiable function.Proof. See Appendix 5.5.Since \\(\\mathcal{}_Y(\\boldsymbol\\theta_0)=\\frac{1}{n}\\mathbf{}(\\boldsymbol\\theta_0)\\), asymptotic covariance matrix MLE \\([\\mathbf{}(\\boldsymbol\\theta_0)]^{-1}\\), :\n\\[\n[\\mathbf{}(\\boldsymbol\\theta_0)]^{-1} = \\left[- \\mathbb{E}_0 \\left( \\frac{\\partial^2 \\log \\mathcal{L}(\\boldsymbol\\theta;\\mathbf{y})}{\\partial \\boldsymbol\\theta \\partial \\boldsymbol\\theta'}\\right) \\right]^{-1}.\n\\]\ndirect (analytical) evaluation expectation often reach. can however estimated , either:\n\\[\\begin{eqnarray}\n\\hat{\\mathbf{}}_1^{-1} &=&  \\left( - \\frac{\\partial^2 \\log \\mathcal{L}({\\boldsymbol\\theta_{MLE}};\\mathbf{y})}{\\partial {\\boldsymbol\\theta} \\partial {\\boldsymbol\\theta}'}\\right)^{-1}, \\tag{3.10}\\\\\n\\hat{\\mathbf{}}_2^{-1} &=&  \\left( \\sum_{=1}^n \\frac{\\partial \\log \\mathcal{L}({\\boldsymbol\\theta_{MLE}};y_i)}{\\partial {\\boldsymbol\\theta}} \\frac{\\partial \\log \\mathcal{L}({\\boldsymbol\\theta_{MLE}};y_i)}{\\partial {\\boldsymbol\\theta'}} \\right)^{-1}.  \\tag{3.11}\n\\end{eqnarray}\\]Asymptotically, \\((\\hat{\\mathbf{}}_1^{-1})\\hat{\\mathbf{}}_2=Id\\), , two formulas provide result.case (suspected) misspecification, one can use -called sandwich estimator covariance matrix.7 covariance matrix given :\n\\[\\begin{equation}\n\\hat{\\mathbf{}}_3^{-1} = \\hat{\\mathbf{}}_2^{-1} \\hat{\\mathbf{}}_1 \\hat{\\mathbf{}}_2^{-1}.\\tag{3.12}\n\\end{equation}\\]","code":""},{"path":"estimation-methods.html","id":"to-sum-up-mle-in-practice","chapter":"3 Estimation Methods","heading":"3.2.3 To sum up – MLE in practice","text":"implement MLE, need:parametric model (depending vector parameters \\(\\boldsymbol\\theta\\) whose “true” value \\(\\boldsymbol\\theta_0\\)) specified...d. sources randomness identified.density associated one observation \\(y_i\\) computed analytically (function \\(\\boldsymbol\\theta\\)): \\(f(y;\\boldsymbol\\theta)\\).log-likelihood \\(\\log \\mathcal{L}(\\boldsymbol\\theta;\\mathbf{y}) = \\sum_i \\log f(y_i;\\boldsymbol\\theta)\\).MLE estimator results optimization problem (Eq. (3.8)):\n\\[\\begin{equation}\n\\boldsymbol\\theta_{MLE} = \\arg \\max_{\\boldsymbol\\theta} \\log \\mathcal{L}(\\boldsymbol\\theta;\\mathbf{y}).\n\\end{equation}\\]: \\(\\boldsymbol\\theta_{MLE} \\sim \\mathcal{N}({\\boldsymbol\\theta}_0,\\mathbf{}(\\boldsymbol\\theta_0)^{-1})\\), \\(\\mathbf{}(\\boldsymbol\\theta_0)^{-1}\\) estimated means Eq. (3.10), Eq. (3.11), Eq. (3.12). time, computation numerical.","code":""},{"path":"estimation-methods.html","id":"example-mle-estimation-of-a-mixture-of-gaussian-distribution","chapter":"3 Estimation Methods","heading":"3.2.4 Example: MLE estimation of a mixture of Gaussian distribution","text":"Consider returns Swiss Market Index (SMI). Assume returns independently drawn mixture Gaussian distributions. p.d.f. \\(f(x;\\boldsymbol\\theta)\\), \\(\\boldsymbol\\theta = [\\mu_1,\\sigma_1,\\mu_2,\\sigma_2,p]'\\), given :\n\\[\np \\frac{1}{\\sqrt{2\\pi\\sigma_1^2}}\\exp\\left(-\\frac{(x - \\mu_1)^2}{2\\sigma_1^2}\\right) + (1-p)\\frac{1}{\\sqrt{2\\pi\\sigma_2^2}}\\exp\\left(-\\frac{(x - \\mu_2)^2}{2\\sigma_2^2}\\right).\n\\]\n(See p.d.f. mixtures Gaussian distributions.)\nFigure 3.4: Time series SMI weekly returns (source: Yahoo Finance).\nBuild log-likelihood function (fucntion log.f), use numerical BFGS algorithm maximize (using optim wrapper):Next, compute estimates covariance matrix MLE (using Eqs. (3.10), (3.11), (3.12)), compare three sets resulting standard deviations five estimated paramters:According first (respectively third) type estimate covariance matrix, 95% confidence interval \\(\\mu_1\\) [0.182, 0.42] (resp. [0.151, 0.451]).Note directly estimated parameter \\(p\\) \\(\\nu = \\log(p/(1-p))\\) (way \\(p = \\exp(\\nu)/(1+\\exp(\\nu))\\)). order get estimate standard deviation esitmate \\(p\\), can implement Delta method. method based fact , function \\(g\\) continuous neighborhood \\(\\boldsymbol\\theta_0\\) large \\(n\\), :\n\\[\\begin{equation}\n\\mathbb{V}ar(g(\\hat{\\boldsymbol\\theta}_n)) \\approx \\frac{\\partial g(\\hat{\\boldsymbol\\theta}_n)}{\\partial \\boldsymbol\\theta'}\\mathbb{V}ar(\\hat{\\boldsymbol\\theta}_n)\\frac{\\partial g(\\hat{\\boldsymbol\\theta}_n)'}{\\partial \\boldsymbol\\theta}.\\tag{3.13}\n\\end{equation}\\]previous results show MLE estimate \\(p\\) 0.8749539, standard deviation approximately equal 0.0612573.finish , let us draw estimated parametric p.d.f. (mixture Gaussian distribution), compare non-parametric (kernel-based) estimate p.d.f. (using function density):\nFigure 3.5: Comparison different estimates distribution returns.\n","code":"\nlibrary(AEC);data(smi)\nT <- dim(smi)[1]\nh <- 5 # holding period (one week)\nsmi$r <- c(rep(NaN,h),\n           100*c(log(smi$Close[(1+h):T]/smi$Close[1:(T-h)])))\nindic.dates <- seq(1,T,by=5)  # weekly returns\nsmi <- smi[indic.dates,]\nsmi <- smi[complete.cases(smi),]\npar(mfrow=c(1,1));par(plt=c(.15,.95,.1,.95))\nplot(smi$Date,smi$r,type=\"l\",xlab=\"\",ylab=\"in percent\")\nabline(h=0,col=\"blue\")\nabline(h=mean(smi$r,na.rm = TRUE)+2*sd(smi$r,na.rm = TRUE),lty=3,col=\"blue\")\nabline(h=mean(smi$r,na.rm = TRUE)-2*sd(smi$r,na.rm = TRUE),lty=3,col=\"blue\")\nf <- function(theta,y){ # Likelihood function\n  mu.1 <- theta[1]; mu.2 <- theta[2]\n  sigma.1 <- theta[3]; sigma.2 <- theta[4]\n  p <- exp(theta[5])/(1+exp(theta[5]))\n  res <- p*1/sqrt(2*pi*sigma.1^2)*exp(-(y-mu.1)^2/(2*sigma.1^2)) +\n    (1-p)*1/sqrt(2*pi*sigma.2^2)*exp(-(y-mu.2)^2/(2*sigma.2^2))\n  return(res)\n}\nlog.f <- function(theta,y){ #log-Likelihood function\n  return(-sum(log(f(theta,y))))\n}\nres.optim <- optim(c(0,0,0.5,1.5,.5),\n                   log.f,\n                   y=smi$r,\n                   method=\"BFGS\", # could be \"Nelder-Mead\"\n                   control=list(trace=FALSE,maxit=100),hessian=TRUE)\ntheta <- res.optim$par\ntheta## [1]  0.3012379 -1.3167476  1.7715072  4.8197596  1.9454889\n# Hessian approach:\nI.1 <- solve(res.optim$hessian)\n# Outer-product of gradient approach:\nlog.f.0 <- log(f(theta,smi$r))\nepsilon <- .00000001\nd.log.f <- NULL\nfor(i in 1:length(theta)){\n  theta.i <- theta\n  theta.i[i] <- theta.i[i] + epsilon\n  log.f.i <- log(f(theta.i,smi$r))\n  d.log.f <- cbind(d.log.f,\n                   (log.f.i - log.f.0)/epsilon)\n}\nI.2 <- solve(t(d.log.f) %*% d.log.f)\n# Misspecification-robust approach (sandwich formula):\nI.3 <- I.1 %*% solve(I.2) %*% I.1\ncbind(diag(I.1),diag(I.2),diag(I.3))##             [,1]        [,2]       [,3]\n## [1,] 0.003683422 0.003199481 0.00586160\n## [2,] 0.226892824 0.194283391 0.38653389\n## [3,] 0.005764271 0.002769579 0.01712255\n## [4,] 0.194081311 0.047466419 0.83130838\n## [5,] 0.092114437 0.040366005 0.31347858\ng <- function(theta){\n  mu.1 <- theta[1]; mu.2 <- theta[2]\n  sigma.1 <- theta[3]; sigma.2 <- theta[4]\n  p <- exp(theta[5])/(1+exp(theta[5]))\n  return(c(mu.1,mu.2,sigma.1,sigma.2,p))\n}\n# Computation of g's gradient around estimated theta:\neps <- .00001\ng.theta <- g(theta)\ng.gradient <- NULL\nfor(i in 1:5){\n  theta.perturb <- theta\n  theta.perturb[i] <- theta[i] + eps\n  g.gradient <- cbind(g.gradient,(g(theta.perturb)-g.theta)/eps)\n}\nVar <- g.gradient %*% I.3 %*% t(g.gradient)\nstdv.g.theta <- sqrt(diag(Var))\nstdv.theta <- sqrt(diag(I.3))\ncbind(theta,stdv.theta,g.theta,stdv.g.theta)##           theta stdv.theta    g.theta stdv.g.theta\n## [1,]  0.3012379 0.07656108  0.3012379   0.07656108\n## [2,] -1.3167476 0.62171850 -1.3167476   0.62171850\n## [3,]  1.7715072 0.13085316  1.7715072   0.13085316\n## [4,]  4.8197596 0.91176114  4.8197596   0.91176114\n## [5,]  1.9454889 0.55989158  0.8749539   0.06125726\nx <- seq(-5,5,by=.01)\npar(plt=c(.1,.95,.1,.95))\nplot(x,f(theta,x),type=\"l\",lwd=2,xlab=\"returns, in percent\",ylab=\"\",\n     ylim=c(0,1.4*max(f(theta,x))))\nlines(density(smi$r),type=\"l\",lwd=2,lty=3)\nlines(x,dnorm(x,mean=mean(smi$r),sd = sd(smi$r)),col=\"red\",lty=2,lwd=2)\nrug(smi$r,col=\"blue\")\nlegend(\"topleft\",\n       c(\"Kernel estimate (non-parametric)\",\n         \"Estimated mixture of Gaussian distr. (MLE, parametric)\",\n         \"Normal distribution\"),\n       lty=c(3,1,2),lwd=c(2), # line width\n       col=c(\"black\",\"black\",\"red\"),pt.bg=c(1),pt.cex = c(1),\n       bg=\"white\",seg.len = 4)"},{"path":"estimation-methods.html","id":"TestMLE","chapter":"3 Estimation Methods","heading":"3.2.5 Test procedures","text":"Suppose want test following parameter restrictions:\n\\[\\begin{equation}\n\\boxed{H_0: \\underbrace{h(\\boldsymbol\\theta)}_{r \\times 1}=0.}\n\\end{equation}\\]context MLE, three tests largely used:Likelihood Ratio (LR) test,Wald (W) test,Lagrange Multiplier (LM) test.rationale behind three tests:8LR: \\(h(\\boldsymbol\\theta)=0\\), imposing restriction estimation (restricted estimator) result large decrease likelihood function (w.r.t unrestricted estimation).Wald: \\(h(\\boldsymbol\\theta)=0\\), \\(h(\\hat{\\boldsymbol\\theta})\\) far \\(0\\) (even restrictions imposed MLE).LM: \\(h(\\boldsymbol\\theta)=0\\), gradient likelihood function small evaluated restricted estimator.terms implementation, LR necessitates estimate restricted unrestricted models, Wald test requires estimation unrestricted model , LM tests requires estimation restricted model .shown , three test statistics associated three tests coincide asymptotically. (Therefore, naturally asymptotic distribution, \\(\\chi^2\\).)Proposition 3.5  (Asymptotic distribution Wald statistic) regularity conditions (Assumptions 3.1) \\(H_0: h(\\boldsymbol\\theta)=0\\), Wald statistic, defined :\n\\[\n\\boxed{\\xi^W = h(\\hat{\\boldsymbol\\theta})' \\mathbb{V}ar[h(\\hat{\\boldsymbol\\theta})]^{-1} h(\\hat{\\boldsymbol\\theta}),}\n\\]\n\n\\[\\begin{equation}\n\\mathbb{V}ar[h(\\hat{\\boldsymbol\\theta})] = \\left(\\frac{\\partial h(\\hat{\\boldsymbol\\theta})}{\\partial \\boldsymbol\\theta'} \\right) \\mathbb{V}ar[\\hat{\\boldsymbol\\theta}]\n\\left(\\frac{\\partial h(\\hat{\\boldsymbol\\theta})'}{\\partial \\boldsymbol\\theta} \\right),\\tag{3.14}\n\\end{equation}\\]\nasymptotically \\(\\chi^2(r)\\), number degrees freedom \\(r\\) corresponds dimension \\(h(\\boldsymbol\\theta)\\). (Note Eq. (3.14) one used Delta method, see Eq. (3.13).)Wald test, defined critical region\n\\[\n\\{\\xi^W \\ge \\chi^2_{1-\\alpha}(r)\\},\n\\]\n\\(\\chi^2_{1-\\alpha}(r)\\) denotes quantile level \\(1-\\alpha\\) \\(\\chi^2(r)\\) distribution, asymptotic level \\(\\alpha\\) consistent.9Proof. See Appendix 5.5.practice, Eq. (3.14), \\(\\mathbb{V}ar[\\hat{\\boldsymbol\\theta}]\\) replaced estimate given, e.g., Eq. (3.10), Eq. (3.11), Eq. (3.12).Proposition 3.6  (Asymptotic distribution LM test statistic) regularity conditions (Assumptions 3.1) \\(H_0: h(\\boldsymbol\\theta)=0\\), LM statistic\n\\[\\begin{equation}\n\\boxed{\\xi^{LM} =\n\\left(\\left.\\frac{\\partial \\log \\mathcal{L}(\\boldsymbol\\theta)}{\\partial \\boldsymbol\\theta'}\\right|_{\\boldsymbol\\theta = \\hat{\\boldsymbol\\theta}^0}  \\right)\n[\\mathbf{}(\\hat{\\boldsymbol\\theta}^0)]^{-1}\n\\left(\\left.\\frac{\\partial \\log \\mathcal{L}(\\boldsymbol\\theta)}{\\partial \\boldsymbol\\theta }\\right|_{\\boldsymbol\\theta = \\hat{\\boldsymbol\\theta}^0}  \\right),} \\tag{3.15}\n\\end{equation}\\]\n(\\(\\hat{\\boldsymbol\\theta}^0\\) restricted MLE estimator) \\(\\chi^2(r)\\).test defined critical region:\n\\[\n\\{\\xi^{LM} \\ge \\chi^2_{1-\\alpha}(r)\\}\n\\]\nasymptotic level \\(\\alpha\\) consistent (see Defs. 5.7 5.8). test called Score Lagrange Multiplier (LM) test.Proof. See Appendix 5.5.Definition 3.8  (Likelihood Ratio test statistics) likelihood ratio associated restriction form \\(H_0: h({\\boldsymbol\\theta})=0\\) given :\n\\[\nLR = \\frac{\\mathcal{L}_R(\\boldsymbol\\theta;\\mathbf{y})}{\\mathcal{L}_U(\\boldsymbol\\theta;\\mathbf{y})} \\quad (\\[0,1]),\n\\]\n\\(\\mathcal{L}_R\\) (respectively \\(\\mathcal{L}_U\\)) likelihood function imposes (resp. impose) restriction. likelihood ratio test statistic given \\(-2\\log(LR)\\), :\n\\[\n\\boxed{\\xi^{LR}= 2 (\\log\\mathcal{L}_U(\\boldsymbol\\theta;\\mathbf{y})-\\log\\mathcal{L}_R(\\boldsymbol\\theta;\\mathbf{y})).}\n\\]Proposition 3.7  (Asymptotic equivalence LR, LM, Wald tests) null hypothesis \\(H_0\\), , asymptotically:\n\\[\n\\xi^{LM} = \\xi^{LR} = \\xi^{W}.\n\\]Proof. See Appendix 5.5.","code":""},{"path":"estimation-methods.html","id":"bayesian-approach","chapter":"3 Estimation Methods","heading":"3.3 Bayesian approach","text":"","code":""},{"path":"estimation-methods.html","id":"introduction","chapter":"3 Estimation Methods","heading":"3.3.1 Introduction","text":"excellent introduction Bayesian methods proposed Martin Haugh, 2017.suggested name approach, starting point Bayes formula:\n\\[\n\\mathbb{P}(|B) = \\frac{\\mathbb{P}(\\& B)}{\\mathbb{P}(B)},\n\\]\n\\(\\) \\(B\\) two “events”. instance, \\(\\) may : parameter \\(\\alpha\\) (conceived something stochastic) lies interval \\([,b]\\). Assume interested probability occurrence \\(\\). Without specific information (“unconditionally”), probability \\(\\mathbb{P}()\\). evaluation probability can better provided additional form information. Typically, event \\(B\\) tends occur simultaneously \\(\\), knowledge \\(B\\) can useful. Bayes formula says additional information (\\(B\\)) can used “update” probability event \\(\\).case, intuition work follows: assume know form data-generating process (DGP). , know structure model used draw stochastic data; also know type distributions used generate data. However, know numerical values parameters characterizing DGP. Let us denote \\({\\boldsymbol\\theta}\\) vector unknown parameters. parameters known exactly, assume –even without observed data– priors distribution. , case example (\\(\\) \\(B\\)), observation data generated model can reduce uncertainty associated \\({\\boldsymbol\\theta}\\). Loosely speaking, combining priors observations data generated model result “thinner” distributions components \\({\\boldsymbol\\theta}\\). latter distributions called posterior distributions.10Let us formalize intuition. Define prior \\(f_{\\boldsymbol\\theta}({\\boldsymbol\\theta})\\) model realizations (“data”) vector \\(\\mathbf{y}\\). joint distribution \\((\\mathbf{y},{\\boldsymbol\\theta})\\) given :\n\\[\nf_{Y,{\\boldsymbol\\theta}}(\\mathbf{y},{\\boldsymbol\\theta}) = f_{Y|{\\boldsymbol\\theta}}(\\mathbf{y},{\\boldsymbol\\theta})f_{\\boldsymbol\\theta}({\\boldsymbol\\theta}),\n\\]\n, symmetrically, \n\\[\nf_{Y,{\\boldsymbol\\theta}}(\\mathbf{y},{\\boldsymbol\\theta}) = f_{{\\boldsymbol\\theta}|Y}({\\boldsymbol\\theta},\\mathbf{y})f_Y(\\mathbf{y}),\n\\]\n\\(f_{{\\boldsymbol\\theta}|Y}(\\cdot,\\mathbf{y})\\), distribution parameters conditional observations, posterior distribution.last two equations imply :\n\\[\\begin{equation}\nf_{{\\boldsymbol\\theta}|Y}({\\boldsymbol\\theta},\\mathbf{y}) = \\frac{f_{Y|{\\boldsymbol\\theta}}(\\mathbf{y},{\\boldsymbol\\theta})f_{{\\boldsymbol\\theta}}({\\boldsymbol\\theta})}{f_Y(\\mathbf{y})}.\\tag{3.16}\n\\end{equation}\\]\nNote \\(f_Y\\) marginal (unconditional) distribution \\(\\mathbf{y}\\), can written:\n\\[\\begin{equation}\nf_Y(\\mathbf{y}) = \\int f_{Y|{\\boldsymbol\\theta}}(\\mathbf{y},{\\boldsymbol\\theta})f_{\\boldsymbol\\theta}({\\boldsymbol\\theta}) d {\\boldsymbol\\theta}.\n\\end{equation}\\]Eq. (3.16) sometimes rewritten follows:\n\\[\\begin{equation}\nf_{{\\boldsymbol\\theta}|Y}({\\boldsymbol\\theta},\\mathbf{y}) \\propto f_{{\\boldsymbol\\theta},Y}({\\boldsymbol\\theta},\\mathbf{y}) := f_{Y|{\\boldsymbol\\theta}}(\\mathbf{y},{\\boldsymbol\\theta})f_{\\boldsymbol\\theta}({\\boldsymbol\\theta}), \\tag{3.17}\n\\end{equation}\\]\n\\(\\propto\\) means, loosely speaking, “proportional ”. rare instances, starting given priors, one can analytically compute posterior distribution \\(f_{\\boldsymbol\\theta}({\\boldsymbol\\theta},\\mathbf{y})\\). However, cases, reach. One resort numerical approaches compute posterior distribution. Monte Carlo Markov Chains (MCMC) one .According Bernstein-von Mises Theorem, Bayesian MLE estimators large sample properties. (particular, Bayesian approach also achieve FDCR bound, see Theorem 3.1.) intuition behind result influence prior diminishes increasing sample sizes.","code":""},{"path":"estimation-methods.html","id":"monte-carlo-markov-chains","chapter":"3 Estimation Methods","heading":"3.3.2 Monte-Carlo Markov Chains","text":"MCMC techniques aim using simulations approach distribution whose distribution difficult obtain analytically. Indeed, circumstances, one can draw distribution even know analytical expression.Definition 3.9  (Markov Chain) sequence \\(\\{z_i\\}\\) said (first-order) Markovian process satisfies:\n\\[\nf(z_i|z_{-1},z_{-2},\\dots) = f(z_i|z_{-1}).\n\\]Metropolis-Hastings (MH) algorithm specific MCMC approach allows generate samples \\({\\boldsymbol\\theta}\\)’s whose distribution approximately corresponds posterior distribution Eq. (3.16).MH algorithm recursive algorithm. , one can draw \\(^{th}\\) value \\({\\boldsymbol\\theta}\\), denoted \\({\\boldsymbol\\theta}_i\\), one already drawn \\({\\boldsymbol\\theta}_{-1}\\). Assume \\({\\boldsymbol\\theta}_{-1}\\). obtain value \\({\\boldsymbol\\theta}_i\\) implementing following steps:Draw \\(\\tilde{{\\boldsymbol\\theta}}_i\\) conditional distribution \\(Q_{\\tilde{{\\boldsymbol\\theta}}|{\\boldsymbol\\theta}}(\\cdot,{\\boldsymbol\\theta}_{-1})\\), called proposal distribution.Draw \\(u\\) uniform distribution \\([0,1]\\).Compute\n\\[\\begin{equation}\n\\alpha(\\tilde{{\\boldsymbol\\theta}}_i,{\\boldsymbol\\theta}_{-1}):= \\min\\left(\\frac{f_{{\\boldsymbol\\theta},Y}(\\tilde{{\\boldsymbol\\theta}}_i,\\mathbf{y})}{f_{{\\boldsymbol\\theta},Y}({\\boldsymbol\\theta}_{-1},\\mathbf{y})}\\times\\frac{Q_{\\tilde{{\\boldsymbol\\theta}}|{\\boldsymbol\\theta}}({\\boldsymbol\\theta}_{-1},\\tilde{{\\boldsymbol\\theta}}_i)}{Q_{\\tilde{{\\boldsymbol\\theta}}|{\\boldsymbol\\theta}}(\\tilde{{\\boldsymbol\\theta}}_i,{\\boldsymbol\\theta}_{-1})},1\\right),\\tag{3.18}\n\\end{equation}\\]\n\\(f_{{\\boldsymbol\\theta},Y}\\) given Eq. (3.17).\\(u<\\alpha(\\tilde{{\\boldsymbol\\theta}}_i,{\\boldsymbol\\theta}_{-1})\\), take \\({\\boldsymbol\\theta}_i = \\tilde{{\\boldsymbol\\theta}}_i\\), otherwise leave \\({\\boldsymbol\\theta}_i\\) equal \\({\\boldsymbol\\theta}_{-1}\\).can shown , distribution draws converges posterior distribution. , sufficiently large number iterations, draws can considered drawn posterior distribution.11To get insights algorithm, consider case symmetric proposal distribution, :\n\\[\\begin{equation}\nQ_{\\tilde{{\\boldsymbol\\theta}}|{\\boldsymbol\\theta}}(\\tilde{{\\boldsymbol\\theta}}_i,{\\boldsymbol\\theta}_{-1})=Q_{\\tilde{{\\boldsymbol\\theta}}|{\\boldsymbol\\theta}}({\\boldsymbol\\theta}_{-1},\\tilde{{\\boldsymbol\\theta}}_i).\\tag{3.19}\n\\end{equation}\\]\n:\n\\[\\begin{equation}\n\\alpha(\\tilde{{\\boldsymbol\\theta}},{\\boldsymbol\\theta}_{-1})= \\min\\left(\\frac{q(\\tilde{{\\boldsymbol\\theta}},y)}{q({\\boldsymbol\\theta}_{-1},y)},1\\right). \\tag{3.20}\n\\end{equation}\\]\nRemember , marginal distribution data (\\(f_Y(\\mathbf{y})\\)), \\(f_{{\\boldsymbol\\theta},Y}(\\tilde{{\\boldsymbol\\theta}},\\mathbf{y})\\) probability observing \\(\\mathbf{y}\\) conditional model parameterized \\(\\tilde{\\boldsymbol\\theta}\\). , Eq. (3.20), appears probability larger \\(\\tilde{\\boldsymbol\\theta}\\) \\({\\boldsymbol\\theta}_{-1}\\) (case \\(\\tilde{\\boldsymbol\\theta}\\) seems “consistent observations \\(\\mathbf{y}\\)” \\({\\boldsymbol\\theta}_{-1}\\)), accept \\({\\boldsymbol\\theta}_i\\). contrast, \\(f_{{\\boldsymbol\\theta},Y}(\\tilde{{\\boldsymbol\\theta}},\\mathbf{y})<f_{{\\boldsymbol\\theta},Y}({\\boldsymbol\\theta}_{-1},\\mathbf{y})\\), necessarily accept proposed value \\(\\tilde{{\\boldsymbol\\theta}}\\), especially \\(f_{{\\boldsymbol\\theta},Y}(\\tilde{{\\boldsymbol\\theta}},\\mathbf{y})\\ll f_{{\\boldsymbol\\theta},Y}({\\boldsymbol\\theta}_{-1},\\mathbf{y})\\) (case \\(\\tilde{\\boldsymbol\\theta}\\) seems far less consistent observations \\(\\mathbf{y}\\) \\({\\boldsymbol\\theta}_{-1}\\), , accordingly, acceptance probability, namely \\(\\alpha(\\tilde{{\\boldsymbol\\theta}},{\\boldsymbol\\theta}_{-1})\\), small).choice proposal distribution \\(Q_{\\tilde{\\boldsymbol\\theta}|{\\boldsymbol\\theta}}\\) crucial get rapid convergence algorithm. Looking Eq. (3.18), easily seen optimal choice \\(Q_{\\tilde{\\boldsymbol\\theta}|{\\boldsymbol\\theta}}(\\cdot,{\\boldsymbol\\theta}_i)=f_{{\\boldsymbol\\theta}|Y}(\\cdot,\\mathbf{y})\\). case, \\(\\alpha(\\tilde{{\\boldsymbol\\theta}}_i,{\\boldsymbol\\theta}_{-1})\\equiv 1\\) (see Eq. (3.18)). accept draws proposal distribution, distribution directly posterior distribution. course, situation realistoc objective algorithm precisely approximate posterior distribution.common choice \\(Q\\) multivariate normal distribution. \\({\\boldsymbol\\theta}\\) dimension \\(K\\), can instance use:\n\\[\nQ(\\tilde{\\boldsymbol\\theta},{\\boldsymbol\\theta})= \\frac{1}{\\left(\\sqrt{2\\pi\\sigma^2}\\right)^K}\\exp\\left(-\\frac{1}{2}\\sum_{j=1}^K\\frac{(\\tilde{\\boldsymbol\\theta}_j-{\\boldsymbol\\theta}_j)^2}{\\sigma^2}\\right),\n\\]\nexample symmetric proposal distribution (see Eq. (3.19)). Equivalently, :\n\\[\n\\tilde{\\boldsymbol\\theta} = {\\boldsymbol\\theta} + \\varepsilon,\n\\]\n\\(\\varepsilon\\) \\(K\\)-dimensional vector independent zero-mean normal disturbances variance \\(\\sigma^2\\).12 One determine appropriate value \\(\\sigma\\). low, \\(\\alpha\\) close 1 (\\(\\tilde{{\\boldsymbol\\theta}}_i\\) close \\({\\boldsymbol\\theta}_{-1}\\)), accept often proposed value (\\(\\tilde{{\\boldsymbol\\theta}}_i\\)). seems favourable situation. may . Indeed, means take large number iterations explore whole distribution \\({\\boldsymbol\\theta}\\). \\(\\sigma\\) large? case, likely porposed values (\\(\\tilde{{\\boldsymbol\\theta}}_i\\)) often result poor likelihoods; probability acceptance low Markov chain may blocked initial value. Therefore, intermediate values \\(\\sigma^2\\) determined. acceptance rate (.e., average value \\(\\alpha(\\tilde{{\\boldsymbol\\theta}},{\\boldsymbol\\theta}_{-1})\\)) can used guide . Indeed, literature explores optimal values acceptance rate (order obtain best possible fit posterior minimum number algorithm iterations). particular, following Roberts, Gelman, Gilks (1997), people often target acceptance rate order magnitude 20%.important note , implement approach, one able compute joint p.d.f. \\(q({\\boldsymbol\\theta},\\mathbf{y})=f_{Y|{\\boldsymbol\\theta}}(\\mathbf{y},{\\boldsymbol\\theta})f_{\\boldsymbol\\theta}({\\boldsymbol\\theta})\\) (Eq. (3.17)). , soon one can evaluate likelihood (\\(f_{Y|{\\boldsymbol\\theta}}(\\mathbf{y},{\\boldsymbol\\theta})\\)) prior (\\(f_{\\boldsymbol\\theta}({\\boldsymbol\\theta})\\)), can employ methodology.","code":""},{"path":"estimation-methods.html","id":"example-ar1-specification","chapter":"3 Estimation Methods","heading":"3.3.3 Example: AR(1) specification","text":"following example, employ MCMC order estimate posterior distributions three parameters defining AR(1) model (see Section ??). specification follows:\n\\[\ny_t = \\mu + \\rho y_{t-1} + \\sigma \\varepsilon_{t}, \\quad \\varepsilon_t \\sim \\,..d.\\,\\mathcal{N}(0,1).\n\\]\nHence, \\({\\boldsymbol\\theta} = [\\mu,\\rho,\\sigma]\\). Let us first simulate process \\(T\\) periods:Next, let us write likelihood function, .e. \\(f_{Y|{\\boldsymbol\\theta}}(\\mathbf{y},{\\boldsymbol\\theta})\\). \\(\\rho\\), expected 0 1, use logistic transformation. \\(\\sigma\\), expected positive, use exponential transformation.Next define function rQ draws (Gaussian) proposal distribution, well function Q, computes \\(Q_{\\tilde{\\boldsymbol\\theta}|{\\boldsymbol\\theta}}(\\tilde{\\boldsymbol\\theta},{\\boldsymbol\\theta})\\):consider Gaussian priors:Function p_tilde corresponds \\(f_{{\\boldsymbol\\theta},Y}\\):can now define function \\(\\alpha\\) (Eq. (3.18)):Now, set us write MCMC function:Specify Gaussian priors:\nFigure 3.6: upper line plot compares prior (black) posterior (red) distributions. vertical dashed blue lines indicate true values parameters. second row plots show sequence \\(\\boldsymbol\\theta_i\\)’s generated MCMC algorithm. sequences ones used produce posterior distributions (red lines) upper plots.\n","code":"\nmu <- .6; rho <- .8; sigma <- .5 # true model specification\nT <- 20 # number of observations\ny0 <- mu/(1-rho)\nY <- NULL\nfor(t in 1:T){\n  if(t==1){y <- y0}\n  y <- mu + rho*y + sigma * rnorm(1)\n  Y <- c(Y,y)}\nplot(Y,type=\"l\",xlab=\"time t\",ylab=expression(y[t]))\nlikelihood <- function(param,Y){\n  mu  <- param[1]\n  rho <- exp(param[2])/(1+exp(param[2]))\n  sigma <- exp(param[3])\n  MU <- mu/(1-rho)\n  SIGMA2 <- sigma^2/(1-rho^2)\n  L <- 1/sqrt(2*pi*SIGMA2)*exp(-(Y[1]-MU)^2/(2*SIGMA2))\n  Y1 <- Y[2:length(Y)]\n  Y0 <- Y[1:(length(Y)-1)]\n  aux <- 1/sqrt(2*pi*sigma^2)*exp(-(Y1-mu-rho*Y0)^2/(2*sigma^2))\n  L <- L * prod(aux)\n  return(L)\n}\nrQ <- function(x,a){\n  n <- length(x)\n  y <- x + a * rnorm(n)\n  return(y)}\nQ <- function(y,x,a){\n  q <- 1/sqrt(2*pi*a^2)*exp(-(y - x)^2/(2*a^2))\n  return(prod(q))}\nprior <- function(param,means_prior,stdv_prior){\n  f <- 1/sqrt(2*pi*stdv_prior^2)*exp(-(param - \n                                         means_prior)^2/(2*stdv_prior^2))\n  return(prod(f))}\np_tilde <- function(param,Y,means_prior,stdv_prior){\n  p <- likelihood(param,Y) * prior(param,means_prior,stdv_prior)\n  return(p)}\nalpha <- function(y,x,means_prior,stdv_prior,a){\n  aux <- p_tilde(y,Y,means_prior,stdv_prior)/\n    p_tilde(x,Y,means_prior,stdv_prior) * Q(y,x,a)/Q(x,y,a)\n  alpha_proba <- min(aux,1)\n  return(alpha_proba)}\nMCMC <- function(Y,means_prior,stdv_prior,a,N){\n  x <- means_prior\n  all_theta <- NULL\n  count_accept <- 0\n  for(i in 1:N){\n    y <- rQ(x,a)\n    alph <- alpha(y,x,means_prior,stdv_prior,a)\n    #print(alph)\n    u <- runif(1)\n    if(u < alph){\n      count_accept <- count_accept + 1\n      x <- y}\n    all_theta <- rbind(all_theta,x)}\n  print(paste(\"Acceptance rate:\",toString(round(count_accept/N,3))))\n  return(all_theta)}\ntrue_values <- c(mu,log(rho/(1-rho)),log(sigma))\nmeans_prior <- c(1,0,0) # as if we did not know the true values\nstdv_prior <- rep(2,3)\nresultMCMC <- MCMC(Y,means_prior,stdv_prior,a=.45,N=20000)## [1] \"Acceptance rate: 0.098\"\npar(mfrow=c(2,3))\nfor(i in 1:length(means_prior)){\n  m <- means_prior[i]\n  s <- stdv_prior[i]\n  x <- seq(m-3*s,m+3*s,length.out = 100)\n  par(mfg=c(1,i))\n  aux <- density(resultMCMC[,i])\n  par(plt=c(.15,.95,.15,.85))\n  plot(x,dnorm(x,m,s),type=\"l\",xlab=\"\",ylab=\"\",main=paste(\"Parameter\",i),\n       ylim=c(0,max(aux$y)))\n  lines(aux$x,aux$y,col=\"red\",lwd=2)\n  abline(v=true_values[i],lty=2,col=\"blue\")\n  par(mfg=c(2,i))\n  plot(resultMCMC[,i],1:length(resultMCMC[,i]),xlim=c(min(x),max(x)),\n       type=\"l\",xlab=\"\",ylab=\"\")}"},{"path":"microeconometrics.html","id":"microeconometrics","chapter":"4 Microeconometrics","heading":"4 Microeconometrics","text":"microeconometric models, variables interest often feature restricted distributions —instance discontinuous support—, necessitates specific models. Typical examples discrete-choice models (binary, multinomial, ordered outcomes), sample selection models (censored truncated outcomes), count-data models (integer outcomes). chapter describes estimation interpretation models. also shows discrete-choice models can emerge (structural) random-utility frameworks.","code":""},{"path":"microeconometrics.html","id":"binary-choice-models","chapter":"4 Microeconometrics","heading":"4.1 Binary-choice models","text":"many instances, variables explained (\\(y_i\\)’s) two possible values (\\(0\\) \\(1\\), say). , binary variables. probability equal either 0 1 may depend independent variables, gathered vectors \\(\\mathbf{x}_i\\) (\\(K \\times 1\\)).spectrum applications wide:Binary decisions (e.g. referendums, owner renter, living city countryside, /labour force,…),Contamination (disease default),Success/failure (exams).Without loss generality, model reads:\n\\[\\begin{equation}\\label{eq:binaryBenroulli}\ny_i | \\mathbf{X} \\sim \\mathcal{B}(g(\\mathbf{x}_i;\\boldsymbol\\theta)),\n\\end{equation}\\]\n\\(g(\\mathbf{x}_i;\\boldsymbol\\theta)\\) parameter Bernoulli distribution. words, conditionally \\(\\mathbf{X}\\):\n\\[\\begin{equation}\ny_i = \\left\\{\n\\begin{array}{cl}\n1 & \\mbox{ probability } g(\\mathbf{x}_i;\\boldsymbol\\theta)\\\\\n0 & \\mbox{ probability } 1-g(\\mathbf{x}_i;\\boldsymbol\\theta),\n\\end{array}\n\\right.\\tag{4.1}\n\\end{equation}\\]\n\\(\\boldsymbol\\theta\\) vector parameters estimated.estimation strategy assume \\(g(\\mathbf{x}_i;\\boldsymbol\\theta)\\) can proxied \\(\\tilde{\\boldsymbol\\theta}'\\mathbf{x}_i\\) run linear regression estimate \\(\\tilde{\\boldsymbol\\theta}\\) (situation called Linear Probability Model, LPM):\n\\[\ny_i = \\tilde{\\boldsymbol\\theta}'\\mathbf{x}_i + \\varepsilon_i.\n\\]\nNotwithstanding fact specification exclude negative probabilities probabilities greater one, compatible assumption zero conditional mean (Hypothesis 1.2) assumption non-correlated residuals (Hypothesis 1.4), difficultly homoskedasticity assumption (Hypothesis 1.3). Moreover, \\(\\varepsilon_i\\)’s Gaussian (\\(y_i \\\\{0,1\\}\\)). Hence, using linear regression study relationship \\(\\mathbf{x}_i\\) \\(y_i\\) can consistent inefficient.Figure 4.1 illustrates fit resulting application LPM model binary (dependent) variables.\nFigure 4.1: Fitting binary variable linear model (Linear Probability Model, LPM). model \\(\\mathbb{P}(y_i=1|x_i)=\\Phi(0.5+2x_i)\\), \\(\\Phi\\) c.d.f. normal distribution \\(x_i \\sim \\,..d.\\,\\mathcal{N}(0,1)\\).\nExcept last row (LPM case), Table 4.1 provides examples functions \\(g\\) valued \\([0,1]\\), can therefore used models type: \\(\\mathbb{P}(y_i=1|\\mathbf{x}_i;\\boldsymbol\\theta) = g(\\boldsymbol\\theta'\\mathbf{x}_i)\\) (see Eq. (4.1)). “linear” case given comparison, note satisfy \\(g(\\boldsymbol\\theta'\\mathbf{x}_i) \\[0,1]\\) value \\(\\boldsymbol\\theta'\\mathbf{x}_i\\).Table 4.1:  table provides examples function \\(g\\), s.t. \\(\\mathbb{P}(y_i=1|\\mathbf{x}_i;\\boldsymbol heta) = g(\\boldsymbol\\theta'\\mathbf{x}_i)\\). LPM case (last row) given comparison , , satisfy \\(g(\\boldsymbol\\theta'\\mathbf{x}_i) \\[0,1]\\) value \\(\\boldsymbol\\theta'\\mathbf{x}_i\\).Figure 4.2 displays first three \\(g\\) functions appearing Table 4.1.\nFigure 4.2: Probit, Logit, Log-log functions.\nprobit logit models popular binary-choice models. probit model, :\n\\[\\begin{equation}\ng(z) = \\Phi(z),\\tag{4.2}\n\\end{equation}\\]\n\\(\\Phi\\) c.d.f. normal distribution. logit model:\n\\[\\begin{equation}\ng(z) = \\frac{1}{1+\\exp(-z)}.\\tag{4.3}\n\\end{equation}\\]Figure 4.3 shows conditional probabilities associated (probit) model used generate data Figure 4.1.\nFigure 4.3: model \\(\\mathbb{P}(y_i=1|x_i)=\\Phi(0.5+2x_i)\\), \\(\\Phi\\) c.d.f. normal distribution \\(x_i \\sim \\,..d.\\,\\mathcal{N}(0,1)\\). Crosses give model-implied probabilities \\(y_i=1\\) (conditional \\(x_i\\)).\n","code":""},{"path":"microeconometrics.html","id":"latent","chapter":"4 Microeconometrics","heading":"4.1.1 Interpretation in terms of latent variable, and utility-based models","text":"probit model interpretation terms latent variables, , turn, often exploited structural models, called Random Utility Models (RUM). structural models, assumed agents take decision selecting outcome provides larger utility (agent \\(\\), two possible outcomes: \\(y_i=0\\) \\(y_i=1\\)). Part utility observed econometrician —depends covariates \\(\\mathbf{x}_i\\)— part latent.probit model, :\n\\[\n\\mathbb{P}(y_i=1|\\mathbf{x}_i;\\boldsymbol\\theta) = \\Phi(\\boldsymbol\\theta'\\mathbf{x}_i) = \\mathbb{P}(-\\varepsilon_{}<\\boldsymbol\\theta'\\mathbf{x}_i),\n\\]\n\\(\\varepsilon_{} \\sim \\mathcal{N}(0,1)\\). :\n\\[\n\\mathbb{P}(y_i=1|\\mathbf{x}_i;\\boldsymbol\\theta) = \\mathbb{P}(0< y_i^*),\n\\]\n\\(y_i^* = \\boldsymbol\\theta'\\mathbf{x}_i + \\varepsilon_i\\), \\(\\varepsilon_{} \\sim \\mathcal{N}(0,1)\\). Variable \\(y_i^*\\) can interpreted (latent) variable determines \\(y_i\\) (since \\(y_i = \\mathbb{}_{\\{y_i^*>0\\}}\\)).Figure 4.4 illustrates situation.\nFigure 4.4: Distribution \\(y_i^*\\) conditional \\(\\mathbf{x}_i\\).\nAssume agent (\\(\\)) chooses \\(y_i=1\\) utility associated choice (\\(U_{,1}\\)) higher one associated \\(y_i=0\\) (\\(U_{,0}\\)). Assume utility agent \\(\\), chooses outcome \\(j\\) (\\(\\\\{0,1\\}\\)), given \n\\[\nU_{,j} = V_{,j} + \\varepsilon_{,j},\n\\]\n\\(V_{,j}\\) deterministic component utility associated choice \\(\\varepsilon_{,j}\\) random (agent-specific) component. Moreover, posit \\(V_{,j} = \\boldsymbol\\theta_j'\\mathbf{x}_i\\). :\n\\[\\begin{eqnarray}\n\\mathbb{P}(y_i = 1|\\mathbf{x}_i;\\boldsymbol\\theta) &=& \\mathbb{P}(\\boldsymbol\\theta_1'\\mathbf{x}_i+\\varepsilon_{,1}>\\boldsymbol\\theta_0'\\mathbf{x}_i+\\varepsilon_{,0}) \\nonumber\\\\\n&=& F(\\boldsymbol\\theta_1'\\mathbf{x}_i-\\boldsymbol\\theta_0'\\mathbf{x}_i) = F([\\boldsymbol\\theta_1-\\boldsymbol\\theta_0]'\\mathbf{x}_i),\\tag{4.4}\n\\end{eqnarray}\\]\n\\(F\\) c.d.f. \\(\\varepsilon_{,0}-\\varepsilon_{,1}\\).Note difference \\(\\boldsymbol\\theta_1-\\boldsymbol\\theta_0\\) identifiable (opposed \\(\\boldsymbol\\theta_1\\) \\(\\boldsymbol\\theta_0\\)). Indeed, replacing \\(U\\) \\(aU\\) (\\(>0\\)) gives model. scaling issue can solved fixing variance \\(\\varepsilon_{,0}-\\varepsilon_{,1}\\).Example 4.1  (Migration income) RUM approach used Nakosteen Zimmer (1980) study migration choices. model based comparison marginal costs benefits associated migration. main ingredients approach follows:Wage can earned present location: \\(y_p^* = \\boldsymbol\\theta_p'\\mathbf{x}_p + \\varepsilon_p\\).Migration cost: \\(C^*= \\boldsymbol\\theta_c'\\mathbf{x}_c + \\varepsilon_c\\).Wage earned elsewhere: \\(y_m^* = \\boldsymbol\\theta_m'\\mathbf{x}_m + \\varepsilon_m\\).context, agents decision migrate \\(y_m^* > y_p^* + C^*\\), .e. \n\\[\ny^* = y_m^* -  y_p^* - C^* =  \\boldsymbol\\theta'\\mathbf{x} + \\underbrace{\\varepsilon}_{=\\varepsilon_m - \\varepsilon_c - \\varepsilon_p}>0,\n\\]\n\\(\\mathbf{x}\\) union \\(\\mathbf{x}_i\\)s, \\(\\\\{p,m,c\\}\\).","code":""},{"path":"microeconometrics.html","id":"Avregressors","chapter":"4 Microeconometrics","heading":"4.1.2 Alternative-Varying Regressors","text":"cases, regressors may depend considered alternative (\\(0\\) \\(1\\)). instance:modeling decision participate labour force (), wage depends alternative. Typically, zero considered agent decided work (strictly positive otherwise).context choice transportation mode, “time cost” depends considered transportation mode.terms utility, :\n\\[\nV_{,j} = {\\theta^{(u)}_{j}}'\\mathbf{u}_{,j} + {\\theta^{(v)}_{j}}'\\mathbf{v}_{},\n\\]\n\\(\\mathbf{u}_{,j}\\)’s regressors associated agent \\(\\), taking different values different choices (\\(j=0\\) \\(j=1\\)). case, Eq. (4.4) becomes:\n\\[\\begin{equation}\n\\mathbb{P}(y_i = 1|\\mathbf{x}_i;\\boldsymbol\\theta)  = F\\left({\\theta^{(u)}_{1}}'\\mathbf{u}_{,1}-{\\theta^{(u)}_{0}}'\\mathbf{u}_{,0}+[\\boldsymbol\\theta_1^{(v)}-\\boldsymbol\\theta_0^{(v)}]'\\mathbf{v}_i\\right),\\tag{4.5}\n\\end{equation}\\]\n, \\(\\theta^{(u)}_{1}=\\theta^{(u)}_{0}=\\theta^{(u)}\\) —customary— get:\n\\[\\begin{equation}\n\\mathbb{P}(y_i = 1|\\mathbf{x}_i;\\boldsymbol\\theta)  = F\\left({\\theta^{(u)}_{1}}'(\\mathbf{u}_{,1}-\\mathbf{u}_{,0})+[\\boldsymbol\\theta_1^{(v)}-\\boldsymbol\\theta_0^{(v)}]'\\mathbf{v}_i\\right).\\tag{4.6}\n\\end{equation}\\]Example 4.2  (Fishing-mode dataset) fishing-mode dataset used Cameron Trivedi (2005) (Chapters 14 15) contains alternative-specific variables. Specifically, individual, price catch rate depend fishing model. table reported , lines price catch correspond prices catch rates associated chosen alternative.","code":"\nlibrary(mlogit)\ndata(\"Fishing\",package=\"mlogit\")\nstargazer::stargazer(Fishing,type=\"text\")## \n## ==========================================================\n## Statistic       N     Mean    St. Dev.    Min      Max    \n## ----------------------------------------------------------\n## price.beach   1,182  103.422   103.641   1.290   843.186  \n## price.pier    1,182  103.422   103.641   1.290   843.186  \n## price.boat    1,182  55.257    62.713    2.290   666.110  \n## price.charter 1,182  84.379    63.545   27.290   691.110  \n## catch.beach   1,182   0.241     0.191    0.068    0.533   \n## catch.pier    1,182   0.162     0.160    0.001    0.452   \n## catch.boat    1,182   0.171     0.210   0.0002    0.737   \n## catch.charter 1,182   0.629     0.706    0.002    2.310   \n## income        1,182 4,099.337 2,461.964 416.667 12,500.000\n## ----------------------------------------------------------"},{"path":"microeconometrics.html","id":"estimation","chapter":"4 Microeconometrics","heading":"4.1.3 Estimation","text":"models can estimated Maximum Likelihood approaches (see Section 3.2).simplify exposition, consider \\(\\mathbf{x}_i\\) vectors covariates deterministic. Moreover, assume r.v. independent across entities \\(\\). write likelihood case? easily checked :\n\\[\nf(y_i|\\mathbf{x}_i;\\boldsymbol\\theta) =   g(\\boldsymbol\\theta'\\mathbf{x}_i)^{y_i}(1-g(\\boldsymbol\\theta'\\mathbf{x}_i))^{1-y_i}.\n\\]Therefore, observations \\((\\mathbf{x}_i,y_i)\\) independent across entities \\(\\), obtain:\n\\[\n\\log \\mathcal{L}(\\boldsymbol\\theta;\\mathbf{y},\\mathbf{X}) = \\sum_{=1}^{n}y_i \\log[g(\\boldsymbol\\theta'\\mathbf{x}_i)] + (1-y_i)\\log[1-g(\\boldsymbol\\theta'\\mathbf{x}_i)].\n\\]likelihood equation reads (FOC optimization program, see Def. 3.7):\n\\[\n\\dfrac{\\partial \\log \\mathcal{L}(\\boldsymbol\\theta;\\mathbf{y},\\mathbf{X})}{\\partial \\boldsymbol\\theta} = \\mathbf{0},\n\\]\n:\n\\[\n\\sum_{=1}^{n} y_i \\mathbf{x}_i\\frac{g'(\\boldsymbol\\theta'\\mathbf{x}_i)}{g(\\boldsymbol\\theta'\\mathbf{x}_i)} - (1-y_i) \\mathbf{x}_i \\frac{g'(\\boldsymbol\\theta'\\mathbf{x}_i)}{1-g(\\boldsymbol\\theta'\\mathbf{x}_i)} = \\mathbf{0}.\n\\]nonlinear (multivariate) equation can solved numerically. regularity conditions (Hypotheses 3.1), approximately (Prop. 3.4):\n\\[\n\\boldsymbol\\theta_{MLE} \\sim \\mathcal{N}(\\boldsymbol\\theta_0,\\mathbf{}(\\boldsymbol\\theta_0)^{-1}),\n\\]\n\n\\[\n\\mathbf{}(\\boldsymbol\\theta_0) = - \\mathbb{E}_0 \\left( \\frac{\\partial^2 \\log \\mathcal{L}(\\theta;\\mathbf{y},\\mathbf{X})}{\\partial \\boldsymbol\\theta \\partial \\boldsymbol\\theta'}\\right) = n \\mathcal{}_Y(\\boldsymbol\\theta_0).\n\\]finite samples, can e.g. approximate \\(\\mathbf{}(\\boldsymbol\\theta_0)^{-1}\\) Eq. (3.10):\n\\[\n\\mathbf{}(\\boldsymbol\\theta_0)^{-1} \\approx -\\left(\\frac{\\partial^2 \\log \\mathcal{L}(\\boldsymbol\\theta_{MLE};\\mathbf{y},\\mathbf{X})}{\\partial \\boldsymbol\\theta \\partial \\boldsymbol\\theta'}\\right)^{-1}.\n\\]Probit case (see Table 4.1), can shown :\n\\[\\begin{eqnarray*}\n&&\\frac{\\partial^2 \\log \\mathcal{L}(\\boldsymbol\\theta;\\mathbf{y},\\mathbf{X})}{\\partial \\boldsymbol\\theta \\partial \\boldsymbol\\theta'} = - \\sum_{=1}^{n} g'(\\boldsymbol\\theta'\\mathbf{x}_i) [\\mathbf{x}_i \\mathbf{x}_i'] \\times \\\\\n&&\\left[y_i \\frac{g'(\\boldsymbol\\theta'\\mathbf{x}_i) + \\boldsymbol\\theta'\\mathbf{x}_ig(\\boldsymbol\\theta'\\mathbf{x}_i)}{g(\\boldsymbol\\theta'\\mathbf{x}_i)^2} + (1-y_i) \\frac{g'(\\boldsymbol\\theta'\\mathbf{x}_i) - \\boldsymbol\\theta'\\mathbf{x}_i (1 - g(\\boldsymbol\\theta'\\mathbf{x}_i))}{(1-g(\\boldsymbol\\theta'\\mathbf{x}_i))^2}\\right].\n\\end{eqnarray*}\\]Logit case (see Table 4.1), can shown :\n\\[\n\\frac{\\partial^2 \\log \\mathcal{L}(\\boldsymbol\\theta;\\mathbf{y},\\mathbf{X})}{\\partial \\boldsymbol\\theta \\partial \\boldsymbol\\theta'} = - \\sum_{=1}^{n} g'(\\boldsymbol\\theta'\\mathbf{x}_i) \\mathbf{x}_i\\mathbf{x}_i',\n\\]\n\\(g'(x)=\\dfrac{\\exp(-x)}{(1 + \\exp(-x))^2}\\).Remark , since \\(g'(x)>0\\), \\(-\\partial^2 \\log \\mathcal{L}(\\boldsymbol\\theta;\\mathbf{y},\\mathbf{X})/\\partial \\boldsymbol\\theta \\partial \\boldsymbol\\theta'\\) positive definite.","code":""},{"path":"microeconometrics.html","id":"marginalFX","chapter":"4 Microeconometrics","heading":"4.1.4 Marginal effects","text":"measure marginal effects, .e. effect probability \\(y_i=1\\) marginal increase \\(x_{,k}\\)? object given :\n\\[\n\\frac{\\partial \\mathbb{P}(y_i=1|\\mathbf{x}_i;\\boldsymbol\\theta)}{\\partial x_{,k}} = \\underbrace{g'(\\boldsymbol\\theta'\\mathbf{x}_i)}_{>0}\\theta_k,\n\\]\nsign \\(\\theta_k\\) function \\(g\\) monotonously increasing.agent \\(\\), marginal effect consistently estimated \\(g'(\\boldsymbol\\theta_{MLE}'\\mathbf{x}_i)\\theta_{MLE,k}\\). important see marginal effect depends \\(\\mathbf{x}_i\\): respective increases 1 unit \\(x_{,k}\\) (entity \\(\\)) \\(x_{j,k}\\) (entity \\(j\\)) necessarily effect \\(\\mathbb{P}(y_i=1|\\mathbf{x}_i;\\boldsymbol\\theta)\\) \\(\\mathbb{P}(y_j=1|\\mathbf{x}_j;\\boldsymbol\\theta)\\). address issue, one can compute measures “average” marginal effect. two main solutions. explanatory variable \\(k\\):Denoting \\(\\hat{\\mathbf{x}}\\) sample average \\(\\mathbf{x}_i\\)s, compute \\(g'(\\boldsymbol\\theta_{MLE}'\\hat{\\mathbf{x}})\\theta_{MLE,k}\\).Compute average (across \\(\\)) \\(g'(\\boldsymbol\\theta_{MLE}'\\mathbf{x}_i)\\theta_{MLE,k}\\).","code":""},{"path":"microeconometrics.html","id":"goodness-of-fit-1","chapter":"4 Microeconometrics","heading":"4.1.5 Goodness of fit","text":"obvious version “\\(R^2\\)” binary-choice models. Existing measures called pseudo-\\(R^2\\) measures.Denoting \\(\\log \\mathcal{L}_0(\\mathbf{y})\\) (maximum) log-likelihood obtained model containing constant term (.e. \\(\\mathbf{x}_i = 1\\) \\(\\)), McFadden’s pseudo-\\(R^2\\) given :\n\\[\nR^2_{MF} = 1 - \\frac{\\log \\mathcal{L}(\\boldsymbol\\theta;\\mathbf{y})}{\\log \\mathcal{L}_0(\\mathbf{y})}.\n\\]\nIntuitively, \\(R^2_{MF}=0\\) explanatory variables convey information outcome \\(y\\). Indeed, case, model better reference model, simply captures fraction \\(y_i\\)’s equal 1.Example 4.3  (Credit defaults (Lending-club dataset)) example makes use credit data package AEC. objective model default probabilities borrowers.Let us first represent relationship fraction households defaulted loan annual income:previous figure suggests effect annual income probability default non-monotonous. therefore include quadratic term one specification (namely eq1 ).consider three specifications. first one (eq0), explanatory variables, trivial. just used compute pseudo-\\(R^2\\). second (eq1), consider covariates (loan amount, ratio amount annual income, number --30 days past-due incidences delinquency borrower’s credit file past 2 years, quadratic function annual income). third model (eq2), add credit rating.Let us compute pseudo R2 last two models:Let us now compute (average) marginal effects, using method ii Section 4.1.4:issue annual_inc variable. Indeed, previous computation realize variable appears twice among explanatory variables (log(annual_inc) (log(annual_inc)^2)). address , one can proceed follows: (1) construct new counterfactual dataset annual incomes increased 1%, (2) use model compute model-implied probabilities default new dataset (3), subtract probabilities resulting original dataset counterfactual probabilities:negative sign means , average across entities considered analysis, 1% increase annual income results decrease default probability. average effect however pretty low. get economic sense size effect, let us compute average effect associated unit increase number delinquencies:can employ likelihood ratio test (see Def. 3.8) see two variables associated annual income jointly statistically significant (context eq1):computation gives p-value 0.0436.Example 4.4  (Replicating Table 14.2 Cameron Trivedi (2005)) following lines codes replicate Table 14.2 Cameron Trivedi (2005) (see Example 4.2).","code":"\nlibrary(AEC)\ncredit$Default <- 0\ncredit$Default[credit$loan_status == \"Charged Off\"] <- 1\ncredit$Default[credit$loan_status ==\n                 \"Does not meet the credit policy. Status:Charged Off\"] <- 1\ncredit$amt2income <- credit$loan_amnt/credit$annual_inc\nplot(as.factor(credit$Default)~log(credit$annual_inc),\n     ylevels=2:1,ylab=\"Default status\",xlab=\"log(annual income)\")\neq0 <- glm(Default ~ 1,data=credit,family=binomial(link=\"probit\"))\neq1 <- glm(Default ~ log(loan_amnt) + amt2income + delinq_2yrs + \n             log(annual_inc)+ I(log(annual_inc)^2),\n           data=credit,family=binomial(link=\"probit\"))\neq2 <- glm(Default ~ grade + log(loan_amnt) + amt2income + delinq_2yrs + \n             log(annual_inc)+ I(log(annual_inc)^2),\n           data=credit,family=binomial(link=\"probit\"))\nstargazer::stargazer(eq0,eq1,eq2,type=\"text\",no.space = TRUE)## \n## ====================================================\n##                           Dependent variable:       \n##                     --------------------------------\n##                                 Default             \n##                        (1)        (2)        (3)    \n## ----------------------------------------------------\n## gradeB                                     0.400*** \n##                                            (0.055)  \n## gradeC                                     0.587*** \n##                                            (0.057)  \n## gradeD                                     0.820*** \n##                                            (0.061)  \n## gradeE                                     0.874*** \n##                                            (0.091)  \n## gradeF                                     1.230*** \n##                                            (0.147)  \n## gradeG                                     1.439*** \n##                                            (0.227)  \n## log(loan_amnt)                  -0.149**  -0.194*** \n##                                 (0.060)    (0.061)  \n## amt2income                      1.266***   1.222*** \n##                                 (0.383)    (0.393)  \n## delinq_2yrs                     0.096***    0.009   \n##                                 (0.034)    (0.035)  \n## log(annual_inc)                 -1.444**    -0.874  \n##                                 (0.569)    (0.586)  \n## I(log(annual_inc)2)             0.064**     0.038   \n##                                 (0.025)    (0.026)  \n## Constant            -1.231***   7.937***    4.749   \n##                      (0.017)    (3.060)    (3.154)  \n## ----------------------------------------------------\n## Observations          9,156      9,156      9,156   \n## Log Likelihood      -3,157.696 -3,120.625 -2,981.343\n## Akaike Inf. Crit.   6,317.392  6,253.250  5,986.686 \n## ====================================================\n## Note:                    *p<0.1; **p<0.05; ***p<0.01\nlogL0 <- logLik(eq0);logL1 <- logLik(eq1);logL2 <- logLik(eq2)\npseudoR2_eq1 <- 1 - logL1/logL0 # pseudo R2\npseudoR2_eq2 <- 1 - logL2/logL0 # pseudo R2\nc(pseudoR2_eq1,pseudoR2_eq2)## [1] 0.01173993 0.05584870\nmean(dnorm(predict(eq2)),na.rm=TRUE)*eq2$coefficients##          (Intercept)               gradeB               gradeC \n##          0.840731198          0.070747353          0.103944305 \n##               gradeD               gradeE               gradeF \n##          0.145089219          0.154773742          0.217702041 \n##               gradeG       log(loan_amnt)           amt2income \n##          0.254722161         -0.034289921          0.216251992 \n##          delinq_2yrs      log(annual_inc) I(log(annual_inc)^2) \n##          0.001574178         -0.154701321          0.006813694\nnew_credit <- credit\nnew_credit$annual_inc <- 1.01 * new_credit$annual_inc\nbas_predict_eq2  <- predict(eq2, newdata = credit, type = \"response\")\n# This is equivalent to pnorm(predict(eq2, newdata = credit))\nnew_predict_eq2  <- predict(eq2, newdata = new_credit, type = \"response\")\nmean(new_predict_eq2 - bas_predict_eq2)## [1] -6.562126e-05\nnew_credit <- credit\nnew_credit$delinq_2yrs <- credit$delinq_2yrs + 1\nnew_predict_eq2  <- predict(eq2, newdata = new_credit, type = \"response\")\nmean(new_predict_eq2 - bas_predict_eq2)## [1] 0.001582332\neq1restr <- glm(Default ~ log(loan_amnt) + amt2income + delinq_2yrs,\n                data=credit,family=binomial(link=\"probit\"))\nLRstat <- 2*(logL1 - logLik(eq1restr))\npvalue <- 1 - c(pchisq(LRstat,df=2))\ndata.reduced <- subset(Fishing,mode %in% c(\"charter\",\"pier\"))\ndata.reduced$lnrelp <- log(data.reduced$price.charter/data.reduced$price.pier)\ndata.reduced$y <- 1*(data.reduced$mode==\"charter\")\n# check first line of Table 14.1:\nprice.charter.y0 <- mean(data.reduced$pcharter[data.reduced$y==0])\nprice.charter.y1 <- mean(data.reduced$pcharter[data.reduced$y==1])\nprice.charter    <- mean(data.reduced$pcharter)\n# Run probit regression:\nreg.probit <- glm(y ~ lnrelp,\n                  data=data.reduced,\n                  family=binomial(link=\"probit\"))\n# Run Logit regression:\nreg.logit <- glm(y ~ lnrelp,\n                 data=data.reduced,\n                 family=binomial(link=\"logit\"))\n# Run OLS regression:\nreg.OLS <- lm(y ~ lnrelp,\n              data=data.reduced)\n# Replicates Table 14.2 of Cameron and Trivedi:\nstargazer::stargazer(reg.logit, reg.probit, reg.OLS,no.space = TRUE,\n                     type=\"text\")## \n## ================================================================\n##                                 Dependent variable:             \n##                     --------------------------------------------\n##                                          y                      \n##                     logistic   probit             OLS           \n##                        (1)       (2)              (3)           \n## ----------------------------------------------------------------\n## lnrelp              -1.823*** -1.056***        -0.243***        \n##                      (0.145)   (0.075)          (0.010)         \n## Constant            2.053***  1.194***          0.784***        \n##                      (0.169)   (0.088)          (0.013)         \n## ----------------------------------------------------------------\n## Observations           630       630              630           \n## R2                                               0.463          \n## Adjusted R2                                      0.462          \n## Log Likelihood      -206.827  -204.411                          \n## Akaike Inf. Crit.    417.654   412.822                          \n## Residual Std. Error                         0.330 (df = 628)    \n## F Statistic                             542.123*** (df = 1; 628)\n## ================================================================\n## Note:                                *p<0.1; **p<0.05; ***p<0.01"},{"path":"microeconometrics.html","id":"predictions-and-roc-curves","chapter":"4 Microeconometrics","heading":"4.1.6 Predictions and ROC curves","text":"compute model-implied predicted outcomes? case \\(y_i\\), predicted outcomes \\(\\hat{y}_i\\) need valued \\(\\{0,1\\}\\). natural choice consists considering \\(\\hat{y}_i=1\\) \\(\\mathbb{P}(y_i=1|\\mathbf{x}_i;\\boldsymbol\\theta) > 0.5\\), .e., taking cutoff \\(c=0.5\\). exist, though, situations relevant. instance, may models predicted probabilities small, less others. context, model-implied probability 10% (say) characterize “high-risk” entity. However, using cutoff 50% identify level riskiness.receiver operating characteristics (ROC) curve consitutes general approach. idea remain agnostic consider possible values cutoff \\(c\\). works follows. potential cutoff \\(c \\[0,1]\\), compute (plot):fraction \\(y = 1\\) values correctly classified (True Positive Rate) againstThe fraction \\(y = 0\\) values incorrectly specified (False Positive Rate).curve mechanically starts (0,0) —corresponds \\(c=1\\)— terminates (1,1) –situation \\(c=0\\).case predictive ability (worst situation), ROC curve straight line (0,0) (1,1).Example 4.5  (ROC fishing-mode dataset) Figure 4.5 shows ROC curve associated probit model estimated Example 4.4.\nFigure 4.5: Application ROC methodology fishing-mode dataset.\n","code":"\nlibrary(pROC)\npredict_model <- predict.glm(reg.probit,type = \"response\")\nroc(data.reduced$y, predict_model, percent=T,\n    boot.n=1000, ci.alpha=0.9, stratified=T, plot=TRUE, grid=TRUE,\n    show.thres=TRUE, legacy.axes = TRUE, reuse.auc = TRUE,\n    print.auc = TRUE, print.thres.col = \"blue\", ci=TRUE,\n    ci.type=\"bars\", print.thres.cex = 0.7, col = 'red',\n    main = paste(\"ROC curve using\",\"(N = \",nrow(data.reduced),\")\") )"},{"path":"microeconometrics.html","id":"multiple-choice-models","chapter":"4 Microeconometrics","heading":"4.2 Multiple Choice Models","text":"now consider cases number possible outcomes (alternatives) larger two. Let us denote \\(J\\) number. \\(y_j \\\\{1,\\dots,J\\}\\). situation arise instance outcome variable reflects:Opinions: strongly opposed / opposed / neutral / support (ranked choices),Occupational field: lawyer / farmer / engineer / doctor / …,Alternative shopping areas,Transportation types.cases, values associated choices meaningful, example, number accidents per day: \\(y = 0, 1,2, \\dots\\) (count data). cases, values meaningless.assume existence covariates, gathered vector \\(\\mathbf{x}_i\\) (\\(K \\times 1\\)), suspected influence probabilities obtaining different outcomes (\\(y_i=j\\), \\(j \\\\{1,\\dots,J\\}\\)).follows, assume \\(y_i\\)’s assumed independently distributed, :\n\\[\\begin{equation}\ny_i = \\left\\{\n\\begin{array}{cl}\n1 & \\mbox{ probability } g_1(\\mathbf{x}_i;\\boldsymbol\\theta)\\\\\n\\vdots \\\\\nJ & \\mbox{ probability } g_J(\\mathbf{x}_i;\\boldsymbol\\theta).\n\\end{array}\n\\right.\\tag{4.7}\n\\end{equation}\\](course, entities (\\(\\)), must \\(\\sum_{j=1}^J g_j(\\mathbf{x}_i;\\boldsymbol\\theta)=1\\).) objective estimate vector population parameters \\(\\boldsymbol\\theta\\) given functional forms \\(g_j\\)’s.","code":""},{"path":"microeconometrics.html","id":"ordered-case","chapter":"4 Microeconometrics","heading":"4.2.1 Ordered case","text":"Sometimes, exists natural order different alternatives. typically case respondents choose level agreement statement, e.g.: (1) Strongly disagree; (2) Disagree; (3) Neither agree disagree; (4) Agree; (5) Strongly agree. Another standard case ratings (F, say).ordered probit model consists extending binary case, considering latent-variable view latter (see Section 4.1.1). Formally, model follows:\n\\[\\begin{equation}\n\\mathbb{P}(y_i = j | \\mathbf{x}_i) = \\mathbb{P}(\\alpha_{j-1} <y^*_i < \\alpha_{j} |\\mathbf{x}_i), \\tag{4.8}\n\\end{equation}\\]\n\n\\[\ny_{}^* = \\boldsymbol\\theta'\\mathbf{x}_i + \\varepsilon_i,\n\\]\n\\(\\varepsilon_i \\sim \\,..d.\\,\\mathcal{N}(0,1)\\). \\(\\alpha_j\\)’s, \\(j \\\\{1,\\dots,J-1\\}\\), (new) parameters estimated, top \\(\\boldsymbol\\theta\\). Naturally, \\(\\alpha_1<\\alpha_2<\\dots<\\alpha_{J-1}\\). Moreover \\(\\alpha_0\\) \\(- \\infty\\) \\(\\alpha_J\\) \\(+ \\infty\\), Eq. (4.8) valid \\(j \\\\{1,\\dots,J\\}\\) (including \\(1\\) \\(J\\)).:\n\\[\\begin{eqnarray*}\ng_j(\\mathbf{x}_i;\\boldsymbol\\theta,\\boldsymbol\\alpha) = \\mathbb{P}(y_i = j | \\mathbf{x}_i) &=& \\mathbb{P}(\\alpha_{j-1} <y^*_i < \\alpha_{j} |\\mathbf{x}_i) \\\\\n&=& \\mathbb{P}(\\alpha_{j-1} - \\boldsymbol\\theta'\\mathbf{x}_i  <\\varepsilon_i < \\alpha_{j} - \\boldsymbol\\theta'\\mathbf{x}_i) \\\\\n&=& \\Phi(\\alpha_{j} - \\boldsymbol\\theta'\\mathbf{x}_i) - \\Phi(\\alpha_{j-1} - \\boldsymbol\\theta'\\mathbf{x}_i),\n\\end{eqnarray*}\\]\n\\(\\Phi\\) c.d.f. \\(\\mathcal{N}(0,1)\\)., \\(\\), one components \\(\\mathbf{x}_i\\) equal 1 (done linear regression introduce intercept specification), one \\(\\alpha_j\\) (\\(j\\\\{1,\\dots,J-1\\}\\)) identified. One can arbitrarily set \\(\\alpha_1=0\\). done binary logit/probit cases.model can estimated maximizing likelihood function (see Section 3.2). function given :\n\\[\\begin{equation}\n\\log \\mathcal{L}(\\boldsymbol\\theta,\\boldsymbol\\alpha;\\mathbf{y},\\mathbf{X}) = \\sum_{=1}^n  \\sum_{j=1}^J \\mathbb{}_{\\{y_i=j\\}} \\log \\left(g_j(\\mathbf{x}_i;\\boldsymbol\\theta,\\boldsymbol\\alpha)\\right). \\tag{4.9}\n\\end{equation}\\]Let us stress two types parameters estimate: included vector \\(\\boldsymbol\\theta\\), \\(\\alpha_j\\)’s, gathered vector \\(\\boldsymbol\\alpha\\).estimated values \\(\\theta_j\\)’s slightly complicated interpret (least term sign) binary case. Indeed, :\n\\[\n\\mathbb{P}(y_i \\le j | \\mathbf{x}_i) = \\Phi(\\alpha_{j} - \\boldsymbol\\theta'\\mathbf{x}_i) \\Rightarrow \\frac{\\partial \\mathbb{P}(y_i \\le j | \\mathbf{x}_i)}{\\mathbf{x}_i} =- \\underbrace{\\Phi'(\\alpha_{j} - \\boldsymbol\\theta'\\mathbf{x}_i)}_{>0}\\boldsymbol\\theta.\n\\]\nHence sign \\(\\theta_k\\) indicates whether \\(\\mathbb{P}(y_i \\le j | \\mathbf{x}_i)\\) increases decreases w.r.t. \\(x_{,k}\\) (\\(k^{th}\\) component \\(\\mathbf{x}_i\\)). contrast:\n\\[\n\\frac{\\partial \\mathbb{P}(y_i = j | \\mathbf{x}_i)}{\\mathbf{x}_i} = \\underbrace{\\left(-F'(\\alpha_{j} + \\boldsymbol\\theta'\\mathbf{x}_i)+F'(\\alpha_{j-1} + \\boldsymbol\\theta'\\mathbf{x}_i)\\right)}_{}\\boldsymbol\\theta.\n\\]\nTherefore signs components \\(\\boldsymbol\\theta\\) necessarily marginal effects. (sign \\(\\) priori unknown.)Example 4.6  (Predicting credit ratings (Lending-club dataset)) Let us use credit dataset (see Example 4.3), let use try model ratings attributed lending-club:Predicted ratings (probabilties given given rating) can computed follows:","code":"\nlibrary(AEC)\nlibrary(MASS)\ncredit$emp_length_low5y   <- credit$emp_length %in%\n  c(\"< 1 year\",\"1 year\",\"2 years\",\"3 years\",\"4 years\")\ncredit$emp_length_high10y <- credit$emp_length==\"10+ years\"\ncredit$annual_inc <- credit$annual_inc/1000\ncredit$loan_amnt  <- credit$loan_amnt/1000\ncredit$income2loan <- credit$annual_inc/credit$loan_amnt\ntraining <- credit[1:20000,] # sample is reduced\ntraining <- subset(training,grade!=c(\"E\",\"F\",\"G\"))\ntraining <- droplevels(training)\ntraining$grade.ordered <- factor(training$grade,ordered=TRUE,\n                                 levels = c(\"D\",\"C\",\"B\",\"A\"))\nmodel1 <- polr(grade.ordered ~ log(loan_amnt) + log(income2loan) + delinq_2yrs,\n               data=training, Hess=TRUE, method=\"probit\")\nmodel2 <- polr(grade.ordered ~ log(loan_amnt) + log(income2loan) + delinq_2yrs +\n                 emp_length_low5y + emp_length_high10y,\n               data=training, Hess=TRUE, method=\"probit\")\nstargazer::stargazer(model1,model2,ord.intercepts = TRUE,type=\"text\",\n                     no.space = TRUE)## \n## ===============================================\n##                        Dependent variable:     \n##                    ----------------------------\n##                           grade.ordered        \n##                         (1)            (2)     \n## -----------------------------------------------\n## log(loan_amnt)         -0.014        -0.040*   \n##                       (0.022)        (0.022)   \n## log(income2loan)      0.115***      0.092***   \n##                       (0.022)        (0.022)   \n## delinq_2yrs          -0.399***      -0.404***  \n##                       (0.025)        (0.025)   \n## emp_length_low5y                    -0.096***  \n##                                      (0.027)   \n## emp_length_high10y                   0.088**   \n##                                      (0.035)   \n## D| C                 -0.937***      -1.073***  \n##                       (0.082)        (0.086)   \n## C| B                  -0.160**      -0.295***  \n##                       (0.082)        (0.085)   \n## B| A                  0.696***      0.564***   \n##                       (0.082)        (0.086)   \n## -----------------------------------------------\n## Observations           8,695          8,695    \n## ===============================================\n## Note:               *p<0.1; **p<0.05; ***p<0.01\npred.grade <- predict(model1,newdata = training)\n# pred.grade = predicted grade, defined as the most likely according model\npred.proba <- predict(model1,newdata = training, type=\"probs\")"},{"path":"microeconometrics.html","id":"MNL","chapter":"4 Microeconometrics","heading":"4.2.2 General multinomial logit model","text":"section introduces general multinomial logit model, natural extension binary logit model (see Table 4.1). general formulation follows:\n\\[\\begin{equation}\ng_j(\\mathbf{x}_i;\\boldsymbol\\theta) = \\frac{\\exp(\\theta_j'\\mathbf{x}_i)}{\\sum_{k=1}^J \\exp(\\theta_k'\\mathbf{x}_i)}.\\tag{4.10}\n\\end{equation}\\]Note , construction, \\(g_j(\\mathbf{x}_i;\\boldsymbol\\theta) \\[0,1]\\) \\(\\sum_{j}g_j(\\mathbf{x}_i;\\boldsymbol\\theta)=1\\).components \\(\\mathbf{x}_i\\) (regressors, covariates) may alternative-specific alternative invariant (see also Section 4.1.2). may, e.g., organize \\(\\mathbf{x}_i\\) follows:\n\\[\\begin{equation}\n\\mathbf{x}_i = [\\mathbf{u}_{,1}',\\dots,\\mathbf{u}_{,J}',\\mathbf{v}_{}']',\\tag{4.11}\n\\end{equation}\\]\nnotations Section 4.1.2, :\\(\\mathbf{u}_{,j}\\) (\\(j \\\\{1,\\dots,J\\}\\)): vector variables associated agent \\(\\) alternative \\(j\\) (alternative-specific regressors). Examples: Travel time per type transportation (transportation choice), wage per type work, cost per type car.\\(\\mathbf{v}_{}\\): vector variables associated agent \\(\\) alternative-invariant. Examples: age gender agent \\(\\),\\(\\mathbf{x}_i\\) Eq. (4.11), obvious notations, \\(\\theta_j\\) form:\n\\[\\begin{equation}\n\\theta_j = [{\\theta^{(u)}_{1,j}}',\\dots,{\\theta^{(u)}_{J,j}}',{\\theta_j^{(v)}}']',\\tag{4.12}\n\\end{equation}\\]\n\\(\\boldsymbol\\theta=[\\theta_1',\\dots,\\theta_J']'\\).literature considered different specific cases general multinomial logit model:13Conditional logit (CL) alternative-varying regressors:\n\\[\\begin{equation}\n\\theta_j = [\\mathbf{0}',\\dots,\\mathbf{0}',\\underbrace{\\boldsymbol\\beta'}_{\\mbox{j$^{th}$ position}},\\mathbf{0}',\\dots]',\\tag{4.13}\n\\end{equation}\\]\n.e., \\(\\boldsymbol\\beta=\\theta^{(u)}_{1,1}=\\dots=\\theta^{(u)}_{J,J}\\) \\(\\theta^{(u)}_{,j}=\\mathbf{0}\\) \\(\\ne j\\).Multinomial logit (MNL) alternative-invariant regressors:\n\\[\\begin{equation}\n\\theta_j = \\left[\\mathbf{0}',\\dots,\\mathbf{0}',{\\theta_j^{(v)}}'\\right]'.\\tag{4.14}\n\\end{equation}\\]Mixed logit:\n\\[\\begin{equation}\n\\theta_j = \\left[\\mathbf{0}',\\dots,\\mathbf{0}',\\boldsymbol\\beta',\\mathbf{0}',\\dots,\\mathbf{0}',{\\theta_j^{(v)}}'\\right]'.\\tag{4.13}\n\\end{equation}\\]Example 4.7  (CL MNL fishing-mode dataset) following lines replicate Table 15.2 Cameron Trivedi (2005) (see also Examples 4.2 4.4):ML estimationGeneral multinomial logit models can estimated Maximum Likelihood techniques (see Section 3.2). Consider general model described Eq. (4.7). can noted :\n\\[\nf(y_i|\\mathbf{x}_i;\\boldsymbol\\theta) = \\prod_{j=1}^J g_j(\\mathbf{x}_i;\\boldsymbol\\theta)^{\\mathbb{}_{\\{y_i=j\\}}},\n\\]\nleads \n\\[\n\\log f(y_i|\\mathbf{x}_i;\\boldsymbol\\theta) = \\sum_{j=1}^J \\mathbb{}_{\\{y_i=j\\}} \\log \\left(g_j(\\mathbf{x}_i;\\boldsymbol\\theta)\\right).\n\\]\nlog-likelihood function therefore given :\n\\[\\begin{equation}\n\\log \\mathcal{L}(\\boldsymbol\\theta;\\mathbf{y},\\mathbf{X}) = \\sum_{=1}^n  \\sum_{j=1}^J \\mathbb{}_{\\{y_i=j\\}} \\log \\left(g_j(\\mathbf{x}_i;\\boldsymbol\\theta)\\right).\\tag{4.9}\n\\end{equation}\\]\nNumerical methods employed order find maximum-likelihood estimate \\(\\boldsymbol\\theta\\). (Standard packages contain fast algorithms.)Marginal EffectsLet us consider computation marginal effects general multinomial logit model (Eq. (4.10)). Using notation \\(p_{,j} \\equiv \\mathbb{P}(y_i=j|\\mathbf{x}_i;\\boldsymbol\\theta)\\), :\n\\[\\begin{eqnarray*}\n\\frac{\\partial p_{,j}}{\\partial x_{,s}} &=& \\frac{\\theta_{j,s}\\exp(\\theta_j'\\mathbf{x}_i)\\sum_{k=1}^J \\exp(\\theta_k'\\mathbf{x}_i)}{(\\sum_{k=1}^J \\exp(\\theta_k'\\mathbf{x}_i))^2} \\\\\n&& - \\frac{\\exp(\\theta_j'\\mathbf{x}_i)\\sum_{k=1}^J \\theta_{k,s} \\exp(\\theta_k'\\mathbf{x}_i)}{(\\sum_{k=1}^J \\exp(\\theta_k'\\mathbf{x}_i))^2}\\\\\n&=& \\theta_{j,s} p_{,j} - \\sum_{k=1}^J \\theta_{k,s} p_{,j}p_{,k}\\\\\n&=&  p_{,j} \\times \\Big(\\theta_{j,s} - \\underbrace{\\sum_{k=1}^J \\theta_{k,s} p_{,k}}_{=\\overline{\\boldsymbol{\\theta}}^{()}_{s}}\\Big),\n\\end{eqnarray*}\\]\n\\(\\overline{\\boldsymbol\\theta}^{()}_{s}\\) depend \\(j\\). Note sign marginal effect necessarily \\(\\theta_{j,s}\\).Random Utility modelsThe general multinomial logit model may arise natural specification arising structural contexts agents compare (random) utilities associated \\(J\\) potential outcomes (see Section 4.1.1 binary situation).Let’s drop \\(\\) subscript simplicity assume utility derived form choosing \\(j\\) given \\(U_j = V_j + \\varepsilon_j\\), \\(V_j\\) deterministic (may depend observed covariates) \\(\\varepsilon_j\\) stochastic. (obvious notations):\n\\[\\begin{eqnarray*}\n\\mathbb{P}(y=j) &=& \\mathbb{P}(U_j>U_k,\\,\\forall k \\ne j)\\\\\n\\mathbb{P}(y=j) &=& \\mathbb{P}(U_k-U_j<0,\\,\\forall k \\ne j)\\\\\n\\mathbb{P}(y=j) &=& \\mathbb{P}(\\underbrace{\\varepsilon_k-\\varepsilon_j}_{=:\\tilde\\varepsilon_{k,j}}<\\underbrace{V_j - V_k}_{=:-\\tilde{V}_{k,j}},\\,\\forall k \\ne j).\n\\end{eqnarray*}\\]last expression \\((J-1)\\)-variate integral. , general, analytical solution, Prop. 4.1 shows case employing Gumbel distributions (see Def. 4.1).Definition 4.1  (Gumbel distribution) c.d.f. Gumbel distribution (\\(\\mathcal{W}\\)) :\n\\[\nF(u) = \\exp(-\\exp(-u)), \\qquad f(u)=\\exp(-u-\\exp(u)).\n\\]Remark: \\(X\\sim\\mathcal{W}\\), \\(\\mathbb{E}(X)=0.577\\) (Euler constant)14 \\(\\mathbb{V}ar(X)=\\pi^2/6\\).\nFigure 4.6: C.d.f. Gumbel distribution (\\(F(x)=\\exp(-\\exp(-x))\\)).\nProposition 4.1  (Weibull) context utility model described , \\(\\varepsilon_j \\sim \\,..d.\\,\\mathcal{W}\\), \n\\[\n\\mathbb{P}(y=j) = \\frac{\\exp(V_j)}{\\sum_{k=1}^J \\exp(V_k)}.\n\\]Proof. :\n\\[\\begin{eqnarray*}\n\\mathbb{P}(y=j) &=& \\mathbb{P}(\\forall\\,k \\ne j,\\,U_k < U_j) =  \\mathbb{P}(\\forall\\,k \\ne j,\\,\\varepsilon_k < V_j - V_k + \\varepsilon_j) \\\\\n&=& \\int \\prod_{k \\ne j} F(V_j - V_k + \\varepsilon) f(\\varepsilon)d\\varepsilon.\n\\end{eqnarray*}\\]\ncomputation, comes \n\\[\n\\prod_{k \\ne j} F(V_j - V_k + \\varepsilon) f(\\varepsilon) = \\exp\\left[-\\varepsilon-\\exp(-\\varepsilon+\\lambda_j)\\right],\n\\]\n\\(\\lambda_j = \\log\\left(1 + \\frac{\\sum_{k \\ne j} \\exp(V_k)}{\\exp(V_j)}\\right)\\). :\n\\[\\begin{eqnarray*}\n\\mathbb{P}(y=j) &=& \\int  \\exp\\left[-\\varepsilon-\\exp(-\\varepsilon+\\lambda_j)\\right] d\\varepsilon\\\\\n&=& \\int  \\exp\\left[-t - \\lambda_j-\\exp(-t)\\right] d\\varepsilon = \\exp(- \\lambda_j),\n\\end{eqnarray*}\\]\nleads result.remarks identification (see Def. 3.5) order.:\n\\[\\begin{eqnarray*}\n\\mathbb{P}(y=j) &=& \\frac{\\exp(V_j)}{\\sum_{k=1}^J \\exp(V_k)}= \\frac{\\exp(V^*_j)}{1 + \\sum_{k=2}^J \\exp(V^*_k)},\n\\end{eqnarray*}\\]\n\\(V^*_j = V_j - V_1\\). can therefore always assume \\(V_{1}=0\\). case \\(V_{,j} = \\theta_j'\\mathbf{x}_i = \\boldsymbol\\beta'\\mathbf{u}_{,j}+{\\theta_j^{(v)}}'\\mathbf{v}_i\\) (see Eqs. (4.11) (4.13)), can instance assume :\n\\[\\begin{eqnarray*}\n&()& \\mathbf{u}_{,1}=0,\\\\\n&(B)& \\theta_1^{(v)} = 0.\n\\end{eqnarray*}\\]\n() hold, can replace \\(\\mathbf{u}_{,j}\\) \\(\\mathbf{u}_{,j}-\\mathbf{u}_{,1}\\).\\(J=2\\) \\(j \\\\{0,1\\}\\) (shift one unit), \\(\\mathbb{P}(y=1|\\mathbf{x})=\\dfrac{\\exp(\\boldsymbol\\theta'\\mathbf{x})}{1+\\exp(\\boldsymbol\\theta'\\mathbf{x})}\\), logit model (Table 4.1).Limitations logit modelsIn Logit model, :\n\\[\\begin{equation}\n\\mathbb{P}(y=j|y \\\\{k,j\\}) = \\frac{\\exp(\\theta_j'\\mathbf{x})}{\\exp(\\theta_j'\\mathbf{x}) + \\exp(\\theta_k'\\mathbf{x})}.\\tag{4.15}\n\\end{equation}\\]\nconditional probability depend alternatives (.e., depend \\(\\theta_m\\), \\(m \\ne j,k\\)). particular, \\(\\mathbf{x} = [\\mathbf{u}_1',\\dots,\\mathbf{u}_J',\\mathbf{v}']'\\), changes \\(\\mathbf{u}_m\\) (\\(m \\ne j,\\,k\\)) impact object shown Eq. (4.15)., Multinomial Logit can seen series pairwise comparisons unaffected characteristics alternatives. model said satisfy independence irrelevant alternatives (IIA) property. , models, individual, ratio probabilities choosing two alternatives independent availability attributes alternatives. may sound alarming, situations like case, instance case want extrapolate results estimated model situation novel outcome highly susbstitutable one previous ones. can illustrated famous “red-blue bus” example:Example 4.8  (Red-blue bus IIA) Assume one logit model capturing decision travel using either car (\\(y=1\\)) (red) bus (\\(y=2\\)). Assume want augment model allow third choice (\\(y=3\\)): travel blue bus. blue bus (\\(y=3\\)) exactly red bus, except color, one expect :\n\\[\n\\mathbb{P}(y=3|y \\\\{2,3\\}) = 0.5,\n\\]\n.e. \\(\\theta_2 = \\theta_3\\).Assume \\(V_1=V_2\\). expect \\(V_2=V_3\\) (hence \\(p_2=p_3\\)). multinomial logit model imply \\(p_1=p_2=p_3=0.33\\). however seem reasonable \\(p_1 = p_2 + p_3 = 0.5\\) \\(p_2=p_3=0.25\\).","code":"\n# Specify data organization:\nlibrary(mlogit)\nlibrary(stargazer)\ndata(\"Fishing\",package=\"mlogit\")\nFish <- mlogit.data(Fishing,\n                    varying = c(2:9),\n                    choice = \"mode\",\n                    shape = \"wide\")\nMNL1 <- mlogit(mode ~ price + catch, data = Fish)\nMNL2 <- mlogit(mode ~ price + catch - 1, data = Fish)\nMNL3 <- mlogit(mode ~ 0 | income, data = Fish)\nMNL4 <- mlogit(mode ~ price + catch | income, data = Fish)\nstargazer(MNL1,MNL2,MNL3,MNL4,type=\"text\",no.space = TRUE,\n          omit.stat = c(\"lr\"))## \n## ===============================================================\n##                                 Dependent variable:            \n##                     -------------------------------------------\n##                                        mode                    \n##                        (1)        (2)        (3)        (4)    \n## ---------------------------------------------------------------\n## (Intercept):boat     0.871***              0.739***   0.527**  \n##                      (0.114)               (0.197)    (0.223)  \n## (Intercept):charter  1.499***              1.341***   1.694*** \n##                      (0.133)               (0.195)    (0.224)  \n## (Intercept):pier     0.307***              0.814***   0.778*** \n##                      (0.115)               (0.229)    (0.220)  \n## price               -0.025***  -0.020***             -0.025*** \n##                      (0.002)    (0.001)               (0.002)  \n## catch                0.377***   0.953***              0.358*** \n##                      (0.110)    (0.089)               (0.110)  \n## income:boat                                0.0001**   0.0001*  \n##                                           (0.00004)   (0.0001) \n## income:charter                             -0.00003   -0.00003 \n##                                           (0.00004)   (0.0001) \n## income:pier                               -0.0001*** -0.0001** \n##                                            (0.0001)   (0.0001) \n## ---------------------------------------------------------------\n## Observations          1,182      1,182      1,182      1,182   \n## R2                    0.178      0.014      0.189              \n## Log Likelihood      -1,230.784 -1,311.980 -1,477.151 -1,215.138\n## ===============================================================\n## Note:                               *p<0.1; **p<0.05; ***p<0.01"},{"path":"microeconometrics.html","id":"nested-logits","chapter":"4 Microeconometrics","heading":"4.2.3 Nested logits","text":"Nested Logits natural extensions logit models choices feature nesting structure. approach relevant makes sense group choices nest, also called limbs. Intuitively, framework consistent idea according , agent, exist unobserved nest-specific variables.setup follows: consider \\(J\\) limbs. limb \\(j\\), \\(K_j\\) branches. Let us denotes \\(y_1\\) limb choice (.e., \\(y_1 \\\\{1,\\dots,J\\}\\)) \\(y_2\\) branch choice (\\(y_2 \\\\{1,\\dots,K_j\\}\\)). utility associated pair choices \\((j,k)\\) given \n\\[\nU_{j,k} = V_{j,k} + \\varepsilon_{j,k}.\n\\]\n:\n\\[\n\\mathbb{P}[(y_1,y_2) = (j,k)|\\mathbf{x}] = \\mathbb{P}(U_{j,k}>U_{l,m},\\,(l,m) \\ne (j,k)|\\mathbf{x}).\n\\]One usually make following two assumptions:deterministic part utility given \\(V_{j,k} = \\mathbf{u}_j'\\boldsymbol\\alpha + \\mathbf{v}_{j,k}'\\boldsymbol\\beta_j\\), \\(\\boldsymbol\\alpha\\) common nests \\(\\boldsymbol\\beta_j\\)’s nest-specific.deterministic part utility given \\(V_{j,k} = \\mathbf{u}_j'\\boldsymbol\\alpha + \\mathbf{v}_{j,k}'\\boldsymbol\\beta_j\\), \\(\\boldsymbol\\alpha\\) common nests \\(\\boldsymbol\\beta_j\\)’s nest-specific.disturbances \\(\\boldsymbol\\varepsilon\\) follow Generalized Extreme Value (GEV) distribution (see Def. 5.15).disturbances \\(\\boldsymbol\\varepsilon\\) follow Generalized Extreme Value (GEV) distribution (see Def. 5.15).following figure displays simulations pairs \\((\\varepsilon_1,\\varepsilon_2)\\) drawn GEV distributions different values \\(\\rho\\). simulation approach based Bhat. code used produce chart provided Appendix 5.6.1.\nFigure 4.7: GEV simulations.\n() (ii), :\n\\[\\begin{eqnarray}\n\\mathbb{P}[(y_1,y_2) = (j,k)|\\mathbf{x}] &=& \\underbrace{\\frac{\\exp(\\mathbf{u}_j'\\boldsymbol\\alpha + \\rho_j I_j)}{\\sum_{m=1}^J \\exp(\\mathbf{u}_m'\\boldsymbol\\alpha + \\rho_m I_m)}}_{= \\mathbb{P}[y_1 = j|\\mathbf{x}]} \\times \\nonumber\\\\\n&& \\underbrace{\\frac{\\exp(\\mathbf{v}_{j,k}'\\boldsymbol\\beta_j/\\rho_j)}{\\sum_{l=1}^{K_j} \\exp(\\mathbf{v}_{j,l}'\\boldsymbol\\beta_j/\\rho_j)}}_{= \\mathbb{P}[y_2 = k|y_1=j,\\mathbf{x}]}, \\tag{4.16}\n\\end{eqnarray}\\]\n\\(I_j\\)’s called inclusive values (log sums), given :\n\\[\nI_j = \\log \\left( \\sum_{l=1}^{K_j} \\exp(\\mathbf{v}_{j,l}'\\boldsymbol\\beta_j/\\rho_j)\\right).\n\\]remarks order:can shown \\(\\rho_j = \\sqrt{1 - \\mathbb{C}(\\varepsilon_{j,k},\\varepsilon_{j,l})}\\), \\(k \\ne l\\).\\(\\rho_j=1\\) implies \\(\\varepsilon_{j,k}\\) \\(\\varepsilon_{j,l}\\) uncorrelated (back multinomial logit case).\\(J=1\\):\n\\[\nF([\\varepsilon_1,\\dots,\\varepsilon_K]',\\rho) = \\exp\\left(-\\left(\\sum_{k=1}^{K} \\exp(-\\varepsilon_k/\\rho)\\right)^{\\rho}\\right).\n\\]:\n\\[\\begin{eqnarray*}\nI_j = \\mathbb{E}(\\max_k(U_{j,k})) &=& \\mathbb{E}(\\max_k(V_{j,k} + \\varepsilon_{j,k})),\n\\end{eqnarray*}\\]\ninclusive values can therefore seen measures relative attractiveness nest.approach allows level correlation across \\(\\varepsilon_{j,k}\\) (given \\(j\\)). can interpreted existence (unobserved) common error component alternatives nest. component contributes making alternatives given nest similar. words, approach can accommodate higher sensitivity (cross-elasticity) alternatives given nest.Note common component reduced zero (.e. \\(\\rho_i=1\\)), model boils multinomial logit model covariance error terms among alternatives.Contrary general multinmial model, nested logits can solve Red-Blue problem described Section 4.2.2 (see Example 4.8). Assume estimated model specifying \\(U_{1} = V_{1} + \\varepsilon_{1}\\) (car choice) \\(U_{2} = V_{2} + \\varepsilon_{2}\\) (red bus choice). can assume blue-bus utility form \\(U_{3} = V_{2} + \\varepsilon_{3}\\) \\(\\varepsilon_{3}\\) perfectly correlated \\(\\varepsilon_{2}\\). done redefining set choices follows:\n\\[\\begin{eqnarray*}\nj=1 &\\Leftrightarrow& (j'=1,k=1) \\\\\nj=2 &\\Leftrightarrow& (j'=2,k=1) \\\\\nj=3 &\\Leftrightarrow& (j'=2,k=2),\n\\end{eqnarray*}\\]\nsetting \\(\\rho_2 \\rightarrow 0\\).IIA holds within nest, considering alternatives different nests. Indeed, using Eq. (4.16):\n\\[\n\\frac{\\mathbb{P}[y_1=j,y_2=k_A|\\mathbf{x}] }{\\mathbb{P}[y_1=j,y_2=k_B|\\mathbf{x}]} = \\frac{\\exp(\\mathbf{v}_{j,k_A}'\\boldsymbol\\beta_j/\\rho_j)}{\\exp(\\mathbf{v}_{j,k_B}'\\boldsymbol\\beta_j/\\rho_j)},\n\\]\n.e. IIA nest \\(j\\).contrast:\n\\[\\begin{eqnarray*}\n\\frac{\\mathbb{P}[y_1=j_A,y_2=k_A|\\mathbf{x}] }{\\mathbb{P}[y_1=j_B,y_2=k_B|\\mathbf{x}]} &=& \\frac{\\exp(\\mathbf{u}_{j_A}'\\boldsymbol\\alpha + \\rho_{j_A} I_{j_A})\\exp(\\mathbf{v}_{{j_A},{k_A}}'\\boldsymbol\\beta_{j_A}/\\rho_{j_A})}{\\exp(\\mathbf{u}_{j_B}'\\boldsymbol\\alpha + \\rho_{j_B} I_{j_B})\\exp(\\mathbf{v}_{{j_B},{k_B}}'\\boldsymbol\\beta_{j_B}/\\rho_{j_B})}\\times\\\\\n&& \\frac{\\sum_{l=1}^{K_{j_B}} \\exp(\\mathbf{v}_{{j_B},l}'\\boldsymbol\\beta_{j_B}/\\rho_{j_B})}{\\sum_{l=1}^{K_{j_A}} \\exp(\\mathbf{v}_{{j_A},l}'\\boldsymbol\\beta_{J_A}/\\rho_{j_A})},\n\\end{eqnarray*}\\]\ndepends expected utilities alternatives nest \\(j_A\\) \\(j_B\\). IIA hold.Example 4.9  (Travel-mode dataset) Let us illustrate nested logits travel-mode dataset used, e.g., Hensher Greene (2002) (see also Heiss (2002)).","code":"\nlibrary(mlogit)\nlibrary(stargazer)\ndata(\"TravelMode\", package = \"AER\")\nPrepared.TravelMode <- mlogit.data(TravelMode,chid.var = \"individual\",\n                                   alt.var = \"mode\",choice = \"choice\",\n                                   shape = \"long\")\n# Fit a multinomial model:\nhl <- mlogit(choice ~ wait + travel + vcost, Prepared.TravelMode,\n             method = \"bfgs\", heterosc = TRUE, tol = 10)\n## Fit a nested logit model:\nTravelMode$avincome <- with(TravelMode, income * (mode == \"air\"))\nTravelMode$time <- with(TravelMode, travel + wait)/60\nTravelMode$timeair <- with(TravelMode, time * I(mode == \"air\"))\nTravelMode$income <- with(TravelMode, income / 10)\n# Hensher and Greene (2002), table 1 p.8-9 model 5\nTravelMode$incomeother <- with(TravelMode,\n                               ifelse(mode %in% c('air', 'car'), income, 0))\nnl1 <- mlogit(choice ~ gcost + wait + incomeother, TravelMode,\n              shape='long', # Indicates how the dataset is organized\n              alt.var='mode', # variable that defines the alternative choices.\n              nests=list(public=c('train', 'bus'),\n                         car='car',air='air'), # defines the \"limbs\".\n              un.nest.el = TRUE)\nnl2 <- mlogit(choice ~ gcost + wait + time, TravelMode,\n              shape='long', # Inidcates how the dataset is organized\n              alt.var='mode', # variable that defines the alternative choices.\n              nests=list(public=c('train', 'bus'),\n                         car='car',air='air'), # defines the \"limbs\".\n              un.nest.el = TRUE)\nstargazer(nl1,nl2,type=\"text\",no.space = TRUE)## \n## ==============================================\n##                       Dependent variable:     \n##                   ----------------------------\n##                              choice           \n##                        (1)            (2)     \n## ----------------------------------------------\n## (Intercept):train     -0.211        -0.284    \n##                      (0.562)        (0.551)   \n## (Intercept):bus       -0.824        -0.712    \n##                      (0.708)        (0.690)   \n## (Intercept):car     -5.237***      -3.845***  \n##                      (0.785)        (0.844)   \n## gcost               -0.013***       -0.004    \n##                      (0.004)        (0.006)   \n## wait                -0.088***      -0.089***  \n##                      (0.011)        (0.011)   \n## incomeother          0.430***                 \n##                      (0.113)                  \n## time                               -0.202***  \n##                                     (0.060)   \n## iv                   0.835***      0.877***   \n##                      (0.192)        (0.198)   \n## ----------------------------------------------\n## Observations           210            210     \n## R2                    0.328          0.313    \n## Log Likelihood       -190.779      -194.841   \n## LR Test (df = 7)    185.959***    177.836***  \n## ==============================================\n## Note:              *p<0.1; **p<0.05; ***p<0.01"},{"path":"microeconometrics.html","id":"tobit","chapter":"4 Microeconometrics","heading":"4.3 Tobit models","text":"situations, dependent variable incompletely observed, may result non-representative sample. Typically, cases, observations dependent variable can lower /upper limit, “true”, underlying, dependent variable . case, OLS regression may lead inconsistent parameter estimates.Tobit models designed address situations. approach named James Tobin, developed model late 50s (see Tobin (1956)).Figure 4.8 illustrates situation. dots (white black) represent “true” observations. Now, assume black observed. ones uses observations OLS regression estimate relatonship \\(x\\) \\(y\\), one gets red line. clear sensitivity \\(y\\) \\(x\\) underestimated. blue line line one obtain white dots also observed; grey line represents model used genrate data (\\(y_i=x_i+\\varepsilon_i\\)).\nFigure 4.8: Bias case sample selection. grey line represents population regression line. model \\(y_i = x_i + \\varepsilon_i\\), \\(\\varepsilon_{,t} \\sim \\mathcal{N}(0,1)\\). red line OLS regression line based black dots .\nAssume (partially) observed dependent variable follows:\n\\[\ny^* = \\boldsymbol\\beta'\\mathbf{x} + \\varepsilon,\n\\]\n\\(\\varepsilon\\) drawn distribution characterized p.d.f. denoted \\(f_{\\boldsymbol\\gamma}^*\\) c.d.f. denoted \\(F_{\\boldsymbol\\gamma}^*\\); functions depend vector parameters \\(\\boldsymbol{\\gamma}\\).observed dependent variable :\n\\[\\begin{eqnarray*}\n\\mbox{Censored case:}&&y = \\left\\{\n\\begin{array}{ccc}\ny^* && y^*>L \\\\\nL && y^*\\le L,\n\\end{array}\n\\right.\\\\\n\\mbox{Truncated case:}&&y = \\left\\{\n\\begin{array}{ccc}\ny^* && y^*>L \\\\\n- && y^*\\le L,\n\\end{array}\n\\right.\n\\end{eqnarray*}\\]\n“\\(-\\)” stands missing observations.formulation easily extended censoring (\\(L \\rightarrow U\\)), censoring .model parameters gathered vector \\(\\theta = [\\boldsymbol\\beta',\\boldsymbol\\gamma']'\\). Let us write conditional p.d.f. observed variable:\n\\[\\begin{eqnarray*}\n\\mbox{Censored case:}&& f(y|\\mathbf{x};\\theta) = \\left\\{\n\\begin{array}{ccc}\nf_{\\boldsymbol\\gamma}^*(y -  \\boldsymbol\\beta'\\mathbf{x}) && y>L \\\\\nF_{\\boldsymbol\\gamma}^*(L-  \\boldsymbol\\beta'\\mathbf{x}) && y = L,\n\\end{array}\n\\right.\\\\\n\\mbox{Truncated case:}&&  f(y|\\mathbf{x};\\theta) =\n\\dfrac{f_{\\boldsymbol\\gamma}^*(y -  \\boldsymbol\\beta'\\mathbf{x})}{1 - F_{\\boldsymbol\\gamma}^*(L-  \\boldsymbol\\beta'\\mathbf{x})} \\quad \\mbox{} \\quad y>L.\n\\end{eqnarray*}\\](conditional) log-likelihood function given :\n\\[\n\\log \\mathcal{L}(\\theta;\\mathbf{y},\\mathbf{X}) = \\sum_{=1}^n \\log f(y_i|\\mathbf{x}_i;\\theta).\n\\]\ncensored case, :\n\\[\\begin{eqnarray*}\n\\log \\mathcal{L}(\\theta;\\mathbf{y},\\mathbf{X}) &=& \\sum_{=1}^n \\left\\{\n\\mathbb{}_{\\{y_i=L\\}}\\log\\left[F_{\\boldsymbol\\gamma}^*(L-  \\boldsymbol\\beta'\\mathbf{x}_i)\\right] + \\right.\\\\\n&& \\left. \\mathbb{}_{\\{y_i>0\\}} \\log \\left[f_{\\boldsymbol\\gamma}^*(y_i -  \\boldsymbol\\beta'\\mathbf{x}_i)\\right]\\right\\}.\n\\end{eqnarray*}\\]Tobit, censored/truncated normal regression model, corresponds case described , Gaussian errors \\(\\varepsilon\\). Specifically:\n\\[\ny^* = \\boldsymbol\\beta'\\mathbf{x} + \\varepsilon,\n\\]\n\\(\\varepsilon \\sim \\,..d.\\,\\mathcal{N}(0,\\sigma^2)\\) (\\(\\Rightarrow\\) \\(\\boldsymbol\\gamma = \\sigma^2\\)).Without loss generality, can assume \\(L=0\\). (One can shift observed data necessary.)censored density (\\(L=0\\)) given :\n\\[\nf(y) = \\left[\n\\frac{1}{\\sqrt{2 \\pi \\sigma^2}}\\exp\\left(-\\frac{1}{2 \\sigma^2}(y - \\boldsymbol\\beta'\\mathbf{x})^2\\right)\n\\right]^{\\mathbb{}_{\\{y>0\\}}}\n\\left[\n1 - \\Phi\\left(\\frac{\\boldsymbol\\beta'\\mathbf{x}}{\\sigma}\\right)\n\\right]^{\\mathbb{}_{\\{y=0\\}}}.\n\\]censored density (\\(L=0\\)) given :\n\\[\nf(y) = \\left[\n\\frac{1}{\\sqrt{2 \\pi \\sigma^2}}\\exp\\left(-\\frac{1}{2 \\sigma^2}(y - \\boldsymbol\\beta'\\mathbf{x})^2\\right)\n\\right]^{\\mathbb{}_{\\{y>0\\}}}\n\\left[\n1 - \\Phi\\left(\\frac{\\boldsymbol\\beta'\\mathbf{x}}{\\sigma}\\right)\n\\right]^{\\mathbb{}_{\\{y=0\\}}}.\n\\]truncated density (\\(L=0\\)) given :\n\\[\nf(y) = \\frac{1}{\\Phi(\\boldsymbol\\beta'\\mathbf{x})}\n\\frac{1}{\\sqrt{2 \\pi \\sigma^2}}\\exp\\left(-\\frac{1}{2 \\sigma^2}(y - \\boldsymbol\\beta'\\mathbf{x})^2\\right).\n\\]truncated density (\\(L=0\\)) given :\n\\[\nf(y) = \\frac{1}{\\Phi(\\boldsymbol\\beta'\\mathbf{x})}\n\\frac{1}{\\sqrt{2 \\pi \\sigma^2}}\\exp\\left(-\\frac{1}{2 \\sigma^2}(y - \\boldsymbol\\beta'\\mathbf{x})^2\\right).\n\\]Results usually heavily rely distributional assumptions (uncensored/untruncated case). framework easy extend heteroskedastic case, instance setting \\(\\sigma_i^2=\\exp(\\alpha'\\mathbf{x}_i)\\). situation illustrated Figure 4.9.\nFigure 4.9: Censored dataset heteroskedasticitiy. model \\(y_i = x_i + \\varepsilon_i\\), \\(\\varepsilon_{,t} \\sim \\mathcal{N}(0,\\sigma_i^2)\\) \\(\\sigma_i = \\exp(-1 + x_i)\\).\nLet us consider conditional means \\(y\\) general case, .e., \\(\\varepsilon\\) distribution. Assume \\(\\mathbf{x}\\) observed, expectations conditional \\(\\mathbf{x}\\).data left-truncated 0, :\n\\[\\begin{eqnarray*}\n\\mathbb{E}(y) &=& \\mathbb{E}(y^*|y^*>0) = \\underbrace{\\boldsymbol\\beta'\\mathbf{x}}_{=\\mathbb{E}(y^*)} + \\underbrace{\\mathbb{E}(\\varepsilon|\\varepsilon>-\\boldsymbol\\beta'\\mathbf{x})}_{>0} > \\mathbb{E}(y^*).\n\\end{eqnarray*}\\]data left-truncated 0, :\n\\[\\begin{eqnarray*}\n\\mathbb{E}(y) &=& \\mathbb{E}(y^*|y^*>0) = \\underbrace{\\boldsymbol\\beta'\\mathbf{x}}_{=\\mathbb{E}(y^*)} + \\underbrace{\\mathbb{E}(\\varepsilon|\\varepsilon>-\\boldsymbol\\beta'\\mathbf{x})}_{>0} > \\mathbb{E}(y^*).\n\\end{eqnarray*}\\]Consider data left-censored 0. Bayes, :\n\\[\nf_{y^*|y^*>0}(u) = \\frac{f_{y^*}(u)}{\\mathbb{P}(y^*>0)}\\mathbb{}_{\\{u>0\\}}.\n\\]\nTherefore:\n\\[\\begin{eqnarray*}\n\\mathbb{E}(y^*|y^*>0) &=& \\frac{1}{\\mathbb{P}(y^*>0)} \\int_{-\\infty}^\\infty u\\, f_{y^*}(u)\\mathbb{}_{\\{u>0\\}} du \\\\\n&=&  \\frac{1}{\\mathbb{P}(y^*>0)} \\mathbb{E}(\\underbrace{y^*\\mathbb{}_{\\{y^*>0\\}}}_{=y}),\n\\end{eqnarray*}\\]\n, :\n\\[\\begin{eqnarray*}\n\\mathbb{E}(y) &=&  \\mathbb{P}(y^*>0)\\mathbb{E}(y^*|y^*>0)\\\\\n&>&  \\mathbb{E}(y^*) =  \\mathbb{P}(y^*>0)\\mathbb{E}(y^*|y^*>0) +  \\mathbb{P}(y^*<0)\\underbrace{\\mathbb{E}(y^*|y^*<0)}_{<0}.\n\\end{eqnarray*}\\]Consider data left-censored 0. Bayes, :\n\\[\nf_{y^*|y^*>0}(u) = \\frac{f_{y^*}(u)}{\\mathbb{P}(y^*>0)}\\mathbb{}_{\\{u>0\\}}.\n\\]\nTherefore:\n\\[\\begin{eqnarray*}\n\\mathbb{E}(y^*|y^*>0) &=& \\frac{1}{\\mathbb{P}(y^*>0)} \\int_{-\\infty}^\\infty u\\, f_{y^*}(u)\\mathbb{}_{\\{u>0\\}} du \\\\\n&=&  \\frac{1}{\\mathbb{P}(y^*>0)} \\mathbb{E}(\\underbrace{y^*\\mathbb{}_{\\{y^*>0\\}}}_{=y}),\n\\end{eqnarray*}\\]\n, :\n\\[\\begin{eqnarray*}\n\\mathbb{E}(y) &=&  \\mathbb{P}(y^*>0)\\mathbb{E}(y^*|y^*>0)\\\\\n&>&  \\mathbb{E}(y^*) =  \\mathbb{P}(y^*>0)\\mathbb{E}(y^*|y^*>0) +  \\mathbb{P}(y^*<0)\\underbrace{\\mathbb{E}(y^*|y^*<0)}_{<0}.\n\\end{eqnarray*}\\]Now, let us come back Tobit (.e., Gaussian case) case.data left-truncated 0:\n\\[\\begin{eqnarray}\n\\mathbb{E}(y) &=& \\boldsymbol\\beta'\\mathbf{x} + \\mathbb{E}(\\varepsilon|\\varepsilon>-\\boldsymbol\\beta'\\mathbf{x}) \\nonumber\\\\\n&=&  \\boldsymbol\\beta'\\mathbf{x} + \\sigma \\underbrace{\\frac{\\phi(\\boldsymbol\\beta'\\mathbf{x}/\\sigma)}{\\Phi(\\boldsymbol\\beta'\\mathbf{x}/\\sigma)}}_{=: \\lambda(\\boldsymbol\\beta'\\mathbf{x}/\\sigma)} = \\sigma \\left( \\frac{\\boldsymbol\\beta'\\mathbf{x}}{\\sigma} + \\lambda\\left(\\frac{\\boldsymbol\\beta'\\mathbf{x}}{\\sigma}\\right)\\right). \\tag{4.17}\n\\end{eqnarray}\\]\npenultimate line obtained using Eq. (5.5).data left-truncated 0:\n\\[\\begin{eqnarray}\n\\mathbb{E}(y) &=& \\boldsymbol\\beta'\\mathbf{x} + \\mathbb{E}(\\varepsilon|\\varepsilon>-\\boldsymbol\\beta'\\mathbf{x}) \\nonumber\\\\\n&=&  \\boldsymbol\\beta'\\mathbf{x} + \\sigma \\underbrace{\\frac{\\phi(\\boldsymbol\\beta'\\mathbf{x}/\\sigma)}{\\Phi(\\boldsymbol\\beta'\\mathbf{x}/\\sigma)}}_{=: \\lambda(\\boldsymbol\\beta'\\mathbf{x}/\\sigma)} = \\sigma \\left( \\frac{\\boldsymbol\\beta'\\mathbf{x}}{\\sigma} + \\lambda\\left(\\frac{\\boldsymbol\\beta'\\mathbf{x}}{\\sigma}\\right)\\right). \\tag{4.17}\n\\end{eqnarray}\\]\npenultimate line obtained using Eq. (5.5).data left-censored 0:\n\\[\\begin{eqnarray*}\n\\mathbb{E}(y) &=&  \\mathbb{P}(y^*>0)\\mathbb{E}(y^*|y^*>0)\\\\\n&=&  \\Phi\\left( \\frac{\\boldsymbol\\beta'\\mathbf{x}}{\\sigma}\\right) \\sigma \\left(\n\\frac{\\boldsymbol\\beta'\\mathbf{x}}{\\sigma} +   \\lambda\\left(\\frac{\\boldsymbol\\beta'\\mathbf{x}}{\\sigma}\\right)\n\\right).\n\\end{eqnarray*}\\]data left-censored 0:\n\\[\\begin{eqnarray*}\n\\mathbb{E}(y) &=&  \\mathbb{P}(y^*>0)\\mathbb{E}(y^*|y^*>0)\\\\\n&=&  \\Phi\\left( \\frac{\\boldsymbol\\beta'\\mathbf{x}}{\\sigma}\\right) \\sigma \\left(\n\\frac{\\boldsymbol\\beta'\\mathbf{x}}{\\sigma} +   \\lambda\\left(\\frac{\\boldsymbol\\beta'\\mathbf{x}}{\\sigma}\\right)\n\\right).\n\\end{eqnarray*}\\]\nFigure 4.10: Conditional means \\(y\\) Tobit models. model \\(y_i = x_i + \\varepsilon_i\\), \\(\\varepsilon_i \\sim \\mathcal{N}(0,1)\\).\nHeckit regressionThe previous formula (Eq. (4.17)) can particular used alternative estimation approach, namely Heckman two-step estimation. approach based two steps:15Using complete sample, fit Probit model \\(\\mathbb{}_{\\{y_i>0\\}}\\) \\(\\mathbf{x}\\). provides consistent estimate \\(\\frac{\\boldsymbol\\beta}{\\sigma}\\), therefore \\(\\lambda(\\boldsymbol\\beta'\\mathbf{x}/\\sigma)\\). (Indeed, \\(z_i \\equiv \\mathbb{}_{\\{y_i>0\\}}\\), \\(\\mathbb{P}(z_i=1|\\mathbf{x}_i;\\boldsymbol\\beta/\\sigma)=\\Phi(\\boldsymbol\\beta'\\mathbf{x}_i/\\sigma)\\).)Using complete sample, fit Probit model \\(\\mathbb{}_{\\{y_i>0\\}}\\) \\(\\mathbf{x}\\). provides consistent estimate \\(\\frac{\\boldsymbol\\beta}{\\sigma}\\), therefore \\(\\lambda(\\boldsymbol\\beta'\\mathbf{x}/\\sigma)\\). (Indeed, \\(z_i \\equiv \\mathbb{}_{\\{y_i>0\\}}\\), \\(\\mathbb{P}(z_i=1|\\mathbf{x}_i;\\boldsymbol\\beta/\\sigma)=\\Phi(\\boldsymbol\\beta'\\mathbf{x}_i/\\sigma)\\).)Using truncated sample : run OLS regression \\(\\mathbf{y}\\) \\(\\left\\{\\mathbf{x},\\lambda(\\boldsymbol\\beta'\\mathbf{x}/\\sigma)\\right\\}\\) (Eq. (4.17) mind). provides consistent estimate \\((\\boldsymbol\\beta,\\sigma)\\).Using truncated sample : run OLS regression \\(\\mathbf{y}\\) \\(\\left\\{\\mathbf{x},\\lambda(\\boldsymbol\\beta'\\mathbf{x}/\\sigma)\\right\\}\\) (Eq. (4.17) mind). provides consistent estimate \\((\\boldsymbol\\beta,\\sigma)\\).underlying specification form:\n\\[\n\\mbox{Conditional mean} + \\mbox{disturbance}.\n\\]\n“Conditional mean” comes Eq. (4.17) “disturbance” error zero conditional mean.approach also applied case sample selection models (Section 4.4).Example 4.10  (Wage prediction) present example based dataset used Mroz (1987) (whicht part sampleSelection package).Figure 4.11 shows , low wages, OLS model tends -predict wages. slope observed Tobit-predicted wages closer one (adjustment line closer 45-degree line.)\nFigure 4.11: Predicted versus observed wages.\nTwo-part modelIn standard Tobit framework, model determining censored —truncated— data censoring mechanism one determining non-censored —observed— data outcome mechanism. two-part model adds flexibility permitting zeros non-zeros generated different densities. second model characterizes outcome conditional outcome observed.seminal paper, Duan et al. (1983) employ methodology account individual annual hospital expenses. two models follows:\\(1^{st}\\) model: \\(\\mathbb{P}(hosp=1|\\mathbf{x}) = \\Phi(\\mathbf{x}_1'\\boldsymbol\\beta_1)\\),\\(2^{nd}\\) model: \\(Expense = \\exp(\\mathbf{x}_2'\\boldsymbol\\beta_2 + \\eta)\\), \\(\\eta \\sim\\,..d.\\, \\mathcal{N}(0,\\sigma_2^2)\\).Specifically:\n\\[\n\\mathbb{E}(Expense|\\mathbf{x}_1,\\mathbf{x}_2) = \\Phi(\\mathbf{x}_1'\\boldsymbol\\beta_1)\\exp\\left(\\mathbf{x}_2'\\boldsymbol\\beta_2+ \\frac{\\sigma_2^2}{2}\\right).\n\\]sample-selection models, studied next section, one specifies joint distribution censoring outcome mechanisms (two parts independent ).","code":"\nlibrary(sampleSelection)\nlibrary(AER)\ndata(\"Mroz87\")\nMroz87$lfp.yesno <- NaN\nMroz87$lfp.yesno[Mroz87$lfp==1] <- \"yes\"\nMroz87$lfp.yesno[Mroz87$lfp==0] <- \"no\"\nMroz87$lfp.yesno <- as.factor(Mroz87$lfp.yesno)\nols <- lm(wage ~ educ + exper + I( exper^2 ) + city,data=subset(Mroz87,lfp==1))\ntobit <- tobit(wage ~ educ + exper + I( exper^2 ) + city,\n               left = 0, right = Inf,\n               data=Mroz87)\nHeckit <- heckit(lfp ~ educ + exper + I( exper^2 ) + city, # selection equation\n                 wage ~ educ + exper + I( exper^2 ) + city, # outcome equation\n                 data=Mroz87 )\n\nstargazer(ols,Heckit,tobit,no.space = TRUE,type=\"text\",omit.stat = \"f\")## \n## ======================================================================\n##                                    Dependent variable:                \n##                     --------------------------------------------------\n##                                            wage                       \n##                           OLS           Heckman           Tobit       \n##                                        selection                      \n##                           (1)             (2)              (3)        \n## ----------------------------------------------------------------------\n## educ                    0.481***       0.759***         0.642***      \n##                         (0.067)         (0.270)          (0.081)      \n## exper                    0.032           0.430          0.461***      \n##                         (0.062)         (0.369)          (0.068)      \n## I(exper2)               -0.0003         -0.008          -0.009***     \n##                         (0.002)         (0.008)          (0.002)      \n## city                     0.449           0.113           -0.087       \n##                         (0.318)         (0.522)          (0.378)      \n## Constant               -2.561***        -12.251        -10.395***     \n##                         (0.929)         (8.853)          (1.095)      \n## ----------------------------------------------------------------------\n## Observations              428             753              753        \n## R2                       0.125           0.128                        \n## Adjusted R2              0.117           0.117                        \n## Log Likelihood                                         -1,462.700     \n## rho                                      1.063                        \n## Inverse Mills Ratio                  5.165 (4.594)                    \n## Residual Std. Error 3.111 (df = 423)                                  \n## Wald Test                                          153.892*** (df = 4)\n## ======================================================================\n## Note:                                      *p<0.1; **p<0.05; ***p<0.01"},{"path":"microeconometrics.html","id":"SSM","chapter":"4 Microeconometrics","heading":"4.4 Sample Selection Models","text":"situation tackled sample-selection models following. dependent variable interest, denoted \\(y_2\\), depends observed variables \\(\\mathbf{x}_2\\). Observing \\(y_2\\), , depends value latent variable (\\(y_1^*\\)) correlated observed variables \\(\\mathbf{x}_1\\). difference w.r.t. two-part model skethed , even conditionally \\((\\mathbf{x}_1,\\mathbf{x}_2)\\), \\(y_1^*\\) \\(y_2\\) may correlated.Tobit case, even simplest case population conditional mean linear regressors (.e. \\(y_2 = \\mathbf{x}_2'\\boldsymbol\\beta_2 + \\varepsilon_2\\)), OLS regression leads inconsistent parameter estimates sample representative population.two latent variables: \\(y_1^*\\) \\(y_2^*\\). observe \\(y_1\\) , considered entity “participates”, also observe \\(y_2\\). specifically:\n\\[\\begin{eqnarray*}\ny_1 &=& \\left\\{\n\\begin{array}{ccc}\n1 &\\mbox{}& y_1^* > 0 \\\\\n0 &\\mbox{}& y_1^* \\le 0\n\\end{array}\n\\right. \\quad \\mbox{(participation equation)}\\\\\ny_2 &=& \\left\\{\n\\begin{array}{ccc}\ny_2^* &\\mbox{}& y_1 = 1 \\\\\n- &\\mbox{}& y_1 = 0\n\\end{array}\n\\right. \\quad \\mbox{(outcome equation).}\n\\end{eqnarray*}\\]Moreover:\n\\[\\begin{eqnarray*}\ny_1^* &=& \\mathbf{x}_1'\\boldsymbol\\beta_1 + \\varepsilon_1 \\\\\ny_2^* &=& \\mathbf{x}_2'\\boldsymbol\\beta_2 + \\varepsilon_2.\n\\end{eqnarray*}\\]Note Tobit model (Section 4.3) special case \\(y_1^*=y_2^*\\).Usually:\n\\[\n\\left[\\begin{array}{c}\\varepsilon_1\\\\\\varepsilon_2\\end{array}\\right] \\sim \\mathcal{N}\\left(\\mathbf{0},\n\\left[\n\\begin{array}{cc}\n1 & \\rho  \\sigma_2 \\\\\n\\rho  \\sigma_2 & \\sigma_2^2\n\\end{array}\n\\right]\n\\right).\n\\]\nLet us derive likelihood associated model. :\n\\[\\begin{eqnarray}\nf(\\underbrace{0}_{=y_1},\\underbrace{-}_{=y_2}|\\mathbf{x};\\theta) &=& \\mathbb{P}(y_1^*\\le 0) = \\Phi(-\\mathbf{x}_1'\\boldsymbol\\beta_1) \\tag{4.18}\\\\\nf(1,y_2|\\mathbf{x};\\theta) &=& f(y_2^*|\\mathbf{x};\\theta) \\mathbb{P}(y_1^*>0|y_2^*,\\mathbf{x};\\theta) \\nonumber \\\\\n&=& \\frac{1}{\\sigma}\\phi\\left(\\frac{y_2 - \\mathbf{x}_2'\\boldsymbol\\beta_2}{\\sigma}\\right)  \\mathbb{P}(y_1^*>0|y_2,\\mathbf{x};\\theta).\\tag{4.19}\n\\end{eqnarray}\\]Let us compute \\(\\mathbb{P}(y_1^*>0|y_2,\\mathbf{x};\\theta)\\). Prop. 5.16 (Appendix 5.4), applied (\\(\\varepsilon_1,\\varepsilon_2\\)), :\n\\[\ny_1^*|y_2 \\sim \\mathcal{N}\\left(\\mathbf{x}_1'\\boldsymbol\\beta_1 + \\frac{\\rho}{\\sigma_2}(y_2-\\mathbf{x}_2'\\boldsymbol\\beta_2),1-\\rho^2\\right).\n\\]\nleads \n\\[\\begin{equation}\n\\mathbb{P}(y_1^*>0|y_2,\\mathbf{x};\\theta) = \\Phi\\left( \\frac{\\mathbf{x}_1'\\boldsymbol\\beta_1 + \\dfrac{\\rho}{\\sigma_2}(y_2-\\mathbf{x}_2'\\boldsymbol\\beta_2)}{\\sqrt{1-\\rho^2}}\\right).\\tag{4.20}\n\\end{equation}\\]Figure 4.12 displays \\(\\mathbb{P}(y_1^*>0|y_2,\\mathbf{x};\\theta)\\) different values \\(y_2\\) \\(\\rho\\), case \\(\\boldsymbol\\beta_1=\\boldsymbol\\beta_2=0\\).\nFigure 4.12: Probability observing \\(y_2\\) depending value, different values conditional correlation \\(y_2\\) \\(y_1^*\\).\nUsing Eqs. (4.18), (4.19) (4.20), one gets log-likelihood function:\n\\[\\begin{eqnarray*}\n\\log \\mathcal{L}(\\theta;\\mathbf{y},\\mathbf{X}) &=& \\sum_{=1}^n  (1 - y_{1,})\\log \\Phi(-\\mathbf{x}_{1,}'\\boldsymbol\\beta_1) + \\\\\n&&  \\sum_{=1}^n y_{1,} \\log \\left(  \\frac{1}{\\sigma}\\phi\\left(\\frac{y_{2,} - \\mathbf{x}_{2,}'\\boldsymbol\\beta_2}{\\sigma}\\right)\\right) + \\\\\n&&  \\sum_{=1}^n y_{1,} \\log \\left(\\Phi\\left( \\frac{\\mathbf{x}_{1,}'\\boldsymbol\\beta_1 + \\dfrac{\\rho}{\\sigma_2}(y_{2,}-\\mathbf{x}_2'\\boldsymbol\\beta_2)}{\\sqrt{1-\\rho^2}}\\right)\\right).\n\\end{eqnarray*}\\]can also compute conditional expectations:\n\\[\\begin{eqnarray}\n\\mathbb{E}(y_2^*|y_1=1,\\mathbf{x}) &=& \\mathbb{E}(\\mathbb{E}(y_2^*|y_1^*,\\mathbf{x})|y_1=1,\\mathbf{x})\\nonumber\\\\\n&=& \\mathbb{E}(\\mathbf{x}_2'\\boldsymbol\\beta_2 + \\rho\\sigma_2(y_1^*-\\mathbf{x}_1'\\boldsymbol\\beta_1)|y_1=1,\\mathbf{x})\\nonumber\\\\\n&=& \\mathbf{x}_2'\\boldsymbol\\beta_2 + \\rho\\sigma_2\\mathbb{E}( \\underbrace{y_1^*-\\mathbf{x}_1'\\boldsymbol\\beta_1}_{=\\varepsilon_1 \\sim\\mathcal{N}(0,1)}|\\varepsilon_1>-\\mathbf{x}_1'\\boldsymbol\\beta_1,\\mathbf{x})\\nonumber\\\\\n&=& \\mathbf{x}_2'\\boldsymbol\\beta_2 + \\rho\\sigma_2\\frac{\\phi(-\\mathbf{x}_1'\\boldsymbol\\beta_1)}{1 - \\Phi(-\\mathbf{x}_1'\\boldsymbol\\beta_1)}\\nonumber\\\\\n&=& \\mathbf{x}_2'\\boldsymbol\\beta_2 + \\rho\\sigma_2\\frac{\\phi(\\mathbf{x}_1'\\boldsymbol\\beta_1)}{\\Phi(\\mathbf{x}_1'\\boldsymbol\\beta_1)}=\\mathbf{x}_2'\\boldsymbol\\beta_2 + \\rho\\sigma_2\\lambda(\\mathbf{x}_1'\\boldsymbol\\beta_1),\\tag{4.21}\n\\end{eqnarray}\\]\n:\n\\[\\begin{eqnarray*}\n\\mathbb{E}(y_2^*|y_1=0,\\mathbf{x}) &=&  \\mathbf{x}_2'\\boldsymbol\\beta_2 + \\rho\\sigma_2\\mathbb{E}(y_1^*-\\mathbf{x}_1'\\boldsymbol\\beta_1|\\varepsilon_1\\le-\\mathbf{x}_1'\\boldsymbol\\beta_1,\\mathbf{x})\\\\\n&=& \\mathbf{x}_2'\\boldsymbol\\beta_2 + \\rho\\sigma_2\\frac{\\phi(-\\mathbf{x}_1'\\boldsymbol\\beta_1)}{1 - \\Phi(-\\mathbf{x}_1'\\boldsymbol\\beta_1)}\\\\\n&=& \\mathbf{x}_2'\\boldsymbol\\beta_2 - \\rho\\sigma_2\\frac{\\phi(-\\mathbf{x}_1'\\boldsymbol\\beta_1)}{\\Phi(-\\mathbf{x}_1'\\boldsymbol\\beta_1)}=\\mathbf{x}_2'\\boldsymbol\\beta_2 - \\rho\\sigma_2\\lambda(-\\mathbf{x}_1'\\boldsymbol\\beta_1).\n\\end{eqnarray*}\\]Heckman procedureAs tobit models (Section 4.3), can use Heckman procedure estimate model. Eq. (4.21) shows \\(\\mathbb{E}(y_2^*|y_1=1,\\mathbf{x}) \\ne \\mathbf{x}_2'\\boldsymbol\\beta_2\\) \\(\\rho \\ne 0\\). Therefore, OLS approach yields biased estimates based employed sub-sample \\(y_1=1\\).Heckman two-step procedure (“Heckit”) consists replacing \\(\\lambda(\\mathbf{x}_1'\\boldsymbol\\beta_1)\\) appearing Eq. (4.21) consistent estimate . precisely:Get estimate \\(\\widehat{\\boldsymbol\\beta_1}\\) \\(\\boldsymbol\\beta_1\\) (probit regression \\(y_1\\) \\(\\mathbf{x}_1\\)).Run OLS regression (using data associated \\(y_1=1\\)):\n\\(\\lambda(\\mathbf{x}_1'\\widehat{\\boldsymbol\\beta_1})\\) regressor.estimate \\(\\sigma_2^2\\)? Eq. (5.6), :\n\\[\n\\mathbb{V}ar(y_2|y_1^*>0,\\mathbf{x}) = \\mathbb{V}ar(\\varepsilon_2|\\varepsilon_1>-\\mathbf{x}_1'\\boldsymbol\\beta_1,\\mathbf{x}).\n\\]\nUsing \\(\\varepsilon_2\\) can decomposed \\(\\rho\\sigma_2\\varepsilon_1 + \\xi\\), \\(\\xi \\sim \\mathcal{N}(0,\\sigma_2^2(1-\\rho^2))\\) independent \\(\\varepsilon_1\\), get:\n\\[\n\\mathbb{V}ar(y_2|y_1^*>0,\\mathbf{x}) = \\sigma_2^2(1-\\rho^2) + \\rho^2\\sigma_2^2 \\mathbb{V}ar(\\varepsilon_1|\\varepsilon_1>-\\mathbf{x}_1'\\boldsymbol\\beta_1,\\mathbf{x}).\n\\]\nUsing Eq. (5.7), get:\n\\[\n\\mathbb{V}ar(\\varepsilon_1|\\varepsilon_1>-\\mathbf{x}_1'\\boldsymbol\\beta_1,\\mathbf{x}) = 1 - \\mathbf{x}_1'\\boldsymbol\\beta_1 \\lambda(\\mathbf{x}_1'\\boldsymbol\\beta_1) - \\lambda(\\mathbf{x}_1'\\boldsymbol\\beta_1)^2,\n\\]\ngives\n\\[\\begin{eqnarray*}\n\\mathbb{V}ar(y_2|y_1^*>0,\\mathbf{x}) &=& \\sigma_2^2(1-\\rho^2) + \\rho^2\\sigma_2^2 (1 - \\mathbf{x}_1'\\boldsymbol\\beta_1 \\lambda(\\mathbf{x}_1'\\boldsymbol\\beta_1) - \\lambda(\\mathbf{x}_1'\\boldsymbol\\beta_1)^2)\\\\\n&=& \\sigma_2^2 - \\rho^2\\sigma_2^2 \\left(\\mathbf{x}_1'\\boldsymbol\\beta_1 \\lambda(\\mathbf{x}_1'\\boldsymbol\\beta_1) + \\lambda(\\mathbf{x}_1'\\boldsymbol\\beta_1)^2\\right),\n\\end{eqnarray*}\\]\n, finally:\n\\[\n\\sigma_2^2 \\approx \\widehat{\\mathbb{V}ar}(y_2|y_1^*>0,\\mathbf{x}) + \\widehat{\\rho \\sigma_2}^2 \\left(\\mathbf{x}_1'\\widehat{\\boldsymbol\\beta_1} \\lambda(\\mathbf{x}_1'\\widehat{\\boldsymbol\\beta_1}) + \\lambda(\\mathbf{x}_1'\\widehat{\\boldsymbol\\beta_1})^2\\right).\n\\]Heckman procedure computationally simple. Although computational costs longer issue, two-step solution allows certain generalisations easily ML, robust certain circumstances. computation parameter standard errors fairly complicated two steps (see Cameron Trivedi (2005), Subsection 16.10.2). Bootstrap can resorted .Example 4.11  (Wage prediction) Example 4.10, let us use Mroz (1987) dataset , objective explaining wage setting.","code":"\nlibrary(sampleSelection)\nlibrary(AER)\ndata(\"Mroz87\")\nMroz87$lfp.yesno <- NaN\nMroz87$lfp.yesno[Mroz87$lfp==1] <- \"yes\"\nMroz87$lfp.yesno[Mroz87$lfp==0] <- \"no\"\nMroz87$lfp.yesno <- as.factor(Mroz87$lfp.yesno)\n#Logit & Probit (selection equation)\nlogitW <- glm(lfp ~ age + I( age^2 ) + kids5 + huswage + educ,\n              family = binomial(link = \"logit\"), data = Mroz87) \nprobitW <- glm(lfp ~ age + I( age^2 ) + kids5 + huswage + educ,\n               family = binomial(link = \"probit\"), data = Mroz87) \n# OLS for outcome:\nols1 <- lm(log(wage) ~ educ+exper+I( exper^2 )+city,data=subset(Mroz87,lfp==1))\n# Two-step Heckman estimation\nheckvan <- \n  heckit( lfp ~ age + I( age^2 ) + kids5 + huswage + educ, # selection equation\n          log(wage) ~ educ + exper + I( exper^2 ) + city, # outcome equation\n          data=Mroz87 )\n# Maximun likelihood estimation of selection model:\nml <- selection(lfp~age+I(age^2)+kids5+huswage+educ, \n                log(wage)~educ+exper+I(exper^2)+city, data = Mroz87)\n# Print selection-equation estimates:\nstargazer(logitW,probitW,heckvan,ml,type = \"text\",no.space = TRUE,\n          selection.equation = TRUE)## \n## ===================================================================\n##                                   Dependent variable:              \n##                     -----------------------------------------------\n##                                           lfp                      \n##                     logistic   probit      Heckman      selection  \n##                                           selection                \n##                        (1)       (2)         (3)           (4)     \n## -------------------------------------------------------------------\n## age                   0.012     0.010       0.010         0.010    \n##                      (0.114)   (0.069)     (0.069)       (0.069)   \n## I(age2)              -0.001    -0.0005     -0.0005       -0.0005   \n##                      (0.001)   (0.001)     (0.001)       (0.001)   \n## kids5               -1.409*** -0.855***   -0.855***     -0.854***  \n##                      (0.198)   (0.116)     (0.115)       (0.116)   \n## huswage             -0.069*** -0.042***   -0.042***     -0.042***  \n##                      (0.020)   (0.012)     (0.012)       (0.013)   \n## educ                0.244***  0.148***    0.148***      0.148***   \n##                      (0.040)   (0.024)     (0.023)       (0.024)   \n## Constant             -0.938    -0.620      -0.620        -0.615    \n##                      (2.508)   (1.506)     (1.516)       (1.518)   \n## -------------------------------------------------------------------\n## Observations           753       753         753           753     \n## R2                                          0.158                  \n## Adjusted R2                                 0.148                  \n## Log Likelihood      -459.955  -459.901                  -891.177   \n## Akaike Inf. Crit.    931.910   931.802                             \n## rho                                         0.018     0.014 (0.203)\n## Inverse Mills Ratio                     0.012 (0.152)              \n## ===================================================================\n## Note:                                   *p<0.1; **p<0.05; ***p<0.01\n# Print outcome-equation estimates:\nstargazer(ols1,heckvan,ml,type = \"text\",no.space = TRUE,omit.stat = \"f\")## \n## ================================================================\n##                                 Dependent variable:             \n##                     --------------------------------------------\n##                                      log(wage)                  \n##                           OLS           Heckman      selection  \n##                                        selection                \n##                           (1)             (2)           (3)     \n## ----------------------------------------------------------------\n## educ                    0.106***       0.106***      0.106***   \n##                         (0.014)         (0.017)       (0.017)   \n## exper                   0.041***       0.041***      0.041***   \n##                         (0.013)         (0.013)       (0.013)   \n## I(exper2)               -0.001**       -0.001**      -0.001**   \n##                         (0.0004)       (0.0004)      (0.0004)   \n## city                     0.054           0.053         0.053    \n##                         (0.068)         (0.069)       (0.069)   \n## Constant               -0.531***        -0.547*      -0.544**   \n##                         (0.199)         (0.289)       (0.272)   \n## ----------------------------------------------------------------\n## Observations              428             753           753     \n## R2                       0.158           0.158                  \n## Adjusted R2              0.150           0.148                  \n## Log Likelihood                                       -891.177   \n## rho                                      0.018     0.014 (0.203)\n## Inverse Mills Ratio                  0.012 (0.152)              \n## Residual Std. Error 0.667 (df = 423)                            \n## ================================================================\n## Note:                                *p<0.1; **p<0.05; ***p<0.01"},{"path":"microeconometrics.html","id":"models-of-count-data","chapter":"4 Microeconometrics","heading":"4.5 Models of Count Data","text":"Count-data models aim explaining dependent variables \\(y_i\\) take integer values. Typically, one may want account number doctor visits, customers, hospital stays, borrowers’ defaults, recreational trips, accidents. Quite often, data feature large proportion zeros (see, e.g., Table 20.1 Cameron Trivedi (2005)), /skewed right.","code":""},{"path":"microeconometrics.html","id":"poisson-model","chapter":"4 Microeconometrics","heading":"4.5.1 Poisson model","text":"basic count-data model Poisson model. model, \\(y \\sim \\mathcal{P}(\\mu)\\), .e.\n\\[\n\\mathbb{P}(y=k) = \\frac{\\mu^k e^{-\\mu}}{k!},\n\\]\nimplying \\(\\mathbb{E}(y) = \\mathbb{V}ar(y) = \\mu\\).Poisson parameter, \\(\\mu\\), assumed depend observed variables, gathered vector \\(\\mathbf{x}_i\\) entity \\(\\). ensure \\(\\mu_i \\ge 0\\), common take \\(\\mu_i = \\exp(\\boldsymbol\\beta'\\mathbf{x}_i)\\), gives:\n\\[\ny_i \\sim \\mathcal{P}(\\exp[\\boldsymbol\\beta'\\mathbf{x}_i]).\n\\]Poisson regression intrinsically heteroskedastic (since \\(\\mathbb{V}ar(y_i) = \\mu_i = \\exp(\\boldsymbol\\beta'\\mathbf{x}_i)\\)).assumption independence across entities, log-likelihood given :\n\\[\n\\log \\mathcal{L}(\\boldsymbol\\beta;\\mathbf{y},\\mathbf{X}) = \\sum_{=1}^n (y_i \\boldsymbol\\beta'\\mathbf{x}_i - \\exp[\\boldsymbol\\beta'\\mathbf{x}_i] - \\ln[y_i!]).\n\\]\nfirst-order condition get MLE :\n\\[\\begin{equation}\n\\sum_{=1}^n (y_i - \\exp[\\boldsymbol\\beta'\\mathbf{x}_i])\\mathbf{x}_i = \\underbrace{\\mathbf{0}}_{K \\times 1}. \\tag{4.23}\n\\end{equation}\\]Eq. (4.23) equivalent define Pseudo Maximum-Likelihood estimator \\(\\boldsymbol\\beta\\) (misspecified) model\n\\[\ny_i \\sim ..d.\\,\\mathcal{N}(\\exp[\\boldsymbol\\beta'\\mathbf{x}_i],\\sigma^2).\n\\]\n, Eq. (4.23) also characterizes (true) ML estimator \\(\\boldsymbol\\beta\\) previous model.Since \\(\\mathbb{E}(y_i|\\mathbf{x}_i) = \\exp(\\boldsymbol\\beta'\\mathbf{x}_i)\\), :\n\\[\ny_i = \\exp(\\boldsymbol\\beta'\\mathbf{x}_i) + \\varepsilon_i,\n\\]\n\\(\\mathbb{E}(\\varepsilon_i|\\mathbf{x}_i) = 0\\). notably implies (N)LS estimator \\(\\boldsymbol\\beta\\) consistent.interpret regression coefficients (components \\(\\boldsymbol\\beta\\))? :\n\\[\n\\frac{\\partial \\mathbb{E}(y_i|\\mathbf{x}_i)}{\\partial x_{,j}} = \\beta_j \\exp(\\boldsymbol\\beta'\\mathbf{x}_i),\n\\]\ndepends considered individual.average estimated response :\n\\[\n\\widehat{\\beta}_j \\frac{1}{n}\\sum_{=1}^n  \\exp(\\widehat{\\boldsymbol\\beta}'\\mathbf{x}_i),\n\\]\nequal \\(\\widehat{\\beta}_j \\overline{y}\\) model includes constant (e.g., \\(x_{1,}=1\\) entities \\(\\)).limitation standard Poisson model distribution \\(y_i\\) conditional \\(\\mathbf{x}_i\\) depends single parameter (\\(\\mu_i\\)). Besides, often tension fitting fraction zeros, .e. \\(\\mathbb{P}(y_i=0|\\mathbf{x}_i)=\\exp[-\\exp(\\boldsymbol\\beta'\\mathbf{x}_i)]\\), distribution \\(y_i|\\mathbf{x}_i,y_i>0\\). following models (negative binomial, NB model, Hurdle model, Zero-Inflated model) designed address points.","code":""},{"path":"microeconometrics.html","id":"negative-binomial-model","chapter":"4 Microeconometrics","heading":"4.5.2 Negative binomial model","text":"negative binomial model, :\n\\[\ny_i|\\lambda_i \\sim \\mathcal{P}(\\lambda_i),\n\\]\n\\(\\lambda_i\\) now random. Specifically, takes form:\n\\[\n\\lambda_i = \\nu_i \\times \\exp(\\boldsymbol\\beta'\\mathbf{x}_i),\n\\]\n\\(\\nu_i \\sim \\,..d.\\,\\Gamma(\\underbrace{\\delta}_{\\mbox{shape}},\\underbrace{1/\\delta}_{\\mbox{scale}})\\). , p.d.f. \\(\\nu_i\\) :\n\\[\ng(\\nu) = \\frac{\\nu^{\\delta - 1}e^{-\\nu\\delta}\\delta^\\delta}{\\Gamma(\\delta)},\n\\]\n\\(\\Gamma:\\,z \\mapsto \\int_0^{+\\infty}t^{z-1}e^{-t}dt\\) (\\(\\Gamma(k+1)=k!\\)).notably implies :\n\\[\n\\mathbb{E}(\\nu_i) = 1 \\quad \\mbox{} \\quad \\mathbb{V}ar(\\nu) = \\frac{1}{\\delta}.\n\\]Hence, p.d.f. \\(y_i\\) conditional \\(\\mu\\) \\(\\delta\\) (\\(\\mu=\\exp(\\boldsymbol\\beta'\\mathbf{x}_i)\\)) obtained mixture densities:\n\\[\n\\mathbb{P}(y_i=k|\\exp(\\boldsymbol\\beta'\\mathbf{x}_i)=\\mu;\\delta)=\\int_0^\\infty \\frac{e^{-\\mu \\nu}(\\mu \\nu)^k}{k!} \\frac{\\nu^{\\delta - 1}e^{-\\nu\\delta}\\delta^\\delta}{\\Gamma(\\delta)} d \\nu.\n\\]can shown :\n\\[\n\\mathbb{E}(y|\\mathbf{x}) = \\mu \\quad \\mbox{}\\quad \\mathbb{V}ar(y|\\mathbf{x}) = \\mu\\left(1+\\alpha \\mu\\right),\n\\]\n\\(\\exp(\\boldsymbol\\beta'\\mathbf{x}_i)=\\mu\\) \\(\\alpha = 1/\\delta\\).one additional degree freedom w.r.t. Poisson model (\\(\\alpha\\)).Note \\(\\mathbb{V}ar(y|\\mathbf{x}) > \\mathbb{E}(y|\\mathbf{x})\\) (often called data). Moreover, conditional variance quadratic mean:\n\\[\n\\mathbb{V}ar(y|\\mathbf{x}) = \\mu+\\alpha \\mu^2.\n\\]\nprevious expression basis -called NB2 specification. \\(\\delta\\) replaced \\(\\mu/\\gamma\\), get NB1 model:\n\\[\n\\mathbb{V}ar(y|\\mathbf{x}) = \\mu(1+\\gamma).\n\\]Example 4.12  (Number doctor visits) following example compares different specifications, namely linear regression model, Poisson model, NB model, account number doctor visits. dataset (`randdata) one used Chapter 20 Cameron Trivedi (2005) (available page).\nFigure 4.13: Distribution number doctor visits.\nModels’ predictions can obtained follows:Let us now compute model-implied probabilities, let’s compare frequencies observed data.appears NB model better capturing relatively large number zeros Poisson model. also case Hurdle Zero-Inflation models:","code":"\nlibrary(AEC)\nlibrary(COUNT)\nlibrary(pscl) # for predprob function and hurdle model\npar(plt=c(.15,.95,.1,.95))\nplot(table(randdata$mdvis))\nranddata$LC <- log(1 + randdata$coins)\nmodel.OLS <- lm(mdvis ~ LC + idp + lpi + fmde + physlm + disea + hlthg + \n                  hlthf + hlthp - 1,data=randdata)\nmodel.poisson <- glm(mdvis ~ LC + idp + lpi + fmde + physlm + disea + \n                       hlthg + hlthf + hlthp - 1,data=randdata,family = poisson)\nmodel.neg.bin <- glm.nb(mdvis ~ LC + idp + lpi + fmde + physlm + disea +\n                          hlthg + hlthf + hlthp - 1,data=randdata)\nmodel.neg.bin.with.intercept <- \n  glm.nb(mdvis ~ LC + idp + lpi + fmde + physlm + disea + hlthg + \n           hlthf + hlthp,data=randdata)\nstargazer::stargazer(model.OLS,model.poisson,model.neg.bin,\n                     model.neg.bin.with.intercept,type=\"text\",\n                     no.space = TRUE,omit.stat = c(\"f\",\"ser\"))## \n## =========================================================================\n##                                     Dependent variable:                  \n##                   -------------------------------------------------------\n##                                            mdvis                         \n##                      OLS      Poisson               negative             \n##                                                     binomial             \n##                      (1)        (2)           (3)              (4)       \n## -------------------------------------------------------------------------\n## LC                -0.155***  -0.051***     -0.057***        -0.058***    \n##                    (0.020)    (0.003)       (0.006)          (0.006)     \n## idp               -0.546***  -0.183***     -0.212***        -0.268***    \n##                    (0.075)    (0.011)       (0.023)          (0.023)     \n## lpi               0.230***   0.095***       0.088***         0.041***    \n##                    (0.012)    (0.002)       (0.004)          (0.004)     \n## fmde              -0.073***  -0.029***     -0.030***        -0.038***    \n##                    (0.012)    (0.002)       (0.004)          (0.003)     \n## physlm            0.945***   0.217***       0.229***         0.269***    \n##                    (0.104)    (0.013)       (0.031)          (0.030)     \n## disea             0.177***   0.050***       0.062***         0.038***    \n##                    (0.004)   (0.0005)       (0.001)          (0.001)     \n## hlthg             0.270***   0.126***       0.068***         -0.044**    \n##                    (0.066)    (0.009)       (0.020)          (0.020)     \n## hlthf             0.455***   0.149***       0.084**           0.017      \n##                    (0.123)    (0.016)       (0.037)          (0.036)     \n## hlthp             1.537***   0.197***       0.185**          0.178**     \n##                    (0.263)    (0.027)       (0.076)          (0.074)     \n## Constant                                                     0.664***    \n##                                                              (0.025)     \n## -------------------------------------------------------------------------\n## Observations       20,190     20,190         20,190           20,190     \n## R2                  0.322                                                \n## Adjusted R2         0.322                                                \n## Log Likelihood              -64,221.340   -43,745.860      -43,384.660   \n## theta                                   0.732*** (0.010) 0.773*** (0.011)\n## Akaike Inf. Crit.           128,460.700    87,509.710       86,789.320   \n## =========================================================================\n## Note:                                         *p<0.1; **p<0.05; ***p<0.01\n# prediction of beta'x, equivalent to \"model.poisson$fitted.values\":\npredict_poisson.beta.x <- predict(model.poisson)\n# prediction of the number of events (exp(beta'x)):\npredict_poisson <- predict(model.poisson,type=\"response\")\npredict_NB <- model.neg.bin$fitted.values\nprop.table.data  <- prop.table(table(randdata$mdvis))\npredprob.poisson <- predprob(model.poisson) # part of pscl package\npredprob.nb      <- predprob(model.neg.bin)\nprint(rbind(prop.table.data[1:6],\n            apply(predprob.poisson[,1:6],2,mean),\n            apply(predprob.nb[,1:6],2,mean)))##              0         1         2          3          4          5\n## [1,] 0.3124319 0.1890540 0.1385339 0.09331352 0.06661714 0.04794453\n## [2,] 0.1220592 0.2230328 0.2271621 0.17173478 0.10888940 0.06244353\n## [3,] 0.3486824 0.1884640 0.1219340 0.08385899 0.05968603 0.04350209"},{"path":"microeconometrics.html","id":"hurdle-model","chapter":"4 Microeconometrics","heading":"4.5.3 Hurdle model","text":"main objective model, w.r.t. Poisson model, generate zeros data predicted previous count models. idea separate modeling number zeros number non-zero counts. Specifically, frequency zeros determined \\(f_1\\), (relative) frequencies non-zero counts determined \\(f_2\\):\n\\[\nf(y) = \\left\\{\n\\begin{array}{lll}\nf_1(0) & \\mbox{$y=0$},\\\\\n\\dfrac{1-f_1(0)}{1-f_2(0)}f_2(y) & \\mbox{$y>0$}.\n\\end{array}\n\\right.\n\\]Note back standard Poisson model \\(f_1 \\equiv f_2\\). model straightforwardly estimated ML.","code":""},{"path":"microeconometrics.html","id":"zero-inflated-model","chapter":"4 Microeconometrics","heading":"4.5.4 Zero-inflated model","text":"objective Hurdle model, modeling slightly different. based mixture binary process \\(B\\) (p.d.f. \\(f_1\\)) process \\(Z\\) (p.d.f. \\(f_2\\)). \\(B\\) \\(Z\\) independent. Formally:\n\\[\ny = B Z,\n\\]\nimplying:\n\\[\nf(y) = \\left\\{\n\\begin{array}{lll}\nf_1(0) + (1-f_1(0))f_2(0) & \\mbox{$y=0$},\\\\\n(1-f_1(0))f_2(y) & \\mbox{$y>0$}.\n\\end{array}\n\\right.\n\\]\nTypically, \\(f_1\\) corresponds logit model \\(f_2\\) Poisson negative binomial density. model easily estimated ML techniques.Example 4.13  (Number doctor visits) Let us come back data used Example 4.12, estimate Hurdle zero-inflation models:Let us test importance LC model using Wald test:Finally, compare average model-implied probabilities frequencies observed data:","code":"\nmodel.hurdle <- \n  hurdle(mdvis ~ LC + idp + lpi + fmde + physlm + disea + hlthg + hlthf + \n           hlthp, data=randdata,\n         dist = \"poisson\", zero.dist = \"binomial\", link = \"logit\")\nmodel.zeroinfl <- zeroinfl(mdvis ~ LC + idp + lpi + fmde + physlm +\n                             disea + hlthg + hlthf + hlthp, data=randdata,\n                           dist = \"poisson\", link = \"logit\")\nstargazer(model.hurdle,model.zeroinfl,zero.component=FALSE,\n          no.space=TRUE,type=\"text\")## \n## ===========================================\n##                    Dependent variable:     \n##                ----------------------------\n##                           mdvis            \n##                    hurdle     zero-inflated\n##                                count data  \n##                     (1)            (2)     \n## -------------------------------------------\n## LC               -0.015***      -0.015***  \n##                   (0.003)        (0.003)   \n## idp              -0.085***      -0.086***  \n##                   (0.011)        (0.011)   \n## lpi               0.010***      0.010***   \n##                   (0.002)        (0.002)   \n## fmde             -0.021***      -0.021***  \n##                   (0.002)        (0.002)   \n## physlm            0.231***      0.231***   \n##                   (0.012)        (0.012)   \n## disea             0.022***      0.022***   \n##                   (0.001)        (0.001)   \n## hlthg             0.027***      0.026***   \n##                   (0.010)        (0.010)   \n## hlthf             0.147***      0.146***   \n##                   (0.016)        (0.016)   \n## hlthp             0.304***      0.303***   \n##                   (0.026)        (0.026)   \n## Constant          1.133***      1.133***   \n##                   (0.012)        (0.012)   \n## -------------------------------------------\n## Observations       20,190        20,190    \n## Log Likelihood  -54,772.100    -54,772.550 \n## ===========================================\n## Note:           *p<0.1; **p<0.05; ***p<0.01\nstargazer(model.hurdle,model.zeroinfl,zero.component=TRUE,\n          no.space=TRUE,type=\"text\")## \n## ===========================================\n##                    Dependent variable:     \n##                ----------------------------\n##                           mdvis            \n##                    hurdle     zero-inflated\n##                                count data  \n##                     (1)            (2)     \n## -------------------------------------------\n## LC               -0.150***      0.154***   \n##                   (0.010)        (0.011)   \n## idp              -0.631***      0.637***   \n##                   (0.038)        (0.040)   \n## lpi               0.102***      -0.105***  \n##                   (0.007)        (0.007)   \n## fmde             -0.062***      0.060***   \n##                   (0.006)        (0.006)   \n## physlm            0.239***      -0.203***  \n##                   (0.056)        (0.058)   \n## disea             0.062***      -0.059***  \n##                   (0.003)        (0.003)   \n## hlthg            -0.142***      0.158***   \n##                   (0.034)        (0.036)   \n## hlthf            -0.352***      0.396***   \n##                   (0.062)        (0.064)   \n## hlthp              -0.181         0.233    \n##                   (0.149)        (0.151)   \n## Constant          0.411***      -0.528***  \n##                   (0.044)        (0.047)   \n## -------------------------------------------\n## Observations       20,190        20,190    \n## Log Likelihood  -54,772.100    -54,772.550 \n## ===========================================\n## Note:           *p<0.1; **p<0.05; ***p<0.01\n# Test whether LC is important in the model:\nmodel.hurdle.reduced <- update(model.hurdle,.~.-LC)\nlmtest::waldtest(model.hurdle, model.hurdle.reduced)## Wald test\n## \n## Model 1: mdvis ~ LC + idp + lpi + fmde + physlm + disea + hlthg + hlthf + \n##     hlthp\n## Model 2: mdvis ~ idp + lpi + fmde + physlm + disea + hlthg + hlthf + hlthp\n##   Res.Df Df  Chisq Pr(>Chisq)    \n## 1  20170                         \n## 2  20172 -2 247.64  < 2.2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\npredprob.hurdle   <- predprob(model.hurdle)\npredprob.zeroinfl <- predprob(model.zeroinfl)\nprint(rbind(prop.table.data[1:6],\n  apply(predprob.poisson[,1:6],2,mean),\n  apply(predprob.nb[,1:6],2,mean),\n  apply(predprob.hurdle[,1:6],2,mean),\n  apply(predprob.zeroinfl[,1:6],2,mean)))##              0          1         2          3          4          5\n## [1,] 0.3124319 0.18905399 0.1385339 0.09331352 0.06661714 0.04794453\n## [2,] 0.1220592 0.22303277 0.2271621 0.17173478 0.10888940 0.06244353\n## [3,] 0.3486824 0.18846395 0.1219340 0.08385899 0.05968603 0.04350209\n## [4,] 0.3124319 0.06056959 0.1083120 0.13262624 0.12553899 0.09847017\n## [5,] 0.3124684 0.06053026 0.1082799 0.13262562 0.12556531 0.09850218"},{"path":"append.html","id":"append","chapter":"5 Appendix","heading":"5 Appendix","text":"","code":""},{"path":"append.html","id":"PCAapp","chapter":"5 Appendix","heading":"5.1 Principal component analysis (PCA)","text":"Principal component analysis (PCA) classical easy--use statistical method reduce dimension large datasets containing variables linearly driven relatively small number factors. approach widely used data analysis image compression.Suppose \\(T\\) observations \\(n\\)-dimensional random vector \\(x\\), denoted \\(x_{1},x_{2},\\ldots,x_{T}\\). suppose component \\(x\\) mean zero.Denote \\(X\\) matrix given \\(\\left[\\begin{array}{cccc} x_{1} & x_{2} & \\ldots & x_{T}\\end{array}\\right]'\\). Denote \\(j^{th}\\) column \\(X\\) \\(X_{j}\\).want find linear combination \\(x_{}\\)’s (\\(x.u\\)), \\(\\left\\Vert u\\right\\Vert =1\\), “maximum variance.” , want solve:\n\\[\\begin{equation}\n\\begin{array}{clll}\n\\underset{u}{\\arg\\max} & u'X'Xu. \\\\\n\\mbox{s.t. } & \\left| u\\right| =1\n\\end{array}\\tag{5.1}\n\\end{equation}\\]Since \\(X'X\\) positive definite matrix, admits following decomposition:\n\\[\\begin{eqnarray*}\nX'X & = & PDP'\\\\\n& = & P\\left[\\begin{array}{ccc}\n\\lambda_{1}\\\\\n& \\ddots\\\\\n&  & \\lambda_{n}\n\\end{array}\\right]P',\n\\end{eqnarray*}\\]\n\\(P\\) orthogonal matrix whose columns eigenvectors \\(X'X\\).can order eigenvalues \\(\\lambda_{1}\\geq\\ldots\\geq\\lambda_{n}\\). (Since \\(X'X\\) positive definite, eigenvalues positive.)Since \\(P\\) orthogonal, \\(u'X'Xu=u'PDP'u=y'Dy\\) \\(\\left\\Vert y\\right\\Vert =1\\). Therefore, \\(y_{}^{2}\\leq 1\\) \\(\\leq n\\).consequence:\n\\[\ny'Dy=\\sum_{=1}^{n}y_{}^{2}\\lambda_{}\\leq\\lambda_{1}\\sum_{=1}^{n}y_{}^{2}=\\lambda_{1}.\n\\]easily seen maximum reached \\(y=\\left[1,0,\\cdots,0\\right]'\\). Therefore, maximum optimization program (Eq. (5.1)) obtained \\(u=P\\left[1,0,\\cdots,0\\right]'\\). , \\(u\\) eigenvector \\(X'X\\) associated larger eigenvalue (first column \\(P\\)).Let us denote \\(F\\) vector given matrix product \\(XP\\) (note last column equal \\(Xu\\)). columns \\(F\\), denoted \\(F_{j}\\), called factors. :\n\\[\nF'F=P'X'XP=D.\n\\]\nTherefore, particular, \\(F_{j}\\)’s orthogonal.Since \\(X=FP'\\), \\(X_{j}\\)’s linear combinations factors. Let us denote \\(\\hat{X}_{,j}\\) part \\(X_{}\\) explained factor \\(F_{j}\\), :\n\\[\\begin{eqnarray*}\n\\hat{X}_{,j} & = & p_{ij}F_{j}\\\\\nX_{} & = & \\sum_{j}\\hat{X}_{,j}=\\sum_{j}p_{ij}F_{j}.\n\\end{eqnarray*}\\]Consider share variance explained –\\(n\\) variables (\\(X_{1},\\ldots,X_{n}\\))– first factor \\(F_{1}\\):\n\\[\\begin{eqnarray*}\n\\frac{\\sum_{}\\hat{X}_{,1}\\hat{X}'_{,1}}{\\sum_{}X_{}X'_{}} & = & \\frac{\\sum_{}p_{i1}F_{1}F'_{1}p_{i1}}{tr(X'X)} = \\frac{\\sum_{}p_{i1}^{2}\\lambda_{1}}{tr(X'X)} = \\frac{\\lambda_{1}}{\\sum_{}\\lambda_{}}.\n\\end{eqnarray*}\\]Intuitively, first eigenvalue large, means first factor embed large share fluctutaions \\(n\\) \\(X_{}\\)’s.Let us illustrate PCA term structure yields. term strucutre yields (yield curve) know driven small number factors (e.g., Litterman Scheinkman (1991)). One can typically employ PCA recover factors. data used example taken Fred database (tickers: “DGS6MO”,“DGS1”, …). second plot shows factor loardings, indicate first factor level factor (loadings = black line), second factor slope factor (loadings = blue line), third factor curvature factor (loadings = red line).run PCA, one simply apply function prcomp matrix data:Let us know visualize results. first plot Figure 5.1 shows share total variance explained different principal components (PCs). second plot shows facotr loadings. two bottom plots show yields (black) fitted linear combinations first two PCs .\nFigure 5.1: PCA results. dataset contains 8 time series U.S. interest rates different maturities.\n","code":"\nlibrary(AEC)\nUSyields <- USyields[complete.cases(USyields),]\nyds <- USyields[c(\"Y1\",\"Y2\",\"Y3\",\"Y5\",\"Y7\",\"Y10\",\"Y20\",\"Y30\")]\nPCA.yds <- prcomp(yds,center=TRUE,scale. = TRUE)\npar(mfrow=c(2,2))\npar(plt=c(.1,.95,.2,.8))\nbarplot(PCA.yds$sdev^2/sum(PCA.yds$sdev^2),\n        main=\"Share of variance expl. by PC's\")\naxis(1, at=1:dim(yds)[2], labels=colnames(PCA.yds$x))\nnb.PC <- 2\nplot(-PCA.yds$rotation[,1],type=\"l\",lwd=2,ylim=c(-1,1),\n     main=\"Factor loadings (1st 3 PCs)\",xaxt=\"n\",xlab=\"\")\naxis(1, at=1:dim(yds)[2], labels=colnames(yds))\nlines(PCA.yds$rotation[,2],type=\"l\",lwd=2,col=\"blue\")\nlines(PCA.yds$rotation[,3],type=\"l\",lwd=2,col=\"red\")\nY1.hat <- PCA.yds$x[,1:nb.PC] %*% PCA.yds$rotation[\"Y1\",1:2]\nY1.hat <- mean(USyields$Y1) + sd(USyields$Y1) * Y1.hat\nplot(USyields$date,USyields$Y1,type=\"l\",lwd=2,\n     main=\"Fit of 1-year yields (2 PCs)\",\n     ylab=\"Obs (black) / Fitted by 2PCs (dashed blue)\")\nlines(USyields$date,Y1.hat,col=\"blue\",lty=2,lwd=2)\nY10.hat <- PCA.yds$x[,1:nb.PC] %*% PCA.yds$rotation[\"Y10\",1:2]\nY10.hat <- mean(USyields$Y10) + sd(USyields$Y10) * Y10.hat\nplot(USyields$date,USyields$Y10,type=\"l\",lwd=2,\n     main=\"Fit of 10-year yields (2 PCs)\",\n     ylab=\"Obs (black) / Fitted by 2PCs (dashed blue)\")\nlines(USyields$date,Y10.hat,col=\"blue\",lty=2,lwd=2)"},{"path":"append.html","id":"LinAlgebra","chapter":"5 Appendix","heading":"5.2 Linear algebra: definitions and results","text":"Definition 5.1  (Eigenvalues) eigenvalues matrix \\(M\\) numbers \\(\\lambda\\) :\n\\[\n|M - \\lambda | = 0,\n\\]\n\\(| \\bullet |\\) determinant operator.Proposition 5.1  (Properties determinant) :\\(|MN|=|M|\\times|N|\\).\\(|M^{-1}|=|M|^{-1}\\).\\(M\\) admits diagonal representation \\(M=TDT^{-1}\\), \\(D\\) diagonal matrix whose diagonal entries \\(\\{\\lambda_i\\}_{=1,\\dots,n}\\), :\n\\[\n|M - \\lambda |=\\prod_{=1}^n (\\lambda_i - \\lambda).\n\\]Definition 5.2  (Moore-Penrose inverse) \\(M \\\\mathbb{R}^{m \\times n}\\), Moore-Penrose pseudo inverse (exists ) unique matrix \\(M^* \\\\mathbb{R}^{n \\times m}\\) satisfies:\\(M M^* M = M\\)\\(M^* M M^* = M^*\\)\\((M M^*)'=M M^*\\)\n.iv \\((M^* M)'=M^* M\\).Proposition 5.2  (Properties Moore-Penrose inverse) \\(M\\) invertible \\(M^* = M^{-1}\\).pseudo-inverse zero matrix transpose.\n*\npseudo-inverse pseudo-inverse original matrix.Definition 5.3  (Idempotent matrix) Matrix \\(M\\) idempotent \\(M^2=M\\).\\(M\\) symmetric idempotent matrix, \\(M'M=M\\).Proposition 5.3  (Roots idempotent matrix) eigenvalues idempotent matrix either 1 0.Proof. \\(\\lambda\\) eigenvalue idempotent matrix \\(M\\) \\(\\exists x \\ne 0\\) s.t. \\(Mx=\\lambda x\\). Hence \\(M^2x=\\lambda M x \\Rightarrow (1-\\lambda)Mx=0\\). Either element \\(Mx\\) zero, case \\(\\lambda=0\\) least one element \\(Mx\\) nonzero, case \\(\\lambda=1\\).Proposition 5.4  (Idempotent matrix chi-square distribution) rank symmetric idempotent matrix equal trace.Proof. result follows Prop. 5.3, combined fact rank symmetric matrix equal number nonzero eigenvalues.Proposition 5.5  (Constrained least squares) solution following optimisation problem:\n\\[\\begin{eqnarray*}\n\\underset{\\boldsymbol\\beta}{\\min} && || \\mathbf{y} - \\mathbf{X}\\boldsymbol\\beta ||^2 \\\\\n&& \\mbox{subject } \\mathbf{R}\\boldsymbol\\beta = \\mathbf{q}\n\\end{eqnarray*}\\]\ngiven :\n\\[\n\\boxed{\\boldsymbol\\beta^r = \\boldsymbol\\beta_0 - (\\mathbf{X}'\\mathbf{X})^{-1} \\mathbf{R}'\\{\\mathbf{R}(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{R}'\\}^{-1}(\\mathbf{R}\\boldsymbol\\beta_0 - \\mathbf{q}),}\n\\]\n\\(\\boldsymbol\\beta_0=(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\mathbf{y}\\).Proof. See instance Jackman, 2007.Proposition 5.6  (Inverse partitioned matrix) :\n\\[\\begin{eqnarray*}\n&&\\left[ \\begin{array}{cc} \\mathbf{}_{11} & \\mathbf{}_{12} \\\\ \\mathbf{}_{21} & \\mathbf{}_{22} \\end{array}\\right]^{-1} = \\\\\n&&\\left[ \\begin{array}{cc} (\\mathbf{}_{11} - \\mathbf{}_{12}\\mathbf{}_{22}^{-1}\\mathbf{}_{21})^{-1} & - \\mathbf{}_{11}^{-1}\\mathbf{}_{12}(\\mathbf{}_{22} - \\mathbf{}_{21}\\mathbf{}_{11}^{-1}\\mathbf{}_{12})^{-1} \\\\\n-(\\mathbf{}_{22} - \\mathbf{}_{21}\\mathbf{}_{11}^{-1}\\mathbf{}_{12})^{-1}\\mathbf{}_{21}\\mathbf{}_{11}^{-1} & (\\mathbf{}_{22} - \\mathbf{}_{21}\\mathbf{}_{11}^{-1}\\mathbf{}_{12})^{-1} \\end{array} \\right].\n\\end{eqnarray*}\\]Definition 5.4  (Matrix derivatives) Consider fonction \\(f: \\mathbb{R}^K \\rightarrow \\mathbb{R}\\). first-order derivative :\n\\[\n\\frac{\\partial f}{\\partial \\mathbf{b}}(\\mathbf{b}) =\n\\left[\\begin{array}{c}\n\\frac{\\partial f}{\\partial b_1}(\\mathbf{b})\\\\\n\\vdots\\\\\n\\frac{\\partial f}{\\partial b_K}(\\mathbf{b})\n\\end{array}\n\\right].\n\\]\nuse notation:\n\\[\n\\frac{\\partial f}{\\partial \\mathbf{b}'}(\\mathbf{b}) = \\left(\\frac{\\partial f}{\\partial \\mathbf{b}}(\\mathbf{b})\\right)'.\n\\]Proposition 5.7  :\\(f(\\mathbf{b}) = ' \\mathbf{b}\\) \\(\\) \\(K \\times 1\\) vector \\(\\frac{\\partial f}{\\partial \\mathbf{b}}(\\mathbf{b}) = \\).\\(f(\\mathbf{b}) = \\mathbf{b}'\\mathbf{b}\\) \\(\\) \\(K \\times K\\) matrix, \\(\\frac{\\partial f}{\\partial \\mathbf{b}}(\\mathbf{b}) = 2A\\mathbf{b}\\).Proposition 5.8  (Square absolute summability) :\n\\[\n\\underbrace{\\sum_{=0}^{\\infty}|\\theta_i| < + \\infty}_{\\mbox{Absolute summability}} \\Rightarrow \\underbrace{\\sum_{=0}^{\\infty} \\theta_i^2 < + \\infty}_{\\mbox{Square summability}}.\n\\]Proof. See Appendix 3.Hamilton. Idea: Absolute summability implies exist \\(N\\) , \\(j>N\\), \\(|\\theta_j| < 1\\) (deduced Cauchy criterion, Theorem 5.2 therefore \\(\\theta_j^2 < |\\theta_j|\\).","code":""},{"path":"append.html","id":"variousResults","chapter":"5 Appendix","heading":"5.3 Statistical analysis: definitions and results","text":"","code":""},{"path":"append.html","id":"moments-and-statistics","chapter":"5 Appendix","heading":"5.3.1 Moments and statistics","text":"Definition 5.5  (Partial correlation) partial correlation \\(y\\) \\(z\\), controlling variables \\(\\mathbf{X}\\) sample correlation \\(y^*\\) \\(z^*\\), latter two variables residuals regressions \\(y\\) \\(\\mathbf{X}\\) \\(z\\) \\(\\mathbf{X}\\), respectively.correlation denoted \\(r_{yz}^\\mathbf{X}\\). definition, :\n\\[\\begin{equation}\nr_{yz}^\\mathbf{X} = \\frac{\\mathbf{z^*}'\\mathbf{y^*}}{\\sqrt{(\\mathbf{z^*}'\\mathbf{z^*})(\\mathbf{y^*}'\\mathbf{y^*})}}.\\tag{5.2}\n\\end{equation}\\]Definition 5.6  (Skewness kurtosis) Let \\(Y\\) random variable whose fourth moment exists. expectation \\(Y\\) denoted \\(\\mu\\).skewness \\(Y\\) given :\n\\[\n\\frac{\\mathbb{E}[(Y-\\mu)^3]}{\\{\\mathbb{E}[(Y-\\mu)^2]\\}^{3/2}}.\n\\]kurtosis \\(Y\\) given :\n\\[\n\\frac{\\mathbb{E}[(Y-\\mu)^4]}{\\{\\mathbb{E}[(Y-\\mu)^2]\\}^{2}}.\n\\]Theorem 5.1  (Cauchy-Schwarz inequality) :\n\\[\n|\\mathbb{C}ov(X,Y)| \\le \\sqrt{\\mathbb{V}ar(X)\\mathbb{V}ar(Y)}\n\\]\n, \\(X \\ne =\\) \\(Y \\ne 0\\), equality holds iff \\(X\\) \\(Y\\) affine transformation.Proof. \\(\\mathbb{V}ar(X)=0\\), trivial. case, let’s define \\(Z\\) \\(Z = Y - \\frac{\\mathbb{C}ov(X,Y)}{\\mathbb{V}ar(X)}X\\). easily seen \\(\\mathbb{C}ov(X,Z)=0\\). , variance \\(Y=Z+\\frac{\\mathbb{C}ov(X,Y)}{\\mathbb{V}ar(X)}X\\) equal sum variance \\(Z\\) variance \\(\\frac{\\mathbb{C}ov(X,Y)}{\\mathbb{V}ar(X)}X\\), :\n\\[\n\\mathbb{V}ar(Y) = \\mathbb{V}ar(Z) + \\left(\\frac{\\mathbb{C}ov(X,Y)}{\\mathbb{V}ar(X)}\\right)^2\\mathbb{V}ar(X) \\ge \\left(\\frac{\\mathbb{C}ov(X,Y)}{\\mathbb{V}ar(X)}\\right)^2\\mathbb{V}ar(X).\n\\]\nequality holds iff \\(\\mathbb{V}ar(Z)=0\\), .e. iff \\(Y = \\frac{\\mathbb{C}ov(X,Y)}{\\mathbb{V}ar(X)}X+cst\\).Definition 5.7  (Asymptotic level) asymptotic test critical region \\(\\Omega_n\\) asymptotic level equal \\(\\alpha\\) :\n\\[\n\\underset{\\theta \\\\Theta}{\\mbox{sup}} \\quad \\underset{n \\rightarrow \\infty}{\\mbox{lim}} \\mathbb{P}_\\theta (S_n \\\\Omega_n) = \\alpha,\n\\]\n\\(S_n\\) test statistic \\(\\Theta\\) null hypothesis \\(H_0\\) equivalent \\(\\theta \\\\Theta\\).Definition 5.8  (Asymptotically consistent test) asymptotic test critical region \\(\\Omega_n\\) consistent :\n\\[\n\\forall \\theta \\\\Theta^c, \\quad \\mathbb{P}_\\theta (S_n \\\\Omega_n) \\rightarrow 1,\n\\]\n\\(S_n\\) test statistic \\(\\Theta^c\\) null hypothesis \\(H_0\\) equivalent \\(\\theta \\notin \\Theta^c\\).Definition 5.9  (Kullback discrepancy) Given two p.d.f. \\(f\\) \\(f^*\\), Kullback discrepancy defined :\n\\[\n(f,f^*) = \\mathbb{E}^* \\left( \\log \\frac{f^*(Y)}{f(Y)} \\right) = \\int \\log \\frac{f^*(y)}{f(y)} f^*(y) dy.\n\\]Proposition 5.9  (Properties Kullback discrepancy) :\\((f,f^*) \\ge 0\\)\\((f,f^*) = 0\\) iff \\(f \\equiv f^*\\).Proof. \\(x \\rightarrow -\\log(x)\\) convex function. Therefore \\(\\mathbb{E}^*(-\\log f(Y)/f^*(Y)) \\ge -\\log \\mathbb{E}^*(f(Y)/f^*(Y)) = 0\\) (proves ()). Since \\(x \\rightarrow -\\log(x)\\) strictly convex, equality () holds \\(f(Y)/f^*(Y)\\) constant (proves (ii)).Definition 5.10  (Characteristic function) real-valued random variable \\(X\\), characteristic function defined :\n\\[\n\\phi_X: u \\rightarrow \\mathbb{E}[\\exp(iuX)].\n\\]","code":""},{"path":"append.html","id":"standard-distributions","chapter":"5 Appendix","heading":"5.3.2 Standard distributions","text":"Definition 5.11  (F distribution) Consider \\(n=n_1+n_2\\) ..d. \\(\\mathcal{N}(0,1)\\) r.v. \\(X_i\\). r.v. \\(F\\) defined :\n\\[\nF = \\frac{\\sum_{=1}^{n_1} X_i^2}{\\sum_{j=n_1+1}^{n_1+n_2} X_j^2}\\frac{n_2}{n_1}\n\\]\n\\(F \\sim \\mathcal{F}(n_1,n_2)\\). (See Table 5.4 quantiles.)Definition 5.12  (Student-t distribution) \\(Z\\) follows Student-t (\\(t\\)) distribution \\(\\nu\\) degrees freedom (d.f.) :\n\\[\nZ = X_0 \\bigg/ \\sqrt{\\frac{\\sum_{=1}^{\\nu}X_i^2}{\\nu}}, \\quad X_i \\sim ..d. \\mathcal{N}(0,1).\n\\]\n\\(\\mathbb{E}(Z)=0\\), \\(\\mathbb{V}ar(Z)=\\frac{\\nu}{\\nu-2}\\) \\(\\nu>2\\). (See Table 5.2 quantiles.)Definition 5.13  (Chi-square distribution) \\(Z\\) follows \\(\\chi^2\\) distribution \\(\\nu\\) d.f. \\(Z = \\sum_{=1}^{\\nu}X_i^2\\) \\(X_i \\sim ..d. \\mathcal{N}(0,1)\\).\n\\(\\mathbb{E}(Z)=\\nu\\). (See Table 5.3 quantiles.)\nFigure 5.2: Pdf Cauchy distribution (\\(\\mu=0\\), \\(\\gamma=1\\)).\nProposition 5.10  (Inner product multivariate Gaussian variable) Let \\(X\\) \\(n\\)-dimensional multivariate Gaussian variable: \\(X \\sim \\mathcal{N}(0,\\Sigma)\\). :\n\\[\nX' \\Sigma^{-1}X \\sim \\chi^2(n).\n\\]Proof. \\(\\Sigma\\) symmetrical definite positive matrix, admits spectral decomposition \\(PDP'\\) \\(P\\) orthogonal matrix (.e. \\(PP'=Id\\)) D diagonal matrix non-negative entries. Denoting \\(\\sqrt{D^{-1}}\\) diagonal matrix whose diagonal entries inverse \\(D\\), easily checked covariance matrix \\(Y:=\\sqrt{D^{-1}}P'X\\) \\(Id\\). Therefore \\(Y\\) vector uncorrelated Gaussian variables. properties Gaussian variables imply components \\(Y\\) also independent. Hence \\(Y'Y=\\sum_i Y_i^2 \\sim \\chi^2(n)\\).remains note \\(Y'Y=X'PD^{-1}P'X=X'\\mathbb{V}ar(X)^{-1}X\\) conclude.Definition 5.15  (Generalized Extreme Value (GEV) distribution) vector disturbances \\(\\boldsymbol\\varepsilon=[\\varepsilon_{1,1},\\dots,\\varepsilon_{1,K_1},\\dots,\\varepsilon_{J,1},\\dots,\\varepsilon_{J,K_J}]'\\) follows Generalized Extreme Value (GEV) distribution c.d.f. :\n\\[\nF(\\boldsymbol\\varepsilon,\\boldsymbol\\rho) = \\exp(-G(e^{-\\varepsilon_{1,1}},\\dots,e^{-\\varepsilon_{J,K_J}};\\boldsymbol\\rho))\n\\]\n\n\\[\\begin{eqnarray*}\nG(\\mathbf{Y};\\boldsymbol\\rho) &\\equiv&  G(Y_{1,1},\\dots,Y_{1,K_1},\\dots,Y_{J,1},\\dots,Y_{J,K_J};\\boldsymbol\\rho) \\\\\n&=& \\sum_{j=1}^J\\left(\\sum_{k=1}^{K_j} Y_{jk}^{1/\\rho_j}\n\\right)^{\\rho_j}\n\\end{eqnarray*}\\]","code":""},{"path":"append.html","id":"StochConvergences","chapter":"5 Appendix","heading":"5.3.3 Stochastic convergences","text":"Proposition 5.11  (Chebychev's inequality) \\(\\mathbb{E}(|X|^r)\\) finite \\(r>0\\) :\n\\[\n\\forall \\varepsilon > 0, \\quad \\mathbb{P}(|X - c|>\\varepsilon) \\le \\frac{\\mathbb{E}[|X - c|^r]}{\\varepsilon^r}.\n\\]\nparticular, \\(r=2\\):\n\\[\n\\forall \\varepsilon > 0, \\quad \\mathbb{P}(|X - c|>\\varepsilon) \\le \\frac{\\mathbb{E}[(X - c)^2]}{\\varepsilon^2}.\n\\]Proof. Remark \\(\\varepsilon^r \\mathbb{}_{\\{|X| \\ge \\varepsilon\\}} \\le |X|^r\\) take expectation sides.Definition 5.16  (Convergence probability) random variable sequence \\(x_n\\) converges probability constant \\(c\\) \\(\\forall \\varepsilon\\), \\(\\lim_{n \\rightarrow \\infty} \\mathbb{P}(|x_n - c|>\\varepsilon) = 0\\).denoted : \\(\\mbox{plim } x_n = c\\).Definition 5.17  (Convergence Lr norm) \\(x_n\\) converges \\(r\\)-th mean (\\(L^r\\)-norm) towards \\(x\\), \\(\\mathbb{E}(|x_n|^r)\\) \\(\\mathbb{E}(|x|^r)\\) exist \n\\[\n\\lim_{n \\rightarrow \\infty} \\mathbb{E}(|x_n - x|^r) = 0.\n\\]\ndenoted : \\(x_n \\overset{L^r}{\\rightarrow} c\\).\\(r=2\\), convergence called mean square convergence.Definition 5.18  (Almost sure convergence) random variable sequence \\(x_n\\) converges almost surely \\(c\\) \\(\\mathbb{P}(\\lim_{n \\rightarrow \\infty} x_n = c) = 1\\).denoted : \\(x_n \\overset{.s.}{\\rightarrow} c\\).Definition 5.19  (Convergence distribution) \\(x_n\\) said converge distribution (law) \\(x\\) \n\\[\n\\lim_{n \\rightarrow \\infty} F_{x_n}(s) = F_{x}(s)\n\\]\n\\(s\\) \\(F_X\\) –cumulative distribution \\(X\\)– continuous.denoted : \\(x_n \\overset{d}{\\rightarrow} x\\).Proposition 5.12  (Rules limiting distributions (Slutsky)) :Slutsky’s theorem: \\(x_n \\overset{d}{\\rightarrow} x\\) \\(y_n \\overset{p}{\\rightarrow} c\\) \n\\[\\begin{eqnarray*}\nx_n y_n &\\overset{d}{\\rightarrow}& x c \\\\\nx_n + y_n &\\overset{d}{\\rightarrow}& x + c \\\\\nx_n/y_n &\\overset{d}{\\rightarrow}& x / c \\quad (\\mbox{}c \\ne 0)\n\\end{eqnarray*}\\]Slutsky’s theorem: \\(x_n \\overset{d}{\\rightarrow} x\\) \\(y_n \\overset{p}{\\rightarrow} c\\) \n\\[\\begin{eqnarray*}\nx_n y_n &\\overset{d}{\\rightarrow}& x c \\\\\nx_n + y_n &\\overset{d}{\\rightarrow}& x + c \\\\\nx_n/y_n &\\overset{d}{\\rightarrow}& x / c \\quad (\\mbox{}c \\ne 0)\n\\end{eqnarray*}\\]Continuous mapping theorem: \\(x_n \\overset{d}{\\rightarrow} x\\) \\(g\\) continuous function \\(g(x_n) \\overset{d}{\\rightarrow} g(x).\\)Continuous mapping theorem: \\(x_n \\overset{d}{\\rightarrow} x\\) \\(g\\) continuous function \\(g(x_n) \\overset{d}{\\rightarrow} g(x).\\)Proposition 5.13  (Implications stochastic convergences) :\n\\[\\begin{align*}\n&\\boxed{\\overset{L^s}{\\rightarrow}}& &\\underset{1 \\le r \\le s}{\\Rightarrow}& &\\boxed{\\overset{L^r}{\\rightarrow}}&\\\\\n&& && &\\Downarrow&\\\\\n&\\boxed{\\overset{.s.}{\\rightarrow}}& &\\Rightarrow& &\\boxed{\\overset{p}{\\rightarrow}}& \\Rightarrow \\qquad \\boxed{\\overset{d}{\\rightarrow}}.\n\\end{align*}\\]Proof. (fact \\(\\left(\\overset{p}{\\rightarrow}\\right) \\Rightarrow \\left( \\overset{d}{\\rightarrow}\\right)\\)). Assume \\(X_n \\overset{p}{\\rightarrow} X\\). Denoting \\(F\\) \\(F_n\\) c.d.f. \\(X\\) \\(X_n\\), respectively:\n\\[\\begin{eqnarray*}\nF_n(x) &=& \\mathbb{P}(X_n \\le x,X\\le x+\\varepsilon) + \\mathbb{P}(X_n \\le x,X > x+\\varepsilon)\\\\\n&\\le& F(x+\\varepsilon) + \\mathbb{P}(|X_n - X|>\\varepsilon).\\tag{5.3}\n\\end{eqnarray*}\\]\nBesides,\n\\[\\begin{eqnarray*}\nF(x-\\varepsilon) &=& \\mathbb{P}(X \\le x-\\varepsilon,X_n \\le x) + \\mathbb{P}(X \\le x-\\varepsilon,X_n > x)\\\\\n&\\le& F_n(x) + \\mathbb{P}(|X_n - X|>\\varepsilon),\n\\end{eqnarray*}\\]\nimplies:\n\\[\\begin{equation}\nF(x-\\varepsilon) - \\mathbb{P}(|X_n - X|>\\varepsilon) \\le F_n(x).\\tag{5.4}\n\\end{equation}\\]\nEqs. (5.3) (5.4) imply:\n\\[\nF(x-\\varepsilon) - \\mathbb{P}(|X_n - X|>\\varepsilon) \\le F_n(x)  \\le F(x+\\varepsilon) + \\mathbb{P}(|X_n - X|>\\varepsilon).\n\\]\nTaking limits \\(n \\rightarrow \\infty\\) yields\n\\[\nF(x-\\varepsilon) \\le \\underset{n \\rightarrow \\infty}{\\mbox{lim inf}}\\; F_n(x) \\le \\underset{n \\rightarrow \\infty}{\\mbox{lim sup}}\\; F_n(x)  \\le F(x+\\varepsilon).\n\\]\nresult obtained taking limits \\(\\varepsilon \\rightarrow 0\\) (\\(F\\) continuous \\(x\\)).Proposition 5.14  (Convergence distribution constant) \\(X_n\\) converges distribution constant \\(c\\), \\(X_n\\) converges probability \\(c\\).Proof. \\(\\varepsilon>0\\), \\(\\mathbb{P}(X_n < c - \\varepsilon) \\underset{n \\rightarrow \\infty}{\\rightarrow} 0\\) .e. \\(\\mathbb{P}(X_n \\ge c - \\varepsilon) \\underset{n \\rightarrow \\infty}{\\rightarrow} 1\\) \\(\\mathbb{P}(X_n < c + \\varepsilon) \\underset{n \\rightarrow \\infty}{\\rightarrow} 1\\). Therefore \\(\\mathbb{P}(c - \\varepsilon \\le X_n < c + \\varepsilon) \\underset{n \\rightarrow \\infty}{\\rightarrow} 1\\),\ngives result.Example 5.1  (Convergence probability $L^r$) Let \\(\\{x_n\\}_{n \\\\mathbb{N}}\\) series random variables defined :\n\\[\nx_n = n u_n,\n\\]\n\\(u_n\\) independent random variables s.t. \\(u_n \\sim \\mathcal{B}(1/n)\\).\\(x_n \\overset{p}{\\rightarrow} 0\\) \\(x_n \\overset{L^r}{\\nrightarrow} 0\\) \\(\\mathbb{E}(|X_n-0|)=\\mathbb{E}(X_n)=1\\).Theorem 5.2  (Cauchy criterion (non-stochastic case)) \\(\\sum_{=0}^{T} a_i\\) converges (\\(T \\rightarrow \\infty\\)) iff, \\(\\eta > 0\\), exists integer \\(N\\) , \\(M\\ge N\\),\n\\[\n\\left|\\sum_{=N+1}^{M} a_i\\right| < \\eta.\n\\]Theorem 5.3  (Cauchy criterion (stochastic case)) \\(\\sum_{=0}^{T} \\theta_i \\varepsilon_{t-}\\) converges mean square (\\(T \\rightarrow \\infty\\)) random variable iff, \\(\\eta > 0\\), exists integer \\(N\\) , \\(M\\ge N\\),\n\\[\n\\mathbb{E}\\left[\\left(\\sum_{=N+1}^{M} \\theta_i \\varepsilon_{t-}\\right)^2\\right] < \\eta.\n\\]","code":""},{"path":"append.html","id":"CLTappend","chapter":"5 Appendix","heading":"5.3.4 Central limit theorem","text":"Theorem 5.4  (Law large numbers) sample mean consistent estimator population mean.Proof. Let’s denote \\(\\phi_{X_i}\\) characteristic function r.v. \\(X_i\\). mean \\(X_i\\) \\(\\mu\\) Talyor expansion characteristic function :\n\\[\n\\phi_{X_i}(u) = \\mathbb{E}(\\exp(iuX)) = 1 + iu\\mu + o(u).\n\\]\nproperties characteristic function (see Def. 5.10) imply :\n\\[\n\\phi_{\\frac{1}{n}(X_1+\\dots+X_n)}(u) = \\prod_{=1}^{n} \\left(1 + \\frac{u}{n}\\mu + o\\left(\\frac{u}{n}\\right) \\right) \\rightarrow e^{iu\\mu}.\n\\]\nfacts () \\(e^{iu\\mu}\\) characteristic function constant \\(\\mu\\) (b) characteristic function uniquely characterises distribution imply sample mean converges distribution constant \\(\\mu\\), implies converges probability \\(\\mu\\).Theorem 5.5  (Lindberg-Levy Central limit theorem, CLT) \\(x_n\\) ..d. sequence random variables mean \\(\\mu\\) variance \\(\\sigma^2\\) (\\(\\]0,+\\infty[\\)), :\n\\[\n\\boxed{\\sqrt{n} (\\bar{x}_n - \\mu) \\overset{d}{\\rightarrow} \\mathcal{N}(0,\\sigma^2), \\quad \\mbox{} \\quad \\bar{x}_n = \\frac{1}{n} \\sum_{=1}^{n} x_i.}\n\\]Proof. Let us introduce r.v. \\(Y_n:= \\sqrt{n}(\\bar{X}_n - \\mu)\\). \\(\\phi_{Y_n}(u) = \\left[ \\mathbb{E}\\left( \\exp(\\frac{1}{\\sqrt{n}} u (X_1 - \\mu)) \\right) \\right]^n\\). :\n\\[\\begin{eqnarray*}\n&&\\left[ \\mathbb{E}\\left( \\exp\\left(\\frac{1}{\\sqrt{n}} u (X_1 - \\mu)\\right) \\right) \\right]^n\\\\\n&=& \\left[ \\mathbb{E}\\left( 1 + \\frac{1}{\\sqrt{n}} u (X_1 - \\mu) - \\frac{1}{2n} u^2 (X_1 - \\mu)^2 + o(u^2) \\right) \\right]^n \\\\\n&=& \\left( 1 - \\frac{1}{2n}u^2\\sigma^2 + o(u^2)\\right)^n.\n\\end{eqnarray*}\\]\nTherefore \\(\\phi_{Y_n}(u) \\underset{n \\rightarrow \\infty}{\\rightarrow} \\exp \\left( - \\frac{1}{2}u^2\\sigma^2 \\right)\\), characteristic function \\(\\mathcal{N}(0,\\sigma^2)\\).","code":""},{"path":"append.html","id":"GaussianVar","chapter":"5 Appendix","heading":"5.4 Some properties of Gaussian variables","text":"Proposition 5.15  \\(\\mathbf{}\\) idempotent \\(\\mathbf{x}\\) Gaussian, \\(\\mathbf{L}\\mathbf{x}\\) \\(\\mathbf{x}'\\mathbf{}\\mathbf{x}\\) independent \\(\\mathbf{L}\\mathbf{}=\\mathbf{0}\\).Proof. \\(\\mathbf{L}\\mathbf{}=\\mathbf{0}\\), two Gaussian vectors \\(\\mathbf{L}\\mathbf{x}\\) \\(\\mathbf{}\\mathbf{x}\\) independent. implies independence function \\(\\mathbf{L}\\mathbf{x}\\) function \\(\\mathbf{}\\mathbf{x}\\). results follows observation \\(\\mathbf{x}'\\mathbf{}\\mathbf{x}=(\\mathbf{}\\mathbf{x})'(\\mathbf{}\\mathbf{x})\\), function \\(\\mathbf{}\\mathbf{x}\\).Proposition 5.16  (Bayesian update vector Gaussian variables) \n\\[\n\\left[\n\\begin{array}{c}\nY_1\\\\\nY_2\n\\end{array}\n\\right]\n\\sim \\mathcal{N}\n\\left(0,\n\\left[\\begin{array}{cc}\n\\Omega_{11} & \\Omega_{12}\\\\\n\\Omega_{21} & \\Omega_{22}\n\\end{array}\\right]\n\\right),\n\\]\n\n\\[\nY_{2}|Y_{1} \\sim \\mathcal{N}\n\\left(\n\\Omega_{21}\\Omega_{11}^{-1}Y_{1},\\Omega_{22}-\\Omega_{21}\\Omega_{11}^{-1}\\Omega_{12}\n\\right).\n\\]\n\\[\nY_{1}|Y_{2} \\sim \\mathcal{N}\n\\left(\n\\Omega_{12}\\Omega_{22}^{-1}Y_{2},\\Omega_{11}-\\Omega_{12}\\Omega_{22}^{-1}\\Omega_{21}\n\\right).\n\\]Proposition 5.17  (Truncated distributions) \\(X\\) random variable distributed according p.d.f. \\(f\\), c.d.f. \\(F\\), infinite support. p.d.f. \\(X|\\le X < b\\) \n\\[\ng(x) = \\frac{f(x)}{F(b)-F()}\\mathbb{}_{\\{\\le x < b\\}},\n\\]\n\\(<b\\).partiucular, Gaussian variable \\(X \\sim \\mathcal{N}(\\mu,\\sigma^2)\\), \n\\[\nf(X=x|\\le X<b) = \\dfrac{\\dfrac{1}{\\sigma}\\phi\\left(\\dfrac{x - \\mu}{\\sigma}\\right)}{Z}.\n\\]\n\\(Z = \\Phi(\\beta)-\\Phi(\\alpha)\\), \\(\\alpha = \\dfrac{- \\mu}{\\sigma}\\) \\(\\beta = \\dfrac{b - \\mu}{\\sigma}\\).Moreover:\n\\[\\begin{eqnarray}\n\\mathbb{E}(X|\\le X<b) &=& \\mu - \\frac{\\phi\\left(\\beta\\right)-\\phi\\left(\\alpha\\right)}{Z}\\sigma. \\tag{5.5}\n\\end{eqnarray}\\]also :\n\\[\\begin{eqnarray}\n&& \\mathbb{V}ar(X|\\le X<b) \\nonumber\\\\\n&=& \\sigma^2\\left[\n1 -  \\frac{\\beta\\phi\\left(\\beta\\right)-\\alpha\\phi\\left(\\alpha\\right)}{Z} -  \\left(\\frac{\\phi\\left(\\beta\\right)-\\phi\\left(\\alpha\\right)}{Z}\\right)^2 \\right] \\tag{5.6}\n\\end{eqnarray}\\]particular, \\(b \\rightarrow \\infty\\), get:\n\\[\\begin{equation}\n\\mathbb{V}ar(X|< X) = \\sigma^2\\left[1 + \\alpha\\lambda(-\\alpha) - \\lambda(-\\alpha)^2 \\right], \\tag{5.7}\n\\end{equation}\\]\n\\(\\lambda(x)=\\dfrac{\\phi(x)}{\\Phi(x)}\\) called inverse Mills ratio.Consider case \\(\\rightarrow - \\infty\\) (.e. conditioning set \\(X<b\\)) \\(\\mu=0\\), \\(\\sigma=1\\). Eq. (5.5) gives \\(\\mathbb{E}(X|X<b) = - \\lambda(b) = - \\dfrac{\\phi(b)}{\\Phi(b)}\\), \\(\\lambda\\) function computing inverse Mills ratio.\nFigure 5.3: \\(\\mathbb{E}(X|X<b)\\) function \\(b\\) \\(X\\sim \\mathcal{N}(0,1)\\) (black).\nProposition 5.18  (p.d.f. multivariate Gaussian variable) \\(Y \\sim \\mathcal{N}(\\mu,\\Omega)\\) \\(Y\\) \\(n\\)-dimensional vector, density function \\(Y\\) :\n\\[\n\\frac{1}{(2 \\pi)^{n/2}|\\Omega|^{1/2}}\\exp\\left[-\\frac{1}{2}\\left(Y-\\mu\\right)'\\Omega^{-1}\\left(Y-\\mu\\right)\\right].\n\\]","code":""},{"path":"append.html","id":"AppendixProof","chapter":"5 Appendix","heading":"5.5 Proofs","text":"Proof Proposition 3.4Proof. Assumptions () (ii) (set Assumptions 3.1) imply \\(\\boldsymbol\\theta_{MLE}\\) exists (\\(=\\mbox{argmax}_\\theta (1/n)\\log \\mathcal{L}(\\boldsymbol\\theta;\\mathbf{y})\\)).\\((1/n)\\log \\mathcal{L}(\\boldsymbol\\theta;\\mathbf{y})\\) can interpreted sample mean r.v. \\(\\log f(Y_i;\\boldsymbol\\theta)\\) ..d. Therefore \\((1/n)\\log \\mathcal{L}(\\boldsymbol\\theta;\\mathbf{y})\\) converges \\(\\mathbb{E}_{\\boldsymbol\\theta_0}(\\log f(Y;\\boldsymbol\\theta))\\) – exists (Assumption iv).latter convergence uniform (Assumption v), solution \\(\\boldsymbol\\theta_{MLE}\\) almost surely converges solution limit problem:\n\\[\n\\mbox{argmax}_\\theta \\mathbb{E}_{\\boldsymbol\\theta_0}(\\log f(Y;\\boldsymbol\\theta)) = \\mbox{argmax}_\\theta \\int_{\\mathcal{Y}} \\log f(y;\\boldsymbol\\theta)f(y;\\boldsymbol\\theta_0) dy.\n\\]Properties Kullback information measure (see Prop. 5.9), together identifiability assumption (ii) implies solution limit problem unique equal \\(\\boldsymbol\\theta_0\\).Consider r.v. sequence \\(\\boldsymbol\\theta\\) converges \\(\\boldsymbol\\theta_0\\). Taylor expansion score neighborood \\(\\boldsymbol\\theta_0\\) yields :\n\\[\n\\frac{\\partial \\log \\mathcal{L}(\\boldsymbol\\theta;\\mathbf{y})}{\\partial \\boldsymbol\\theta} = \\frac{\\partial \\log \\mathcal{L}(\\boldsymbol\\theta_0;\\mathbf{y})}{\\partial \\boldsymbol\\theta} + \\frac{\\partial^2 \\log \\mathcal{L}(\\boldsymbol\\theta_0;\\mathbf{y})}{\\partial \\boldsymbol\\theta \\partial \\boldsymbol\\theta'}(\\boldsymbol\\theta - \\boldsymbol\\theta_0) + o_p(\\boldsymbol\\theta - \\boldsymbol\\theta_0)\n\\]\\(\\boldsymbol\\theta_{MLE}\\) converges \\(\\boldsymbol\\theta_0\\) satisfies likelihood equation \\(\\frac{\\partial \\log \\mathcal{L}(\\boldsymbol\\theta;\\mathbf{y})}{\\partial \\boldsymbol\\theta} = \\mathbf{0}\\). Therefore:\n\\[\n\\frac{\\partial \\log \\mathcal{L}(\\boldsymbol\\theta_0;\\mathbf{y})}{\\partial \\boldsymbol\\theta} \\approx - \\frac{\\partial^2 \\log \\mathcal{L}(\\boldsymbol\\theta_0;\\mathbf{y})}{\\partial \\boldsymbol\\theta \\partial \\boldsymbol\\theta'}(\\boldsymbol\\theta_{MLE} - \\boldsymbol\\theta_0),\n\\]\nequivalently:\n\\[\n\\frac{1}{\\sqrt{n}} \\frac{\\partial \\log \\mathcal{L}(\\boldsymbol\\theta_0;\\mathbf{y})}{\\partial \\boldsymbol\\theta} \\approx\n\\left(- \\frac{1}{n} \\sum_{=1}^n \\frac{\\partial^2 \\log f(y_i;\\boldsymbol\\theta_0)}{\\partial \\boldsymbol\\theta \\partial \\boldsymbol\\theta'} \\right)\\sqrt{n}(\\boldsymbol\\theta_{MLE} - \\boldsymbol\\theta_0),\n\\]law large numbers, : \\(\\left(- \\frac{1}{n} \\sum_{=1}^n \\frac{\\partial^2 \\log f(y_i;\\boldsymbol\\theta_0)}{\\partial \\boldsymbol\\theta \\partial \\boldsymbol\\theta'} \\right) \\overset{}\\rightarrow \\frac{1}{n} \\mathbf{}(\\boldsymbol\\theta_0) = \\mathcal{}_Y(\\boldsymbol\\theta_0)\\).Besides, :\n\\[\\begin{eqnarray*}\n\\frac{1}{\\sqrt{n}} \\frac{\\partial \\log \\mathcal{L}(\\boldsymbol\\theta_0;\\mathbf{y})}{\\partial \\boldsymbol\\theta} &=& \\sqrt{n} \\left( \\frac{1}{n} \\sum_i \\frac{\\partial \\log f(y_i;\\boldsymbol\\theta_0)}{\\partial \\boldsymbol\\theta} \\right) \\\\\n&=& \\sqrt{n} \\left( \\frac{1}{n} \\sum_i \\left\\{ \\frac{\\partial \\log f(y_i;\\boldsymbol\\theta_0)}{\\partial \\boldsymbol\\theta} - \\mathbb{E}_{\\boldsymbol\\theta_0} \\frac{\\partial \\log f(Y_i;\\boldsymbol\\theta_0)}{\\partial \\boldsymbol\\theta} \\right\\} \\right)\n\\end{eqnarray*}\\]\nconverges \\(\\mathcal{N}(0,\\mathcal{}_Y(\\boldsymbol\\theta_0))\\) CLT.Collecting preceding results leads (b). fact \\(\\boldsymbol\\theta_{MLE}\\) achieves FDCR bound proves (c).Proof Proposition 3.5Proof. \\(\\sqrt{n}(\\hat{\\boldsymbol\\theta}_{n} - \\boldsymbol\\theta_{0}) \\overset{d}{\\rightarrow} \\mathcal{N}(0,\\mathcal{}(\\boldsymbol\\theta_0)^{-1})\\) (Eq. (3.9)). Taylor expansion around \\(\\boldsymbol\\theta_0\\) yields :\n\\[\\begin{equation}\n\\sqrt{n}(h(\\hat{\\boldsymbol\\theta}_{n}) - h(\\boldsymbol\\theta_{0})) \\overset{d}{\\rightarrow} \\mathcal{N}\\left(0,\\frac{\\partial h(\\boldsymbol\\theta_{0})}{\\partial \\boldsymbol\\theta'}\\mathcal{}(\\boldsymbol\\theta_0)^{-1}\\frac{\\partial h(\\boldsymbol\\theta_{0})'}{\\partial \\boldsymbol\\theta}\\right). \\tag{5.8}\n\\end{equation}\\]\n\\(H_0\\), \\(h(\\boldsymbol\\theta_{0})=0\\) therefore:\n\\[\\begin{equation}\n\\sqrt{n} h(\\hat{\\boldsymbol\\theta}_{n}) \\overset{d}{\\rightarrow} \\mathcal{N}\\left(0,\\frac{\\partial h(\\boldsymbol\\theta_{0})}{\\partial \\boldsymbol\\theta'}\\mathcal{}(\\boldsymbol\\theta_0)^{-1}\\frac{\\partial h(\\boldsymbol\\theta_{0})'}{\\partial \\boldsymbol\\theta}\\right). \\tag{5.9}\n\\end{equation}\\]\nHence\n\\[\n\\sqrt{n} \\left(\n\\frac{\\partial h(\\boldsymbol\\theta_{0})}{\\partial \\boldsymbol\\theta'}\\mathcal{}(\\boldsymbol\\theta_0)^{-1}\\frac{\\partial h(\\boldsymbol\\theta_{0})'}{\\partial \\boldsymbol\\theta}\n\\right)^{-1/2} h(\\hat{\\boldsymbol\\theta}_{n}) \\overset{d}{\\rightarrow} \\mathcal{N}\\left(0,Id\\right).\n\\]\nTaking quadratic form, obtain:\n\\[\nn h(\\hat{\\boldsymbol\\theta}_{n})'  \\left(\n\\frac{\\partial h(\\boldsymbol\\theta_{0})}{\\partial \\boldsymbol\\theta'}\\mathcal{}(\\boldsymbol\\theta_0)^{-1}\\frac{\\partial h(\\boldsymbol\\theta_{0})'}{\\partial \\boldsymbol\\theta}\n\\right)^{-1} h(\\hat{\\boldsymbol\\theta}_{n}) \\overset{d}{\\rightarrow} \\chi^2(r).\n\\]fact test asymptotic level \\(\\alpha\\) directly stems precedes. Consistency test: Consider \\(\\theta_0 \\\\Theta\\). MLE consistent, \\(h(\\hat{\\boldsymbol\\theta}_{n})\\) converges \\(h(\\boldsymbol\\theta_0) \\ne 0\\). Eq. (5.8) still valid. implies \\(\\xi^W_n\\) converges \\(+\\infty\\) therefore \\(\\mathbb{P}_{\\boldsymbol\\theta}(\\xi^W_n \\ge \\chi^2_{1-\\alpha}(r)) \\rightarrow 1\\).Proof Proposition 3.6Proof. Notations: “\\(\\approx\\)” means “equal term converges 0 probability”. \\(H_0\\). \\(\\hat{\\boldsymbol\\theta}^0\\) constrained ML estimator; \\(\\hat{\\boldsymbol\\theta}\\) denotes unconstrained one.combine two Taylor expansion: \\(h(\\hat{\\boldsymbol\\theta}_n) \\approx \\dfrac{\\partial h(\\boldsymbol\\theta_0)}{\\partial \\boldsymbol\\theta'}(\\hat{\\boldsymbol\\theta}_n - \\boldsymbol\\theta_0)\\) \\(h(\\hat{\\boldsymbol\\theta}_n^0) \\approx \\dfrac{\\partial h(\\boldsymbol\\theta_0)}{\\partial \\boldsymbol\\theta'}(\\hat{\\boldsymbol\\theta}_n^0 - \\boldsymbol\\theta_0)\\) use \\(h(\\hat{\\boldsymbol\\theta}_n^0)=0\\) (definition) get:\n\\[\\begin{equation}\n\\sqrt{n}h(\\hat{\\boldsymbol\\theta}_n) \\approx \\dfrac{\\partial h(\\boldsymbol\\theta_0)}{\\partial \\boldsymbol\\theta'}\\sqrt{n}(\\hat{\\boldsymbol\\theta}_n - \\hat{\\boldsymbol\\theta}^0_n). \\tag{5.10}\n\\end{equation}\\]\nBesides, (using definition information matrix):\n\\[\\begin{equation}\n\\frac{1}{\\sqrt{n}}\\frac{\\partial \\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}^0_n;\\mathbf{y})}{\\partial \\boldsymbol\\theta} \\approx\n\\frac{1}{\\sqrt{n}}\\frac{\\partial \\log \\mathcal{L}(\\boldsymbol\\theta_0;\\mathbf{y})}{\\partial \\boldsymbol\\theta} - \\mathcal{}(\\boldsymbol\\theta_0)\\sqrt{n}(\\hat{\\boldsymbol\\theta}^0_n-\\boldsymbol\\theta_0) \\tag{5.11}\n\\end{equation}\\]\n:\n\\[\\begin{equation}\n0=\\frac{1}{\\sqrt{n}}\\frac{\\partial \\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}_n;\\mathbf{y})}{\\partial \\boldsymbol\\theta} \\approx\n\\frac{1}{\\sqrt{n}}\\frac{\\partial \\log \\mathcal{L}(\\boldsymbol\\theta_0;\\mathbf{y})}{\\partial \\boldsymbol\\theta} - \\mathcal{}(\\boldsymbol\\theta_0)\\sqrt{n}(\\hat{\\boldsymbol\\theta}_n-\\boldsymbol\\theta_0).\\tag{5.12}\n\\end{equation}\\]\nTaking difference multiplying \\(\\mathcal{}(\\boldsymbol\\theta_0)^{-1}\\):\n\\[\\begin{equation}\n\\sqrt{n}(\\hat{\\boldsymbol\\theta}_n-\\hat{\\boldsymbol\\theta}_n^0) \\approx\n\\mathcal{}(\\boldsymbol\\theta_0)^{-1}\\frac{1}{\\sqrt{n}}\\frac{\\partial \\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}^0_n;\\mathbf{y})}{\\partial \\boldsymbol\\theta}\n\\mathcal{}(\\boldsymbol\\theta_0).\\tag{5.13}\n\\end{equation}\\]\nEqs. (5.10) (5.13) yield :\n\\[\\begin{equation}\n\\sqrt{n}h(\\hat{\\boldsymbol\\theta}_n) \\approx \\dfrac{\\partial h(\\boldsymbol\\theta_0)}{\\partial \\boldsymbol\\theta'} \\mathcal{}(\\boldsymbol\\theta_0)^{-1}\\frac{1}{\\sqrt{n}}\\frac{\\partial \\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}^0_n;\\mathbf{y})}{\\partial \\boldsymbol\\theta}.\\tag{5.14}\n\\end{equation}\\]Recall \\(\\hat{\\boldsymbol\\theta}^0_n\\) MLE \\(\\boldsymbol\\theta_0\\) constraint \\(h(\\boldsymbol\\theta)=0\\). vector Lagrange multipliers \\(\\hat\\lambda_n\\) associated program satisfies:\n\\[\\begin{equation}\n\\frac{\\partial \\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}^0_n;\\mathbf{y})}{\\partial \\boldsymbol\\theta}+ \\frac{\\partial h'(\\hat{\\boldsymbol\\theta}^0_n;\\mathbf{y})}{\\partial \\boldsymbol\\theta}\\hat\\lambda_n = 0.\\tag{5.15}\n\\end{equation}\\]\nSubstituting latter equation Eq. (5.14) gives:\n\\[\\begin{eqnarray*}\n\\sqrt{n}h(\\hat{\\boldsymbol\\theta}_n) &\\approx&\n- \\dfrac{\\partial h(\\boldsymbol\\theta_0)}{\\partial \\boldsymbol\\theta'} \\mathcal{}(\\boldsymbol\\theta_0)^{-1}\n\\frac{\\partial h'(\\hat{\\boldsymbol\\theta}^0_n;\\mathbf{y})}{\\partial \\boldsymbol\\theta} \\frac{\\hat\\lambda_n}{\\sqrt{n}} \\\\\n&\\approx&\n- \\dfrac{\\partial h(\\boldsymbol\\theta_0)}{\\partial \\boldsymbol\\theta'} \\mathcal{}(\\boldsymbol\\theta_0)^{-1}\n\\frac{\\partial h'(\\boldsymbol\\theta_0;\\mathbf{y})}{\\partial \\boldsymbol\\theta} \\frac{\\hat\\lambda_n}{\\sqrt{n}},\n\\end{eqnarray*}\\]\nyields:\n\\[\\begin{equation}\n\\frac{\\hat\\lambda_n}{\\sqrt{n}} \\approx - \\left(\n\\dfrac{\\partial h(\\boldsymbol\\theta_0)}{\\partial \\boldsymbol\\theta'} \\mathcal{}(\\boldsymbol\\theta_0)^{-1}\n\\frac{\\partial h'(\\boldsymbol\\theta_0;\\mathbf{y})}{\\partial \\boldsymbol\\theta}\n\\right)^{-1}\n\\sqrt{n}h(\\hat{\\boldsymbol\\theta}_n).\\tag{5.16}\n\\end{equation}\\]\nfollows, Eq. (5.9), :\n\\[\n\\frac{\\hat\\lambda_n}{\\sqrt{n}} \\overset{d}{\\rightarrow} \\mathcal{N}\\left(0,\\left(\n\\dfrac{\\partial h(\\boldsymbol\\theta_0)}{\\partial \\boldsymbol\\theta'} \\mathcal{}(\\boldsymbol\\theta_0)^{-1}\n\\frac{\\partial h'(\\boldsymbol\\theta_0;\\mathbf{y})}{\\partial \\boldsymbol\\theta}\n\\right)^{-1}\\right).\n\\]\nTaking quadratic form last equation gives:\n\\[\n\\frac{1}{n}\\hat\\lambda_n' \\dfrac{\\partial h(\\hat{\\boldsymbol\\theta}^0_n)}{\\partial \\boldsymbol\\theta'} \\mathcal{}(\\hat{\\boldsymbol\\theta}^0_n)^{-1}\n\\frac{\\partial h'(\\hat{\\boldsymbol\\theta}^0_n;\\mathbf{y})}{\\partial \\boldsymbol\\theta} \\hat\\lambda_n \\overset{d}{\\rightarrow} \\chi^2(r).\n\\]\nUsing Eq. (5.15), appears left-hand side term last equation \\(\\xi^{LM}\\) defined Eq. (3.15). Consistency: see Remark 17.3 Gouriéroux Monfort (1995).Proof Proposition 3.7Proof. Let us first demonstrate asymptotic equivalence \\(\\xi^{LM}\\) \\(\\xi^{LR}\\).second-order Taylor expansions \\(\\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}^0_n,\\mathbf{y})\\) \\(\\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}_n,\\mathbf{y})\\) :\n\\[\\begin{eqnarray*}\n\\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}_n,\\mathbf{y}) &\\approx& \\log \\mathcal{L}(\\boldsymbol\\theta_0,\\mathbf{y})\n+ \\frac{\\partial \\log \\mathcal{L}(\\boldsymbol\\theta_0,\\mathbf{y})}{\\partial \\boldsymbol\\theta'}(\\hat{\\boldsymbol\\theta}_n-\\boldsymbol\\theta_0) \\\\\n&& - \\frac{n}{2} (\\hat{\\boldsymbol\\theta}_n-\\boldsymbol\\theta_0)' \\mathcal{}(\\boldsymbol\\theta_0) (\\hat{\\boldsymbol\\theta}_n-\\boldsymbol\\theta_0)\\\\\n\\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}^0_n,\\mathbf{y}) &\\approx& \\log \\mathcal{L}(\\boldsymbol\\theta_0,\\mathbf{y})\n+ \\frac{\\partial \\log \\mathcal{L}(\\boldsymbol\\theta_0,\\mathbf{y})}{\\partial \\boldsymbol\\theta'}(\\hat{\\boldsymbol\\theta}^0_n-\\boldsymbol\\theta_0) \\\\\n&& - \\frac{n}{2} (\\hat{\\boldsymbol\\theta}^0_n-\\boldsymbol\\theta_0)' \\mathcal{}(\\boldsymbol\\theta_0) (\\hat{\\boldsymbol\\theta}^0_n-\\boldsymbol\\theta_0).\n\\end{eqnarray*}\\]\nTaking difference, obtain:\n\\[\\begin{eqnarray*}\n\\xi_n^{LR} &\\approx& 2\\frac{\\partial \\log \\mathcal{L}(\\boldsymbol\\theta_0,\\mathbf{y})}{\\partial \\boldsymbol\\theta'}\n(\\hat{\\boldsymbol\\theta}_n-\\hat{\\boldsymbol\\theta}^0_n) + n (\\hat{\\boldsymbol\\theta}^0_n-\\boldsymbol\\theta_0)' \\mathcal{}(\\boldsymbol\\theta_0) (\\hat{\\boldsymbol\\theta}^0_n-\\boldsymbol\\theta_0)\\\\\n&& - n (\\hat{\\boldsymbol\\theta}_n-\\boldsymbol\\theta_0)' \\mathcal{}(\\boldsymbol\\theta_0) (\\hat{\\boldsymbol\\theta}_n-\\boldsymbol\\theta_0).\n\\end{eqnarray*}\\]\nUsing \\(\\dfrac{1}{\\sqrt{n}}\\frac{\\partial \\log \\mathcal{L}(\\boldsymbol\\theta_0;\\mathbf{y})}{\\partial \\boldsymbol\\theta} \\approx \\mathcal{}(\\boldsymbol\\theta_0)\\sqrt{n}(\\hat{\\boldsymbol\\theta}_n-\\boldsymbol\\theta_0)\\) (Eq. (5.12)), :\n\\[\\begin{eqnarray*}\n\\xi_n^{LR} &\\approx&\n2n(\\hat{\\boldsymbol\\theta}_n-\\boldsymbol\\theta_0)'\\mathcal{}(\\boldsymbol\\theta_0)\n(\\hat{\\boldsymbol\\theta}_n-\\hat{\\boldsymbol\\theta}^0_n)\n+ n (\\hat{\\boldsymbol\\theta}^0_n-\\boldsymbol\\theta_0)' \\mathcal{}(\\boldsymbol\\theta_0) (\\hat{\\boldsymbol\\theta}^0_n-\\boldsymbol\\theta_0) \\\\\n&& - n (\\hat{\\boldsymbol\\theta}_n-\\boldsymbol\\theta_0)' \\mathcal{}(\\boldsymbol\\theta_0) (\\hat{\\boldsymbol\\theta}_n-\\boldsymbol\\theta_0).\n\\end{eqnarray*}\\]\nsecond three terms sum, replace \\((\\hat{\\boldsymbol\\theta}^0_n-\\boldsymbol\\theta_0)\\) \\((\\hat{\\boldsymbol\\theta}^0_n-\\hat{\\boldsymbol\\theta}_n+\\hat{\\boldsymbol\\theta}_n-\\boldsymbol\\theta_0)\\) develop associated product. leads :\n\\[\\begin{equation}\n\\xi_n^{LR} \\approx n (\\hat{\\boldsymbol\\theta}^0_n-\\hat{\\boldsymbol\\theta}_n)' \\mathcal{}(\\boldsymbol\\theta_0)^{-1} (\\hat{\\boldsymbol\\theta}^0_n-\\hat{\\boldsymbol\\theta}_n). \\tag{5.17}\n\\end{equation}\\]\ndifference Eqs. (5.11) (5.12) implies:\n\\[\n\\frac{1}{\\sqrt{n}}\\frac{\\partial \\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}^0_n;\\mathbf{y})}{\\partial \\boldsymbol\\theta} \\approx\n\\mathcal{}(\\boldsymbol\\theta_0)\\sqrt{n}(\\hat{\\boldsymbol\\theta}_n-\\hat{\\boldsymbol\\theta}^0_n),\n\\]\n, associated Eq. @(eq:lr10), gives:\n\\[\n\\xi_n^{LR} \\approx \\frac{1}{n} \\frac{\\partial \\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}^0_n;\\mathbf{y})}{\\partial \\boldsymbol\\theta'} \\mathcal{}(\\boldsymbol\\theta_0)^{-1} \\frac{\\partial \\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}^0_n;\\mathbf{y})}{\\partial \\boldsymbol\\theta} \\approx \\xi_n^{LM}.\n\\]\nHence \\(\\xi_n^{LR}\\) asymptotic distribution \\(\\xi_n^{LM}\\).Let’s show LR test consistent. , note :\n\\[\\begin{eqnarray*}\n\\frac{\\log \\mathcal{L}(\\hat{\\boldsymbol\\theta},\\mathbf{y}) - \\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}^0,\\mathbf{y})}{n} &=& \\frac{1}{n} \\sum_{=1}^n[\\log f(y_i;\\hat{\\boldsymbol\\theta}_n) - \\log f(y_i;\\hat{\\boldsymbol\\theta}_n^0)]\\\\\n&\\rightarrow& \\mathbb{E}_0[\\log f(Y;\\boldsymbol\\theta_0) - \\log f(Y;\\boldsymbol\\theta_\\infty)],\n\\end{eqnarray*}\\]\n\\(\\boldsymbol\\theta_\\infty\\), pseudo true value, \\(h(\\boldsymbol\\theta_\\infty) \\ne 0\\) (definition \\(H_1\\)). Kullback inequality asymptotic identifiability \\(\\boldsymbol\\theta_0\\), follows \\(\\mathbb{E}_0[\\log f(Y;\\boldsymbol\\theta_0) - \\log f(Y;\\boldsymbol\\theta_\\infty)] >0\\). Therefore \\(\\xi_n^{LR} \\rightarrow + \\infty\\) \\(H_1\\).Let us now demonstrate equivalence \\(\\xi^{LM} \\xi^{W}\\).(using Eq. (eq:multiplier)):\n\\[\n\\xi^{LM}_n = \\frac{1}{n}\\hat\\lambda_n' \\dfrac{\\partial h(\\hat{\\boldsymbol\\theta}^0_n)}{\\partial \\boldsymbol\\theta'} \\mathcal{}(\\hat{\\boldsymbol\\theta}^0_n)^{-1}\n\\frac{\\partial h'(\\hat{\\boldsymbol\\theta}^0_n;\\mathbf{y})}{\\partial \\boldsymbol\\theta} \\hat\\lambda_n.\n\\]\nSince, \\(H_0\\), \\(\\hat{\\boldsymbol\\theta}_n^0\\approx\\hat{\\boldsymbol\\theta}_n \\approx {\\boldsymbol\\theta}_0\\), Eq. (5.16) therefore implies :\n\\[\n\\xi^{LM} \\approx n h(\\hat{\\boldsymbol\\theta}_n)' \\left(\n\\dfrac{\\partial h(\\hat{\\boldsymbol\\theta}_n)}{\\partial \\boldsymbol\\theta'} \\mathcal{}(\\hat{\\boldsymbol\\theta}_n)^{-1}\n\\frac{\\partial h'(\\hat{\\boldsymbol\\theta}_n;\\mathbf{y})}{\\partial \\boldsymbol\\theta}\n\\right)^{-1}\nh(\\hat{\\boldsymbol\\theta}_n) = \\xi^{W},\n\\]\ngives result.Proof Eq. (??)Proof. :\n\\[\\begin{eqnarray*}\n&&T\\mathbb{E}\\left[(\\bar{y}_T - \\mu)^2\\right]\\\\\n&=& T\\mathbb{E}\\left[\\left(\\frac{1}{T}\\sum_{t=1}^T(y_t - \\mu)\\right)^2\\right] = \\frac{1}{T} \\mathbb{E}\\left[\\sum_{t=1}^T(y_t - \\mu)^2+2\\sum_{s<t\\le T}(y_t - \\mu)(y_s - \\mu)\\right]\\\\\n&=& \\gamma_0 +\\frac{2}{T}\\left(\\sum_{t=2}^{T}\\mathbb{E}\\left[(y_t - \\mu)(y_{t-1} - \\mu)\\right]\\right) +\\frac{2}{T}\\left(\\sum_{t=3}^{T}\\mathbb{E}\\left[(y_t - \\mu)(y_{t-2} - \\mu)\\right]\\right) + \\dots \\\\\n&&+ \\frac{2}{T}\\left(\\sum_{t=T-1}^{T}\\mathbb{E}\\left[(y_t - \\mu)(y_{t-(T-2)} - \\mu)\\right]\\right) + \\frac{2}{T}\\mathbb{E}\\left[(y_t - \\mu)(y_{t-(T-1)} - \\mu)\\right]\\\\\n&=&  \\gamma_0 + 2 \\frac{T-1}{T}\\gamma_1 + \\dots + 2 \\frac{1}{T}\\gamma_{T-1} .\n\\end{eqnarray*}\\]\nTherefore:\n\\[\\begin{eqnarray*}\n&& T\\mathbb{E}\\left[(\\bar{y}_T - \\mu)^2\\right] - \\sum_{j=-\\infty}^{+\\infty} \\gamma_j \\\\\n&=& - 2\\frac{1}{T}\\gamma_1 - 2\\frac{2}{T}\\gamma_2 - \\dots - 2\\frac{T-1}{T}\\gamma_{T-1} - 2\\gamma_T - 2 \\gamma_{T+1} + \\dots\n\\end{eqnarray*}\\]\n:\n\\[\\begin{eqnarray*}\n&& \\left|T\\mathbb{E}\\left[(\\bar{y}_T - \\mu)^2\\right] - \\sum_{j=-\\infty}^{+\\infty} \\gamma_j\\right|\\\\\n&\\le& 2\\frac{1}{T}|\\gamma_1| + 2\\frac{2}{T}|\\gamma_2| + \\dots + 2\\frac{T-1}{T}|\\gamma_{T-1}| + 2|\\gamma_T| + 2 |\\gamma_{T+1}| + \\dots\n\\end{eqnarray*}\\]\\(q \\le T\\), :\n\\[\\begin{eqnarray*}\n\\left|T\\mathbb{E}\\left[(\\bar{y}_T - \\mu)^2\\right] - \\sum_{j=-\\infty}^{+\\infty} \\gamma_j\\right| &\\le& 2\\frac{1}{T}|\\gamma_1| + 2\\frac{2}{T}|\\gamma_2| + \\dots + 2\\frac{q-1}{T}|\\gamma_{q-1}| +2\\frac{q}{T}|\\gamma_q| +\\\\\n&&2\\frac{q+1}{T}|\\gamma_{q+1}| + \\dots  + 2\\frac{T-1}{T}|\\gamma_{T-1}| + 2|\\gamma_T| + 2 |\\gamma_{T+1}| + \\dots\\\\\n&\\le& \\frac{2}{T}\\left(|\\gamma_1| + 2|\\gamma_2| + \\dots + (q-1)|\\gamma_{q-1}| +q|\\gamma_q|\\right) +\\\\\n&&2|\\gamma_{q+1}| + \\dots  + 2|\\gamma_{T-1}| + 2|\\gamma_T| + 2 |\\gamma_{T+1}| + \\dots\n\\end{eqnarray*}\\]Consider \\(\\varepsilon > 0\\). fact autocovariances absolutely summable implies exists \\(q_0\\) (Cauchy criterion, Theorem 5.2):\n\\[\n2|\\gamma_{q_0+1}|+2|\\gamma_{q_0+2}|+2|\\gamma_{q_0+3}|+\\dots < \\varepsilon/2.\n\\]\n, \\(T > q_0\\), comes :\n\\[\n\\left|T \\mathbb{E}\\left[(\\bar{y}_T - \\mu)^2\\right] - \\sum_{j=-\\infty}^{+\\infty} \\gamma_j\\right|\\le \\frac{2}{T}\\left(|\\gamma_1| + 2|\\gamma_2| + \\dots + (q_0-1)|\\gamma_{q_0-1}| +q_0|\\gamma_{q_0}|\\right) + \\varepsilon/2.\n\\]\n\\(T \\ge 2\\left(|\\gamma_1| + 2|\\gamma_2| + \\dots + (q_0-1)|\\gamma_{q_0-1}| +q_0|\\gamma_{q_0}|\\right)/(\\varepsilon/2)\\) (\\(= f(q_0)\\), say) \n\\[\n\\frac{2}{T}\\left(|\\gamma_1| + 2|\\gamma_2| + \\dots + (q_0-1)|\\gamma_{q_0-1}| +q_0|\\gamma_{q_0}|\\right) \\le \\varepsilon/2.\n\\]\n, \\(T>f(q_0)\\) \\(T>q_0\\), .e. \\(T>\\max(f(q_0),q_0)\\), :\n\\[\n\\left|T \\mathbb{E}\\left[(\\bar{y}_T - \\mu)^2\\right] - \\sum_{j=-\\infty}^{+\\infty} \\gamma_j\\right|\\le \\varepsilon.\n\\]Proof Proposition ??Proof. :\n\\[\\begin{eqnarray}\n\\mathbb{E}([y_{t+1} - y^*_{t+1}]^2) &=& \\mathbb{E}\\left([\\color{blue}{\\{y_{t+1} - \\mathbb{E}(y_{t+1}|x_t)\\}} + \\color{red}{\\{\\mathbb{E}(y_{t+1}|x_t)  - y^*_{t+1}\\}}]^2\\right)\\nonumber\\\\\n&=&  \\mathbb{E}\\left(\\color{blue}{[y_{t+1} - \\mathbb{E}(y_{t+1}|x_t)]}^2\\right) + \\mathbb{E}\\left(\\color{red}{[\\mathbb{E}(y_{t+1}|x_t)  - y^*_{t+1}]}^2\\right)\\nonumber\\\\\n&& + 2\\mathbb{E}\\left( \\color{blue}{[y_{t+1} - \\mathbb{E}(y_{t+1}|x_t)]}\\color{red}{ [\\mathbb{E}(y_{t+1}|x_t)  - y^*_{t+1}]}\\right). \\tag{5.18}\n\\end{eqnarray}\\]\nLet us focus last term. :\n\\[\\begin{eqnarray*}\n&&\\mathbb{E}\\left(\\color{blue}{[y_{t+1} - \\mathbb{E}(y_{t+1}|x_t)]}\\color{red}{ [\\mathbb{E}(y_{t+1}|x_t)  - y^*_{t+1}]}\\right)\\\\\n&=& \\mathbb{E}( \\mathbb{E}( \\color{blue}{[y_{t+1} - \\mathbb{E}(y_{t+1}|x_t)]}\\color{red}{ \\underbrace{[\\mathbb{E}(y_{t+1}|x_t)  - y^*_{t+1}]}_{\\mbox{function $x_t$}}}|x_t))\\\\\n&=& \\mathbb{E}( \\color{red}{ [\\mathbb{E}(y_{t+1}|x_t)  - y^*_{t+1}]} \\mathbb{E}( \\color{blue}{[y_{t+1} - \\mathbb{E}(y_{t+1}|x_t)]}|x_t))\\\\\n&=& \\mathbb{E}( \\color{red}{ [\\mathbb{E}(y_{t+1}|x_t)  - y^*_{t+1}]} \\color{blue}{\\underbrace{[\\mathbb{E}(y_{t+1}|x_t) - \\mathbb{E}(y_{t+1}|x_t)]}_{=0}})=0.\n\\end{eqnarray*}\\]Therefore, Eq. (5.18) becomes:\n\\[\\begin{eqnarray*}\n&&\\mathbb{E}([y_{t+1} - y^*_{t+1}]^2) \\\\\n&=&  \\underbrace{\\mathbb{E}\\left(\\color{blue}{[y_{t+1} - \\mathbb{E}(y_{t+1}|x_t)]}^2\\right)}_{\\mbox{$\\ge 0$ depend $y^*_{t+1}$}} + \\underbrace{\\mathbb{E}\\left(\\color{red}{[\\mathbb{E}(y_{t+1}|x_t)  - y^*_{t+1}]}^2\\right)}_{\\mbox{$\\ge 0$ depends $y^*_{t+1}$}}.\n\\end{eqnarray*}\\]\nimplies \\(\\mathbb{E}([y_{t+1} - y^*_{t+1}]^2)\\) always larger \\(\\color{blue}{\\mathbb{E}([y_{t+1} - \\mathbb{E}(y_{t+1}|x_t)]^2)}\\), therefore minimized second term equal zero, \\(\\mathbb{E}(y_{t+1}|x_t) = y^*_{t+1}\\).Proof Proposition ??Proof. Using Proposition 5.18, obtain , conditionally \\(x_1\\), log-likelihood given \n\\[\\begin{eqnarray*}\n\\log\\mathcal{L}(Y_{T};\\theta) & = & -(Tn/2)\\log(2\\pi)+(T/2)\\log\\left|\\Omega^{-1}\\right|\\\\\n&  & -\\frac{1}{2}\\sum_{t=1}^{T}\\left[\\left(y_{t}-\\Pi'x_{t}\\right)'\\Omega^{-1}\\left(y_{t}-\\Pi'x_{t}\\right)\\right].\n\\end{eqnarray*}\\]\nLet’s rewrite last term log-likelihood:\n\\[\\begin{eqnarray*}\n\\sum_{t=1}^{T}\\left[\\left(y_{t}-\\Pi'x_{t}\\right)'\\Omega^{-1}\\left(y_{t}-\\Pi'x_{t}\\right)\\right] & =\\\\\n\\sum_{t=1}^{T}\\left[\\left(y_{t}-\\hat{\\Pi}'x_{t}+\\hat{\\Pi}'x_{t}-\\Pi'x_{t}\\right)'\\Omega^{-1}\\left(y_{t}-\\hat{\\Pi}'x_{t}+\\hat{\\Pi}'x_{t}-\\Pi'x_{t}\\right)\\right] & =\\\\\n\\sum_{t=1}^{T}\\left[\\left(\\hat{\\varepsilon}_{t}+(\\hat{\\Pi}-\\Pi)'x_{t}\\right)'\\Omega^{-1}\\left(\\hat{\\varepsilon}_{t}+(\\hat{\\Pi}-\\Pi)'x_{t}\\right)\\right],\n\\end{eqnarray*}\\]\n\\(j^{th}\\) element \\((n\\times1)\\) vector \\(\\hat{\\varepsilon}_{t}\\) sample residual, observation \\(t\\), OLS regression \\(y_{j,t}\\) \\(x_{t}\\). Expanding previous equation, get:\n\\[\\begin{eqnarray*}\n&&\\sum_{t=1}^{T}\\left[\\left(y_{t}-\\Pi'x_{t}\\right)'\\Omega^{-1}\\left(y_{t}-\\Pi'x_{t}\\right)\\right]  = \\sum_{t=1}^{T}\\hat{\\varepsilon}_{t}'\\Omega^{-1}\\hat{\\varepsilon}_{t}\\\\\n&&+2\\sum_{t=1}^{T}\\hat{\\varepsilon}_{t}'\\Omega^{-1}(\\hat{\\Pi}-\\Pi)'x_{t}+\\sum_{t=1}^{T}x'_{t}(\\hat{\\Pi}-\\Pi)\\Omega^{-1}(\\hat{\\Pi}-\\Pi)'x_{t}.\n\\end{eqnarray*}\\]\nLet’s apply trace operator second term (scalar):\n\\[\\begin{eqnarray*}\n\\sum_{t=1}^{T}\\hat{\\varepsilon}_{t}'\\Omega^{-1}(\\hat{\\Pi}-\\Pi)'x_{t} & = & Tr\\left(\\sum_{t=1}^{T}\\hat{\\varepsilon}_{t}'\\Omega^{-1}(\\hat{\\Pi}-\\Pi)'x_{t}\\right)\\\\\n=  Tr\\left(\\sum_{t=1}^{T}\\Omega^{-1}(\\hat{\\Pi}-\\Pi)'x_{t}\\hat{\\varepsilon}_{t}'\\right) & = & Tr\\left(\\Omega^{-1}(\\hat{\\Pi}-\\Pi)'\\sum_{t=1}^{T}x_{t}\\hat{\\varepsilon}_{t}'\\right).\n\\end{eqnarray*}\\]\nGiven , construction (property OLS estimates), sample residuals orthogonal explanatory variables, term zero. Introducing \\(\\tilde{x}_{t}=(\\hat{\\Pi}-\\Pi)'x_{t}\\), \n\\[\\begin{eqnarray*}\n\\sum_{t=1}^{T}\\left[\\left(y_{t}-\\Pi'x_{t}\\right)'\\Omega^{-1}\\left(y_{t}-\\Pi'x_{t}\\right)\\right] =\\sum_{t=1}^{T}\\hat{\\varepsilon}_{t}'\\Omega^{-1}\\hat{\\varepsilon}_{t}+\\sum_{t=1}^{T}\\tilde{x}'_{t}\\Omega^{-1}\\tilde{x}_{t}.\n\\end{eqnarray*}\\]\nSince \\(\\Omega\\) positive definite matrix, \\(\\Omega^{-1}\\) well. Consequently, smallest value last term can take obtained \\(\\tilde{x}_{t}=0\\), .e. \\(\\Pi=\\hat{\\Pi}.\\)MLE \\(\\Omega\\) matrix \\(\\hat{\\Omega}\\) maximizes \\(\\Omega\\overset{\\ell}{\\rightarrow}L(Y_{T};\\hat{\\Pi},\\Omega)\\). :\n\\[\\begin{eqnarray*}\n\\log\\mathcal{L}(Y_{T};\\hat{\\Pi},\\Omega) & = & -(Tn/2)\\log(2\\pi)+(T/2)\\log\\left|\\Omega^{-1}\\right| -\\frac{1}{2}\\sum_{t=1}^{T}\\left[\\hat{\\varepsilon}_{t}'\\Omega^{-1}\\hat{\\varepsilon}_{t}\\right].\n\\end{eqnarray*}\\]Matrix \\(\\hat{\\Omega}\\) symmetric positive definite. easily checked (unrestricted) matrix maximizes latter expression symmetric positive definite matrix. Indeed:\n\\[\n\\frac{\\partial \\log\\mathcal{L}(Y_{T};\\hat{\\Pi},\\Omega)}{\\partial\\Omega}=\\frac{T}{2}\\Omega'-\\frac{1}{2}\\sum_{t=1}^{T}\\hat{\\varepsilon}_{t}\\hat{\\varepsilon}'_{t}\\Rightarrow\\hat{\\Omega}'=\\frac{1}{T}\\sum_{t=1}^{T}\\hat{\\varepsilon}_{t}\\hat{\\varepsilon}'_{t},\n\\]\nleads result.Proof Proposition ??Proof. Let us drop \\(\\) subscript. Rearranging Eq. (??), :\n\\[\n\\sqrt{T}(\\mathbf{b}-\\boldsymbol{\\beta}) =  (X'X/T)^{-1}\\sqrt{T}(X'\\boldsymbol\\varepsilon/T).\n\\]\nLet us consider autocovariances \\(\\mathbf{v}_t = x_t \\varepsilon_t\\), denoted \\(\\gamma^v_j\\). Using fact \\(x_t\\) linear combination past \\(\\varepsilon_t\\)s \\(\\varepsilon_t\\) white noise, get \\(\\mathbb{E}(\\varepsilon_t x_t)=0\\). Therefore\n\\[\n\\gamma^v_j = \\mathbb{E}(\\varepsilon_t\\varepsilon_{t-j}x_tx_{t-j}').\n\\]\n\\(j>0\\), \\(\\mathbb{E}(\\varepsilon_t\\varepsilon_{t-j}x_tx_{t-j}')=\\mathbb{E}(\\mathbb{E}[\\varepsilon_t\\varepsilon_{t-j}x_tx_{t-j}'|\\varepsilon_{t-j},x_t,x_{t-j}])=\\) \\(\\mathbb{E}(\\varepsilon_{t-j}x_tx_{t-j}'\\mathbb{E}[\\varepsilon_t|\\varepsilon_{t-j},x_t,x_{t-j}])=0\\). Note \\(\\mathbb{E}[\\varepsilon_t|\\varepsilon_{t-j},x_t,x_{t-j}]=0\\) \\(\\{\\varepsilon_t\\}\\) ..d. white noise sequence. \\(j=0\\), :\n\\[\n\\gamma^v_0 = \\mathbb{E}(\\varepsilon_t^2x_tx_{t}')= \\mathbb{E}(\\varepsilon_t^2) \\mathbb{E}(x_tx_{t}')=\\sigma^2\\mathbf{Q}.\n\\]\nconvergence distribution \\(\\sqrt{T}(X'\\boldsymbol\\varepsilon/T)=\\sqrt{T}\\frac{1}{T}\\sum_{t=1}^Tv_t\\) results Central Limit Theorem covariance-stationary processes, using \\(\\gamma_j^v\\) computed .","code":""},{"path":"append.html","id":"additional-codes","chapter":"5 Appendix","heading":"5.6 Additional codes","text":"","code":""},{"path":"append.html","id":"App:GEV","chapter":"5 Appendix","heading":"5.6.1 Simulating GEV distributions","text":"following lines code used generate Figure 4.7.","code":"\nn.sim <- 4000\npar(mfrow=c(1,3),\n    plt=c(.2,.95,.2,.85))\nall.rhos <- c(.3,.6,.95)\nfor(j in 1:length(all.rhos)){\n  theta <- 1/all.rhos[j]\n  v1 <- runif(n.sim)\n  v2 <- runif(n.sim)\n  w <- rep(.000001,n.sim)\n  # solve for f(w) = w*(1 - log(w)/theta) - v2 = 0\n  for(i in 1:20){\n    f.i <- w * (1 - log(w)/theta) - v2\n    f.prime <- 1 - log(w)/theta - 1/theta\n    w <- w - f.i/f.prime\n  }\n  u1 <- exp(v1^(1/theta) * log(w))\n  u2 <- exp((1-v1)^(1/theta) * log(w))\n\n  # Get eps1 and eps2 using the inverse of\n  # the Gumbel distribution's cdf:\n  eps1 <- -log(-log(u1))\n  eps2 <- -log(-log(u2))\n  cbind(cor(eps1,eps2),1-all.rhos[j]^2)\n  plot(eps1,eps2,pch=19,col=\"#FF000044\",\n       main=paste(\"rho = \",toString(all.rhos[j]),sep=\"\"),\n       xlab=expression(epsilon[1]),\n       ylab=expression(epsilon[2]),\n       cex.lab=2,cex.main=1.5)\n}"},{"path":"append.html","id":"IRFDELTA","chapter":"5 Appendix","heading":"5.6.2 Computing the covariance matrix of IRF using the delta method","text":"","code":"\nirf.function <- function(THETA){\n  c <- THETA[1]\n  phi <- THETA[2:(p+1)]\n  if(q>0){\n    theta <- c(1,THETA[(1+p+1):(1+p+q)])\n  }else{\n    theta <- 1\n  }\n  sigma <- THETA[1+p+q+1]\n  r <- dim(Matrix.of.Exog)[2] - 1\n  beta <- THETA[(1+p+q+1+1):(1+p+q+1+(r+1))]\n  \n  irf <- sim.arma(0,phi,beta,sigma=sd(Ramey$ED3_TC,na.rm=TRUE),T=60,\n                  y.0=rep(0,length(x$phi)),nb.sim=1,make.IRF=1,\n                  X=NaN,beta=NaN)\n  return(irf)\n}\n\nIRF.0 <- 100*irf.function(x$THETA)\neps <- .00000001\nd.IRF <- NULL\nfor(i in 1:length(x$THETA)){\n  THETA.i <- x$THETA\n  THETA.i[i] <- THETA.i[i] + eps\n  IRF.i <- 100*irf.function(THETA.i)\n  d.IRF <- cbind(d.IRF,\n                 (IRF.i - IRF.0)/eps\n                 )\n}\nmat.var.cov.IRF <- d.IRF %*% x$I %*% t(d.IRF)"},{"path":"append.html","id":"statistical-tables","chapter":"5 Appendix","heading":"5.7 Statistical Tables","text":"Table 5.1: Quantiles \\(\\mathcal{N}(0,1)\\) distribution. \\(\\) \\(b\\) respectively row column number; corresponding cell gives \\(\\mathbb{P}(0<X\\le +b)\\), \\(X \\sim \\mathcal{N}(0,1)\\).Table 5.2: Quantiles Student-\\(t\\) distribution. rows correspond different degrees freedom (\\(\\nu\\), say); columns correspond different probabilities (\\(z\\), say). cell gives \\(q\\) s.t. \\(\\mathbb{P}(-q<X<q)=z\\), \\(X \\sim t(\\nu)\\).Table 5.3: Quantiles \\(\\chi^2\\) distribution. rows correspond different degrees freedom; columns correspond different probabilities.Table 5.4: Quantiles \\(\\mathcal{F}\\) distribution. columns rows correspond different degrees freedom (resp. \\(n_1\\) \\(n_2\\)). different panels correspond different probabilities (\\(\\alpha\\)) corresponding cell gives \\(z\\) s.t. \\(\\mathbb{P}(X \\le z)=\\alpha\\), \\(X \\sim \\mathcal{F}(n_1,n_2)\\).","code":""},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]
