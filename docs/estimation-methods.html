<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 3 Estimation Methods | Micro-Econometrics</title>
<meta name="author" content="Jean-Paul Renne">
<meta name="description" content="This chapter presents three approaches to estimate parametric models: the General Method of Moments (GMM), the Maximum Likelihood approach (ML), and the Bayesian approach. The general context is...">
<meta name="generator" content="bookdown 0.27 with bs4_book()">
<meta property="og:title" content="Chapter 3 Estimation Methods | Micro-Econometrics">
<meta property="og:type" content="book">
<meta property="og:description" content="This chapter presents three approaches to estimate parametric models: the General Method of Moments (GMM), the Maximum Likelihood approach (ML), and the Bayesian approach. The general context is...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 3 Estimation Methods | Micro-Econometrics">
<meta name="twitter:description" content="This chapter presents three approaches to estimate parametric models: the General Method of Moments (GMM), the Maximum Likelihood approach (ML), and the Bayesian approach. The general context is...">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.4.0/transition.js"></script><script src="libs/bs3compat-0.4.0/tabs.js"></script><script src="libs/bs3compat-0.4.0/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
<style type="text/css">
    /* Used with Pandoc 2.11+ new --citeproc when CSL is used */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
        }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
<link rel="stylesheet" href="my-style.css">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Micro-Econometrics</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Micro-Econometrics</a></li>
<li><a class="" href="ChapterLS.html"><span class="header-section-number">1</span> Linear Regressions</a></li>
<li><a class="" href="Panel.html"><span class="header-section-number">2</span> Panel regressions</a></li>
<li><a class="active" href="estimation-methods.html"><span class="header-section-number">3</span> Estimation Methods</a></li>
<li><a class="" href="binary-choice-models.html"><span class="header-section-number">4</span> Binary-choice models</a></li>
<li><a class="" href="multiple-choice-models.html"><span class="header-section-number">5</span> Multiple Choice Models</a></li>
<li><a class="" href="tobit-and-sample-selection-models.html"><span class="header-section-number">6</span> Tobit and sample-selection models</a></li>
<li><a class="" href="models-of-count-data.html"><span class="header-section-number">7</span> Models of Count Data</a></li>
<li><a class="" href="append.html"><span class="header-section-number">8</span> Appendix</a></li>
<li><a class="" href="references.html">References</a></li>
</ul>

        <div class="book-extra">
          
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="estimation-methods" class="section level1" number="3">
<h1>
<span class="header-section-number">3</span> Estimation Methods<a class="anchor" aria-label="anchor" href="#estimation-methods"><i class="fas fa-link"></i></a>
</h1>
<p>This chapter presents three approaches to estimate parametric models: the General Method of Moments (GMM), the Maximum Likelihood approach (ML), and the Bayesian approach. The general context is the following: You observe a sample <span class="math inline">\(\mathbf{y}=\{y_1,\dots,y_n\}\)</span>, you assume that these data have been generated by a model parameterized by <span class="math inline">\({\boldsymbol\theta} \in \mathbb{R}^K\)</span>, and you want to estimate this vector <span class="math inline">\({\boldsymbol\theta}_0\)</span>.</p>
<div id="secGMM" class="section level2" number="3.1">
<h2>
<span class="header-section-number">3.1</span> Generalized Method of Moments (GMM)<a class="anchor" aria-label="anchor" href="#secGMM"><i class="fas fa-link"></i></a>
</h2>
<div id="definition-of-the-gmm-estimator" class="section level3" number="3.1.1">
<h3>
<span class="header-section-number">3.1.1</span> Definition of the GMM estimator<a class="anchor" aria-label="anchor" href="#definition-of-the-gmm-estimator"><i class="fas fa-link"></i></a>
</h3>
<p>We denote by <span class="math inline">\(y_i\)</span> a <span class="math inline">\(p \times 1\)</span> vector of variables; by <span class="math inline">\(\boldsymbol\theta\)</span> an <span class="math inline">\(K \times 1\)</span> vector of parameters, and by <span class="math inline">\(h(y_i;\boldsymbol\theta)\)</span> a continuous <span class="math inline">\(r \times 1\)</span> vector-valued function.</p>
<p>We denote by <span class="math inline">\(\boldsymbol\theta_0\)</span> the true value of <span class="math inline">\(\boldsymbol\theta\)</span> and we assume that <span class="math inline">\(\boldsymbol\theta_0\)</span> satisfies:
<span class="math display">\[
\mathbb{E}[h(y_i;\boldsymbol\theta_0)] = \mathbf{0}.
\]</span></p>
<p>We denote by <span class="math inline">\(\underline{y_i}\)</span> the information contained in the current and past observations of <span class="math inline">\(y_i\)</span>, that is: <span class="math inline">\(\underline{y_i} = \{y_i,y_{i-1},\dots,y_1\}\)</span>. We denote by <span class="math inline">\(g(\underline{y_n};\boldsymbol\theta)\)</span> the sample average of the <span class="math inline">\(h(y_i;\boldsymbol\theta)\)</span> vectors, i.e.:
<span class="math display">\[
g(\underline{y_n};\boldsymbol\theta) = \frac{1}{n} \sum_{i=1}^{n} h(y_i;\boldsymbol\theta).
\]</span></p>
<p>The intuition behind the GMM estimator is the following: choose <span class="math inline">\(\boldsymbol\theta\)</span> so as to make the sample moment as close as possible to their population values, that is 0.</p>
<div class="definition">
<p><span id="def:GMM" class="definition"><strong>Definition 3.1  </strong></span>A GMM estimator of <span class="math inline">\(\boldsymbol\theta_0\)</span> is given by:
<span class="math display">\[
\hat{\boldsymbol\theta}_n = \mbox{argmin}_{\boldsymbol\theta} \quad g(\underline{y_n};\boldsymbol\theta)'\, W_n \, g(\underline{y_n};\boldsymbol\theta),
\]</span>
where <span class="math inline">\(W_n\)</span> is a positive definite matrix (that may depend on <span class="math inline">\(\underline{y_n}\)</span>).</p>
</div>
<p>In the specific case where <span class="math inline">\(K = r\)</span> (the dimension of <span class="math inline">\(\boldsymbol\theta\)</span> is the same as that of <span class="math inline">\(h(y_i;\boldsymbol\theta)\)</span> —or of <span class="math inline">\(g(\underline{y_n};\boldsymbol\theta)\)</span>— then <span class="math inline">\(\hat{\boldsymbol\theta}_n\)</span> satisfies:
<span class="math display">\[
g(\underline{y_n};\hat{\boldsymbol\theta}_n) = \mathbf{0}.
\]</span>
Under regularity and identification conditions, this estimator is consistent, that is <span class="math inline">\(\hat{\boldsymbol\theta}_{n}\)</span> converges towards <span class="math inline">\(\boldsymbol\theta_0\)</span> in probability, which we denote by:
<span class="math display" id="eq:consistGMM">\[\begin{equation}
\mbox{plim}_n\;\hat{\boldsymbol\theta}_{n}= \boldsymbol\theta_0,\quad \mbox{or} \quad\hat{\boldsymbol\theta}_{n} \overset{p}{\rightarrow} \boldsymbol\theta_0,\tag{3.1}
\end{equation}\]</span>
i.e. <span class="math inline">\(\forall \varepsilon&gt;0\)</span>, <span class="math inline">\(\lim_{n \rightarrow \infty} \mathbb{P}(|\hat{\boldsymbol\theta}_{n} - \boldsymbol\theta_0|&gt;\varepsilon) = 0\)</span> (this is Definition <a href="append.html#def:convergenceproba">8.16</a>).</p>
<p>Definition <a href="estimation-methods.html#def:GMM">3.1</a> involves a positive definite matrix <span class="math inline">\(W_n\)</span>. While one can take any positive definite matrix to have consistency (Eq. <a href="estimation-methods.html#eq:consistGMM">(3.1)</a>), it can be shown that he GMM estimator achieves the minimum asymptotic variance when <span class="math inline">\(W_n\)</span> is the inverse of matrix <span class="math inline">\(S\)</span>, the latter being defined by:
<span class="math display">\[
S = Asy.\mathbb{V}ar\left(\sqrt{n}g(\underline{y_n};\hat{\boldsymbol\theta}_n)\right).
\]</span>
In this case, <span class="math inline">\(W_n\)</span> is said to be the <em>optimal weighting matrix</em>.</p>
<p>The intuition behind this result is the same that underlies Generalized Least Squares (see Section <a href="ChapterLS.html#GLS">1.5.2</a>), that is: it is beneficial to use a criterion in which the weights are inversely proportional to the variances of the moments.</p>
<p>If <span class="math inline">\(h(x_i;\boldsymbol\theta_0)\)</span> is not correlated to <span class="math inline">\(h(x_j;\boldsymbol\theta_0)\)</span>, for <span class="math inline">\(i \ne j\)</span>, then we have:
<span class="math display">\[
S = \mathbb{V}ar(h(x_i;\boldsymbol\theta_0)),
\]</span>
which can be approximated by
<span class="math display">\[
\hat{\Gamma}_{0,n}=\frac{1}{n}\sum_{i=1}^{n} h(x_i;\hat{\boldsymbol\theta}_n)h(x_{i};\hat{\boldsymbol\theta}_n)'.
\]</span></p>
<p>In a time series context, we often have correlation between <span class="math inline">\(x_i\)</span> and <span class="math inline">\(x_{i+k}\)</span>, especially for small <span class="math inline">\(k\)</span>’s. In this case, and if the time series <span class="math inline">\(\{y_i\}\)</span> is covariance stationary, then we have:
<span class="math display">\[
S := \sum_{\nu = -\infty}^{\infty} \Gamma_\nu,
\]</span>
where <span class="math inline">\(\Gamma_\nu := \mathbb{E}[h(x_i;\boldsymbol\theta_0) h(x_{i-\nu};\boldsymbol\theta_0)']\)</span>. Matrix <span class="math inline">\(S\)</span> is called the <em>long-run variance</em> of process <span class="math inline">\(\{y_i\}\)</span>.</p>
<p>For <span class="math inline">\(\nu \ge 0\)</span>, let us define <span class="math inline">\(\hat{\Gamma}_{\nu,n}\)</span> by:
<span class="math display">\[
\hat{\Gamma}_{\nu,n} = \frac{1}{n} \sum_{i=\nu + 1}^{n} h(x_i;\hat{\boldsymbol\theta}_n)h(x_{i-\nu};\hat{\boldsymbol\theta}_n)',
\]</span>
then <span class="math inline">\(S\)</span> can be approximated by the <span class="citation">Newey and West (<a href="references.html#ref-Newey_West_1987" role="doc-biblioref">1987</a>)</span> formula:
<span class="math display" id="eq:Shat">\[\begin{equation}
\hat{\Gamma}_{0,n} + \sum_{\nu=1}^{q}\left[1-\frac{\nu}{q+1}\right](\hat{\Gamma}_{\nu,n}+\hat{\Gamma}_{\nu,n}').    \tag{3.2}
\end{equation}\]</span></p>
</div>
<div id="asymptotic-distribution-of-the-gmm-estimator" class="section level3" number="3.1.2">
<h3>
<span class="header-section-number">3.1.2</span> Asymptotic distribution of the GMM estimator<a class="anchor" aria-label="anchor" href="#asymptotic-distribution-of-the-gmm-estimator"><i class="fas fa-link"></i></a>
</h3>
<p>We have:
<span class="math display" id="eq:asymptGMM">\[\begin{equation}
\boxed{\sqrt{n}(\hat{\boldsymbol\theta}_n - \boldsymbol\theta_0) \overset{d}{\rightarrow} \mathcal{N}(0,V),}\tag{3.3}
\end{equation}\]</span>
where <span class="math inline">\(V = (DS^{-1}D')^{-1}\)</span>, with
<span class="math display">\[
D := \left.\mathbb{E}\left(\frac{\partial h(x_i;\boldsymbol\theta)}{\partial \boldsymbol\theta'}\right)\right|_{\boldsymbol\theta = \boldsymbol\theta_0}.
\]</span></p>
<p>Matrix <span class="math inline">\(V\)</span> can be approximated by
<span class="math display" id="eq:VGMM">\[\begin{equation}
\hat{V}_n = (\hat{D}_n\hat{S}_n^{-1}\hat{D}_n')^{-1},\tag{3.4}
\end{equation}\]</span>
where <span class="math inline">\(\hat{S}_n\)</span> is given by Eq. <a href="estimation-methods.html#eq:Shat">(3.2)</a> and
<span class="math display">\[
\hat{D}'_n := \left.\frac{\partial g(\underline{y_n};\boldsymbol\theta)}{\partial \boldsymbol\theta'}\right|_{\boldsymbol\theta = \hat{\boldsymbol\theta}_n}.
\]</span>
In practice, the previous matrix is computed numerically.</p>
</div>
<div id="overidentif" class="section level3" number="3.1.3">
<h3>
<span class="header-section-number">3.1.3</span> Testing hypotheses in the GMM framework<a class="anchor" aria-label="anchor" href="#overidentif"><i class="fas fa-link"></i></a>
</h3>
<p>A first important test is the one concerning the validity of the moment restrictions (Sargan-Hansen test; <span class="citation">Sargan (<a href="references.html#ref-Sargan_1958" role="doc-biblioref">1958</a>)</span> and <span class="citation">Hansen (<a href="references.html#ref-Hansen_1982" role="doc-biblioref">1982</a>)</span>). Assume that the number of restrictions imposed is larger than the number of parameters to estimate (<span class="math inline">\(r&gt;K\)</span>). In this case, the restrictions are said to be over-identifiying.</p>
<p>Under correct specification, we asymptotically have:
<span class="math display">\[
\sqrt{n}g(\underline{y_n};{\boldsymbol\theta}_0)  \sim \mathcal{N}(0,S).
\]</span>
As a result, it comes that:
<span class="math display" id="eq:HansenSargan">\[\begin{equation}
J_n = \left(\sqrt{n}g(\underline{y_n};{\boldsymbol\theta}_0)\right)'S^{-1}\left(\sqrt{n}g(\underline{y_n};{\boldsymbol\theta}_0)\right) \tag{3.5}
\end{equation}\]</span>
asymptotically follows a <span class="math inline">\(\chi^2\)</span> distribution. The number of degrees of freedom is equal to <span class="math inline">\(r-K\)</span>. (Note that, for <span class="math inline">\(r=K\)</span>, we have, as expected, <span class="math inline">\(J=0\)</span>.) That is, asymptotically:
<span class="math display">\[
J_n \sim \chi^2(r-K).
\]</span>
The GMM framework also allows to easily test linear restrictions on the parameters. First, given Eq. <a href="estimation-methods.html#eq:asymptGMM">(3.3)</a>, Wald tests (see Eq. <a href="ChapterLS.html#eq:W1">(1.13)</a> in Section <a href="ChapterLS.html#Ftest">1.2.5</a>) are readily available. Second, one can also resort to a test equivalent to the <em>likelihood ratio tests</em> (see Definition <a href="estimation-methods.html#def:LR">3.8</a>). More precisely, consider an unconstrained model and a constrained version of this model, the number of restrictions being equal to <span class="math inline">\(k\)</span>. If the two models are estimated by considering the same moment constraints, and the same weighting matrix —using Eq. <a href="estimation-methods.html#eq:VGMM">(3.4)</a>, based on the unrestricted model—, then we have that:
<span class="math display">\[
n \left[(g(\underline{y_n};\hat{{\boldsymbol\theta}}^*_n)-g(\underline{y_n};\hat{{\boldsymbol\theta}}_n)\right] \sim \chi^2(k),
\]</span>
where <span class="math inline">\(\hat{{\boldsymbol\theta}}^*_n\)</span> is the constrained estimate of <span class="math inline">\({\boldsymbol\theta}_0\)</span>.</p>
</div>
<div id="example-estimation-of-the-stochastic-discount-factor-s.d.f." class="section level3" number="3.1.4">
<h3>
<span class="header-section-number">3.1.4</span> Example: Estimation of the Stochastic Discount Factor (s.d.f.)<a class="anchor" aria-label="anchor" href="#example-estimation-of-the-stochastic-discount-factor-s.d.f."><i class="fas fa-link"></i></a>
</h3>
<p>Under the no-arbitrage assumption, there exists a random variable <span class="math inline">\(\mathcal{M}_{t,t+1}\)</span> (a s.d.f.) such that
<span class="math display">\[
\mathbb{E}_t(\mathcal{M}_{t,t+1}R_{t+1})=1
\]</span>
for any (gross) asset return <span class="math inline">\(R_t\)</span>. In the following, <span class="math inline">\(R_t\)</span> denotes a <span class="math inline">\(n_r\)</span>-dimensional vector of gross returns.</p>
<p>We consider the following specification of the s.d.f.:
<span class="math display" id="eq:sdf">\[\begin{equation}
\mathcal{M}_{t,t+1} = 1 - \textbf{b}_M'(F_{t+1} - \mathbb{E}_t(F_{t+1})), \tag{3.6}
\end{equation}\]</span>
where <span class="math inline">\(F_t\)</span> is a vector of factors. Eq. <a href="estimation-methods.html#eq:sdf">(3.6)</a> then reads:
<span class="math display">\[
\mathbb{E}_t([1 - \textbf{b}_M'(F_{t+1} - \mathbb{E}_t(F_{t+1}))]R_{t+1})=1.
\]</span></p>
<p>Assume that the date-<span class="math inline">\(t\)</span> information set is <span class="math inline">\(\mathcal{I}_t=\{\textbf{z}_t,\mathcal{I}_{t-1}\}\)</span>, where <span class="math inline">\(\textbf{z}_t\)</span> is a vector of variables observed on date <span class="math inline">\(t\)</span>. (We then have <span class="math inline">\(\mathbb{E}_t(\bullet) \equiv \mathbb{E}(\bullet|\mathcal{I}_t)\)</span>.)</p>
<p>We can use <span class="math inline">\(\textbf{z}_t\)</span> as an instrument. Indeed, we have:
<span class="math display" id="eq:momF">\[\begin{eqnarray}
&amp;&amp;\mathbb{E}(z_{i,t} [\textbf{b}_M'\{F_{t+1} - \mathbb{E}_t(F_{t+1})\}R_{t+1}-R_{t+1}+1]) \nonumber \\
&amp;=&amp;\mathbb{E}(\mathbb{E}_t\{z_{i,t} [\textbf{b}_M'\{F_{t+1} - \mathbb{E}_t(F_{t+1})\}R_{t+1}-R_{t+1}+1]\})\nonumber\\
&amp;=&amp;\mathbb{E}(z_{i,t} \underbrace{\mathbb{E}_t\{\textbf{b}_M'\{F_{t+1} - \mathbb{E}_t(F_{t+1})\}R_{t+1}-R_{t+1}+1\}}_{1 - \mathbb{E}_t(\mathcal{M}_{t,t+1}R_{t+1})=0})=0.\tag{3.7}
\end{eqnarray}\]</span>
We have then converted a conditional moment condition into a unconditional one (which we need to implement the GMM approach described above). However, at that stage, we can still not directly use the GMM formulas because of the conditional expectation <span class="math inline">\(\mathbb{E}_t(F_{t+1})\)</span> that appears in <span class="math inline">\(\mathbb{E}(z_{i,t} [\textbf{b}_M'\{F_{t+1} - \mathbb{E}_t(F_{t+1})\}R_{t+1}-R_{t+1}+1])=0\)</span>.</p>
<p>To go further, let us assume that:
<span class="math display">\[
\mathbb{E}_t(F_{t+1}) = \textbf{b}_F \textbf{z}_t.
\]</span>
We can then easily estimate matrix <span class="math inline">\(\textbf{b}_F\)</span> (of dimension <span class="math inline">\(n_F \times n_z\)</span>) by OLS. Note here that these OLS can be seen as a special GMM case. Indeed, as was done in Eq. <a href="estimation-methods.html#eq:momF">(3.7)</a>, we can show that, for the <span class="math inline">\(j^{th}\)</span> component of <span class="math inline">\(F_t\)</span>, we have:
<span class="math display">\[
\mathbb{E}( [F_{j,t+1} - \textbf{b}_{F,j} \textbf{z}_t]\textbf{z}_{t})=0,
\]</span>
where <span class="math inline">\(\textbf{b}_{F,j}\)</span> denotes the <span class="math inline">\(j^{th}\)</span> row of <span class="math inline">\(\textbf{b}_{F}\)</span>. This yields the OLS formula.</p>
<p>Equipped with <span class="math inline">\(\textbf{b}_F\)</span>, we rely on the following moment restrictions to estimate <span class="math inline">\(\textbf{b}_M\)</span>:
<span class="math display">\[
\mathbb{E}(z_{i,t} [\textbf{b}_M'\{F_{t+1} - \textbf{b}_F \textbf{z}_t\}R_{t+1}-R_{t+1}+1])=0.
\]</span>
Specifically, the number of restrictions is <span class="math inline">\(n_R \times n_z\)</span>. Let us implement this approach in the U.S. context, using data extracted from the <a href="https://fred.stlouisfed.org">FRED database</a>. In factor <span class="math inline">\(F_t\)</span>, we use the changes in the VIX and in the personal consumption expenditures. The returns (<span class="math inline">\(R_t\)</span>) are based on the Wilshire 5000 Price Index (a stock price index) and on the ICE BofA BBB US Corporate Index Total Return Index (a bond return index).</p>
<div class="sourceCode" id="cb75"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/sboysel/fredr">fredr</a></span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/fredr/man/fredr-key.html">fredr_set_key</a></span><span class="op">(</span><span class="st">"df65e14c054697a52b4511e77fcfa1f3"</span><span class="op">)</span></span>
<span><span class="va">start_date</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/zoo/man/yearmon.html">as.Date</a></span><span class="op">(</span><span class="st">"1990-01-01"</span><span class="op">)</span>; <span class="va">end_date</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/zoo/man/yearmon.html">as.Date</a></span><span class="op">(</span><span class="st">"2022-01-01"</span><span class="op">)</span></span>
<span><span class="va">f</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">ticker</span><span class="op">)</span><span class="op">{</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/pkg/fredr/man/fredr.html">fredr</a></span><span class="op">(</span>series_id <span class="op">=</span> <span class="va">ticker</span>,</span>
<span>        observation_start <span class="op">=</span> <span class="va">start_date</span>,observation_end <span class="op">=</span> <span class="va">end_date</span>,</span>
<span>        frequency <span class="op">=</span> <span class="st">"m"</span>,aggregation_method <span class="op">=</span> <span class="st">"avg"</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span><span class="va">vix</span> <span class="op">&lt;-</span> <span class="fu">f</span><span class="op">(</span><span class="st">"VIXCLS"</span><span class="op">)</span> <span class="co"># VIX</span></span>
<span><span class="va">pce</span> <span class="op">&lt;-</span> <span class="fu">f</span><span class="op">(</span><span class="st">"PCE"</span><span class="op">)</span> <span class="co"># Personal consumption expenditures</span></span>
<span><span class="va">sto</span> <span class="op">&lt;-</span> <span class="fu">f</span><span class="op">(</span><span class="st">"WILL5000PRFC"</span><span class="op">)</span> <span class="co"># Wilshire 5000 Full Cap Price Index</span></span>
<span><span class="va">bdr</span> <span class="op">&lt;-</span> <span class="fu">f</span><span class="op">(</span><span class="st">"BAMLCC0A4BBBTRIV"</span><span class="op">)</span> <span class="co"># ICE BofA BBB US Corp. Index Tot. Return</span></span>
<span><span class="cn">T</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/dim.html">dim</a></span><span class="op">(</span><span class="va">vix</span><span class="op">)</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span></span>
<span><span class="va">dvix</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">vix</span><span class="op">$</span><span class="va">value</span><span class="op">[</span><span class="fl">3</span><span class="op">:</span><span class="cn">T</span><span class="op">]</span><span class="op">/</span><span class="va">vix</span><span class="op">$</span><span class="va">value</span><span class="op">[</span><span class="fl">2</span><span class="op">:</span><span class="op">(</span><span class="cn">T</span><span class="op">-</span><span class="fl">1</span><span class="op">)</span><span class="op">]</span><span class="op">)</span> <span class="co"># change in VIX t+1</span></span>
<span><span class="va">dpce</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">pce</span><span class="op">$</span><span class="va">value</span><span class="op">[</span><span class="fl">3</span><span class="op">:</span><span class="cn">T</span><span class="op">]</span><span class="op">/</span><span class="va">pce</span><span class="op">$</span><span class="va">value</span><span class="op">[</span><span class="fl">2</span><span class="op">:</span><span class="op">(</span><span class="cn">T</span><span class="op">-</span><span class="fl">1</span><span class="op">)</span><span class="op">]</span><span class="op">)</span> <span class="co"># change in PCE t+1</span></span>
<span><span class="va">dsto</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">sto</span><span class="op">$</span><span class="va">value</span><span class="op">[</span><span class="fl">3</span><span class="op">:</span><span class="cn">T</span><span class="op">]</span><span class="op">/</span><span class="va">sto</span><span class="op">$</span><span class="va">value</span><span class="op">[</span><span class="fl">2</span><span class="op">:</span><span class="op">(</span><span class="cn">T</span><span class="op">-</span><span class="fl">1</span><span class="op">)</span><span class="op">]</span><span class="op">)</span> <span class="co"># return t+1</span></span>
<span><span class="va">dbdr</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">bdr</span><span class="op">$</span><span class="va">value</span><span class="op">[</span><span class="fl">3</span><span class="op">:</span><span class="cn">T</span><span class="op">]</span><span class="op">/</span><span class="va">bdr</span><span class="op">$</span><span class="va">value</span><span class="op">[</span><span class="fl">2</span><span class="op">:</span><span class="op">(</span><span class="cn">T</span><span class="op">-</span><span class="fl">1</span><span class="op">)</span><span class="op">]</span><span class="op">)</span> <span class="co"># return t+1</span></span>
<span><span class="va">dvix_1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">vix</span><span class="op">$</span><span class="va">value</span><span class="op">[</span><span class="fl">2</span><span class="op">:</span><span class="op">(</span><span class="cn">T</span><span class="op">-</span><span class="fl">1</span><span class="op">)</span><span class="op">]</span><span class="op">/</span><span class="va">vix</span><span class="op">$</span><span class="va">value</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="op">(</span><span class="cn">T</span><span class="op">-</span><span class="fl">2</span><span class="op">)</span><span class="op">]</span><span class="op">)</span> <span class="co"># change in VIX t</span></span>
<span><span class="va">dpce_1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">pce</span><span class="op">$</span><span class="va">value</span><span class="op">[</span><span class="fl">2</span><span class="op">:</span><span class="op">(</span><span class="cn">T</span><span class="op">-</span><span class="fl">1</span><span class="op">)</span><span class="op">]</span><span class="op">/</span><span class="va">pce</span><span class="op">$</span><span class="va">value</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="op">(</span><span class="cn">T</span><span class="op">-</span><span class="fl">2</span><span class="op">)</span><span class="op">]</span><span class="op">)</span> <span class="co"># change in PCE t</span></span>
<span><span class="va">dsto_1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">sto</span><span class="op">$</span><span class="va">value</span><span class="op">[</span><span class="fl">2</span><span class="op">:</span><span class="op">(</span><span class="cn">T</span><span class="op">-</span><span class="fl">1</span><span class="op">)</span><span class="op">]</span><span class="op">/</span><span class="va">sto</span><span class="op">$</span><span class="va">value</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="op">(</span><span class="cn">T</span><span class="op">-</span><span class="fl">2</span><span class="op">)</span><span class="op">]</span><span class="op">)</span> <span class="co"># return t</span></span>
<span><span class="va">dbdr_1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">bdr</span><span class="op">$</span><span class="va">value</span><span class="op">[</span><span class="fl">2</span><span class="op">:</span><span class="op">(</span><span class="cn">T</span><span class="op">-</span><span class="fl">1</span><span class="op">)</span><span class="op">]</span><span class="op">/</span><span class="va">bdr</span><span class="op">$</span><span class="va">value</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="op">(</span><span class="cn">T</span><span class="op">-</span><span class="fl">2</span><span class="op">)</span><span class="op">]</span><span class="op">)</span> <span class="co"># return t</span></span></code></pre></div>
<p>Define the matrices containing the <span class="math inline">\(F_{t+1}\)</span>, <span class="math inline">\(\textbf{z}_t\)</span>, and <span class="math inline">\(R_{t+1}\)</span> vectors:</p>
<div class="sourceCode" id="cb76"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">F_tp1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/cbind.html">cbind</a></span><span class="op">(</span><span class="va">dvix</span>,<span class="va">dpce</span><span class="op">)</span></span>
<span><span class="va">Z</span>     <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/cbind.html">cbind</a></span><span class="op">(</span><span class="fl">1</span>,<span class="va">dvix_1</span>,<span class="va">dpce_1</span>,<span class="va">dsto_1</span>,<span class="va">dbdr_1</span><span class="op">)</span></span>
<span><span class="va">b_F</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/t.html">t</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/pkg/Matrix/man/solve-methods.html">solve</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/t.html">t</a></span><span class="op">(</span><span class="va">Z</span><span class="op">)</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="va">Z</span><span class="op">)</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="fu"><a href="https://rdrr.io/r/base/t.html">t</a></span><span class="op">(</span><span class="va">Z</span><span class="op">)</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="va">F_tp1</span><span class="op">)</span></span>
<span><span class="va">F_innov</span> <span class="op">&lt;-</span> <span class="va">F_tp1</span> <span class="op">-</span> <span class="va">Z</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="fu"><a href="https://rdrr.io/r/base/t.html">t</a></span><span class="op">(</span><span class="va">b_F</span><span class="op">)</span></span>
<span><span class="va">R_tp1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/cbind.html">cbind</a></span><span class="op">(</span><span class="va">dsto</span>,<span class="va">dbdr</span><span class="op">)</span></span>
<span><span class="va">n_F</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/dim.html">dim</a></span><span class="op">(</span><span class="va">F_tp1</span><span class="op">)</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span>; <span class="va">n_R</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/dim.html">dim</a></span><span class="op">(</span><span class="va">R_tp1</span><span class="op">)</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span>; <span class="va">n_z</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/dim.html">dim</a></span><span class="op">(</span><span class="va">Z</span><span class="op">)</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span></span></code></pre></div>
<p>Function <code>f_aux</code> compute the <span class="math inline">\(h(x_t;{\boldsymbol\theta})\)</span> and the <span class="math inline">\(g(\underline{y_T};{\boldsymbol\theta})\)</span>; function <code>f2beMin</code> is the function to be minimized.</p>
<div class="sourceCode" id="cb77"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">f_aux</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">theta</span><span class="op">)</span><span class="op">{</span></span>
<span>  <span class="va">b_M</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span><span class="op">(</span><span class="va">theta</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="va">n_F</span><span class="op">]</span>,ncol<span class="op">=</span><span class="fl">1</span><span class="op">)</span></span>
<span>  <span class="va">R_aux</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span><span class="op">(</span><span class="va">F_innov</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="va">b_M</span>,<span class="cn">T</span><span class="op">-</span><span class="fl">2</span>,<span class="va">n_R</span><span class="op">)</span> <span class="op">*</span> <span class="va">R_tp1</span> <span class="op">-</span> <span class="va">R_tp1</span> <span class="op">+</span> <span class="fl">1</span></span>
<span>  <span class="va">H</span> <span class="op">&lt;-</span> <span class="op">(</span><span class="va">R_aux</span> <span class="op"><a href="https://rdrr.io/r/base/kronecker.html">%x%</a></span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span><span class="op">(</span><span class="fl">1</span>,<span class="fl">1</span>,<span class="va">n_z</span><span class="op">)</span><span class="op">)</span> <span class="op">*</span> <span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span><span class="op">(</span><span class="fl">1</span>,<span class="fl">1</span>,<span class="va">n_R</span><span class="op">)</span> <span class="op"><a href="https://rdrr.io/r/base/kronecker.html">%x%</a></span> <span class="va">Z</span><span class="op">)</span></span>
<span>  <span class="va">g</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/apply.html">apply</a></span><span class="op">(</span><span class="va">H</span>,<span class="fl">2</span>,<span class="va">mean</span><span class="op">)</span>,ncol<span class="op">=</span><span class="fl">1</span><span class="op">)</span></span>
<span>  <span class="kw"><a href="https://rdrr.io/r/base/function.html">return</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span>g<span class="op">=</span><span class="va">g</span>,H<span class="op">=</span><span class="va">H</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span><span class="va">f2beMin</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">theta</span>,<span class="va">W</span><span class="op">)</span><span class="op">{</span><span class="co"># function to be minimized</span></span>
<span>  <span class="va">res</span> <span class="op">&lt;-</span> <span class="fu">f_aux</span><span class="op">(</span><span class="va">theta</span><span class="op">)</span></span>
<span>  <span class="kw"><a href="https://rdrr.io/r/base/function.html">return</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/t.html">t</a></span><span class="op">(</span><span class="va">res</span><span class="op">$</span><span class="va">g</span><span class="op">)</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="va">W</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="va">res</span><span class="op">$</span><span class="va">g</span><span class="op">)</span></span>
<span><span class="op">}</span></span></code></pre></div>
<p>Now, let’s minimize this function, using use the BFGS numerical algorithm (part of the <code>optim</code> wrapper). We run 5 iterations (where <span class="math inline">\(W\)</span> is updated).</p>
<div class="sourceCode" id="cb78"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">AEC</span><span class="op">)</span></span>
<span><span class="va">theta</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="fl">0</span>,<span class="va">n_F</span><span class="op">)</span><span class="op">)</span> <span class="co"># inital value</span></span>
<span><span class="kw">for</span><span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="fl">10</span><span class="op">)</span><span class="op">{</span><span class="co"># recursion on W</span></span>
<span>  <span class="va">res</span> <span class="op">&lt;-</span> <span class="fu">f_aux</span><span class="op">(</span><span class="va">theta</span><span class="op">)</span></span>
<span>  <span class="va">W</span> <span class="op">&lt;-</span>  <span class="fu"><a href="https://rdrr.io/pkg/Matrix/man/solve-methods.html">solve</a></span><span class="op">(</span><span class="fu">NW.LongRunVariance</span><span class="op">(</span><span class="va">res</span><span class="op">$</span><span class="va">H</span>,q<span class="op">=</span><span class="fl">6</span><span class="op">)</span><span class="op">)</span></span>
<span>  <span class="va">res.optim</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/optim.html">optim</a></span><span class="op">(</span><span class="va">theta</span>,<span class="va">f2beMin</span>,W<span class="op">=</span><span class="va">W</span>,</span>
<span>                     method<span class="op">=</span><span class="st">"BFGS"</span>, <span class="co"># could be "Nelder-Mead"</span></span>
<span>                     control<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span>trace<span class="op">=</span><span class="cn">FALSE</span>,maxit<span class="op">=</span><span class="fl">200</span><span class="op">)</span>,hessian<span class="op">=</span><span class="cn">TRUE</span><span class="op">)</span></span>
<span>  <span class="va">theta</span> <span class="op">&lt;-</span> <span class="va">res.optim</span><span class="op">$</span><span class="va">par</span></span>
<span><span class="op">}</span></span></code></pre></div>
<p>Finally, let’s compute the standard deviation of the parameter estimates, using Eq. <a href="estimation-methods.html#eq:VGMM">(3.4)</a>:</p>
<div class="sourceCode" id="cb79"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">eps</span> <span class="op">&lt;-</span> <span class="fl">.0001</span></span>
<span><span class="va">g0</span> <span class="op">&lt;-</span> <span class="fu">f_aux</span><span class="op">(</span><span class="va">theta</span><span class="op">)</span><span class="op">$</span><span class="va">g</span></span>
<span><span class="va">D</span> <span class="op">&lt;-</span> <span class="cn">NULL</span></span>
<span><span class="kw">for</span><span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">theta</span><span class="op">)</span><span class="op">)</span><span class="op">{</span></span>
<span>  <span class="va">theta.i</span> <span class="op">&lt;-</span> <span class="va">theta</span></span>
<span>  <span class="va">theta.i</span><span class="op">[</span><span class="va">i</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="va">theta.i</span><span class="op">[</span><span class="va">i</span><span class="op">]</span> <span class="op">+</span> <span class="va">eps</span></span>
<span>  <span class="va">gi</span> <span class="op">&lt;-</span> <span class="fu">f_aux</span><span class="op">(</span><span class="va">theta.i</span><span class="op">)</span><span class="op">$</span><span class="va">g</span></span>
<span>  <span class="va">D</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/cbind.html">cbind</a></span><span class="op">(</span><span class="va">D</span>,<span class="op">(</span><span class="va">gi</span><span class="op">-</span><span class="va">g0</span><span class="op">)</span><span class="op">/</span><span class="va">eps</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span><span class="va">V</span> <span class="op">&lt;-</span> <span class="fl">1</span><span class="op">/</span><span class="cn">T</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/pkg/Matrix/man/solve-methods.html">solve</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/t.html">t</a></span><span class="op">(</span><span class="va">D</span><span class="op">)</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="va">W</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="va">D</span><span class="op">)</span></span>
<span><span class="va">std.dev</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/diag.html">diag</a></span><span class="op">(</span><span class="va">V</span><span class="op">)</span><span class="op">)</span>;<span class="va">t.stud</span> <span class="op">&lt;-</span> <span class="va">theta</span><span class="op">/</span><span class="va">std.dev</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cbind.html">cbind</a></span><span class="op">(</span><span class="va">theta</span>,<span class="va">std.dev</span>,<span class="va">t.stud</span><span class="op">)</span></span></code></pre></div>
<pre><code>##            theta    std.dev     t.stud
## [1,]  -0.7180716  0.4646617 -1.5453642
## [2,] -11.2042452 17.1039449 -0.6550679</code></pre>
<p>The Hansen statistic can be used to test the model (see Eq. <a href="estimation-methods.html#eq:HansenSargan">(3.5)</a>). If the model is correct, we have:
<span class="math display">\[
T g(\underline{y_T};{\boldsymbol\theta})'\, S^{-1} \, g(\underline{y_T};{\boldsymbol\theta}) \sim \,i.i.d.\,\chi^2(J - K),
\]</span>
where <span class="math inline">\(J\)</span> is the number of moment constraints (<span class="math inline">\(n_z \times n_r\)</span> here) and <span class="math inline">\(K\)</span> is the number of estimated parameters (<span class="math inline">\(=n_F\)</span> here).</p>
<div class="sourceCode" id="cb81"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">g</span> <span class="op">&lt;-</span> <span class="fu">f_aux</span><span class="op">(</span><span class="va">theta</span><span class="op">)</span><span class="op">$</span><span class="va">g</span></span>
<span><span class="va">Hansen_stat</span> <span class="op">&lt;-</span> <span class="cn">T</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/t.html">t</a></span><span class="op">(</span><span class="va">g</span><span class="op">)</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="va">W</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="va">g</span></span>
<span><span class="va">pvalue</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Chisquare.html">pchisq</a></span><span class="op">(</span>q <span class="op">=</span> <span class="va">Hansen_stat</span>,df <span class="op">=</span> <span class="va">n_R</span><span class="op">*</span><span class="va">n_z</span> <span class="op">-</span> <span class="va">n_F</span><span class="op">)</span></span>
<span><span class="va">pvalue</span></span></code></pre></div>
<pre><code>##           [,1]
## [1,] 0.8789782</code></pre>
</div>
</div>
<div id="secMLE" class="section level2" number="3.2">
<h2>
<span class="header-section-number">3.2</span> Maximum Likelihood Estimation<a class="anchor" aria-label="anchor" href="#secMLE"><i class="fas fa-link"></i></a>
</h2>
<div id="intuition" class="section level3" number="3.2.1">
<h3>
<span class="header-section-number">3.2.1</span> Intuition<a class="anchor" aria-label="anchor" href="#intuition"><i class="fas fa-link"></i></a>
</h3>
<p>Intuitively, the <em>Maximum Likelihood Estimation (MLE)</em> consists in looking for the value of <span class="math inline">\({\boldsymbol\theta}\)</span> that is such that the probability of having observed <span class="math inline">\(\mathbf{y}\)</span> (the sample at hand) is the highest possible.</p>
<p>To set an example, assume that the time periods between the arrivals of two customers in a shop, denoted by <span class="math inline">\(y_i\)</span>, are i.i.d. and follow an exponential distribution, i.e. <span class="math inline">\(y_i \sim \,i.i.d.\, \mathcal{E}(\lambda)\)</span>. You have observed these arrivals for some time, thereby constituting a sample <span class="math inline">\(\mathbf{y}=\{y_1,\dots,y_n\}\)</span>. You want to estimate <span class="math inline">\(\lambda\)</span> (i.e. in that case, the vector of parameters is simply <span class="math inline">\({\boldsymbol\theta} = \lambda\)</span>).</p>
<p>The density of <span class="math inline">\(Y\)</span> (one observation) is <span class="math inline">\(f(y;\lambda) = \dfrac{1}{\lambda}\exp(-y/\lambda)\)</span>. Fig. <a href="estimation-methods.html#fig:MLE1">3.1</a> represents such density functions for different values of <span class="math inline">\(\lambda\)</span>.</p>
<p>Your 200 observations are reported at the bottom of Fig. <a href="estimation-methods.html#fig:MLE1">3.1</a> (red bars). You build the histogram and display it on the same chart.</p>
<div class="figure" style="text-align: left-aligned">
<span style="display:block;" id="fig:MLE1"></span>
<img src="MicroEc_files/figure-html/MLE1-1.png" alt="The red ticks, at the bottom, indicate observations (there are 200 of them). The historgram is based on these 200 observations" width="95%"><p class="caption">
Figure 3.1: The red ticks, at the bottom, indicate observations (there are 200 of them). The historgram is based on these 200 observations
</p>
</div>
<p>What is your estimate of <span class="math inline">\(\lambda\)</span>? Intuitively, one is led to take the <span class="math inline">\(\lambda\)</span> for which the (theoretical) distribution is the closest to the histogram (that can be seen as an “empirical distribution”). This approach is consistent with the idea of picking the <span class="math inline">\(\lambda\)</span> for which the probability of observing the values included in <span class="math inline">\(\mathbf{y}\)</span> is the highest.</p>
<p>Let us be more formal. Assume that you have only four observations: <span class="math inline">\(y_1=1.1\)</span>, <span class="math inline">\(y_2=2.2\)</span>, <span class="math inline">\(y_3=0.7\)</span> and <span class="math inline">\(y_4=5.0\)</span>. What was the probability of jointly observing:</p>
<ul>
<li>
<span class="math inline">\(1.1-\varepsilon \le Y_1 &lt; 1.1+\varepsilon\)</span>,</li>
<li>
<span class="math inline">\(2.2-\varepsilon \le Y_2 &lt; 2.2+\varepsilon\)</span>,</li>
<li>
<span class="math inline">\(0.7-\varepsilon \le Y_3 &lt; 0.7+\varepsilon\)</span>, and</li>
<li>
<span class="math inline">\(5.0-\varepsilon \le Y_4 &lt; 5.0+\varepsilon\)</span>?</li>
</ul>
<p>Because the <span class="math inline">\(y_i\)</span>’s are i.i.d., this probability is <span class="math inline">\(\prod_{i=1}^4(2\varepsilon f(y_i,\lambda))\)</span>.
The next plot shows the probability (divided by <span class="math inline">\(16\varepsilon^4\)</span>, which does not depend on <span class="math inline">\(\lambda\)</span>) as a function of <span class="math inline">\(\lambda\)</span>.</p>
<div class="figure" style="text-align: left-aligned">
<span style="display:block;" id="fig:MLE2"></span>
<img src="MicroEc_files/figure-html/MLE2-1.png" alt="Proba. that $y_i-\varepsilon \le Y_i &lt; y_i+\varepsilon$, $i \in \{1,2,3,4\}$. The vertical red line indicates the maximum of the function." width="95%"><p class="caption">
Figure 3.2: Proba. that <span class="math inline">\(y_i-\varepsilon \le Y_i &lt; y_i+\varepsilon\)</span>, <span class="math inline">\(i \in \{1,2,3,4\}\)</span>. The vertical red line indicates the maximum of the function.
</p>
</div>
<p>The value of <span class="math inline">\(\lambda\)</span> that maximizes the probability is 2.26.</p>
<p>Let us come back to the example with 200 observations:</p>
<div class="figure" style="text-align: left-aligned">
<span style="display:block;" id="fig:MLE3"></span>
<img src="MicroEc_files/figure-html/MLE3-1.png" alt="Log-likelihood function associated with the 200 i.i.d. observations. The vertical red line indicates the maximum of the function." width="95%"><p class="caption">
Figure 3.3: Log-likelihood function associated with the 200 i.i.d. observations. The vertical red line indicates the maximum of the function.
</p>
</div>
<p>In that case, the value of <span class="math inline">\(\lambda\)</span> that maxmimizes the probability is 3.42.</p>
</div>
<div id="definition-and-properties" class="section level3" number="3.2.2">
<h3>
<span class="header-section-number">3.2.2</span> Definition and properties<a class="anchor" aria-label="anchor" href="#definition-and-properties"><i class="fas fa-link"></i></a>
</h3>
<p><span class="math inline">\(f(y;\boldsymbol\theta)\)</span> denotes the probability density function (p.d.f.) of a random variable <span class="math inline">\(Y\)</span> which depends on a set of parameters <span class="math inline">\(\boldsymbol\theta\)</span>. The density of <span class="math inline">\(n\)</span> independent and identically distributed (i.i.d.) observations of <span class="math inline">\(Y\)</span> is given by:
<span class="math display">\[
f(\mathbf{y};\boldsymbol\theta) = \prod_{i=1}^n f(y_i;\boldsymbol\theta),
\]</span>
where <span class="math inline">\(\mathbf{y}\)</span> denotes the vector of observations; <span class="math inline">\(\mathbf{y} = \{y_1,\dots,y_n\}\)</span>.</p>
<div class="definition">
<p><span id="def:likelihood" class="definition"><strong>Definition 3.2  (Likelihood function) </strong></span>The likelihood function is:
<span class="math display">\[
\mathcal{L}: \boldsymbol\theta \rightarrow  \mathcal{L}(\boldsymbol\theta;\mathbf{y})=f(\mathbf{y};\boldsymbol\theta)=f(y_1,\dots,y_n;\boldsymbol\theta).
\]</span></p>
</div>
<p>We often work with <span class="math inline">\(\log \mathcal{L}\)</span>, the <strong>log-likelihood function</strong>.</p>
<div class="example">
<p><span id="exm:normal" class="example"><strong>Example 3.1  (Gaussian distribution) </strong></span>If <span class="math inline">\(y_i \sim \mathcal{N}(\mu,\sigma^2)\)</span>, then
<span class="math display">\[
\log \mathcal{L}(\boldsymbol\theta;\mathbf{y}) = - \frac{1}{2}\sum_{i=1}^n\left( \log \sigma^2 + \log 2\pi + \frac{(y_i-\mu)^2}{\sigma^2} \right).
\]</span></p>
</div>
<div class="definition">
<p><span id="def:score" class="definition"><strong>Definition 3.3  (Score) </strong></span>The score <span class="math inline">\(S(y;\boldsymbol\theta)\)</span> is given by <span class="math inline">\(\frac{\partial \log f(y;\boldsymbol\theta)}{\partial \boldsymbol\theta}\)</span>.</p>
</div>
<p>If <span class="math inline">\(y_i \sim \mathcal{N}(\mu,\sigma^2)\)</span> (Example <a href="estimation-methods.html#exm:normal">3.1</a>), then
<span class="math display">\[
\frac{\partial \log f(y;\boldsymbol\theta)}{\partial \boldsymbol\theta} =
\left[\begin{array}{c}
\dfrac{\partial \log f(y;\boldsymbol\theta)}{\partial \mu}\\
\dfrac{\partial \log f(y;\boldsymbol\theta)}{\partial \sigma^2}
\end{array}\right] =
\left[\begin{array}{c}
\dfrac{y-\mu}{\sigma^2}\\
\frac{1}{2\sigma^2}\left(\frac{(y-\mu)^2}{\sigma^2}-1\right)
\end{array}\right].
\]</span></p>
<div class="proposition">
<p><span id="prp:score" class="proposition"><strong>Proposition 3.1  (Score expectation) </strong></span>The expectation of the score is zero.</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-16" class="proof"><em>Proof</em>. </span>We have:
<span class="math display">\[\begin{eqnarray*}
\mathbb{E}\left(\frac{\partial \log f(Y;\boldsymbol\theta)}{\partial \boldsymbol\theta}\right) &amp;=&amp;
\int \frac{\partial \log f(y;\boldsymbol\theta)}{\partial \boldsymbol\theta} f(y;\boldsymbol\theta) dy \\
&amp;=&amp; \int \frac{\partial f(y;\boldsymbol\theta)/\partial \boldsymbol\theta}{f(y;\boldsymbol\theta)} f(y;\boldsymbol\theta) dy =
\frac{\partial}{\partial \boldsymbol\theta} \int f(y;\boldsymbol\theta) dy\\
&amp;=&amp;\partial 1 /\partial \boldsymbol\theta = 0,
\end{eqnarray*}\]</span>
which gives the result.</p>
</div>
<div class="definition">
<p><span id="def:Fisher" class="definition"><strong>Definition 3.4  (Fisher information matrix) </strong></span>The information matrix is (minus) the the expectation of the second derivatives of the log-likelihood function:
<span class="math display">\[
\mathcal{I}_Y(\boldsymbol\theta) = - \mathbb{E} \left( \frac{\partial^2 \log f(Y;\boldsymbol\theta)}{\partial \boldsymbol\theta \partial \boldsymbol\theta'} \right).
\]</span></p>
</div>
<div class="proposition">
<p><span id="prp:Fisher" class="proposition"><strong>Proposition 3.2  </strong></span>We have
<span class="math display">\[
\mathcal{I}_Y(\boldsymbol\theta) = \mathbb{E} \left[ \left( \frac{\partial \log f(Y;\boldsymbol\theta)}{\partial \boldsymbol\theta} \right)
\left( \frac{\partial \log f(Y;\boldsymbol\theta)}{\partial \boldsymbol\theta} \right)' \right] = \mathbb{V}ar[S(Y;\boldsymbol\theta)].
\]</span></p>
</div>
<div class="proof">
<p><span id="unlabeled-div-17" class="proof"><em>Proof</em>. </span>We have <span class="math inline">\(\frac{\partial^2 \log f(Y;\boldsymbol\theta)}{\partial \boldsymbol\theta \partial \boldsymbol\theta'} = \frac{\partial^2 f(Y;\boldsymbol\theta)}{\partial \boldsymbol\theta \partial \boldsymbol\theta'}\frac{1}{f(Y;\boldsymbol\theta)} - \frac{\partial \log f(Y;\boldsymbol\theta)}{\partial \boldsymbol\theta}\frac{\partial \log f(Y;\boldsymbol\theta)}{\partial \boldsymbol\theta'}\)</span>. The expectation of the first right-hand side term is <span class="math inline">\(\partial^2 1 /(\partial \boldsymbol\theta \partial \boldsymbol\theta') = \mathbf{0}\)</span>, which gives the result.</p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-18" class="example"><strong>Example 3.2  </strong></span>If <span class="math inline">\(y_i \sim\,i.i.d.\, \mathcal{N}(\mu,\sigma^2)\)</span>, let <span class="math inline">\(\boldsymbol\theta = [\mu,\sigma^2]'\)</span> then
<span class="math display">\[
\frac{\partial \log f(y;\boldsymbol\theta)}{\partial \boldsymbol\theta} = \left[\frac{y-\mu}{\sigma^2} \quad \frac{1}{2\sigma^2}\left(\frac{(y-\mu)^2}{\sigma^2}-1\right) \right]',
\]</span>
and
<span class="math display">\[
\mathcal{I}_Y(\boldsymbol\theta) = \mathbb{E}\left( \frac{1}{\sigma^4}
\left[
\begin{array}{cc}
\sigma^2&amp;y-\mu\\
y-\mu &amp; \frac{(y-\mu)^2}{\sigma^2}-\frac{1}{2}
\end{array}\right]
\right)=
\left[
\begin{array}{cc}
1/\sigma^2&amp;0\\
0 &amp; 1/(2\sigma^4)
\end{array}\right].
\]</span></p>
</div>
<div class="proposition">
<p><span id="prp:additiv" class="proposition"><strong>Proposition 3.3  (Additive property of the Information matrix) </strong></span>The information matrix resulting from two independent experiments is the sum of the information matrices:
<span class="math display">\[
\mathcal{I}_{X,Y}(\boldsymbol\theta) = \mathcal{I}_X(\boldsymbol\theta) + \mathcal{I}_Y(\boldsymbol\theta).
\]</span></p>
</div>
<div class="proof">
<p><span id="unlabeled-div-19" class="proof"><em>Proof</em>. </span>Directly deduced from the definition of the information matrix (Def. <a href="estimation-methods.html#def:Fisher">3.4</a>), using that the epxectation of a product of independent variables is the product of the expectations.</p>
</div>
<div class="theorem">
<p><span id="thm:FDCR" class="theorem"><strong>Theorem 3.1  (Frechet-Darmois-Cramer-Rao bound) </strong></span>Consider an unbiased estimator of <span class="math inline">\(\boldsymbol\theta\)</span> denoted by <span class="math inline">\(\hat{\boldsymbol\theta}(Y)\)</span>. The variance of the random variable <span class="math inline">\(\boldsymbol\omega'\hat{\boldsymbol\theta}\)</span> (which is a linear combination of the components of <span class="math inline">\(\hat{\boldsymbol\theta}\)</span>) is larger than:
<span class="math display">\[
(\boldsymbol\omega'\boldsymbol\omega)^2/(\boldsymbol\omega' \mathcal{I}_Y(\boldsymbol\theta) \boldsymbol\omega).
\]</span></p>
</div>
<div class="proof">
<p><span id="unlabeled-div-20" class="proof"><em>Proof</em>. </span>The Cauchy-Schwarz inequality implies that <span class="math inline">\(\sqrt{\mathbb{V}ar(\boldsymbol\omega'\hat{\boldsymbol\theta}(Y))\mathbb{V}ar(\boldsymbol\omega'S(Y;\boldsymbol\theta))} \ge |\boldsymbol\omega'\mathbb{C}ov[\hat{\boldsymbol\theta}(Y),S(Y;\boldsymbol\theta)]\boldsymbol\omega |\)</span>. Now, <span class="math inline">\(\mathbb{C}ov[\hat{\boldsymbol\theta}(Y),S(Y;\boldsymbol\theta)] = \int_y \hat{\boldsymbol\theta}(y) \frac{\partial \log f(y;\boldsymbol\theta)}{\partial \boldsymbol\theta} f(y;\boldsymbol\theta)dy = \frac{\partial}{\partial \boldsymbol\theta}\int_y \hat{\boldsymbol\theta}(y) f(y;\boldsymbol\theta)dy = \mathbf{I}\)</span> because <span class="math inline">\(\hat{\boldsymbol\theta}\)</span> is unbiased. Therefore <span class="math inline">\(\mathbb{V}ar(\boldsymbol\omega'\hat{\boldsymbol\theta}(Y)) \ge \mathbb{V}ar(\boldsymbol\omega'S(Y;\boldsymbol\theta))^{-1} (\boldsymbol\omega'\boldsymbol\omega)^2\)</span>. Prop. <a href="estimation-methods.html#prp:Fisher">3.2</a> leads to the result.</p>
</div>
<div class="definition">
<p><span id="def:identif" class="definition"><strong>Definition 3.5  (Identifiability) </strong></span>The vector of parameters <span class="math inline">\(\boldsymbol\theta\)</span> is identifiable if, for any other vector <span class="math inline">\(\boldsymbol\theta^*\)</span>:
<span class="math display">\[
\boldsymbol\theta^* \ne \boldsymbol\theta \Rightarrow \mathcal{L}(\boldsymbol\theta^*;\mathbf{y}) \ne \mathcal{L}(\boldsymbol\theta;\mathbf{y}).
\]</span></p>
</div>
<div class="definition">
<p><span id="def:MLEest" class="definition"><strong>Definition 3.6  (Maximum Likelihood Estimator (MLE)) </strong></span>The maximum likelihood estimator (MLE) is the vector <span class="math inline">\(\boldsymbol\theta\)</span> that maximizes the likelihood function. Formally:
<span class="math display" id="eq:MLEestimator">\[\begin{equation}
\boldsymbol\theta_{MLE} = \arg \max_{\boldsymbol\theta} \mathcal{L}(\boldsymbol\theta;\mathbf{y})  = \arg \max_{\boldsymbol\theta} \log \mathcal{L}(\boldsymbol\theta;\mathbf{y}).\tag{3.8}
\end{equation}\]</span></p>
</div>
<div class="definition">
<p><span id="def:likFunction" class="definition"><strong>Definition 3.7  (Likelihood equation) </strong></span>A necessary condition for maximizing the likelihood function (under regularity assumption, see Hypotheses <a href="estimation-methods.html#hyp:MLEregularity">3.1</a>) is:
<span class="math display">\[\begin{equation}
\dfrac{\partial \log \mathcal{L}(\boldsymbol\theta;\mathbf{y})}{\partial \boldsymbol\theta} = \mathbf{0}.
\end{equation}\]</span></p>
</div>
<div class="hypothesis">
<p><span id="hyp:MLEregularity" class="hypothesis"><strong>Hypothesis 3.1  (Regularity assumptions) </strong></span>We have:</p>
<ol style="list-style-type: lower-roman">
<li>
<span class="math inline">\(\boldsymbol\theta \in \Theta\)</span> where <span class="math inline">\(\Theta\)</span> is compact.</li>
<li>
<span class="math inline">\(\boldsymbol\theta_0\)</span> is identified.</li>
<li>The log-likelihood function is continuous in <span class="math inline">\(\boldsymbol\theta\)</span>.</li>
<li>
<span class="math inline">\(\mathbb{E}_{\boldsymbol\theta_0}(\log f(Y;\boldsymbol\theta))\)</span> exists.</li>
<li>The log-likelihood function is such that <span class="math inline">\((1/n)\log\mathcal{L}(\boldsymbol\theta;\mathbf{y})\)</span> converges almost surely to <span class="math inline">\(\mathbb{E}_{\boldsymbol\theta_0}(\log f(Y;\boldsymbol\theta))\)</span>, uniformly in <span class="math inline">\(\boldsymbol\theta \in \Theta\)</span>.</li>
<li>The log-likelihood function is twice continuously differentiable in an open neighborood of <span class="math inline">\(\boldsymbol\theta_0\)</span>.</li>
<li>The matrix <span class="math inline">\(\mathbf{I}(\boldsymbol\theta_0) = - \mathbb{E}_0 \left( \frac{\partial^2 \log \mathcal{L}(\boldsymbol\theta;\mathbf{y})}{\partial \boldsymbol\theta \partial \boldsymbol\theta'}\right)\)</span> —the Fisher Information matrix— exists and is nonsingular.</li>
</ol>
</div>
<div class="proposition">
<p><span id="prp:MLEproperties" class="proposition"><strong>Proposition 3.4  (Properties of MLE) </strong></span>Under regularity conditions (Assumptions <a href="estimation-methods.html#hyp:MLEregularity">3.1</a>), the MLE is:</p>
<ol style="list-style-type: lower-alpha">
<li>
<strong>Consistent</strong>: <span class="math inline">\(\mbox{plim}\; \boldsymbol\theta_{MLE} = {\boldsymbol\theta}_0\)</span> (<span class="math inline">\({\boldsymbol\theta}_0\)</span> is the true vector of parameters).</li>
<li>
<strong>Asymptotically normal</strong>:
<span class="math display" id="eq:normMLE">\[\begin{equation}
\boxed{\sqrt{n}(\boldsymbol\theta_{MLE} - \boldsymbol\theta_{0}) \overset{d}{\rightarrow} \mathcal{N}(0,\mathcal{I}_Y(\boldsymbol\theta_0)^{-1}).} \tag{3.9}
\end{equation}\]</span>
</li>
<li>
<strong>Asymptotically efficient</strong>: <span class="math inline">\(\boldsymbol\theta_{MLE}\)</span> is asymptotically efficient and achieves the Freechet-Darmois-Cramer-Rao lower bound for consistent estimators.</li>
<li>
<strong>Invariant</strong>: The MLE of <span class="math inline">\(g(\boldsymbol\theta_0)\)</span> is <span class="math inline">\(g(\boldsymbol\theta_{MLE})\)</span> if <span class="math inline">\(g\)</span> is a continuous and continuously differentiable function.</li>
</ol>
</div>
<div class="proof">
<p><span id="unlabeled-div-21" class="proof"><em>Proof</em>. </span>See Appendix <a href="append.html#AppendixProof">8.4</a>.</p>
</div>
<p>Since <span class="math inline">\(\mathcal{I}_Y(\boldsymbol\theta_0)=\frac{1}{n}\mathbf{I}(\boldsymbol\theta_0)\)</span>, the asymptotic covariance matrix of the MLE is <span class="math inline">\([\mathbf{I}(\boldsymbol\theta_0)]^{-1}\)</span>, that is:
<span class="math display">\[
[\mathbf{I}(\boldsymbol\theta_0)]^{-1} = \left[- \mathbb{E}_0 \left( \frac{\partial^2 \log \mathcal{L}(\boldsymbol\theta;\mathbf{y})}{\partial \boldsymbol\theta \partial \boldsymbol\theta'}\right) \right]^{-1}.
\]</span>
A direct (analytical) evaluation of this expectation is often out of reach. It can however be estimated by, either:
<span class="math display" id="eq:I2">\[\begin{eqnarray}
\hat{\mathbf{I}}_1^{-1} &amp;=&amp;  \left( - \frac{\partial^2 \log \mathcal{L}({\boldsymbol\theta_{MLE}};\mathbf{y})}{\partial {\boldsymbol\theta} \partial {\boldsymbol\theta}'}\right)^{-1}, \tag{3.10}\\
\hat{\mathbf{I}}_2^{-1} &amp;=&amp;  \left( \sum_{i=1}^n \frac{\partial \log \mathcal{L}({\boldsymbol\theta_{MLE}};y_i)}{\partial {\boldsymbol\theta}} \frac{\partial \log \mathcal{L}({\boldsymbol\theta_{MLE}};y_i)}{\partial {\boldsymbol\theta'}} \right)^{-1}.  \tag{3.11}
\end{eqnarray}\]</span></p>
<p>Asymptotically, we have <span class="math inline">\((\hat{\mathbf{I}}_1^{-1})\hat{\mathbf{I}}_2=Id\)</span>, that is, the two formulas provide the same result.</p>
<p>In case of (suspected) misspecification, one can use the so-called <em>sandwich estimator</em> of the covariance matrix.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;For more details, see, e.g., &lt;a href="https://www.stat.umn.edu/geyer/5601/notes/sand.pdf"&gt;Charles Geyer’s lectures notes&lt;/a&gt;.&lt;/p&gt;'><sup>8</sup></a> This covariance matrix is given by:
<span class="math display" id="eq:III3">\[\begin{equation}
\hat{\mathbf{I}}_3^{-1} = \hat{\mathbf{I}}_2^{-1} \hat{\mathbf{I}}_1 \hat{\mathbf{I}}_2^{-1}.\tag{3.12}
\end{equation}\]</span></p>
</div>
<div id="to-sum-up-mle-in-practice" class="section level3" number="3.2.3">
<h3>
<span class="header-section-number">3.2.3</span> To sum up – MLE in practice<a class="anchor" aria-label="anchor" href="#to-sum-up-mle-in-practice"><i class="fas fa-link"></i></a>
</h3>
<p>To implement MLE, we need:</p>
<ul>
<li>A parametric model (depending on the vector of parameters <span class="math inline">\(\boldsymbol\theta\)</span> whose “true” value is <span class="math inline">\(\boldsymbol\theta_0\)</span>) is specified.</li>
<li>i.i.d. sources of randomness are identified.</li>
<li>The density associated to one observation <span class="math inline">\(y_i\)</span> is computed analytically (as a function of <span class="math inline">\(\boldsymbol\theta\)</span>): <span class="math inline">\(f(y;\boldsymbol\theta)\)</span>.</li>
<li>The log-likelihood is <span class="math inline">\(\log \mathcal{L}(\boldsymbol\theta;\mathbf{y}) = \sum_i \log f(y_i;\boldsymbol\theta)\)</span>.</li>
<li>The MLE estimator results from the optimization problem (this is Eq. <a href="estimation-methods.html#eq:MLEestimator">(3.8)</a>):
<span class="math display">\[\begin{equation}
\boldsymbol\theta_{MLE} = \arg \max_{\boldsymbol\theta} \log \mathcal{L}(\boldsymbol\theta;\mathbf{y}).
\end{equation}\]</span>
</li>
<li>We have: <span class="math inline">\(\boldsymbol\theta_{MLE} \sim \mathcal{N}({\boldsymbol\theta}_0,\mathbf{I}(\boldsymbol\theta_0)^{-1})\)</span>, where <span class="math inline">\(\mathbf{I}(\boldsymbol\theta_0)^{-1}\)</span> is estimated by means of Eq. <a href="estimation-methods.html#eq:III1">(3.10)</a>, Eq. <a href="estimation-methods.html#eq:I2">(3.11)</a>, or Eq. <a href="estimation-methods.html#eq:III3">(3.12)</a>. Most of the time, this computation is numerical.</li>
</ul>
</div>
<div id="example-mle-estimation-of-a-mixture-of-gaussian-distribution" class="section level3" number="3.2.4">
<h3>
<span class="header-section-number">3.2.4</span> Example: MLE estimation of a mixture of Gaussian distribution<a class="anchor" aria-label="anchor" href="#example-mle-estimation-of-a-mixture-of-gaussian-distribution"><i class="fas fa-link"></i></a>
</h3>
<p>Consider the returns of the Swiss Market Index (SMI). Assume that these returns are independently drawn from a mixture of Gaussian distributions. The p.d.f. <span class="math inline">\(f(x;\boldsymbol\theta)\)</span>, with <span class="math inline">\(\boldsymbol\theta = [\mu_1,\sigma_1,\mu_2,\sigma_2,p]'\)</span>, is given by:
<span class="math display">\[
p \frac{1}{\sqrt{2\pi\sigma_1^2}}\exp\left(-\frac{(x - \mu_1)^2}{2\sigma_1^2}\right) + (1-p)\frac{1}{\sqrt{2\pi\sigma_2^2}}\exp\left(-\frac{(x - \mu_2)^2}{2\sigma_2^2}\right).
\]</span>
(See <a href="https://jrenne.shinyapps.io/density/">p.d.f. of mixtures of Gaussian distributions</a>.)</p>
<!-- The maximum likelihood estimate is $\boldsymbol\theta_{MLE}=[0.30,1.40,-1.45,3.61,0.87]'$. -->
<!-- The first two entries of the diagonal of $\hat{\bv{I}}_1^{-1}$ are $0.00528$ and $0.00526$. They are the estimates of $\mathbb{V}ar(\mu_{1,MLE})$ and of $\mathbb{V}ar(\sigma_{1,MLE})$, respectively. -->
<!-- 95\% confidence intervals for $\mu_1$ and $\sigma_1$ are, respectively: -->
<!-- $$ -->
<!-- 0.30 \pm 1.96\underbrace{\sqrt{0.00528}}_{=0.0726} \quad \mbox{ and } \quad 1.40 \pm 1.96\underbrace{\sqrt{0.00526}.}_{=0.0725} -->
<!-- $$ -->
<div class="sourceCode" id="cb83"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">AEC</span><span class="op">)</span>;<span class="fu"><a href="https://rdrr.io/r/utils/data.html">data</a></span><span class="op">(</span><span class="va">smi</span><span class="op">)</span></span>
<span><span class="cn">T</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/dim.html">dim</a></span><span class="op">(</span><span class="va">smi</span><span class="op">)</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span></span>
<span><span class="va">h</span> <span class="op">&lt;-</span> <span class="fl">5</span> <span class="co"># holding period (one week)</span></span>
<span><span class="va">smi</span><span class="op">$</span><span class="va">r</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="cn">NaN</span>,<span class="va">h</span><span class="op">)</span>,</span>
<span>           <span class="fl">100</span><span class="op">*</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">smi</span><span class="op">$</span><span class="va">Close</span><span class="op">[</span><span class="op">(</span><span class="fl">1</span><span class="op">+</span><span class="va">h</span><span class="op">)</span><span class="op">:</span><span class="cn">T</span><span class="op">]</span><span class="op">/</span><span class="va">smi</span><span class="op">$</span><span class="va">Close</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="op">(</span><span class="cn">T</span><span class="op">-</span><span class="va">h</span><span class="op">)</span><span class="op">]</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">indic.dates</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span><span class="op">(</span><span class="fl">1</span>,<span class="cn">T</span>,by<span class="op">=</span><span class="fl">5</span><span class="op">)</span>  <span class="co"># weekly returns</span></span>
<span><span class="va">smi</span> <span class="op">&lt;-</span> <span class="va">smi</span><span class="op">[</span><span class="va">indic.dates</span>,<span class="op">]</span></span>
<span><span class="va">smi</span> <span class="op">&lt;-</span> <span class="va">smi</span><span class="op">[</span><span class="fu"><a href="https://rdrr.io/r/stats/complete.cases.html">complete.cases</a></span><span class="op">(</span><span class="va">smi</span><span class="op">)</span>,<span class="op">]</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>mfrow<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>,<span class="fl">1</span><span class="op">)</span><span class="op">)</span>;<span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>plt<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">.15</span>,<span class="fl">.95</span>,<span class="fl">.1</span>,<span class="fl">.95</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">smi</span><span class="op">$</span><span class="va">Date</span>,<span class="va">smi</span><span class="op">$</span><span class="va">r</span>,type<span class="op">=</span><span class="st">"l"</span>,xlab<span class="op">=</span><span class="st">""</span>,ylab<span class="op">=</span><span class="st">"in percent"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/abline.html">abline</a></span><span class="op">(</span>h<span class="op">=</span><span class="fl">0</span>,col<span class="op">=</span><span class="st">"blue"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/abline.html">abline</a></span><span class="op">(</span>h<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">smi</span><span class="op">$</span><span class="va">r</span>,na.rm <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span><span class="op">+</span><span class="fl">2</span><span class="op">*</span><span class="fu"><a href="https://rdrr.io/r/stats/sd.html">sd</a></span><span class="op">(</span><span class="va">smi</span><span class="op">$</span><span class="va">r</span>,na.rm <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>,lty<span class="op">=</span><span class="fl">3</span>,col<span class="op">=</span><span class="st">"blue"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/abline.html">abline</a></span><span class="op">(</span>h<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">smi</span><span class="op">$</span><span class="va">r</span>,na.rm <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span><span class="op">-</span><span class="fl">2</span><span class="op">*</span><span class="fu"><a href="https://rdrr.io/r/stats/sd.html">sd</a></span><span class="op">(</span><span class="va">smi</span><span class="op">$</span><span class="va">r</span>,na.rm <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>,lty<span class="op">=</span><span class="fl">3</span>,col<span class="op">=</span><span class="st">"blue"</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: left-aligned">
<span style="display:block;" id="fig:smiData"></span>
<img src="MicroEc_files/figure-html/smiData-1.png" alt="Time series of SMI weekly returns (source: Yahoo Finance)." width="95%"><p class="caption">
Figure 3.4: Time series of SMI weekly returns (source: Yahoo Finance).
</p>
</div>
<p>Build the log-likelihood function (fucntion <code>log.f</code>), and use the numerical BFGS algorithm to maximize it (using the <code>optim</code> wrapper):</p>
<div class="sourceCode" id="cb84"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">f</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">theta</span>,<span class="va">y</span><span class="op">)</span><span class="op">{</span> <span class="co"># Likelihood function</span></span>
<span>  <span class="va">mu.1</span> <span class="op">&lt;-</span> <span class="va">theta</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span>; <span class="va">mu.2</span> <span class="op">&lt;-</span> <span class="va">theta</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span></span>
<span>  <span class="va">sigma.1</span> <span class="op">&lt;-</span> <span class="va">theta</span><span class="op">[</span><span class="fl">3</span><span class="op">]</span>; <span class="va">sigma.2</span> <span class="op">&lt;-</span> <span class="va">theta</span><span class="op">[</span><span class="fl">4</span><span class="op">]</span></span>
<span>  <span class="va">p</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">exp</a></span><span class="op">(</span><span class="va">theta</span><span class="op">[</span><span class="fl">5</span><span class="op">]</span><span class="op">)</span><span class="op">/</span><span class="op">(</span><span class="fl">1</span><span class="op">+</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html">exp</a></span><span class="op">(</span><span class="va">theta</span><span class="op">[</span><span class="fl">5</span><span class="op">]</span><span class="op">)</span><span class="op">)</span></span>
<span>  <span class="va">res</span> <span class="op">&lt;-</span> <span class="va">p</span><span class="op">*</span><span class="fl">1</span><span class="op">/</span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="fl">2</span><span class="op">*</span><span class="va">pi</span><span class="op">*</span><span class="va">sigma.1</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span><span class="op">*</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html">exp</a></span><span class="op">(</span><span class="op">-</span><span class="op">(</span><span class="va">y</span><span class="op">-</span><span class="va">mu.1</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span><span class="op">/</span><span class="op">(</span><span class="fl">2</span><span class="op">*</span><span class="va">sigma.1</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="op">(</span><span class="fl">1</span><span class="op">-</span><span class="va">p</span><span class="op">)</span><span class="op">*</span><span class="fl">1</span><span class="op">/</span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="fl">2</span><span class="op">*</span><span class="va">pi</span><span class="op">*</span><span class="va">sigma.2</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span><span class="op">*</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html">exp</a></span><span class="op">(</span><span class="op">-</span><span class="op">(</span><span class="va">y</span><span class="op">-</span><span class="va">mu.2</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span><span class="op">/</span><span class="op">(</span><span class="fl">2</span><span class="op">*</span><span class="va">sigma.2</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span><span class="op">)</span></span>
<span>  <span class="kw"><a href="https://rdrr.io/r/base/function.html">return</a></span><span class="op">(</span><span class="va">res</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span><span class="va">log.f</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">theta</span>,<span class="va">y</span><span class="op">)</span><span class="op">{</span> <span class="co">#log-Likelihood function</span></span>
<span>  <span class="kw"><a href="https://rdrr.io/r/base/function.html">return</a></span><span class="op">(</span><span class="op">-</span><span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="fu">f</span><span class="op">(</span><span class="va">theta</span>,<span class="va">y</span><span class="op">)</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span><span class="va">res.optim</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/optim.html">optim</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span>,<span class="fl">0</span>,<span class="fl">0.5</span>,<span class="fl">1.5</span>,<span class="fl">.5</span><span class="op">)</span>,</span>
<span>                   <span class="va">log.f</span>,</span>
<span>                   y<span class="op">=</span><span class="va">smi</span><span class="op">$</span><span class="va">r</span>,</span>
<span>                   method<span class="op">=</span><span class="st">"BFGS"</span>, <span class="co"># could be "Nelder-Mead"</span></span>
<span>                   control<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span>trace<span class="op">=</span><span class="cn">FALSE</span>,maxit<span class="op">=</span><span class="fl">100</span><span class="op">)</span>,hessian<span class="op">=</span><span class="cn">TRUE</span><span class="op">)</span></span>
<span><span class="va">theta</span> <span class="op">&lt;-</span> <span class="va">res.optim</span><span class="op">$</span><span class="va">par</span></span>
<span><span class="va">theta</span></span></code></pre></div>
<pre><code>## [1]  0.3012379 -1.3167476  1.7715072  4.8197596  1.9454889</code></pre>
<p>Next, compute estimates of the covariance matrix of the MLE (using Eqs. <a href="estimation-methods.html#eq:III1">(3.10)</a>, <a href="estimation-methods.html#eq:I2">(3.11)</a>, and <a href="estimation-methods.html#eq:III3">(3.12)</a>), and compare the three sets of resulting standard deviations for the five estimated paramters:</p>
<div class="sourceCode" id="cb86"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Hessian approach:</span></span>
<span><span class="va">I.1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/Matrix/man/solve-methods.html">solve</a></span><span class="op">(</span><span class="va">res.optim</span><span class="op">$</span><span class="va">hessian</span><span class="op">)</span></span>
<span><span class="co"># Outer-product of gradient approach:</span></span>
<span><span class="va">log.f.0</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="fu">f</span><span class="op">(</span><span class="va">theta</span>,<span class="va">smi</span><span class="op">$</span><span class="va">r</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">epsilon</span> <span class="op">&lt;-</span> <span class="fl">.00000001</span></span>
<span><span class="va">d.log.f</span> <span class="op">&lt;-</span> <span class="cn">NULL</span></span>
<span><span class="kw">for</span><span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">theta</span><span class="op">)</span><span class="op">)</span><span class="op">{</span></span>
<span>  <span class="va">theta.i</span> <span class="op">&lt;-</span> <span class="va">theta</span></span>
<span>  <span class="va">theta.i</span><span class="op">[</span><span class="va">i</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="va">theta.i</span><span class="op">[</span><span class="va">i</span><span class="op">]</span> <span class="op">+</span> <span class="va">epsilon</span></span>
<span>  <span class="va">log.f.i</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="fu">f</span><span class="op">(</span><span class="va">theta.i</span>,<span class="va">smi</span><span class="op">$</span><span class="va">r</span><span class="op">)</span><span class="op">)</span></span>
<span>  <span class="va">d.log.f</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/cbind.html">cbind</a></span><span class="op">(</span><span class="va">d.log.f</span>,</span>
<span>                   <span class="op">(</span><span class="va">log.f.i</span> <span class="op">-</span> <span class="va">log.f.0</span><span class="op">)</span><span class="op">/</span><span class="va">epsilon</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span><span class="va">I.2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/Matrix/man/solve-methods.html">solve</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/t.html">t</a></span><span class="op">(</span><span class="va">d.log.f</span><span class="op">)</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="va">d.log.f</span><span class="op">)</span></span>
<span><span class="co"># Misspecification-robust approach (sandwich formula):</span></span>
<span><span class="va">I.3</span> <span class="op">&lt;-</span> <span class="va">I.1</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="fu"><a href="https://rdrr.io/pkg/Matrix/man/solve-methods.html">solve</a></span><span class="op">(</span><span class="va">I.2</span><span class="op">)</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="va">I.1</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cbind.html">cbind</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/diag.html">diag</a></span><span class="op">(</span><span class="va">I.1</span><span class="op">)</span>,<span class="fu"><a href="https://rdrr.io/r/base/diag.html">diag</a></span><span class="op">(</span><span class="va">I.2</span><span class="op">)</span>,<span class="fu"><a href="https://rdrr.io/r/base/diag.html">diag</a></span><span class="op">(</span><span class="va">I.3</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<pre><code>##             [,1]        [,2]       [,3]
## [1,] 0.003683422 0.003199481 0.00586160
## [2,] 0.226892824 0.194283391 0.38653389
## [3,] 0.005764271 0.002769579 0.01712255
## [4,] 0.194081311 0.047466419 0.83130838
## [5,] 0.092114437 0.040366005 0.31347858</code></pre>
<p>According to the first (respectively third) type of estimate for the covariance matrix, a 95% confidence interval for <span class="math inline">\(\mu_1\)</span> is [0.182, 0.42] (resp. [0.151, 0.451]).</p>
<p>Note that we have not directly estimated parameter <span class="math inline">\(p\)</span> but <span class="math inline">\(\nu = \log(p/(1-p))\)</span> (in such a way that <span class="math inline">\(p = \exp(\nu)/(1+\exp(\nu))\)</span>). In order to get an estimate of the standard deviation of our esitmate of <span class="math inline">\(p\)</span>, we can implement the <strong>Delta method</strong>. This method is based on the fact that, for a function <span class="math inline">\(g\)</span> that is continuous in the neighborhood of <span class="math inline">\(\boldsymbol\theta_0\)</span> and for large <span class="math inline">\(n\)</span>, we have:
<span class="math display" id="eq:DeltaMethod">\[\begin{equation}
\mathbb{V}ar(g(\hat{\boldsymbol\theta}_n)) \approx \frac{\partial g(\hat{\boldsymbol\theta}_n)}{\partial \boldsymbol\theta'}\mathbb{V}ar(\hat{\boldsymbol\theta}_n)\frac{\partial g(\hat{\boldsymbol\theta}_n)'}{\partial \boldsymbol\theta}.\tag{3.13}
\end{equation}\]</span></p>
<div class="sourceCode" id="cb88"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">g</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">theta</span><span class="op">)</span><span class="op">{</span></span>
<span>  <span class="va">mu.1</span> <span class="op">&lt;-</span> <span class="va">theta</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span>; <span class="va">mu.2</span> <span class="op">&lt;-</span> <span class="va">theta</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span></span>
<span>  <span class="va">sigma.1</span> <span class="op">&lt;-</span> <span class="va">theta</span><span class="op">[</span><span class="fl">3</span><span class="op">]</span>; <span class="va">sigma.2</span> <span class="op">&lt;-</span> <span class="va">theta</span><span class="op">[</span><span class="fl">4</span><span class="op">]</span></span>
<span>  <span class="va">p</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">exp</a></span><span class="op">(</span><span class="va">theta</span><span class="op">[</span><span class="fl">5</span><span class="op">]</span><span class="op">)</span><span class="op">/</span><span class="op">(</span><span class="fl">1</span><span class="op">+</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html">exp</a></span><span class="op">(</span><span class="va">theta</span><span class="op">[</span><span class="fl">5</span><span class="op">]</span><span class="op">)</span><span class="op">)</span></span>
<span>  <span class="kw"><a href="https://rdrr.io/r/base/function.html">return</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">mu.1</span>,<span class="va">mu.2</span>,<span class="va">sigma.1</span>,<span class="va">sigma.2</span>,<span class="va">p</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span><span class="co"># Computation of g's gradient around estimated theta:</span></span>
<span><span class="va">eps</span> <span class="op">&lt;-</span> <span class="fl">.00001</span></span>
<span><span class="va">g.theta</span> <span class="op">&lt;-</span> <span class="fu">g</span><span class="op">(</span><span class="va">theta</span><span class="op">)</span></span>
<span><span class="va">g.gradient</span> <span class="op">&lt;-</span> <span class="cn">NULL</span></span>
<span><span class="kw">for</span><span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="fl">5</span><span class="op">)</span><span class="op">{</span></span>
<span>  <span class="va">theta.perturb</span> <span class="op">&lt;-</span> <span class="va">theta</span></span>
<span>  <span class="va">theta.perturb</span><span class="op">[</span><span class="va">i</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="va">theta</span><span class="op">[</span><span class="va">i</span><span class="op">]</span> <span class="op">+</span> <span class="va">eps</span></span>
<span>  <span class="va">g.gradient</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/cbind.html">cbind</a></span><span class="op">(</span><span class="va">g.gradient</span>,<span class="op">(</span><span class="fu">g</span><span class="op">(</span><span class="va">theta.perturb</span><span class="op">)</span><span class="op">-</span><span class="va">g.theta</span><span class="op">)</span><span class="op">/</span><span class="va">eps</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span><span class="va">Var</span> <span class="op">&lt;-</span> <span class="va">g.gradient</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="va">I.3</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="fu"><a href="https://rdrr.io/r/base/t.html">t</a></span><span class="op">(</span><span class="va">g.gradient</span><span class="op">)</span></span>
<span><span class="va">stdv.g.theta</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/diag.html">diag</a></span><span class="op">(</span><span class="va">Var</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">stdv.theta</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/diag.html">diag</a></span><span class="op">(</span><span class="va">I.3</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cbind.html">cbind</a></span><span class="op">(</span><span class="va">theta</span>,<span class="va">stdv.theta</span>,<span class="va">g.theta</span>,<span class="va">stdv.g.theta</span><span class="op">)</span></span></code></pre></div>
<pre><code>##           theta stdv.theta    g.theta stdv.g.theta
## [1,]  0.3012379 0.07656108  0.3012379   0.07656108
## [2,] -1.3167476 0.62171850 -1.3167476   0.62171850
## [3,]  1.7715072 0.13085316  1.7715072   0.13085316
## [4,]  4.8197596 0.91176114  4.8197596   0.91176114
## [5,]  1.9454889 0.55989158  0.8749539   0.06125726</code></pre>
<p>The previous results show that the MLE estimate of <span class="math inline">\(p\)</span> is 0.8749539, and its standard deviation is approximately equal to 0.0612573.</p>
<p>To finish with, let us draw the estimated parametric p.d.f. (the mixture of Gaussian distribution), and compare it to a non-parametric (kernel-based) estimate of this p.d.f. (using function <code>density</code>):</p>
<div class="sourceCode" id="cb90"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">x</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span><span class="op">(</span><span class="op">-</span><span class="fl">5</span>,<span class="fl">5</span>,by<span class="op">=</span><span class="fl">.01</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>plt<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">.1</span>,<span class="fl">.95</span>,<span class="fl">.1</span>,<span class="fl">.95</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">x</span>,<span class="fu">f</span><span class="op">(</span><span class="va">theta</span>,<span class="va">x</span><span class="op">)</span>,type<span class="op">=</span><span class="st">"l"</span>,lwd<span class="op">=</span><span class="fl">2</span>,xlab<span class="op">=</span><span class="st">"returns, in percent"</span>,ylab<span class="op">=</span><span class="st">""</span>,</span>
<span>     ylim<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span>,<span class="fl">1.4</span><span class="op">*</span><span class="fu"><a href="https://rdrr.io/r/base/Extremes.html">max</a></span><span class="op">(</span><span class="fu">f</span><span class="op">(</span><span class="va">theta</span>,<span class="va">x</span><span class="op">)</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/lines.html">lines</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/density.html">density</a></span><span class="op">(</span><span class="va">smi</span><span class="op">$</span><span class="va">r</span><span class="op">)</span>,type<span class="op">=</span><span class="st">"l"</span>,lwd<span class="op">=</span><span class="fl">2</span>,lty<span class="op">=</span><span class="fl">3</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/lines.html">lines</a></span><span class="op">(</span><span class="va">x</span>,<span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">dnorm</a></span><span class="op">(</span><span class="va">x</span>,mean<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">smi</span><span class="op">$</span><span class="va">r</span><span class="op">)</span>,sd <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/sd.html">sd</a></span><span class="op">(</span><span class="va">smi</span><span class="op">$</span><span class="va">r</span><span class="op">)</span><span class="op">)</span>,col<span class="op">=</span><span class="st">"red"</span>,lty<span class="op">=</span><span class="fl">2</span>,lwd<span class="op">=</span><span class="fl">2</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/rug.html">rug</a></span><span class="op">(</span><span class="va">smi</span><span class="op">$</span><span class="va">r</span>,col<span class="op">=</span><span class="st">"blue"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/legend.html">legend</a></span><span class="op">(</span><span class="st">"topleft"</span>,</span>
<span>       <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"Kernel estimate (non-parametric)"</span>,</span>
<span>         <span class="st">"Estimated mixture of Gaussian distr. (MLE, parametric)"</span>,</span>
<span>         <span class="st">"Normal distribution"</span><span class="op">)</span>,</span>
<span>       lty<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">3</span>,<span class="fl">1</span>,<span class="fl">2</span><span class="op">)</span>,lwd<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">2</span><span class="op">)</span>, <span class="co"># line width</span></span>
<span>       col<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"black"</span>,<span class="st">"black"</span>,<span class="st">"red"</span><span class="op">)</span>,pt.bg<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span><span class="op">)</span>,pt.cex <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span><span class="op">)</span>,</span>
<span>       bg<span class="op">=</span><span class="st">"white"</span>,seg.len <span class="op">=</span> <span class="fl">4</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: left-aligned">
<span style="display:block;" id="fig:smidistri"></span>
<img src="MicroEc_files/figure-html/smidistri-1.png" alt="Comparison of different estimates of the distribution of returns." width="95%"><p class="caption">
Figure 3.5: Comparison of different estimates of the distribution of returns.
</p>
</div>
<!-- \begin{figure} -->
<!-- \caption{Density of 5-day returns on SMI index} -->
<!-- \includegraphics[width=.9\linewidth]{../../figures/Figure_kernel_smi.pdf} -->
<!-- \label{fig:illuskernel_smi} -->
<!-- \begin{tiny} -->
<!-- Gaussian kernel, $h=0.5$ (in percent). -->
<!-- The data spans the period from 2 June 2006 to 23 February 2016 at the daily frequency. -->
<!-- Left-hand plot: the blue lines indicates $\mu \pm 2 \sigma$, where $\mu$ is the sample mean of the returns and $\sigma$ is their sample standard deviation. Right-hand plot: the red dotted line is the density $\mathcal{N}(\mu,\sigma^2)$ -->
<!-- \end{tiny} -->
<!-- \end{figure} -->
<!-- \end{exmpl} -->
<!-- \end{scriptsize} -->
<!-- \end{frame} -->
<!-- \begin{frame}{} -->
<!-- \begin{scriptsize} -->
<!-- \addtocounter{exmpl}{-1} -->
<!-- \begin{exmpl}[MLE estim. of a mixture of Gaussian distri. (cont'd)] -->
<!-- \begin{figure} -->
<!-- \caption{Estimated density (vs kernel-based estimate)} -->
<!-- \includegraphics[width=1\linewidth]{../../figures/Figure_kernel_smiMLE.pdf} -->
<!-- \label{fig:MLE1} -->
<!-- \end{figure} -->
<!-- \end{exmpl} -->
<!-- \end{scriptsize} -->
<!-- \end{frame} -->
</div>
<div id="TestMLE" class="section level3" number="3.2.5">
<h3>
<span class="header-section-number">3.2.5</span> Test procedures<a class="anchor" aria-label="anchor" href="#TestMLE"><i class="fas fa-link"></i></a>
</h3>
<p>Suppose we want to test the following parameter restrictions:
<span class="math display">\[\begin{equation}
\boxed{H_0: \underbrace{h(\boldsymbol\theta)}_{r \times 1}=0.}
\end{equation}\]</span></p>
<p>In the context of MLE, three tests are largely used:</p>
<ul>
<li>Likelihood Ratio (LR) test,</li>
<li>Wald (W) test,</li>
<li>Lagrange Multiplier (LM) test.</li>
</ul>
<p>Here is the rationale behind these three tests:<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;An interesting graphical presentation of the tests is proposed in &lt;a href="http://hedibert.org/wp-content/uploads/2014/04/LR-W-LM-Tests-Buse1982.pdf"&gt;Buse (1982)&lt;/a&gt;.&lt;/p&gt;'><sup>9</sup></a></p>
<ul>
<li>LR: If <span class="math inline">\(h(\boldsymbol\theta)=0\)</span>, then imposing this restriction during the estimation (restricted estimator) should not result in a large decrease in the likelihood function (w.r.t the unrestricted estimation).</li>
<li>Wald: If <span class="math inline">\(h(\boldsymbol\theta)=0\)</span>, then <span class="math inline">\(h(\hat{\boldsymbol\theta})\)</span> should not be far from <span class="math inline">\(0\)</span> (even if these restrictions are not imposed during the MLE).</li>
<li>LM: If <span class="math inline">\(h(\boldsymbol\theta)=0\)</span>, then the gradient of the likelihood function should be small when evaluated at the restricted estimator.</li>
</ul>
<p>In terms of implementation, while the LR necessitates to estimate both restricted and unrestricted models, the Wald test requires the estimation of the unrestricted model only, and the LM tests requires the estimation of the restricted model only.</p>
<p>As shown below, the three test statistics associated with these three tests coincide asymptotically. (Therefore, they naturally have the same asymptotic distribution, that are <span class="math inline">\(\chi^2\)</span>.)</p>
<!-- :::{.proof} -->
<!-- (sketch of proof) The restricted estimation could be done by maximizing the Lagrangian function $\log\mathcal{L}(\boldsymbol\theta) + \lambda' h(\boldsymbol\theta)$. The first order conditions associated to $\boldsymbol\theta$ are $(\partial \log\mathcal{L}(\boldsymbol\theta) / \partial \boldsymbol\theta) = - (\partial h(\boldsymbol\theta) / \partial \theta) \lambda$. Under $H_0$, the expectation of the latter term is 0. Besides, the covariance matrix of the score is the information matrix. -->
<!-- ::: -->
<div class="proposition">
<p><span id="prp:Walddistri" class="proposition"><strong>Proposition 3.5  (Asymptotic distribution of the Wald statistic) </strong></span>Under regularity conditions (Assumptions <a href="estimation-methods.html#hyp:MLEregularity">3.1</a>) and under <span class="math inline">\(H_0: h(\boldsymbol\theta)=0\)</span>, the Wald statistic, defined by:
<span class="math display">\[
\boxed{\xi^W = h(\hat{\boldsymbol\theta})' \mathbb{V}ar[h(\hat{\boldsymbol\theta})]^{-1} h(\hat{\boldsymbol\theta}),}
\]</span>
where
<span class="math display" id="eq:varinWald">\[\begin{equation}
\mathbb{V}ar[h(\hat{\boldsymbol\theta})] = \left(\frac{\partial h(\hat{\boldsymbol\theta})}{\partial \boldsymbol\theta'} \right) \mathbb{V}ar[\hat{\boldsymbol\theta}]
\left(\frac{\partial h(\hat{\boldsymbol\theta})'}{\partial \boldsymbol\theta} \right),\tag{3.14}
\end{equation}\]</span>
is asymptotically <span class="math inline">\(\chi^2(r)\)</span>, where the number of degrees of freedom <span class="math inline">\(r\)</span> corresponds to the dimension of <span class="math inline">\(h(\boldsymbol\theta)\)</span>. (Note that Eq. <a href="estimation-methods.html#eq:varinWald">(3.14)</a> is the same as the one used in the Delta method, see Eq. <a href="estimation-methods.html#eq:DeltaMethod">(3.13)</a>.)</p>
<p>The Wald test, defined by the critical region
<span class="math display">\[
\{\xi^W \ge \chi^2_{1-\alpha}(r)\},
\]</span>
where <span class="math inline">\(\chi^2_{1-\alpha}(r)\)</span> denotes the quantile of level <span class="math inline">\(1-\alpha\)</span> of the <span class="math inline">\(\chi^2(r)\)</span> distribution, has asymptotic level <span class="math inline">\(\alpha\)</span> and is consistent.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;See Defs. &lt;a href="append.html#def:asmyptlevel"&gt;8.7&lt;/a&gt; and &lt;a href="append.html#def:asmyptconsisttest"&gt;8.8&lt;/a&gt; for definitions of the asymptotic levels and consistency of tests.&lt;/p&gt;'><sup>10</sup></a></p>
</div>
<div class="proof">
<p><span id="unlabeled-div-22" class="proof"><em>Proof</em>. </span>See Appendix <a href="append.html#AppendixProof">8.4</a>.</p>
</div>
<p>In practice, in Eq. <a href="estimation-methods.html#eq:varinWald">(3.14)</a>, <span class="math inline">\(\mathbb{V}ar[\hat{\boldsymbol\theta}]\)</span> is replaced by an estimate given, e.g., by Eq. <a href="estimation-methods.html#eq:III1">(3.10)</a>, Eq. <a href="estimation-methods.html#eq:I2">(3.11)</a>, or Eq. <a href="estimation-methods.html#eq:III3">(3.12)</a>.</p>
<div class="proposition">
<p><span id="prp:LMdistri" class="proposition"><strong>Proposition 3.6  (Asymptotic distribution of the LM test statistic) </strong></span>Under regularity conditions (Assumptions <a href="estimation-methods.html#hyp:MLEregularity">3.1</a>) and under <span class="math inline">\(H_0: h(\boldsymbol\theta)=0\)</span>, the LM statistic
<span class="math display" id="eq:xiLM">\[\begin{equation}
\boxed{\xi^{LM} =
\left(\left.\frac{\partial \log \mathcal{L}(\boldsymbol\theta)}{\partial \boldsymbol\theta'}\right|_{\boldsymbol\theta = \hat{\boldsymbol\theta}^0}  \right)
[\mathbf{I}(\hat{\boldsymbol\theta}^0)]^{-1}
\left(\left.\frac{\partial \log \mathcal{L}(\boldsymbol\theta)}{\partial \boldsymbol\theta }\right|_{\boldsymbol\theta = \hat{\boldsymbol\theta}^0}  \right),} \tag{3.15}
\end{equation}\]</span>
(where <span class="math inline">\(\hat{\boldsymbol\theta}^0\)</span> is the restricted MLE estimator) is <span class="math inline">\(\chi^2(r)\)</span>.</p>
<p>The test defined by the critical region:
<span class="math display">\[
\{\xi^{LM} \ge \chi^2_{1-\alpha}(r)\}
\]</span>
has asymptotic level <span class="math inline">\(\alpha\)</span> and is consistent (see Defs. <a href="append.html#def:asmyptlevel">8.7</a> and <a href="append.html#def:asmyptconsisttest">8.8</a>). This test is called <em>Score</em> or <em>Lagrange Multiplier (LM)</em> test.</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-23" class="proof"><em>Proof</em>. </span>See Appendix <a href="append.html#AppendixProof">8.4</a>.</p>
</div>
<div class="definition">
<p><span id="def:LR" class="definition"><strong>Definition 3.8  (Likelihood Ratio test statistics) </strong></span>The likelihood ratio associated to a restriction of the form <span class="math inline">\(H_0: h({\boldsymbol\theta})=0\)</span> is given by:
<span class="math display">\[
LR = \frac{\mathcal{L}_R(\boldsymbol\theta;\mathbf{y})}{\mathcal{L}_U(\boldsymbol\theta;\mathbf{y})} \quad (\in [0,1]),
\]</span>
where <span class="math inline">\(\mathcal{L}_R\)</span> (respectively <span class="math inline">\(\mathcal{L}_U\)</span>) is the likelihood function that imposes (resp. that does not impose) the restriction. The likelihood ratio test statistic is given by <span class="math inline">\(-2\log(LR)\)</span>, that is:
<span class="math display">\[
\boxed{\xi^{LR}= 2 (\log\mathcal{L}_U(\boldsymbol\theta;\mathbf{y})-\log\mathcal{L}_R(\boldsymbol\theta;\mathbf{y})).}
\]</span></p>
</div>
<div class="proposition">
<p><span id="prp:equivLRLMW" class="proposition"><strong>Proposition 3.7  (Asymptotic equivalence of LR, LM, and Wald tests) </strong></span>Under the null hypothesis <span class="math inline">\(H_0\)</span>, we have, asymptotically:
<span class="math display">\[
\xi^{LM} = \xi^{LR} = \xi^{W}.
\]</span></p>
</div>
<div class="proof">
<p><span id="unlabeled-div-24" class="proof"><em>Proof</em>. </span>See Appendix <a href="append.html#AppendixProof">8.4</a>.</p>
</div>
</div>
</div>
<div id="bayesian-approach" class="section level2" number="3.3">
<h2>
<span class="header-section-number">3.3</span> Bayesian approach<a class="anchor" aria-label="anchor" href="#bayesian-approach"><i class="fas fa-link"></i></a>
</h2>
<div id="introduction" class="section level3" number="3.3.1">
<h3>
<span class="header-section-number">3.3.1</span> Introduction<a class="anchor" aria-label="anchor" href="#introduction"><i class="fas fa-link"></i></a>
</h3>
<p>An excellent introduction to Bayesian methods is proposed by <a href="http://www.columbia.edu/~mh2078/MonteCarlo/MCMC_Bayes.pdf">Martin Haugh, 2017</a>.</p>
<p>As suggested by the name of this approach, the starting point is the Bayes formula:
<span class="math display">\[
\mathbb{P}(A|B) = \frac{\mathbb{P}(A \&amp; B)}{\mathbb{P}(B)},
\]</span>
where <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are two “events”. For instance, <span class="math inline">\(A\)</span> may be: parameter <span class="math inline">\(\alpha\)</span> (conceived as something stochastic) lies in interval <span class="math inline">\([a,b]\)</span>. Assume that you are interested in the probability of occurrence of <span class="math inline">\(A\)</span>. Without any specific information (or “unconditionally”), this probability if <span class="math inline">\(\mathbb{P}(A)\)</span>. Your evaluation of this probability can only be better if you are provided with any additional form of information. Typically, if the event <span class="math inline">\(B\)</span> tends to occur simultaneously with <span class="math inline">\(A\)</span>, then knowledge of <span class="math inline">\(B\)</span> can be useful. The Bayes formula says how this additional information (on <span class="math inline">\(B\)</span>) can be used to “update” the probability of event <span class="math inline">\(A\)</span>.</p>
<p>In our case, this intuition will work as follows: assume that you know the form of the data-generating process (DGP). That is, you know the structure of the model used to draw some stochastic data; you also know the type of distributions used to generate these data. However, you do not know the numerical values of all the parameters characterizing the DGP. Let us denote by <span class="math inline">\({\boldsymbol\theta}\)</span> the vector of unknown parameters. While these parameters are not known exactly, assume that we have –even without having observed any data– some <strong>priors</strong> on their distribution. Then, as was the case in the example above (with <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>), the observation of data generated by the model can only reduce the uncertainty associated with <span class="math inline">\({\boldsymbol\theta}\)</span>. Loosely speaking, combining the priors and the observations of data generated by the model should result in “thinner” distributions for the components of <span class="math inline">\({\boldsymbol\theta}\)</span>. The latter distributions are called the <strong>posterior distributions</strong>.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;The output of the Bayesian approach will be the (posterior) distribution of the vector of parameters (&lt;span class="math inline"&gt;\(\boldsymbol\theta\)&lt;/span&gt;). When we speak about the &lt;em&gt;distributions of the components of &lt;span class="math inline"&gt;\({\boldsymbol\theta}\)&lt;/span&gt;&lt;/em&gt;, we mean the marginal distributions of each component of the vector.&lt;/p&gt;'><sup>11</sup></a></p>
<p>Let us formalize this intuition. Define the prior by <span class="math inline">\(f_{\boldsymbol\theta}({\boldsymbol\theta})\)</span> and the model realizations (the “data”) by vector <span class="math inline">\(\mathbf{y}\)</span>. The joint distribution of <span class="math inline">\((\mathbf{y},{\boldsymbol\theta})\)</span> is given by:
<span class="math display">\[
f_{Y,{\boldsymbol\theta}}(\mathbf{y},{\boldsymbol\theta}) = f_{Y|{\boldsymbol\theta}}(\mathbf{y},{\boldsymbol\theta})f_{\boldsymbol\theta}({\boldsymbol\theta}),
\]</span>
and, symmetrically, by
<span class="math display">\[
f_{Y,{\boldsymbol\theta}}(\mathbf{y},{\boldsymbol\theta}) = f_{{\boldsymbol\theta}|Y}({\boldsymbol\theta},\mathbf{y})f_Y(\mathbf{y}),
\]</span>
where <span class="math inline">\(f_{{\boldsymbol\theta}|Y}(\cdot,\mathbf{y})\)</span>, the distribution of the parameters conditional on the observations, is the <strong>posterior</strong> distribution.</p>
<p>The last two equations imply that:
<span class="math display" id="eq:post1">\[\begin{equation}
f_{{\boldsymbol\theta}|Y}({\boldsymbol\theta},\mathbf{y}) = \frac{f_{Y|{\boldsymbol\theta}}(\mathbf{y},{\boldsymbol\theta})f_{{\boldsymbol\theta}}({\boldsymbol\theta})}{f_Y(\mathbf{y})}.\tag{3.16}
\end{equation}\]</span>
Note that <span class="math inline">\(f_Y\)</span> is the marginal (or unconditional) distribution of <span class="math inline">\(\mathbf{y}\)</span>, that can be written:
<span class="math display">\[\begin{equation}
f_Y(\mathbf{y}) = \int f_{Y|{\boldsymbol\theta}}(\mathbf{y},{\boldsymbol\theta})f_{\boldsymbol\theta}({\boldsymbol\theta}) d {\boldsymbol\theta}.
\end{equation}\]</span></p>
<p>Eq. <a href="estimation-methods.html#eq:post1">(3.16)</a> is sometimes rewritten as follows:
<span class="math display" id="eq:post2">\[\begin{equation}
f_{{\boldsymbol\theta}|Y}({\boldsymbol\theta},\mathbf{y}) \propto f_{{\boldsymbol\theta},Y}({\boldsymbol\theta},\mathbf{y}) := f_{Y|{\boldsymbol\theta}}(\mathbf{y},{\boldsymbol\theta})f_{\boldsymbol\theta}({\boldsymbol\theta}), \tag{3.17}
\end{equation}\]</span>
where <span class="math inline">\(\propto\)</span> means, loosely speaking, “<em>proportional to</em>”. In rare instances, starting from given priors, one can analytically compute the posterior distribution <span class="math inline">\(f_{\boldsymbol\theta}({\boldsymbol\theta},\mathbf{y})\)</span>. However, in most cases, this is out of reach. One then has to resort to numerical approaches to compute the posterior distribution. Monte Carlo Markov Chains (MCMC) is one of them.</p>
<p>According to the Bernstein-von Mises Theorem, Bayesian and MLE estimators have the same large sample properties. (In particular, the Bayesian approach also achieve the FDCR bound, see Theorem <a href="estimation-methods.html#thm:FDCR">3.1</a>.) The intuition behind this result is that the influence of the prior diminishes with increasing sample sizes.</p>
</div>
<div id="monte-carlo-markov-chains" class="section level3" number="3.3.2">
<h3>
<span class="header-section-number">3.3.2</span> Monte-Carlo Markov Chains<a class="anchor" aria-label="anchor" href="#monte-carlo-markov-chains"><i class="fas fa-link"></i></a>
</h3>
<p>MCMC techniques aim at using simulations to approach a distribution whose distribution is difficult to obtain analytically. Indeed, in some circumstances, one can draw in a distribution even if we do not know its analytical expression.</p>
<div class="definition">
<p><span id="def:MC" class="definition"><strong>Definition 3.9  (Markov Chain) </strong></span>The sequence <span class="math inline">\(\{z_i\}\)</span> is said to be a (first-order) Markovian process is it satisfies:
<span class="math display">\[
f(z_i|z_{i-1},z_{i-2},\dots) = f(z_i|z_{i-1}).
\]</span></p>
</div>
<p>The Metropolis-Hastings (MH) algorithm is a specific MCMC approach that allows to generate samples of <span class="math inline">\({\boldsymbol\theta}\)</span>’s whose distribution approximately corresponds to the posterior distribution of Eq. <a href="estimation-methods.html#eq:post1">(3.16)</a>.</p>
<p>The MH algorithm is a recursive algorithm. That is, one can draw the <span class="math inline">\(i^{th}\)</span> value of <span class="math inline">\({\boldsymbol\theta}\)</span>, denoted by <span class="math inline">\({\boldsymbol\theta}_i\)</span>, if one has already drawn <span class="math inline">\({\boldsymbol\theta}_{i-1}\)</span>. Assume we have <span class="math inline">\({\boldsymbol\theta}_{i-1}\)</span>. We obtain a value for <span class="math inline">\({\boldsymbol\theta}_i\)</span> by implementing the following steps:</p>
<ol style="list-style-type: decimal">
<li>Draw <span class="math inline">\(\tilde{{\boldsymbol\theta}}_i\)</span> from the conditional distribution <span class="math inline">\(Q_{\tilde{{\boldsymbol\theta}}|{\boldsymbol\theta}}(\cdot,{\boldsymbol\theta}_{i-1})\)</span>, called <strong>proposal distribution</strong>.</li>
<li>Draw <span class="math inline">\(u\)</span> in a uniform distribution on <span class="math inline">\([0,1]\)</span>.</li>
<li>Compute
<span class="math display" id="eq:alphaXXX">\[\begin{equation}
\alpha(\tilde{{\boldsymbol\theta}}_i,{\boldsymbol\theta}_{i-1}):= \min\left(\frac{f_{{\boldsymbol\theta},Y}(\tilde{{\boldsymbol\theta}}_i,\mathbf{y})}{f_{{\boldsymbol\theta},Y}({\boldsymbol\theta}_{i-1},\mathbf{y})}\times\frac{Q_{\tilde{{\boldsymbol\theta}}|{\boldsymbol\theta}}({\boldsymbol\theta}_{i-1},\tilde{{\boldsymbol\theta}}_i)}{Q_{\tilde{{\boldsymbol\theta}}|{\boldsymbol\theta}}(\tilde{{\boldsymbol\theta}}_i,{\boldsymbol\theta}_{i-1})},1\right),\tag{3.18}
\end{equation}\]</span>
where <span class="math inline">\(f_{{\boldsymbol\theta},Y}\)</span> is given in Eq. <a href="estimation-methods.html#eq:post2">(3.17)</a>.</li>
<li>If <span class="math inline">\(u&lt;\alpha(\tilde{{\boldsymbol\theta}}_i,{\boldsymbol\theta}_{i-1})\)</span>, then take <span class="math inline">\({\boldsymbol\theta}_i = \tilde{{\boldsymbol\theta}}_i\)</span>, otherwise we leave <span class="math inline">\({\boldsymbol\theta}_i\)</span> equal to <span class="math inline">\({\boldsymbol\theta}_{i-1}\)</span>.</li>
</ol>
<p>It can be shown that, the distribution of the draws converges to the posterior distribution. That is, after a sufficiently large number of iterations, the draws can be considered to be drawn from the posterior distribution.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;The proof of this claim is based on the fact that, if &lt;span class="math inline"&gt;\({\boldsymbol\theta}_{i-1}\)&lt;/span&gt; is drawn from the posterior distribution, then it is also the case for &lt;span class="math inline"&gt;\({\boldsymbol\theta}_i\)&lt;/span&gt;.&lt;/p&gt;'><sup>12</sup></a></p>
<p>To get some insights into the algorithm, consider the case of a <strong>symmetric proposal distribution</strong>, that is:
<span class="math display" id="eq:symmQ">\[\begin{equation}
Q_{\tilde{{\boldsymbol\theta}}|{\boldsymbol\theta}}(\tilde{{\boldsymbol\theta}}_i,{\boldsymbol\theta}_{i-1})=Q_{\tilde{{\boldsymbol\theta}}|{\boldsymbol\theta}}({\boldsymbol\theta}_{i-1},\tilde{{\boldsymbol\theta}}_i).\tag{3.19}
\end{equation}\]</span>
We then have:
<span class="math display" id="eq:hypoQ">\[\begin{equation}
\alpha(\tilde{{\boldsymbol\theta}},{\boldsymbol\theta}_{i-1})= \min\left(\frac{q(\tilde{{\boldsymbol\theta}},y)}{q({\boldsymbol\theta}_{i-1},y)},1\right). \tag{3.20}
\end{equation}\]</span>
Remember that, up to the marginal distribution of the data (<span class="math inline">\(f_Y(\mathbf{y})\)</span>), <span class="math inline">\(f_{{\boldsymbol\theta},Y}(\tilde{{\boldsymbol\theta}},\mathbf{y})\)</span> is the probability of observing <span class="math inline">\(\mathbf{y}\)</span> conditional on having a model parameterized by <span class="math inline">\(\tilde{\boldsymbol\theta}\)</span>. Then, under Eq. <a href="estimation-methods.html#eq:hypoQ">(3.20)</a>, it appears that if this probability is larger for <span class="math inline">\(\tilde{\boldsymbol\theta}\)</span> than for <span class="math inline">\({\boldsymbol\theta}_{i-1}\)</span> (in which case <span class="math inline">\(\tilde{\boldsymbol\theta}\)</span> seems “more consistent with the observations <span class="math inline">\(\mathbf{y}\)</span>” than <span class="math inline">\({\boldsymbol\theta}_{i-1}\)</span>), we accept <span class="math inline">\({\boldsymbol\theta}_i\)</span>. By contrast, if <span class="math inline">\(f_{{\boldsymbol\theta},Y}(\tilde{{\boldsymbol\theta}},\mathbf{y})&lt;f_{{\boldsymbol\theta},Y}({\boldsymbol\theta}_{i-1},\mathbf{y})\)</span>, then we do not necessarily accept the proposed value <span class="math inline">\(\tilde{{\boldsymbol\theta}}\)</span>, especially if <span class="math inline">\(f_{{\boldsymbol\theta},Y}(\tilde{{\boldsymbol\theta}},\mathbf{y})\ll f_{{\boldsymbol\theta},Y}({\boldsymbol\theta}_{i-1},\mathbf{y})\)</span> (in which case <span class="math inline">\(\tilde{\boldsymbol\theta}\)</span> seems far less consistent with the observations <span class="math inline">\(\mathbf{y}\)</span> than <span class="math inline">\({\boldsymbol\theta}_{i-1}\)</span>, and, accordingly, the acceptance probability, namely <span class="math inline">\(\alpha(\tilde{{\boldsymbol\theta}},{\boldsymbol\theta}_{i-1})\)</span>, is small).</p>
<p>The choice of the <strong>proposal distribution</strong> <span class="math inline">\(Q_{\tilde{\boldsymbol\theta}|{\boldsymbol\theta}}\)</span> is crucial to get a rapid convergence of the algorithm. Looking at Eq. <a href="estimation-methods.html#eq:alphaXXX">(3.18)</a>, it is easily seen that the optimal choice would be <span class="math inline">\(Q_{\tilde{\boldsymbol\theta}|{\boldsymbol\theta}}(\cdot,{\boldsymbol\theta}_i)=f_{{\boldsymbol\theta}|Y}(\cdot,\mathbf{y})\)</span>. In that case, we would have <span class="math inline">\(\alpha(\tilde{{\boldsymbol\theta}}_i,{\boldsymbol\theta}_{i-1})\equiv 1\)</span> (see Eq. <a href="estimation-methods.html#eq:alphaXXX">(3.18)</a>). We would then accept all draws from the proposal distribution, as this distribution would directly be the posterior distribution. Of course, this situation is not realistoc as the objective of the algorithm is precisely to approximate the posterior distribution.</p>
<p>A common choice for <span class="math inline">\(Q\)</span> is a multivariate normal distribution. If <span class="math inline">\({\boldsymbol\theta}\)</span> is of dimension <span class="math inline">\(K\)</span>, we can for instance use:
<span class="math display">\[
Q(\tilde{\boldsymbol\theta},{\boldsymbol\theta})= \frac{1}{\left(\sqrt{2\pi\sigma^2}\right)^K}\exp\left(-\frac{1}{2}\sum_{j=1}^K\frac{(\tilde{\boldsymbol\theta}_j-{\boldsymbol\theta}_j)^2}{\sigma^2}\right),
\]</span>
which is an example of symmetric proposal distribution (see Eq. <a href="estimation-methods.html#eq:symmQ">(3.19)</a>). Equivalently, we then have:
<span class="math display">\[
\tilde{\boldsymbol\theta} = {\boldsymbol\theta} + \varepsilon,
\]</span>
where <span class="math inline">\(\varepsilon\)</span> is a <span class="math inline">\(K\)</span>-dimensional vector of independent zero-mean normal disturbances of variance <span class="math inline">\(\sigma^2\)</span>.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;We could also have different variances for the different components of &lt;span class="math inline"&gt;\({\boldsymbol\theta}\)&lt;/span&gt;. However, this may lead to complicated settings. A useful practice consists in looking for model (re)parametrization –based, e.g., on exponential and/or logistic functions– that are such that the components of &lt;span class="math inline"&gt;\({\boldsymbol\theta}\)&lt;/span&gt; are all expected to be of the order of magnitude of the unity.&lt;/p&gt;'><sup>13</sup></a> One then has to determine an appropriate value for <span class="math inline">\(\sigma\)</span>. If it is too low, then <span class="math inline">\(\alpha\)</span> will be close to 1 (as <span class="math inline">\(\tilde{{\boldsymbol\theta}}_i\)</span> will be close to <span class="math inline">\({\boldsymbol\theta}_{i-1}\)</span>), and we will accept very often the proposed value (<span class="math inline">\(\tilde{{\boldsymbol\theta}}_i\)</span>). This seems to be a favourable situation. But it may not be. Indeed, it means that it will take a large number of iterations to explore the whole distribution of <span class="math inline">\({\boldsymbol\theta}\)</span>. What if <span class="math inline">\(\sigma\)</span> is very large? In this case, it is likely that the porposed values (<span class="math inline">\(\tilde{{\boldsymbol\theta}}_i\)</span>) will often result in poor likelihoods; The probability of acceptance will then be low and the Markov chain may be blocked at its initial value. Therefore, intermediate values of <span class="math inline">\(\sigma^2\)</span> have to be determined. The acceptance rate (i.e., the average value of <span class="math inline">\(\alpha(\tilde{{\boldsymbol\theta}},{\boldsymbol\theta}_{i-1})\)</span>) can be used as a guide for that. Indeed, a literature explores the optimal values for such acceptance rate (in order to obtain the best possible fit of the posterior for a minimum number of algorithm iterations). In particular, following <span class="citation">Roberts, Gelman, and Gilks (<a href="references.html#ref-Roberts_Gelman_Gilks_1997" role="doc-biblioref">1997</a>)</span>, people often target acceptance rate of the order of magnitude of 20%.</p>
<p>It is important to note that, to implement this approach, one only has to be able to compute the joint p.d.f. <span class="math inline">\(q({\boldsymbol\theta},\mathbf{y})=f_{Y|{\boldsymbol\theta}}(\mathbf{y},{\boldsymbol\theta})f_{\boldsymbol\theta}({\boldsymbol\theta})\)</span> (Eq. <a href="estimation-methods.html#eq:post2">(3.17)</a>). That is, as soon as one can evaluate the likelihood (<span class="math inline">\(f_{Y|{\boldsymbol\theta}}(\mathbf{y},{\boldsymbol\theta})\)</span>) and the prior (<span class="math inline">\(f_{\boldsymbol\theta}({\boldsymbol\theta})\)</span>), we can employ this methodology.</p>
</div>
<div id="example-ar1-specification" class="section level3" number="3.3.3">
<h3>
<span class="header-section-number">3.3.3</span> Example: AR(1) specification<a class="anchor" aria-label="anchor" href="#example-ar1-specification"><i class="fas fa-link"></i></a>
</h3>
<p>In the following example, we employ MCMC in order to estimate the posterior distributions of the three parameters defining an AR(1) model. The specification is as follows:
<span class="math display">\[
y_t = \mu + \rho y_{t-1} + \sigma \varepsilon_{t}, \quad \varepsilon_t \sim \,i.i.d.\,\mathcal{N}(0,1).
\]</span>
Hence, we have <span class="math inline">\({\boldsymbol\theta} = [\mu,\rho,\sigma]\)</span>. Let us first simulate the process on <span class="math inline">\(T\)</span> periods:</p>
<div class="sourceCode" id="cb91"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">mu</span> <span class="op">&lt;-</span> <span class="fl">.6</span>; <span class="va">rho</span> <span class="op">&lt;-</span> <span class="fl">.8</span>; <span class="va">sigma</span> <span class="op">&lt;-</span> <span class="fl">.5</span> <span class="co"># true model specification</span></span>
<span><span class="cn">T</span> <span class="op">&lt;-</span> <span class="fl">20</span> <span class="co"># number of observations</span></span>
<span><span class="va">y0</span> <span class="op">&lt;-</span> <span class="va">mu</span><span class="op">/</span><span class="op">(</span><span class="fl">1</span><span class="op">-</span><span class="va">rho</span><span class="op">)</span></span>
<span><span class="va">Y</span> <span class="op">&lt;-</span> <span class="cn">NULL</span></span>
<span><span class="kw">for</span><span class="op">(</span><span class="va">t</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="cn">T</span><span class="op">)</span><span class="op">{</span></span>
<span>  <span class="kw">if</span><span class="op">(</span><span class="va">t</span><span class="op">==</span><span class="fl">1</span><span class="op">)</span><span class="op">{</span><span class="va">y</span> <span class="op">&lt;-</span> <span class="va">y0</span><span class="op">}</span></span>
<span>  <span class="va">y</span> <span class="op">&lt;-</span> <span class="va">mu</span> <span class="op">+</span> <span class="va">rho</span><span class="op">*</span><span class="va">y</span> <span class="op">+</span> <span class="va">sigma</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="fl">1</span><span class="op">)</span></span>
<span>  <span class="va">Y</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">Y</span>,<span class="va">y</span><span class="op">)</span><span class="op">}</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">Y</span>,type<span class="op">=</span><span class="st">"l"</span>,xlab<span class="op">=</span><span class="st">"time t"</span>,ylab<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/expression.html">expression</a></span><span class="op">(</span><span class="va">y</span><span class="op">[</span><span class="va">t</span><span class="op">]</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="MicroEc_files/figure-html/MCMC1-1.png" width="672"></div>
<p>Next, let us write the likelihood function, i.e. <span class="math inline">\(f_{Y|{\boldsymbol\theta}}(\mathbf{y},{\boldsymbol\theta})\)</span>. For <span class="math inline">\(\rho\)</span>, which is expected to be between 0 and 1, we use a logistic transformation. For <span class="math inline">\(\sigma\)</span>, that is expected to be positive, we use an exponential transformation.</p>
<div class="sourceCode" id="cb92"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">likelihood</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">param</span>,<span class="va">Y</span><span class="op">)</span><span class="op">{</span></span>
<span>  <span class="va">mu</span>  <span class="op">&lt;-</span> <span class="va">param</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span></span>
<span>  <span class="va">rho</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">exp</a></span><span class="op">(</span><span class="va">param</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span><span class="op">)</span><span class="op">/</span><span class="op">(</span><span class="fl">1</span><span class="op">+</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html">exp</a></span><span class="op">(</span><span class="va">param</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span><span class="op">)</span><span class="op">)</span></span>
<span>  <span class="va">sigma</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">exp</a></span><span class="op">(</span><span class="va">param</span><span class="op">[</span><span class="fl">3</span><span class="op">]</span><span class="op">)</span></span>
<span>  <span class="va">MU</span> <span class="op">&lt;-</span> <span class="va">mu</span><span class="op">/</span><span class="op">(</span><span class="fl">1</span><span class="op">-</span><span class="va">rho</span><span class="op">)</span></span>
<span>  <span class="va">SIGMA2</span> <span class="op">&lt;-</span> <span class="va">sigma</span><span class="op">^</span><span class="fl">2</span><span class="op">/</span><span class="op">(</span><span class="fl">1</span><span class="op">-</span><span class="va">rho</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span></span>
<span>  <span class="va">L</span> <span class="op">&lt;-</span> <span class="fl">1</span><span class="op">/</span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="fl">2</span><span class="op">*</span><span class="va">pi</span><span class="op">*</span><span class="va">SIGMA2</span><span class="op">)</span><span class="op">*</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html">exp</a></span><span class="op">(</span><span class="op">-</span><span class="op">(</span><span class="va">Y</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span><span class="op">-</span><span class="va">MU</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span><span class="op">/</span><span class="op">(</span><span class="fl">2</span><span class="op">*</span><span class="va">SIGMA2</span><span class="op">)</span><span class="op">)</span></span>
<span>  <span class="va">Y1</span> <span class="op">&lt;-</span> <span class="va">Y</span><span class="op">[</span><span class="fl">2</span><span class="op">:</span><span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">Y</span><span class="op">)</span><span class="op">]</span></span>
<span>  <span class="va">Y0</span> <span class="op">&lt;-</span> <span class="va">Y</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">Y</span><span class="op">)</span><span class="op">-</span><span class="fl">1</span><span class="op">)</span><span class="op">]</span></span>
<span>  <span class="va">aux</span> <span class="op">&lt;-</span> <span class="fl">1</span><span class="op">/</span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="fl">2</span><span class="op">*</span><span class="va">pi</span><span class="op">*</span><span class="va">sigma</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span><span class="op">*</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html">exp</a></span><span class="op">(</span><span class="op">-</span><span class="op">(</span><span class="va">Y1</span><span class="op">-</span><span class="va">mu</span><span class="op">-</span><span class="va">rho</span><span class="op">*</span><span class="va">Y0</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span><span class="op">/</span><span class="op">(</span><span class="fl">2</span><span class="op">*</span><span class="va">sigma</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span><span class="op">)</span></span>
<span>  <span class="va">L</span> <span class="op">&lt;-</span> <span class="va">L</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/prod.html">prod</a></span><span class="op">(</span><span class="va">aux</span><span class="op">)</span></span>
<span>  <span class="kw"><a href="https://rdrr.io/r/base/function.html">return</a></span><span class="op">(</span><span class="va">L</span><span class="op">)</span></span>
<span><span class="op">}</span></span></code></pre></div>
<p>Next define function <code>rQ</code> that draws from the (Gaussian) proposal distribution, as well as function <code>Q</code>, that computes <span class="math inline">\(Q_{\tilde{\boldsymbol\theta}|{\boldsymbol\theta}}(\tilde{\boldsymbol\theta},{\boldsymbol\theta})\)</span>:</p>
<div class="sourceCode" id="cb93"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">rQ</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span>,<span class="va">a</span><span class="op">)</span><span class="op">{</span></span>
<span>  <span class="va">n</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span></span>
<span>  <span class="va">y</span> <span class="op">&lt;-</span> <span class="va">x</span> <span class="op">+</span> <span class="va">a</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span><span class="op">)</span></span>
<span>  <span class="kw"><a href="https://rdrr.io/r/base/function.html">return</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span><span class="op">}</span></span>
<span><span class="va">Q</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">y</span>,<span class="va">x</span>,<span class="va">a</span><span class="op">)</span><span class="op">{</span></span>
<span>  <span class="va">q</span> <span class="op">&lt;-</span> <span class="fl">1</span><span class="op">/</span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="fl">2</span><span class="op">*</span><span class="va">pi</span><span class="op">*</span><span class="va">a</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span><span class="op">*</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html">exp</a></span><span class="op">(</span><span class="op">-</span><span class="op">(</span><span class="va">y</span> <span class="op">-</span> <span class="va">x</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span><span class="op">/</span><span class="op">(</span><span class="fl">2</span><span class="op">*</span><span class="va">a</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span><span class="op">)</span></span>
<span>  <span class="kw"><a href="https://rdrr.io/r/base/function.html">return</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/prod.html">prod</a></span><span class="op">(</span><span class="va">q</span><span class="op">)</span><span class="op">)</span><span class="op">}</span></span></code></pre></div>
<p>We consider Gaussian priors:</p>
<div class="sourceCode" id="cb94"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">prior</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">param</span>,<span class="va">means_prior</span>,<span class="va">stdv_prior</span><span class="op">)</span><span class="op">{</span></span>
<span>  <span class="va">f</span> <span class="op">&lt;-</span> <span class="fl">1</span><span class="op">/</span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="fl">2</span><span class="op">*</span><span class="va">pi</span><span class="op">*</span><span class="va">stdv_prior</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span><span class="op">*</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html">exp</a></span><span class="op">(</span><span class="op">-</span><span class="op">(</span><span class="va">param</span> <span class="op">-</span> </span>
<span>                                         <span class="va">means_prior</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span><span class="op">/</span><span class="op">(</span><span class="fl">2</span><span class="op">*</span><span class="va">stdv_prior</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span><span class="op">)</span></span>
<span>  <span class="kw"><a href="https://rdrr.io/r/base/function.html">return</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/prod.html">prod</a></span><span class="op">(</span><span class="va">f</span><span class="op">)</span><span class="op">)</span><span class="op">}</span></span></code></pre></div>
<p>Function <code>p_tilde</code> corresponds to <span class="math inline">\(f_{{\boldsymbol\theta},Y}\)</span>:</p>
<div class="sourceCode" id="cb95"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">p_tilde</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">param</span>,<span class="va">Y</span>,<span class="va">means_prior</span>,<span class="va">stdv_prior</span><span class="op">)</span><span class="op">{</span></span>
<span>  <span class="va">p</span> <span class="op">&lt;-</span> <span class="fu">likelihood</span><span class="op">(</span><span class="va">param</span>,<span class="va">Y</span><span class="op">)</span> <span class="op">*</span> <span class="fu">prior</span><span class="op">(</span><span class="va">param</span>,<span class="va">means_prior</span>,<span class="va">stdv_prior</span><span class="op">)</span></span>
<span>  <span class="kw"><a href="https://rdrr.io/r/base/function.html">return</a></span><span class="op">(</span><span class="va">p</span><span class="op">)</span><span class="op">}</span></span></code></pre></div>
<p>We can now define function <span class="math inline">\(\alpha\)</span> (Eq. <a href="estimation-methods.html#eq:alphaXXX">(3.18)</a>):</p>
<div class="sourceCode" id="cb96"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">alpha</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">y</span>,<span class="va">x</span>,<span class="va">means_prior</span>,<span class="va">stdv_prior</span>,<span class="va">a</span><span class="op">)</span><span class="op">{</span></span>
<span>  <span class="va">aux</span> <span class="op">&lt;-</span> <span class="fu">p_tilde</span><span class="op">(</span><span class="va">y</span>,<span class="va">Y</span>,<span class="va">means_prior</span>,<span class="va">stdv_prior</span><span class="op">)</span><span class="op">/</span></span>
<span>    <span class="fu">p_tilde</span><span class="op">(</span><span class="va">x</span>,<span class="va">Y</span>,<span class="va">means_prior</span>,<span class="va">stdv_prior</span><span class="op">)</span> <span class="op">*</span> <span class="fu">Q</span><span class="op">(</span><span class="va">y</span>,<span class="va">x</span>,<span class="va">a</span><span class="op">)</span><span class="op">/</span><span class="fu">Q</span><span class="op">(</span><span class="va">x</span>,<span class="va">y</span>,<span class="va">a</span><span class="op">)</span></span>
<span>  <span class="va">alpha_proba</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/Extremes.html">min</a></span><span class="op">(</span><span class="va">aux</span>,<span class="fl">1</span><span class="op">)</span></span>
<span>  <span class="kw"><a href="https://rdrr.io/r/base/function.html">return</a></span><span class="op">(</span><span class="va">alpha_proba</span><span class="op">)</span><span class="op">}</span></span></code></pre></div>
<p>Now, all is set for us to write the MCMC function:</p>
<div class="sourceCode" id="cb97"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">MCMC</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">Y</span>,<span class="va">means_prior</span>,<span class="va">stdv_prior</span>,<span class="va">a</span>,<span class="va">N</span><span class="op">)</span><span class="op">{</span></span>
<span>  <span class="va">x</span> <span class="op">&lt;-</span> <span class="va">means_prior</span></span>
<span>  <span class="va">all_theta</span> <span class="op">&lt;-</span> <span class="cn">NULL</span></span>
<span>  <span class="va">count_accept</span> <span class="op">&lt;-</span> <span class="fl">0</span></span>
<span>  <span class="kw">for</span><span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="va">N</span><span class="op">)</span><span class="op">{</span></span>
<span>    <span class="va">y</span> <span class="op">&lt;-</span> <span class="fu">rQ</span><span class="op">(</span><span class="va">x</span>,<span class="va">a</span><span class="op">)</span></span>
<span>    <span class="va">alph</span> <span class="op">&lt;-</span> <span class="fu">alpha</span><span class="op">(</span><span class="va">y</span>,<span class="va">x</span>,<span class="va">means_prior</span>,<span class="va">stdv_prior</span>,<span class="va">a</span><span class="op">)</span></span>
<span>    <span class="co">#print(alph)</span></span>
<span>    <span class="va">u</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html">runif</a></span><span class="op">(</span><span class="fl">1</span><span class="op">)</span></span>
<span>    <span class="kw">if</span><span class="op">(</span><span class="va">u</span> <span class="op">&lt;</span> <span class="va">alph</span><span class="op">)</span><span class="op">{</span></span>
<span>      <span class="va">count_accept</span> <span class="op">&lt;-</span> <span class="va">count_accept</span> <span class="op">+</span> <span class="fl">1</span></span>
<span>      <span class="va">x</span> <span class="op">&lt;-</span> <span class="va">y</span><span class="op">}</span></span>
<span>    <span class="va">all_theta</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/cbind.html">rbind</a></span><span class="op">(</span><span class="va">all_theta</span>,<span class="va">x</span><span class="op">)</span><span class="op">}</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste</a></span><span class="op">(</span><span class="st">"Acceptance rate:"</span>,<span class="fu"><a href="https://rdrr.io/r/base/toString.html">toString</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="va">count_accept</span><span class="op">/</span><span class="va">N</span>,<span class="fl">3</span><span class="op">)</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span>  <span class="kw"><a href="https://rdrr.io/r/base/function.html">return</a></span><span class="op">(</span><span class="va">all_theta</span><span class="op">)</span><span class="op">}</span></span></code></pre></div>
<p>Specify the Gaussian priors:</p>
<div class="sourceCode" id="cb98"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">true_values</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">mu</span>,<span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">rho</span><span class="op">/</span><span class="op">(</span><span class="fl">1</span><span class="op">-</span><span class="va">rho</span><span class="op">)</span><span class="op">)</span>,<span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">sigma</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">means_prior</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>,<span class="fl">0</span>,<span class="fl">0</span><span class="op">)</span> <span class="co"># as if we did not know the true values</span></span>
<span><span class="va">stdv_prior</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="fl">2</span>,<span class="fl">3</span><span class="op">)</span></span>
<span><span class="va">resultMCMC</span> <span class="op">&lt;-</span> <span class="fu">MCMC</span><span class="op">(</span><span class="va">Y</span>,<span class="va">means_prior</span>,<span class="va">stdv_prior</span>,a<span class="op">=</span><span class="fl">.45</span>,N<span class="op">=</span><span class="fl">20000</span><span class="op">)</span></span></code></pre></div>
<pre><code>## [1] "Acceptance rate: 0.098"</code></pre>
<div class="sourceCode" id="cb100"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>mfrow<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">2</span>,<span class="fl">3</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="kw">for</span><span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">means_prior</span><span class="op">)</span><span class="op">)</span><span class="op">{</span></span>
<span>  <span class="va">m</span> <span class="op">&lt;-</span> <span class="va">means_prior</span><span class="op">[</span><span class="va">i</span><span class="op">]</span></span>
<span>  <span class="va">s</span> <span class="op">&lt;-</span> <span class="va">stdv_prior</span><span class="op">[</span><span class="va">i</span><span class="op">]</span></span>
<span>  <span class="va">x</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span><span class="op">(</span><span class="va">m</span><span class="op">-</span><span class="fl">3</span><span class="op">*</span><span class="va">s</span>,<span class="va">m</span><span class="op">+</span><span class="fl">3</span><span class="op">*</span><span class="va">s</span>,length.out <span class="op">=</span> <span class="fl">100</span><span class="op">)</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>mfg<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>,<span class="va">i</span><span class="op">)</span><span class="op">)</span></span>
<span>  <span class="va">aux</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/density.html">density</a></span><span class="op">(</span><span class="va">resultMCMC</span><span class="op">[</span>,<span class="va">i</span><span class="op">]</span><span class="op">)</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>plt<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">.15</span>,<span class="fl">.95</span>,<span class="fl">.15</span>,<span class="fl">.85</span><span class="op">)</span><span class="op">)</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">x</span>,<span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">dnorm</a></span><span class="op">(</span><span class="va">x</span>,<span class="va">m</span>,<span class="va">s</span><span class="op">)</span>,type<span class="op">=</span><span class="st">"l"</span>,xlab<span class="op">=</span><span class="st">""</span>,ylab<span class="op">=</span><span class="st">""</span>,main<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste</a></span><span class="op">(</span><span class="st">"Parameter"</span>,<span class="va">i</span><span class="op">)</span>,</span>
<span>       ylim<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span>,<span class="fu"><a href="https://rdrr.io/r/base/Extremes.html">max</a></span><span class="op">(</span><span class="va">aux</span><span class="op">$</span><span class="va">y</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/graphics/lines.html">lines</a></span><span class="op">(</span><span class="va">aux</span><span class="op">$</span><span class="va">x</span>,<span class="va">aux</span><span class="op">$</span><span class="va">y</span>,col<span class="op">=</span><span class="st">"red"</span>,lwd<span class="op">=</span><span class="fl">2</span><span class="op">)</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/graphics/abline.html">abline</a></span><span class="op">(</span>v<span class="op">=</span><span class="va">true_values</span><span class="op">[</span><span class="va">i</span><span class="op">]</span>,lty<span class="op">=</span><span class="fl">2</span>,col<span class="op">=</span><span class="st">"blue"</span><span class="op">)</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>mfg<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">2</span>,<span class="va">i</span><span class="op">)</span><span class="op">)</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">resultMCMC</span><span class="op">[</span>,<span class="va">i</span><span class="op">]</span>,<span class="fl">1</span><span class="op">:</span><span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">resultMCMC</span><span class="op">[</span>,<span class="va">i</span><span class="op">]</span><span class="op">)</span>,xlim<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Extremes.html">min</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span>,<span class="fu"><a href="https://rdrr.io/r/base/Extremes.html">max</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span><span class="op">)</span>,</span>
<span>       type<span class="op">=</span><span class="st">"l"</span>,xlab<span class="op">=</span><span class="st">""</span>,ylab<span class="op">=</span><span class="st">""</span><span class="op">)</span><span class="op">}</span></span></code></pre></div>
<div class="figure">
<span style="display:block;" id="fig:MCMC8"></span>
<img src="MicroEc_files/figure-html/MCMC8-1.png" alt="The upper line of plot compares prior (black) and posterior (red) distributions. The vertical dashed blue lines indicate the true values of the parameters. The second row of plots show the sequence of $\boldsymbol\theta_i$'s generated by the MCMC algorithm. These sequences are the ones used to produce the posterior distributions (red lines) in the upper plots." width="100%"><p class="caption">
Figure 3.6: The upper line of plot compares prior (black) and posterior (red) distributions. The vertical dashed blue lines indicate the true values of the parameters. The second row of plots show the sequence of <span class="math inline">\(\boldsymbol\theta_i\)</span>’s generated by the MCMC algorithm. These sequences are the ones used to produce the posterior distributions (red lines) in the upper plots.
</p>
</div>

</div>
</div>
</div>

  <div class="chapter-nav">
<div class="prev"><a href="Panel.html"><span class="header-section-number">2</span> Panel regressions</a></div>
<div class="next"><a href="binary-choice-models.html"><span class="header-section-number">4</span> Binary-choice models</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#estimation-methods"><span class="header-section-number">3</span> Estimation Methods</a></li>
<li>
<a class="nav-link" href="#secGMM"><span class="header-section-number">3.1</span> Generalized Method of Moments (GMM)</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#definition-of-the-gmm-estimator"><span class="header-section-number">3.1.1</span> Definition of the GMM estimator</a></li>
<li><a class="nav-link" href="#asymptotic-distribution-of-the-gmm-estimator"><span class="header-section-number">3.1.2</span> Asymptotic distribution of the GMM estimator</a></li>
<li><a class="nav-link" href="#overidentif"><span class="header-section-number">3.1.3</span> Testing hypotheses in the GMM framework</a></li>
<li><a class="nav-link" href="#example-estimation-of-the-stochastic-discount-factor-s.d.f."><span class="header-section-number">3.1.4</span> Example: Estimation of the Stochastic Discount Factor (s.d.f.)</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#secMLE"><span class="header-section-number">3.2</span> Maximum Likelihood Estimation</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#intuition"><span class="header-section-number">3.2.1</span> Intuition</a></li>
<li><a class="nav-link" href="#definition-and-properties"><span class="header-section-number">3.2.2</span> Definition and properties</a></li>
<li><a class="nav-link" href="#to-sum-up-mle-in-practice"><span class="header-section-number">3.2.3</span> To sum up – MLE in practice</a></li>
<li><a class="nav-link" href="#example-mle-estimation-of-a-mixture-of-gaussian-distribution"><span class="header-section-number">3.2.4</span> Example: MLE estimation of a mixture of Gaussian distribution</a></li>
<li><a class="nav-link" href="#TestMLE"><span class="header-section-number">3.2.5</span> Test procedures</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#bayesian-approach"><span class="header-section-number">3.3</span> Bayesian approach</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#introduction"><span class="header-section-number">3.3.1</span> Introduction</a></li>
<li><a class="nav-link" href="#monte-carlo-markov-chains"><span class="header-section-number">3.3.2</span> Monte-Carlo Markov Chains</a></li>
<li><a class="nav-link" href="#example-ar1-specification"><span class="header-section-number">3.3.3</span> Example: AR(1) specification</a></li>
</ul>
</li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
          
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Micro-Econometrics</strong>" was written by Jean-Paul Renne. It was last built on 2023-02-20.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
