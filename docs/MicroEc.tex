% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
  12pt,
]{book}
\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\usepackage{booktabs}
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage[]{natbib}
\bibliographystyle{apalike}
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Micro-Econometrics},
  pdfauthor={Jean-Paul Renne},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\title{Micro-Econometrics}
\author{Jean-Paul Renne}
\date{2023-01-28}

\usepackage{amsthm}
\newtheorem{theorem}{Theorem}[chapter]
\newtheorem{lemma}{Lemma}[chapter]
\newtheorem{corollary}{Corollary}[chapter]
\newtheorem{proposition}{Proposition}[chapter]
\newtheorem{conjecture}{Conjecture}[chapter]
\theoremstyle{definition}
\newtheorem{definition}{Definition}[chapter]
\theoremstyle{definition}
\newtheorem{example}{Example}[chapter]
\theoremstyle{definition}
\newtheorem{exercise}{Exercise}[chapter]
\theoremstyle{definition}
\newtheorem{hypothesis}{Hypothesis}[chapter]
\theoremstyle{remark}
\newtheorem*{remark}{Remark}
\newtheorem*{solution}{Solution}
\begin{document}
\maketitle

{
\setcounter{tocdepth}{1}
\tableofcontents
}
\newcommand{\bv}[1]{\mathbf{#1}}

\hypertarget{intro}{%
\chapter*{Micro-Econometrics}\label{intro}}

In microeconometric models, the variables of interest often feature restricted distributions---for instance with discontinuous support---, which necessitates specific models. Typical examples are discrete-choice models (binary, multinomial, ordered outcomes), sample selection models (censored or truncated outcomes), and count-data models (integer outcomes). The course describes the estimation and interpretation of these models. It also shows how the discrete-choice models can emerge from (structural) random-utility frameworks.

The R codes use various packages that can be obtained from \href{https://cran.r-project.org}{CRAN}. This \texttt{AEC} package is available on GitHub. To install it, one need to employ the \texttt{devtools} library:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{install.packages}\NormalTok{(}\StringTok{"devtools"}\NormalTok{) }\CommentTok{\# in case this library has not been loaded yet}
\FunctionTok{library}\NormalTok{(devtools)}
\FunctionTok{install\_github}\NormalTok{(}\StringTok{"jrenne/AEC"}\NormalTok{)}
\FunctionTok{library}\NormalTok{(AEC)}
\end{Highlighting}
\end{Shaded}

\textbf{Useful (R) links:}

\begin{itemize}
\item
  Download R:

  \begin{itemize}
  \tightlist
  \item
    R software: \url{https://cran.r-project.org} (the basic R software)
  \item
    RStudio: \url{https://www.rstudio.com} (a convenient R editor)
  \end{itemize}
\item
  Tutorials:

  \begin{itemize}
  \tightlist
  \item
    Rstudio: \url{https://dss.princeton.edu/training/RStudio101.pdf} (by Oscar Torres-Reyna)
  \item
    R: \url{https://cran.r-project.org/doc/contrib/Paradis-rdebuts_en.pdf} (by Emmanuel Paradis)
  \item
    My own tutorial: \url{https://jrenne.shinyapps.io/Rtuto_publiShiny/}
  \end{itemize}
\end{itemize}

\hypertarget{Panel}{%
\chapter{Panel regressions}\label{Panel}}

\hypertarget{specification-and-notations}{%
\section{Specification and notations}\label{specification-and-notations}}

A standard panel situation is as follows: the sample covers a lot of ``entities'', indexed by \(i \in \{1,\dots,n\}\), with \(n\) large, and, for each entity, we observe different variables over a small number of periods \(t \in \{1,\dots,T\}\). This is a \emph{longitudinal dataset}.

The linear panel regression model is:
\begin{equation}
y_{i,t} = \mathbf{x}'_{i,t}\underbrace{\boldsymbol\beta}_{K \times 1} + \underbrace{\mathbf{z}'_{i}\boldsymbol\alpha}_{\mbox{Individual effects}} + \varepsilon_{i,t}.\label{eq:panel1}
\end{equation}

When running panel regressions, the usual objective is to estimate \(\boldsymbol\beta\).

Figure \ref{fig:simulPanel} illustrates a panel-data situation. The model is \(y_i = \alpha_i + \beta x_{i,t} + \varepsilon_{i,t}\), \(t \in \{1,2\}\). On Panel (b), blue dots are for \(t=1\), red dots are for \(t=2\). The lines relate the dots associated with the same entity \(i\). What is remarkable in the simulated model is that, while the unconditional correlation between \(y\) and \(x\) is negative, the conditional correlation (conditional on \(\alpha_i\)) is positive. Indeed, the sign of this conditional correlation is the sign of \(\beta\), which is positive in th simulated example (\(\beta=5\)). In other words, if one did not know the panel nature of the data, that would be tempting to say that \(\beta<0\), but this is not the case, due to \textbf{fixed effects} (the \(\alpha_i\)'s) that are negatively correlated to the \(x_i\)'s.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{T }\OtherTok{\textless{}{-}} \DecValTok{2}\NormalTok{; n }\OtherTok{\textless{}{-}} \DecValTok{12} \CommentTok{\# 2 periods and 12 entities}
\NormalTok{alpha }\OtherTok{\textless{}{-}} \DecValTok{5}\SpecialCharTok{*}\FunctionTok{rnorm}\NormalTok{(n) }\CommentTok{\# draw fixed effects}
\NormalTok{x}\FloatTok{.1} \OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(n) }\SpecialCharTok{{-}}\NormalTok{ .}\DecValTok{5}\SpecialCharTok{*}\NormalTok{alpha }\CommentTok{\# note: x\_i\textquotesingle{}s correlate to alpha\_i\textquotesingle{}s}
\NormalTok{x}\FloatTok{.2} \OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(n) }\SpecialCharTok{{-}}\NormalTok{ .}\DecValTok{5}\SpecialCharTok{*}\NormalTok{alpha}
\NormalTok{beta }\OtherTok{\textless{}{-}} \DecValTok{5}\NormalTok{; sigma }\OtherTok{\textless{}{-}}\NormalTok{ .}\DecValTok{3}
\NormalTok{y}\FloatTok{.1} \OtherTok{\textless{}{-}}\NormalTok{ alpha }\SpecialCharTok{+}\NormalTok{ x}\FloatTok{.1} \SpecialCharTok{+}\NormalTok{ sigma}\SpecialCharTok{*}\FunctionTok{rnorm}\NormalTok{(n);y}\FloatTok{.2} \OtherTok{\textless{}{-}}\NormalTok{ alpha }\SpecialCharTok{+}\NormalTok{ x}\FloatTok{.2} \SpecialCharTok{+}\NormalTok{ sigma}\SpecialCharTok{*}\FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(x}\FloatTok{.1}\NormalTok{,x}\FloatTok{.2}\NormalTok{) }\CommentTok{\# pooled x}
\NormalTok{y }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(y}\FloatTok{.1}\NormalTok{,y}\FloatTok{.2}\NormalTok{) }\CommentTok{\# pooled y}
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\FunctionTok{plot}\NormalTok{(x,y,}\AttributeTok{col=}\StringTok{"black"}\NormalTok{,}\AttributeTok{pch=}\DecValTok{19}\NormalTok{,}\AttributeTok{xlab=}\StringTok{"x"}\NormalTok{,}\AttributeTok{ylab=}\StringTok{"y"}\NormalTok{,}\AttributeTok{main=}\StringTok{"(a)"}\NormalTok{)}
\FunctionTok{plot}\NormalTok{(x,y,}\AttributeTok{col=}\StringTok{"black"}\NormalTok{,}\AttributeTok{pch=}\DecValTok{19}\NormalTok{,}\AttributeTok{xlab=}\StringTok{"x"}\NormalTok{,}\AttributeTok{ylab=}\StringTok{"y"}\NormalTok{,}\AttributeTok{main=}\StringTok{"(b)"}\NormalTok{)}
\FunctionTok{points}\NormalTok{(x}\FloatTok{.1}\NormalTok{,y}\FloatTok{.1}\NormalTok{,}\AttributeTok{col=}\StringTok{"blue"}\NormalTok{,}\AttributeTok{pch=}\DecValTok{19}\NormalTok{);}\FunctionTok{points}\NormalTok{(x}\FloatTok{.2}\NormalTok{,y}\FloatTok{.2}\NormalTok{,}\AttributeTok{col=}\StringTok{"red"}\NormalTok{,}\AttributeTok{pch=}\DecValTok{19}\NormalTok{)}
\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{n)\{}\FunctionTok{lines}\NormalTok{(}\FunctionTok{c}\NormalTok{(x}\FloatTok{.1}\NormalTok{[i],x}\FloatTok{.2}\NormalTok{[i]),}\FunctionTok{c}\NormalTok{(y}\FloatTok{.1}\NormalTok{[i],y}\FloatTok{.2}\NormalTok{[i]))\}}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\includegraphics[width=0.95\linewidth]{MicroEc_files/figure-latex/simulPanel-1} \caption{The data are the same for both panels. On Panel (b), blue dots are for $t=1$, red dots are for $t=2$. The lines relate the dots associated to the same entity $i$.}\label{fig:simulPanel}
\end{figure}

Figure \ref{fig:cigarettes} presents the same type of plot based on the Cigarette Consumption Panel dataset (\texttt{CigarettesSW} dataset, used in \citet{Stock_Watson_2003}). This dataset documents the average consumption of cigarettes in 48 continental US states for two dates (1985 and 1995).

\begin{figure}
\includegraphics[width=0.95\linewidth]{MicroEc_files/figure-latex/cigarettes-1} \caption{Cigarette consumption versus real price in the CigarettesSW panel dataset.}\label{fig:cigarettes}
\end{figure}

We will make use of the following notations:
\[
\mathbf{y}_i =
\underbrace{\left[
\begin{array}{c}
y_{i,1}\\
\vdots\\
y_{i,T}
\end{array}\right]}_{T \times 1}, \quad
\boldsymbol\varepsilon_i =
\underbrace{\left[
\begin{array}{c}
\varepsilon_{i,1}\\
\vdots\\
\varepsilon_{i,T}
\end{array}\right]}_{T \times 1}, \quad
\mathbf{x}_i =
\underbrace{\left[
\begin{array}{c}
\mathbf{x}_{i,1}'\\
\vdots\\
\mathbf{x}_{i,T}'
\end{array}\right]}_{T \times K}, \quad
\mathbf{X} =
\underbrace{\left[
\begin{array}{c}
\mathbf{x}_{1}\\
\vdots\\
\mathbf{x}_{n}
\end{array}\right]}_{(nT) \times K}.
\]
\[
\tilde{\mathbf{y}}_i =
\left[
\begin{array}{c}
y_{i,1} - \bar{y}_i\\
\vdots\\
y_{i,T} - \bar{y}_i
\end{array}\right], \quad
\tilde{\boldsymbol\varepsilon}_i =
\left[
\begin{array}{c}
\varepsilon_{i,1} - \bar{\varepsilon}_i\\
\vdots\\
\varepsilon_{i,T} - \bar{\varepsilon}_i
\end{array}\right],
\]
\[
\tilde{\mathbf{x}}_i =
\left[
\begin{array}{c}
\mathbf{x}_{i,1}' - \bar{\mathbf{x}}_i'\\
\vdots\\
\mathbf{x}_{i,T}' - \bar{\mathbf{x}}_i'
\end{array}\right], \quad
\tilde{\mathbf{X}} =
\left[
\begin{array}{c}
\tilde{\mathbf{x}}_{1}\\
\vdots\\
\tilde{\mathbf{x}}_{n}
\end{array}\right], \quad
\tilde{\mathbf{Y}} =
\left[
\begin{array}{c}
\tilde{\mathbf{y}}_{1}\\
\vdots\\
\tilde{\mathbf{y}}_{n}
\end{array}\right],
\]
where
\[
\bar{y}_i = \frac{1}{T} \sum_{t=1}^T y_{i,t}, \quad \bar{\varepsilon}_i = \frac{1}{T}\sum_{t=1}^T \varepsilon_{i,t} \quad \mbox{and} \quad \bar{\mathbf{x}}_i = \frac{1}{T}\sum_{t=1}^T \mathbf{x}_{i,t}.
\]

\hypertarget{three-standard-cases}{%
\section{Three standard cases}\label{three-standard-cases}}

There are three typical situations:

\begin{itemize}
\tightlist
\item
  \textbf{Pooled regression}: \(\mathbf{z}_i \equiv 1\). This case amounts to the case studied in Chapter \ref{ChapterLS}.
\item
  \textbf{Fixed Effects} (Section \ref{FixedEffect}): \(\mathbf{z}_i\) is unobserved, but correlates with \(\mathbf{x}_i\) \(\Rightarrow\) \(\mathbf{b}\) is biased and inconsistent in the OLS regression of \(\mathbf{y}\) on \(\mathbf{X}\) (omitted variable, see Section \ref{Omitted}).
\item
  \textbf{Random Effects} (Section \ref{RandomEffect}): \(\mathbf{z}_i\) is unobserved, but uncorrelated with \(\mathbf{x}_i\). The model writes:
  \[
  y_{i,t} = \mathbf{x}'_{i,t}\boldsymbol\beta + \alpha +  \underbrace{{\color{blue}u_i + \varepsilon_{i,t}}}_{\mbox{compound error}},
  \]
  where \(\alpha = \mathbb{E}(\mathbf{z}'_{i}\boldsymbol\alpha)\) and \(u_i = \mathbf{z}'_{i}\boldsymbol\alpha - \mathbb{E}(\mathbf{z}'_{i}\boldsymbol\alpha) \perp \mathbf{x}_i\). In that case, the OLS is consistent, but not efficient. GLS can be used to gain efficiencies over OLS (see Section \ref{GLS} for a presentation of the GLS approach).
\end{itemize}

\hypertarget{FixedEffect}{%
\section{Estimation of Fixed-Effects Models}\label{FixedEffect}}

\begin{hypothesis}[Fixed-effect model]
\protect\hypertarget{hyp:FE}{}\label{hyp:FE}

We assume that:

\begin{enumerate}
\def\labelenumi{\roman{enumi}.}
\tightlist
\item
  There is no perfect multicollinearity among the regressors.
\item
  \(\mathbb{E}(\varepsilon_{i,t}|\mathbf{X})=0\), for all \(i,t\).
\item
  We have:
  \[
  \mathbb{E}(\varepsilon_{i,t}\varepsilon_{j,s}|\mathbf{X}) =
  \left\{
  \begin{array}{cl}
  \sigma^2 & \mbox{if $i=j$ and $s=t$},\\
  0 & \mbox{otherwise.}
  \end{array}\right.
  \]
\end{enumerate}

\end{hypothesis}

These assumptions are analogous to those introduced in the standard linear regression:

\begin{enumerate}
\def\labelenumi{(\roman{enumi})}
\tightlist
\item
  \(\leftrightarrow\) Hyp. \ref{hyp:fullrank}, (ii) \(\leftrightarrow\) Hyp. \ref{hyp:exogeneity}, (iii) \(\leftrightarrow\) Hyp. \ref{hyp:homoskedasticity} + \ref{hyp:noncorrelResid}.
\end{enumerate}

In matrix form, for a given \(i\), the model writes:
\[
\mathbf{y}_i = \mathbf{x}_i \boldsymbol\beta + \mathbf{1}\alpha_i + \boldsymbol\varepsilon_i,
\]
where \(\mathbf{1}\) is a \(T\)-dimensional vector of ones.

This is the \textbf{Least Square Dummy Variable (LSDV)} model:
\begin{equation}
\mathbf{y} = [\mathbf{X} \quad \mathbf{D}]
\left[
\begin{array}{c}
\boldsymbol\beta\\
\boldsymbol\alpha
\end{array}
\right]
+ \boldsymbol\varepsilon, \label{eq:LSDV}
\end{equation}
with:
\[
\mathbf{D} = \underbrace{ \left[\begin{array}{cccc}
\mathbf{1}&\mathbf{0}&\dots&\mathbf{0}\\
\mathbf{0}&\mathbf{1}&\dots&\mathbf{0}\\
&&\vdots&\\
\mathbf{0}&\mathbf{0}&\dots&\mathbf{1}\\
\end{array}\right]}_{(nT \times n)}.
\]

The linear regression (Eq. \eqref{eq:LSDV}) ---with the dummy variables--- satisfies the Gauss-Markov conditions (Theorem \ref{thm:GaussMarkov}). Hence, in this context, the OLS estimator is the \emph{best linear unbiased estimator} (BLUE).

Denoting by \(\mathbf{Z}\) the matrix \([\mathbf{X} \quad \mathbf{D}]\), and by \(\mathbf{b}\) and \(\mathbf{a}\) the respective OLS estimates of \(\boldsymbol\beta\) and of \(\boldsymbol\alpha\), we have:
\begin{equation}
\boxed{
\left[
\begin{array}{c}
\mathbf{b}\\
\mathbf{a}
\end{array}
\right]
= [\mathbf{Z}'\mathbf{Z}]^{-1}\mathbf{Z}'\mathbf{y}.} \label{eq:bfixedeffects11}
\end{equation}

The asymptotical distribution of \([\mathbf{b}',\mathbf{a}']'\) derives from the standard OLS context: Prop. \ref{prp:asymptOLS} can be used after having replaced \(\mathbf{X}\) by \(\mathbf{Z}=[\mathbf{X} \quad \mathbf{D}]\).

We have:
\begin{equation}
\boxed{\left[
\begin{array}{c}
\mathbf{b}\\
\mathbf{a}
\end{array}
\right] \overset{d}{\rightarrow}
\mathcal{N}\left(
\left[
\begin{array}{c}
\boldsymbol\beta\\
\boldsymbol\alpha
\end{array}
\right],
\sigma^2 \frac{Q^{-1}}{nT}
\right),}
\end{equation}
where
\[
Q = \mbox{plim}_{nT \rightarrow \infty} \frac{1}{nT} \mathbf{Z}'\mathbf{Z},
\]
assuming the previous limit exists.

In practice, an estimator of the covariance matrix of \([\mathbf{b}',\mathbf{a}']'\) is:
\[
s^2 \left( \mathbf{Z}'\mathbf{Z}\right)^{-1} \quad with \quad s^2 = \frac{\mathbf{e}'\mathbf{e}}{nT - K - n},
\]
where \(\mathbf{e}\) is the \((nT) \times 1\) vector of OLS residuals.

There is an alternative way of expressing the LSDV estimators. It involves the residual-maker matrix matrix \(\mathbf{M_D}=\mathbf{I} - \mathbf{D}(\mathbf{D}'\mathbf{D})^{-1}\mathbf{D}'\) (see Eq. \eqref{eq:Mres}), which acts as an operator that removes entity-specific means, i.e.:
\[
\tilde{\mathbf{Y}} = \mathbf{M_D}\mathbf{Y}, \quad \tilde{\mathbf{X}} = \mathbf{M_D}\mathbf{X} \quad and \quad \tilde{\boldsymbol\varepsilon} = \mathbf{M_D}\boldsymbol\varepsilon.
\]

With these notations, using the Frisch-Waugh theorem (Theorem \ref{thm:FW}), we get another expression for the estimator \(\mathbf{b}\) appearing in Eq. \eqref{eq:bfixedeffects11}:
\begin{equation}
\boxed{\mathbf{b} = [\mathbf{X}'\mathbf{M_D}\mathbf{X}]^{-1}\mathbf{X}'\mathbf{M_D}\mathbf{y}.}\label{eq:bfixedeffects}
\end{equation}

This amounts to regressing the \(\tilde{y}_{i,t}\)'s (\(= y_{i,t} - \bar{y}_i\)) on the \(\tilde{\mathbf{x}}_{i,t}\)'s (\(=\mathbf{x}_{i,t} - \bar{\mathbf{x}}_i\)).

The estimate of \(\boldsymbol\alpha\) is given by:
\begin{equation}
\boxed{\mathbf{a} = (\mathbf{D}'\mathbf{D})^{-1}\mathbf{D}'(\mathbf{y} - \mathbf{X}\mathbf{b}),} \label{eq:a}
\end{equation}
which is obtained by developing the second row of
\[
\left[
\begin{array}{cc}
\mathbf{X}'\mathbf{X} & \mathbf{X}'\mathbf{D}\\
\mathbf{D}'\mathbf{X} & \mathbf{D}'\mathbf{D}
\end{array}\right]
\left[
\begin{array}{c}
\mathbf{b}\\
\mathbf{a}
\end{array}\right] =
\left[
\begin{array}{c}
\mathbf{X}'\mathbf{Y}\\
\mathbf{D}'\mathbf{Y}
\end{array}\right],
\]
which are the first-order conditions resulting from the least squares problem (see Eq. \eqref{eq:OLSFOC}).

One can use different types of fixed effects in the same regression. Typically, one can have time and entity fixed effects. In that case, the model writes:
\[
y_{i,t} = \mathbf{x}_i'\boldsymbol\beta + \alpha_i + \gamma_t + \varepsilon_{i,t}.
\]

The LSDV approach (Eq. \eqref{eq:LSDV}) can still be resorted to. It suffices to extend the \(\mathbf{Z}\) matrix with additional columns (then called \emph{time dummies}):
\begin{equation}
\mathbf{y} = [\mathbf{X} \quad \mathbf{D} \quad \mathbf{C}]
\left[
\begin{array}{c}
\boldsymbol\beta\\
\boldsymbol\alpha\\
\boldsymbol\gamma
\end{array}
\right]
+ \boldsymbol\varepsilon, \label{eq:LSDV2}
\end{equation}
with:
\[
\mathbf{C} = \left[\begin{array}{cccc}
\boldsymbol{\delta}_1&\boldsymbol{\delta}_2&\dots&\boldsymbol{\delta}_{T-1}\\
\vdots&\vdots&&\vdots\\
\boldsymbol{\delta}_1&\boldsymbol{\delta}_2&\dots&\boldsymbol{\delta}_{T-1}\\
\end{array}\right],
\]
where the \(T\)-dimensional vector \(\boldsymbol\delta_t\) (the \emph{time dummy}) is
\[
[0,\dots,0,\underbrace{1}_{\mbox{t$^{th}$ entry}},0,\dots,0]'.
\]

Using state and year fixed effects in the \texttt{CigarettesSW} panel dataset yields the following results:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{CigarettesSW}\SpecialCharTok{$}\NormalTok{rincome }\OtherTok{\textless{}{-}} \FunctionTok{with}\NormalTok{(CigarettesSW, income}\SpecialCharTok{/}\NormalTok{population}\SpecialCharTok{/}\NormalTok{cpi)}
\NormalTok{eq.pooled }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(}\FunctionTok{log}\NormalTok{(packs)}\SpecialCharTok{\textasciitilde{}}\FunctionTok{log}\NormalTok{(rprice)}\SpecialCharTok{+}\FunctionTok{log}\NormalTok{(rincome),}\AttributeTok{data=}\NormalTok{CigarettesSW)}
\NormalTok{eq.LSDV }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(}\FunctionTok{log}\NormalTok{(packs)}\SpecialCharTok{\textasciitilde{}}\FunctionTok{log}\NormalTok{(rprice)}\SpecialCharTok{+}\FunctionTok{log}\NormalTok{(rincome)}\SpecialCharTok{+}\NormalTok{state,}
              \AttributeTok{data=}\NormalTok{CigarettesSW)}
\NormalTok{CigarettesSW}\SpecialCharTok{$}\NormalTok{year }\OtherTok{\textless{}{-}} \FunctionTok{as.factor}\NormalTok{(CigarettesSW}\SpecialCharTok{$}\NormalTok{year)}
\NormalTok{eq.LSDV2 }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(}\FunctionTok{log}\NormalTok{(packs)}\SpecialCharTok{\textasciitilde{}}\FunctionTok{log}\NormalTok{(rprice)}\SpecialCharTok{+}\FunctionTok{log}\NormalTok{(rincome)}\SpecialCharTok{+}\NormalTok{state}\SpecialCharTok{+}\NormalTok{year,}
               \AttributeTok{data=}\NormalTok{CigarettesSW)}
\NormalTok{stargazer}\SpecialCharTok{::}\FunctionTok{stargazer}\NormalTok{(eq.pooled,eq.LSDV,eq.LSDV2,}\AttributeTok{type=}\StringTok{"text"}\NormalTok{,}\AttributeTok{no.space =} \ConstantTok{TRUE}\NormalTok{,}
                     \AttributeTok{omit=}\FunctionTok{c}\NormalTok{(}\StringTok{"state"}\NormalTok{,}\StringTok{"year"}\NormalTok{),}
                     \AttributeTok{add.lines=}\FunctionTok{list}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}State FE\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}No\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}Yes\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}Yes\textquotesingle{}}\NormalTok{),}
                                    \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}Year FE\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}No\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}No\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}Yes\textquotesingle{}}\NormalTok{)),}
                     \AttributeTok{omit.stat=}\FunctionTok{c}\NormalTok{(}\StringTok{"f"}\NormalTok{,}\StringTok{"ser"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## ==========================================
##                   Dependent variable:     
##              -----------------------------
##                       log(packs)          
##                 (1)       (2)       (3)   
## ------------------------------------------
## log(rprice)  -1.334*** -1.210*** -1.056***
##               (0.135)   (0.114)   (0.149) 
## log(rincome)  0.318**    0.121     0.497  
##               (0.136)   (0.190)   (0.304) 
## Constant     10.067*** 9.954***  8.360*** 
##               (0.516)   (0.264)   (1.049) 
## ------------------------------------------
## State FE        No        Yes       Yes   
## Year FE         No        No        Yes   
## Observations    96        96        96    
## R2             0.552     0.966     0.967  
## Adjusted R2    0.542     0.929     0.931  
## ==========================================
## Note:          *p<0.1; **p<0.05; ***p<0.01
\end{verbatim}

\begin{example}[Housing prices and interest rates]
\protect\hypertarget{exm:JSTPanel}{}\label{exm:JSTPanel}

In this example, we want to estimate the effect of short and long-term interest rate on housing prices. The data come from the \citet{JST_2017} dataset (\href{https://www.macrohistory.net}{see this website}).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(AEC);}\FunctionTok{library}\NormalTok{(sandwich)}
\FunctionTok{data}\NormalTok{(JST); JST }\OtherTok{\textless{}{-}} \FunctionTok{subset}\NormalTok{(JST,year}\SpecialCharTok{\textgreater{}}\DecValTok{1950}\NormalTok{);N }\OtherTok{\textless{}{-}} \FunctionTok{dim}\NormalTok{(JST)[}\DecValTok{1}\NormalTok{]}
\NormalTok{JST}\SpecialCharTok{$}\NormalTok{hpreal }\OtherTok{\textless{}{-}}\NormalTok{ JST}\SpecialCharTok{$}\NormalTok{hpnom}\SpecialCharTok{/}\NormalTok{JST}\SpecialCharTok{$}\NormalTok{cpi }\CommentTok{\# real house price index}
\NormalTok{JST}\SpecialCharTok{$}\NormalTok{dhpreal }\OtherTok{\textless{}{-}} \DecValTok{100}\SpecialCharTok{*}\FunctionTok{log}\NormalTok{(JST}\SpecialCharTok{$}\NormalTok{hpreal}\SpecialCharTok{/}\FunctionTok{c}\NormalTok{(}\ConstantTok{NaN}\NormalTok{,JST}\SpecialCharTok{$}\NormalTok{hpreal[}\DecValTok{1}\SpecialCharTok{:}\NormalTok{(N}\DecValTok{{-}1}\NormalTok{)]))}
\CommentTok{\# Put NA\textquotesingle{}s when change in country:}
\NormalTok{JST}\SpecialCharTok{$}\NormalTok{dhpreal[}\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,JST}\SpecialCharTok{$}\NormalTok{iso[}\DecValTok{2}\SpecialCharTok{:}\NormalTok{N]}\SpecialCharTok{!=}\NormalTok{JST}\SpecialCharTok{$}\NormalTok{iso[}\DecValTok{1}\SpecialCharTok{:}\NormalTok{(N}\DecValTok{{-}1}\NormalTok{)])] }\OtherTok{\textless{}{-}} \ConstantTok{NaN}
\NormalTok{JST}\SpecialCharTok{$}\NormalTok{dhpreal[}\FunctionTok{abs}\NormalTok{(JST}\SpecialCharTok{$}\NormalTok{dhpreal)}\SpecialCharTok{\textgreater{}}\DecValTok{30}\NormalTok{] }\OtherTok{\textless{}{-}} \ConstantTok{NaN} \CommentTok{\# remove extreme price change}
\NormalTok{JST}\SpecialCharTok{$}\NormalTok{YEAR }\OtherTok{\textless{}{-}} \FunctionTok{as.factor}\NormalTok{(JST}\SpecialCharTok{$}\NormalTok{year) }\CommentTok{\# to have time fixed effects}
\NormalTok{eq1\_noFE }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(dhpreal }\SpecialCharTok{\textasciitilde{}}\NormalTok{ stir }\SpecialCharTok{+}\NormalTok{ ltrate,}\AttributeTok{data=}\NormalTok{JST)}
\NormalTok{eq1\_FE   }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(dhpreal }\SpecialCharTok{\textasciitilde{}}\NormalTok{ stir }\SpecialCharTok{+}\NormalTok{ ltrate }\SpecialCharTok{+}\NormalTok{ iso }\SpecialCharTok{+}\NormalTok{ YEAR,}\AttributeTok{data=}\NormalTok{JST)}
\NormalTok{eq2\_noFE }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(dhpreal }\SpecialCharTok{\textasciitilde{}} \FunctionTok{I}\NormalTok{(ltrate}\SpecialCharTok{{-}}\NormalTok{stir),}\AttributeTok{data=}\NormalTok{JST)}
\NormalTok{eq2\_FE }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(dhpreal }\SpecialCharTok{\textasciitilde{}} \FunctionTok{I}\NormalTok{(ltrate}\SpecialCharTok{{-}}\NormalTok{stir) }\SpecialCharTok{+}\NormalTok{ iso }\SpecialCharTok{+}\NormalTok{ YEAR,}\AttributeTok{data=}\NormalTok{JST)}
\NormalTok{vcov\_cluster1\_noFE }\OtherTok{\textless{}{-}} \FunctionTok{vcovHC}\NormalTok{(eq1\_noFE, }\AttributeTok{cluster =}\NormalTok{ JST[, }\FunctionTok{c}\NormalTok{(}\StringTok{"iso"}\NormalTok{,}\StringTok{"YEAR"}\NormalTok{)])}
\NormalTok{vcov\_cluster1\_FE   }\OtherTok{\textless{}{-}} \FunctionTok{vcovHC}\NormalTok{(eq1\_FE, }\AttributeTok{cluster =}\NormalTok{ JST[, }\FunctionTok{c}\NormalTok{(}\StringTok{"iso"}\NormalTok{,}\StringTok{"YEAR"}\NormalTok{)])}
\NormalTok{vcov\_cluster2\_noFE }\OtherTok{\textless{}{-}} \FunctionTok{vcovHC}\NormalTok{(eq2\_noFE, }\AttributeTok{cluster =}\NormalTok{ JST[, }\FunctionTok{c}\NormalTok{(}\StringTok{"iso"}\NormalTok{,}\StringTok{"YEAR"}\NormalTok{)])}
\NormalTok{vcov\_cluster2\_FE   }\OtherTok{\textless{}{-}} \FunctionTok{vcovHC}\NormalTok{(eq2\_FE, }\AttributeTok{cluster =}\NormalTok{ JST[, }\FunctionTok{c}\NormalTok{(}\StringTok{"iso"}\NormalTok{,}\StringTok{"YEAR"}\NormalTok{)])}
\NormalTok{robust\_se\_FE1\_noFE }\OtherTok{\textless{}{-}} \FunctionTok{sqrt}\NormalTok{(}\FunctionTok{diag}\NormalTok{(vcov\_cluster1\_noFE))}
\NormalTok{robust\_se\_FE1\_FE   }\OtherTok{\textless{}{-}} \FunctionTok{sqrt}\NormalTok{(}\FunctionTok{diag}\NormalTok{(vcov\_cluster1\_FE))}
\NormalTok{robust\_se\_FE2\_noFE }\OtherTok{\textless{}{-}} \FunctionTok{sqrt}\NormalTok{(}\FunctionTok{diag}\NormalTok{(vcov\_cluster2\_noFE))}
\NormalTok{robust\_se\_FE2\_FE   }\OtherTok{\textless{}{-}} \FunctionTok{sqrt}\NormalTok{(}\FunctionTok{diag}\NormalTok{(vcov\_cluster2\_FE))}
\NormalTok{stargazer}\SpecialCharTok{::}\FunctionTok{stargazer}\NormalTok{(eq1\_noFE, eq1\_FE, eq2\_noFE, eq2\_FE, }\AttributeTok{type =} \StringTok{"text"}\NormalTok{,}
                     \AttributeTok{column.labels =} \FunctionTok{c}\NormalTok{(}\StringTok{"no FE"}\NormalTok{, }\StringTok{"with FE"}\NormalTok{, }\StringTok{"no FE"}\NormalTok{,}\StringTok{"with FE"}\NormalTok{),}
                     \AttributeTok{omit =} \FunctionTok{c}\NormalTok{(}\StringTok{"iso"}\NormalTok{,}\StringTok{"YEAR"}\NormalTok{,}\StringTok{"Constant"}\NormalTok{),}\AttributeTok{keep.stat =} \StringTok{"n"}\NormalTok{,}
                     \AttributeTok{add.lines=}\FunctionTok{list}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}Country FE\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}No\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}Yes\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}No\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}Yes\textquotesingle{}}\NormalTok{),}
                                    \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}Year FE\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}No\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}Yes\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}No\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}Yes\textquotesingle{}}\NormalTok{)),}
                     \AttributeTok{se =} \FunctionTok{list}\NormalTok{(robust\_se\_FE1\_noFE,robust\_se\_FE1\_FE,}
\NormalTok{                               robust\_se\_FE2\_noFE,robust\_se\_FE2\_FE))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## =======================================================
##                           Dependent variable:          
##                  --------------------------------------
##                                 dhpreal                
##                    no FE   with FE    no FE    with FE 
##                     (1)      (2)       (3)       (4)   
## -------------------------------------------------------
## stir             0.485***  0.532***                    
##                   (0.149)  (0.170)                     
##                                                        
## ltrate           -0.690*** -0.384**                    
##                   (0.164)  (0.182)                     
##                                                        
## I(ltrate - stir)                    -0.476*** -0.475***
##                                      (0.145)   (0.159) 
##                                                        
## -------------------------------------------------------
## Country FE          No       Yes       No        Yes   
## Year FE             No       Yes       No        Yes   
## Observations       1,141    1,141     1,141     1,141  
## =======================================================
## Note:                       *p<0.1; **p<0.05; ***p<0.01
\end{verbatim}

\end{example}

\hypertarget{RandomEffect}{%
\section{Estimation of random effects models}\label{RandomEffect}}

Here, the individual effects are assumed to be not correlated to other variables (the \(\mathbf{x}_i\)'s). In that context, the OLS estimator is consistent. However, it is not efficient. The GLS approach can be employed to gain efficiency.

\textbf{Random-effect models} write:
\[
y_{i,t}=\mathbf{x}'_{it}\boldsymbol\beta + (\alpha + \underbrace{u_i}_{\substack{\text{Random}\\\text{heterogeneity}}}) + \varepsilon_{i,t},
\]
with
\begin{eqnarray*}
\mathbb{E}(\varepsilon_{i,t}|\mathbf{X})&=&\mathbb{E}(u_{i}|\mathbf{X}) =0,\\
\mathbb{E}(\varepsilon_{i,t}\varepsilon_{j,s}|\mathbf{X}) &=&
\left\{
\begin{array}{cl}
\sigma_\varepsilon^2 & \mbox{ if $i=j$ and $s=t$},\\
0 & \mbox{ otherwise.}
\end{array}
\right.\\
\mathbb{E}(u_{i}u_{j}|\mathbf{X}) &=&
\left\{
\begin{array}{cl}
\sigma_u^2 & \mbox{ if $i=j$},\\
0 & \mbox{otherwise.}
\end{array}
\right.\\
\mathbb{E}(\varepsilon_{i,t}u_{j}|\mathbf{X})&=&0 \quad \text{for all $i$, $j$ and $t$}.
\end{eqnarray*}

Introducing the notations \(\eta_{i,t} = u_i + \varepsilon_{i,t}\) and \(\boldsymbol\eta_i = [\eta_{i,1},\dots,\eta_{i,T}]'\), we have \(\mathbb{E}(\boldsymbol\eta_i |\mathbf{X}) = \mathbf{0}\) and \(\mathbb{V}ar(\boldsymbol\eta_i | \mathbf{X}) = \boldsymbol\Gamma\), where
\[
\boldsymbol\Gamma = \left[  \begin{array}{ccccc}
\sigma_\varepsilon^2+\sigma_u^2 & \sigma_u^2 & \sigma_u^2 & \dots & \sigma_u^2\\
\sigma_u^2 & \sigma_\varepsilon^2+\sigma_u^2 & \sigma_u^2 & \dots & \sigma_u^2\\
\vdots && \ddots && \vdots \\
\sigma_u^2 & \sigma_u^2 & \sigma_u^2 & \dots & \sigma_\varepsilon^2+\sigma_u^2\\
\end{array}
\right] = \sigma_\varepsilon^2\mathbf{I} + \sigma_u^2\mathbf{1}\mathbf{1}'.
\]

Denoting by \(\boldsymbol\Sigma\) the covariance matrix of \(\boldsymbol\eta = [\boldsymbol\eta_1',\dots,\boldsymbol\eta_n']'\), we have:
\[
\boldsymbol\Sigma = \mathbf{I} \otimes \boldsymbol\Gamma.
\]

If we knew \(\boldsymbol\Sigma\), we would apply (feasible) GLS (Eq. \eqref{eq:betaGLS}, in Section \ref{GLS}):
\[
\boldsymbol\beta = (\mathbf{X}'\boldsymbol\Sigma^{-1}\mathbf{X})^{-1}\mathbf{X}'\boldsymbol\Sigma^{-1}\mathbf{y}.
\]
(As explained in Section \ref{GLS}, this amounts to regressing \({\boldsymbol\Sigma^{-1/2}}'\mathbf{y}\) on \({\boldsymbol\Sigma^{-1/2}}'\mathbf{X}\).)

It can be checked that \(\boldsymbol\Sigma^{-1/2} = \mathbf{I} \otimes (\boldsymbol\Gamma^{-1/2})\) where
\[
\boldsymbol\Gamma^{-1/2} = \frac{1}{\sigma_\varepsilon}\left( \mathbf{I} - \frac{\theta}{T}\mathbf{1}\mathbf{1}'\right),\quad \mbox{with}\quad\theta = 1 - \frac{\sigma_\varepsilon}{\sqrt{\sigma_\varepsilon^2+T\sigma_u^2}}.
\]

Hence, if we knew \(\boldsymbol\Sigma\), we would transform the data as follows:
\[
\boldsymbol\Gamma^{-1/2}\mathbf{y}_i = \frac{1}{\sigma_\varepsilon}\left[\begin{array}{c}y_{i,1} - \theta\bar{y}_i\\y_{i,2} - \theta\bar{y}_i\\\vdots\\y_{i,T} - \theta\bar{y}_i\\\end{array}\right].
\]

What about when \(\boldsymbol\Sigma\) is unknown? One can take deviations from group means to remove heterogeneity:
\begin{equation}
y_{i,t} - \bar{y}_i = [\mathbf{x}_{i,t} - \bar{\mathbf{x}}_i]'\boldsymbol\beta + (\varepsilon_{i,t} - \bar{\varepsilon}_i).\label{eq:OLSRUM}
\end{equation}
The previous equation can be consistently estimated by OLS. (Although the residuals are correlated across \(t\)'s for the observations pertaining to a given entity, the OLS remain consistent; see Prop. \ref{prp:XXX}.)

We have \(\mathbb{E}\left[\sum_{i=1}^{T}(\varepsilon_{i,t}-\bar{\varepsilon}_i)^2\right] = (T-1)\sigma_{\varepsilon}^2\).

The \(\varepsilon_{i,t}\)'s are not observed but \(\mathbf{b}\), the OLS estimator of \(\boldsymbol\beta\) in Eq. \eqref{eq:OLSRUM}, is a consistent estimator of \(\boldsymbol\beta\). Using an adjustment for the degrees of freedom, we can approximate their variance with:
\[
\hat{\sigma}_e^2 = \frac{1}{nT-n-K}\sum_{i=1}^{n}\sum_{t=1}^{T}(e_{i,t} - \bar{e}_i)^2.
\]

What about \(\sigma_u^2\)? We can exploit the fact that OLS are consistent in the pooled regression:
\[
\mbox{plim }s^2_{pooled} = \mbox{plim }\frac{\mathbf{e}'\mathbf{e}}{nT-K-1} = \sigma_u^2 + \sigma_\varepsilon^2,
\]
and therefore use \(s^2_{pooled} - \hat{\sigma}_e^2\) as an approximation to \(\sigma_u^2\).

Let us come back to Example \ref{exm:JSTPanel} (relationship between changes in housing prices and interest rates). In the following, we use the random effect specification; and compare the results with those obtained with the pooled regression and with the fixed-effect model. For that, we use the function \texttt{plm} of the package of the same name. (Note that \texttt{eq.FE} is similar to \texttt{eq1} in Example \ref{exm:JSTPanel}.)

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(plm);}\FunctionTok{library}\NormalTok{(stargazer)}
\NormalTok{eq.RE }\OtherTok{\textless{}{-}} \FunctionTok{plm}\NormalTok{(dhpreal }\SpecialCharTok{\textasciitilde{}}\NormalTok{ stir }\SpecialCharTok{+}\NormalTok{ ltrate,}\AttributeTok{data=}\NormalTok{JST,}\AttributeTok{index=}\FunctionTok{c}\NormalTok{(}\StringTok{"iso"}\NormalTok{,}\StringTok{"YEAR"}\NormalTok{),}
             \AttributeTok{model=}\StringTok{"random"}\NormalTok{,}\AttributeTok{effect=}\StringTok{"twoways"}\NormalTok{)}
\NormalTok{eq.FE }\OtherTok{\textless{}{-}} \FunctionTok{plm}\NormalTok{(dhpreal }\SpecialCharTok{\textasciitilde{}}\NormalTok{ stir }\SpecialCharTok{+}\NormalTok{ ltrate,}\AttributeTok{data=}\NormalTok{JST,}\AttributeTok{index=}\FunctionTok{c}\NormalTok{(}\StringTok{"iso"}\NormalTok{,}\StringTok{"YEAR"}\NormalTok{),}
             \AttributeTok{model=}\StringTok{"within"}\NormalTok{,}\AttributeTok{effect=}\StringTok{"twoways"}\NormalTok{)}
\NormalTok{eq0   }\OtherTok{\textless{}{-}} \FunctionTok{plm}\NormalTok{(dhpreal }\SpecialCharTok{\textasciitilde{}}\NormalTok{ stir }\SpecialCharTok{+}\NormalTok{ ltrate,}\AttributeTok{data=}\NormalTok{JST,}\AttributeTok{index=}\FunctionTok{c}\NormalTok{(}\StringTok{"iso"}\NormalTok{,}\StringTok{"YEAR"}\NormalTok{),}
             \AttributeTok{model=}\StringTok{"pooling"}\NormalTok{) }
\FunctionTok{stargazer}\NormalTok{(eq0, eq.RE, eq.FE, }\AttributeTok{type =} \StringTok{"text"}\NormalTok{,}\AttributeTok{no.space =} \ConstantTok{TRUE}\NormalTok{,}
                     \AttributeTok{column.labels=}\FunctionTok{c}\NormalTok{(}\StringTok{"Pooled"}\NormalTok{,}\StringTok{"Random Effect"}\NormalTok{,}\StringTok{"Fixed Effects"}\NormalTok{),}
                     \AttributeTok{add.lines=}\FunctionTok{list}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}State FE\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}No\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}Yes\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}Yes\textquotesingle{}}\NormalTok{),}
                                    \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}Year FE\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}No\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}Yes\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}Yes\textquotesingle{}}\NormalTok{)),}
                     \AttributeTok{omit.stat=}\FunctionTok{c}\NormalTok{(}\StringTok{"f"}\NormalTok{,}\StringTok{"ser"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## ==================================================
##                       Dependent variable:         
##              -------------------------------------
##                             dhpreal               
##               Pooled   Random Effect Fixed Effects
##                 (1)         (2)           (3)     
## --------------------------------------------------
## stir         0.485***    0.456***      0.532***   
##               (0.114)     (0.019)       (0.134)   
## ltrate       -0.690***   -0.541***     -0.384***  
##               (0.127)     (0.020)       (0.145)   
## Constant     4.103***    3.341***                 
##               (0.421)     (0.096)                 
## --------------------------------------------------
## State FE        No          Yes           Yes     
## Year FE         No          Yes           Yes     
## Observations   1,141       1,141         1,141    
## R2             0.027       0.024         0.015    
## Adjusted R2    0.025       0.022        -0.067    
## ==================================================
## Note:                  *p<0.1; **p<0.05; ***p<0.01
\end{verbatim}

One can run an \citet{Hausman_1978} test in order to check whether or not the fixed-effect model is needed. Indeed, if this is not the case (i.e., if the covariates are not correlated to the disturbances), then it is preferable to use the random-effect estimation as the latter is more efficient.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{phtest}\NormalTok{(eq.FE,eq.RE)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  Hausman Test
## 
## data:  dhpreal ~ stir + ltrate
## chisq = 3.8386, df = 2, p-value = 0.1467
## alternative hypothesis: one model is inconsistent
\end{verbatim}

The p-value being high, we do not reject the null hypothesis according to which the covariates and the errors are uncorrelated. We should therefore prefer the random-effect model.

\begin{example}[Spatial data]
\protect\hypertarget{exm:airbnb}{}\label{exm:airbnb}This example makes use of Airbnb prices (ZÃ¼rich, 22 June 2017), collected from \href{http://tomslee.net/airbnb-data-collection-get-the-data}{Tom Slee's website}. The covariates are the number of bedrooms and the number of people that can be accommodated. We consider the use of district fixed effects. Figure \ref{fig:airbnb} shows the price to explain (the size of the circles is proportional to the prices). The white lines delineate the 12 districts of the city.

\begin{figure}
\includegraphics[width=0.95\linewidth]{MicroEc_files/figure-latex/airbnb-1} \caption{Airbnb prices for the Zurich area, 22 June 2017. The size of the circles is proportional to the prices. White lines delineate the 12 districts of the city.}\label{fig:airbnb}
\end{figure}

Let us regress prices on the covariates as well as on district dummies:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{eq\_noFE }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(price}\SpecialCharTok{\textasciitilde{}}\NormalTok{bedrooms}\SpecialCharTok{+}\NormalTok{accommodates,}\AttributeTok{data=}\NormalTok{airbnb)}
\NormalTok{eq\_FE   }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(price}\SpecialCharTok{\textasciitilde{}}\NormalTok{bedrooms}\SpecialCharTok{+}\NormalTok{accommodates}\SpecialCharTok{+}\NormalTok{neighborhood,}\AttributeTok{data=}\NormalTok{airbnb)}
\CommentTok{\# Adjust standard errors:}
\NormalTok{cov\_FE          }\OtherTok{\textless{}{-}} \FunctionTok{vcovHC}\NormalTok{(eq\_FE, }\AttributeTok{cluster =}\NormalTok{ airbnb[, }\FunctionTok{c}\NormalTok{(}\StringTok{"neighborhood"}\NormalTok{)])}
\NormalTok{robust\_se\_FE    }\OtherTok{\textless{}{-}} \FunctionTok{sqrt}\NormalTok{(}\FunctionTok{diag}\NormalTok{(cov\_FE))}
\NormalTok{cov\_noFE        }\OtherTok{\textless{}{-}} \FunctionTok{vcovHC}\NormalTok{(eq\_noFE, }\AttributeTok{cluster =}\NormalTok{ airbnb[, }\FunctionTok{c}\NormalTok{(}\StringTok{"neighborhood"}\NormalTok{)])}
\NormalTok{robust\_se\_noFE  }\OtherTok{\textless{}{-}} \FunctionTok{sqrt}\NormalTok{(}\FunctionTok{diag}\NormalTok{(cov\_noFE))}
\NormalTok{stargazer}\SpecialCharTok{::}\FunctionTok{stargazer}\NormalTok{(eq\_FE, eq\_noFE, eq\_FE, eq\_noFE, }\AttributeTok{type =} \StringTok{"text"}\NormalTok{,}
                     \AttributeTok{column.labels =} \FunctionTok{c}\NormalTok{(}\StringTok{"FE (no HAC)"}\NormalTok{, }\StringTok{"No FE (no HAC)"}\NormalTok{,}
                                       \StringTok{"FE (with HAC)"}\NormalTok{, }\StringTok{"No FE (with HAC)"}\NormalTok{),}
                     \AttributeTok{omit =} \FunctionTok{c}\NormalTok{(}\StringTok{"neighborhood"}\NormalTok{),}\AttributeTok{no.space =} \ConstantTok{TRUE}\NormalTok{,}
                     \AttributeTok{omit.labels =} \FunctionTok{c}\NormalTok{(}\StringTok{"District FE"}\NormalTok{),}\AttributeTok{keep.stat =} \StringTok{"n"}\NormalTok{,}
                     \AttributeTok{se =} \FunctionTok{list}\NormalTok{(}\ConstantTok{NULL}\NormalTok{, }\ConstantTok{NULL}\NormalTok{, robust\_se\_FE, robust\_se\_noFE))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## ======================================================================
##                                 Dependent variable:                   
##              ---------------------------------------------------------
##                                        price                          
##              FE (no HAC) No FE (no HAC) FE (with HAC) No FE (with HAC)
##                  (1)          (2)            (3)            (4)       
## ----------------------------------------------------------------------
## bedrooms      7.229***      5.629**       7.229***        5.629***    
##                (2.135)      (2.194)        (2.052)        (2.073)     
## accommodates  16.426***    17.449***      16.426***      17.449***    
##                (1.284)      (1.323)        (1.431)        (1.428)     
## Constant      95.118***    68.417***      95.118***      68.417***    
##                (5.323)      (3.223)        (5.664)        (3.527)     
## ----------------------------------------------------------------------
## District FE      Yes           No            Yes             No       
## ----------------------------------------------------------------------
## Observations    1,321        1,321          1,321          1,321      
## ======================================================================
## Note:                                      *p<0.1; **p<0.05; ***p<0.01
\end{verbatim}

Figure \ref{fig:airbnb3} compares the residuals with and without fixed effects. The sizes of the circles are proportional to the absolute values of the residuals, the color indicates the sign (blue for positive).

\begin{figure}
\includegraphics[width=0.95\linewidth]{MicroEc_files/figure-latex/airbnb3-1} \caption{Regression residuals. The sizes of the circles are proportional to the absolute values of the residuals, the color indicates the sign (blue for negative).}\label{fig:airbnb3}
\end{figure}

With fixed effects, the colors are better balanced within each district.
\end{example}

\hypertarget{DynPanel}{%
\section{Dynamic Panel Regressions}\label{DynPanel}}

In what precedes, it has been assumed that there is no correlation between the observations indexed by \((i,t)\) and those indexed by \((j,s)\) as long as \(j \ne i\) or \(t \ne s\). If one suspects that the errors \(\varepsilon_{i,t}\) are correlated (across entities \(i\) for a given date \(t\), or across dates for a given entity, or both), then one should employ a robust covariance matrix (see Section \ref{Clusters}).

In several cases, auto-correlation in the variable of interest may stem from an auto-regressive specification. That is, Eq. \eqref{eq:panel1} is then replaced by:
\begin{equation}
y_{i,t} = \rho y_{i,t-1} + \mathbf{x}'_{i,t}\underbrace{\boldsymbol\beta}_{K \times 1} + \underbrace{\alpha_i}_{\mbox{Individual effects}} + \varepsilon_{i,t}.\label{eq:paneldyn}
\end{equation}

In that case, even if the explanatory variables \(\mathbf{x}_{i,t}\) are uncorrelated to the errors \(\varepsilon_{i,t}\), we have that the additional \emph{explanatory variable} \(y_{i,t-1}\) correlates to the errors \(\varepsilon_{i,t-1},\varepsilon_{i,t-2},\dots,\varepsilon_{i,1}\). As a result, the LSDV estimate of the model parameters \(\{\rho,\boldsymbol\beta\}\) may be biased, even if \(n\) is large. To see this, notice that the LSDV regression amounts to regressing \(\widetilde{\mathbf{y}}\) on \(\widetilde{\mathbf{X}}\) (see Eq. \eqref{eq:bfixedeffects}), where the elements of \(\widetilde{\mathbf{X}}\) are the explanatory variables to which we subtract their within-sample means. In particular, we have:
\[
\tilde{y}_{i,t-1} = y_{i,t-1} - \frac{1}{T} \sum_{s=1}^{T} y_{i,s-1},
\]
which correlates to the corresponding error, that is:
\[
\tilde{\varepsilon}_{i,t} = \varepsilon_{i,t} - \frac{1}{T} \sum_{s=1}^{T} \varepsilon_{i,s}.
\]

The previous equation shows that the \emph{within-group} estimator (LSDV) introduces all realizations of the \(\varepsilon_{i,t}\) errors into the transformed error term (\(\tilde{\varepsilon}_{i,t}\)). As a result, in large-\(n\) fixed-\(T\) panels, it is consistent only if all the right-hand-side variables of the regression are strictly exogenous (i.e., do not correlate to past, present, and future errors \(\varepsilon_{i,t}\)).\footnote{Although the bias may vanish for large \(T\)'s, it does not if \(n\) only goes to infinity.} This is not the case when there are lags of \(y_{i,t}\) on the right-hand side of the regression formula.

The following simulation illustrate this bias. The \(x\)-coordinates of the dots are the fixed effects \(\alpha_i\)'s, and the \(y\)-coordinates are their LSDV estimates. The blue line is the 45-degree line.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n }\OtherTok{\textless{}{-}} \DecValTok{400}\NormalTok{;T }\OtherTok{\textless{}{-}} \DecValTok{10}
\NormalTok{rho }\OtherTok{\textless{}{-}} \FloatTok{0.8}\NormalTok{;sigma }\OtherTok{\textless{}{-}}\NormalTok{ .}\DecValTok{5}
\NormalTok{alpha }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{y }\OtherTok{\textless{}{-}}\NormalTok{ alpha }\SpecialCharTok{/}\NormalTok{(}\DecValTok{1}\SpecialCharTok{{-}}\NormalTok{rho) }\SpecialCharTok{+}\NormalTok{ sigma}\SpecialCharTok{\^{}}\DecValTok{2}\SpecialCharTok{/}\NormalTok{(}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ rho}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{) }\SpecialCharTok{*} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{all\_y }\OtherTok{\textless{}{-}}\NormalTok{ y}
\ControlFlowTok{for}\NormalTok{(t }\ControlFlowTok{in} \DecValTok{2}\SpecialCharTok{:}\NormalTok{T)\{}
\NormalTok{  y }\OtherTok{\textless{}{-}}\NormalTok{ rho }\SpecialCharTok{*}\NormalTok{ y }\SpecialCharTok{+}\NormalTok{ alpha }\SpecialCharTok{+}\NormalTok{ sigma }\SpecialCharTok{*} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{  all\_y }\OtherTok{\textless{}{-}} \FunctionTok{rbind}\NormalTok{(all\_y,y)}
\NormalTok{\}}
\NormalTok{y   }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(all\_y[}\DecValTok{2}\SpecialCharTok{:}\NormalTok{T,]);y\_1 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(all\_y[}\DecValTok{1}\SpecialCharTok{:}\NormalTok{(T}\DecValTok{{-}1}\NormalTok{),])}
\NormalTok{D }\OtherTok{\textless{}{-}} \FunctionTok{diag}\NormalTok{(n) }\SpecialCharTok{\%x\%} \FunctionTok{rep}\NormalTok{(}\DecValTok{1}\NormalTok{,T}\DecValTok{{-}1}\NormalTok{)}
\NormalTok{Z }\OtherTok{\textless{}{-}} \FunctionTok{cbind}\NormalTok{(}\FunctionTok{c}\NormalTok{(y\_1),D)}
\NormalTok{b }\OtherTok{\textless{}{-}} \FunctionTok{solve}\NormalTok{(}\FunctionTok{t}\NormalTok{(Z) }\SpecialCharTok{\%*\%}\NormalTok{ Z) }\SpecialCharTok{\%*\%} \FunctionTok{t}\NormalTok{(Z) }\SpecialCharTok{\%*\%}\NormalTok{ y}
\NormalTok{a }\OtherTok{\textless{}{-}}\NormalTok{ b[}\DecValTok{2}\SpecialCharTok{:}\NormalTok{(n}\SpecialCharTok{+}\DecValTok{1}\NormalTok{)]}
\FunctionTok{plot}\NormalTok{(alpha,a)}
\FunctionTok{lines}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{10}\NormalTok{,}\DecValTok{10}\NormalTok{),}\FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{10}\NormalTok{,}\DecValTok{10}\NormalTok{),}\AttributeTok{col=}\StringTok{"blue"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\includegraphics[width=0.95\linewidth]{MicroEc_files/figure-latex/dynpanel1-1} \caption{illustration of the bias pertianing to the LSDV estimation approach in the presence of auto-correlation of the depend variable.}\label{fig:dynpanel1}
\end{figure}

In the previous example, the estimate of \(\rho\) (whose true value is 0.8) is 0.531.

To address this, one can resort to instrumental-variable regressions. \citet{Anderson_Hsiao_1982} have, in particular, proposed a first-differenced Two Stage Least Squares (2SLS) estimator (see Eq. \eqref{eq:IV} in Section \ref{IV}). This estimation is based on the following transformation of the model:
\begin{equation}
\Delta y_{i,t} = \rho \Delta y_{i,t-1} + (\Delta \mathbf{x}_{i,t})'\boldsymbol\beta + \Delta\varepsilon_{i,t}.\label{eq:paneldynFisrtDiff}
\end{equation}
The OLS estimates of the parameters are biased because \(\varepsilon_{i,t-1}\) ---which is part of the error \(\Delta\varepsilon_{i,t}\)--- is correlated to \(y_{i,t-1}\) ---which is part of the ``explanatory variable'', namely \(\Delta y_{i,t-1}\). But consistent estimates can be obtained using 2SLS with instrumental variables that are correlated with \(\Delta y_{i,t}\) but orthogonal to \(\Delta\varepsilon_{i,t}\). One can for instance use \(\{y_{i,t-2},\mathbf{x}_{i,t-2}\}\) as instruments. Note that this approach can be implemented only if there are more than 3 time observations per entity \(i\).

If the explanatory variables \(\mathbf{x}_{i,t}\) are assumed to be predetermined (i.e., do not contemporaneous correlate with the errors \(\varepsilon_{i,t}\)), then \(\mathbf{x}_{i,t-1}\) can be added to the instruments associated with \(\Delta y_{i,t}\). Further, if these variables (the \(\mathbf{x}_{i,t}\)'s) are exogenous (i.e., do not contemporaneous correlate with any of the errors \(\varepsilon_{i,s}\), \(\forall s\)), then \(\mathbf{x}_{i,t}\) also constitute a valid instrument.

Using the previous (simulated) example, this approach consists in the following steps:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Dy   }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(all\_y[}\DecValTok{3}\SpecialCharTok{:}\NormalTok{T,]) }\SpecialCharTok{{-}} \FunctionTok{c}\NormalTok{(all\_y[}\DecValTok{2}\SpecialCharTok{:}\NormalTok{(T}\DecValTok{{-}1}\NormalTok{),])}
\NormalTok{Dy\_1 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(all\_y[}\DecValTok{2}\SpecialCharTok{:}\NormalTok{(T}\DecValTok{{-}1}\NormalTok{),]) }\SpecialCharTok{{-}} \FunctionTok{c}\NormalTok{(all\_y[}\DecValTok{1}\SpecialCharTok{:}\NormalTok{(T}\DecValTok{{-}2}\NormalTok{),])}
\NormalTok{y\_2  }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(all\_y[}\DecValTok{1}\SpecialCharTok{:}\NormalTok{(T}\DecValTok{{-}2}\NormalTok{),])}
\NormalTok{Z }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(y\_2,}\AttributeTok{ncol=}\DecValTok{1}\NormalTok{)}
\NormalTok{Pz }\OtherTok{\textless{}{-}}\NormalTok{ Z }\SpecialCharTok{\%*\%} \FunctionTok{solve}\NormalTok{(}\FunctionTok{t}\NormalTok{(Z) }\SpecialCharTok{\%*\%}\NormalTok{ Z) }\SpecialCharTok{\%*\%} \FunctionTok{t}\NormalTok{(Z)}
\NormalTok{Dy\_1hat }\OtherTok{\textless{}{-}}\NormalTok{ Pz }\SpecialCharTok{\%*\%}\NormalTok{ Dy\_1}
\NormalTok{rho\_2SLS }\OtherTok{\textless{}{-}} \FunctionTok{solve}\NormalTok{(}\FunctionTok{t}\NormalTok{(Dy\_1hat) }\SpecialCharTok{\%*\%}\NormalTok{ Dy\_1hat) }\SpecialCharTok{\%*\%} \FunctionTok{t}\NormalTok{(Dy\_1hat) }\SpecialCharTok{\%*\%}\NormalTok{ Dy}
\end{Highlighting}
\end{Shaded}

While the OLS estimate of \(\rho\) (whose true value is 0.8) was 0.531, we obtain here \texttt{rho\_2SLS} \(=\) 0.89.

Let us come back to the general case (with covariates \(\mathbf{x}_{i,k}\)'s). For \(t=3\), \(y_{i,1}\) (and \(\mathbf{x}_{i,1}\)) is the only possible instrument. However, for \(t=4\), one could use \(y_{i,2}\) and \(y_{i,1}\) (as well as \(\mathbf{x}_{i,2}\) and \(\mathbf{x}_{i,1}\)). More generally, defining matrix \(Z_i\) as follows:
\[
Z_i = \left[
\begin{array}{ccccccccccccccccc}
\mathbf{z}_{i,1}' & 0 & \dots \\
0 & \mathbf{z}_{i,1}' & \mathbf{z}_{i,2}' & 0 & \dots \\
0 &0 &0 & \mathbf{z}_{i,1} & \mathbf{z}_{i,2}' & \mathbf{z}_{i,3}' & 0 & \dots \\
\vdots \\
0 & \dots &&&&&& 0 & \mathbf{z}_{i,1}' &  \dots &   \mathbf{z}_{i,T-2}'
\end{array}
\right],
\]
where \(\mathbf{z}_{i,t} = [y_{i,t},\mathbf{x}_{i,t}']'\), we have the moment conditions:\footnote{If \(\mathbf{x}_{i,t}\) is predetermined (exogenous), we can use \(\mathbf{z}_{i,t} = [y_{i,t},\mathbf{x}_{i,t+1},\mathbf{x}_{i,t}']'\) (respectively \(\mathbf{z}_{i,t} = [y_{i,t},\mathbf{x}_{i,t+2},\mathbf{x}_{i,t+1},\mathbf{x}_{i,t}']'\)).}
\[
\mathbb{E}(Z_i'\Delta  {\boldsymbol\varepsilon}_i)=0,
\]

with \(\Delta{\boldsymbol\varepsilon}_i = [ \Delta \varepsilon_{i,3},\dots,\Delta \varepsilon_{i,T}]'\).

These restrictions are used in the GMM approach employed by \citet{Arellano_Bond_1991}. Specifically, a GMM estimator of the model parameters is given by:
\[
\mbox{argmin}\;\left(\frac{1}{n} \sum_{i=1}^n Z_i' \Delta \boldsymbol\varepsilon_i\right)'W_n\left(\frac{1}{n} \sum_{i=1}^n Z_i' \Delta \boldsymbol\varepsilon_i\right),
\]
using the weighting matrix
\[
W_n = \left(\frac{1}{n}\sum_{i=1}^n Z_i'\widehat{\Delta\boldsymbol\varepsilon_i}\widehat{\Delta\boldsymbol\varepsilon_i}'Z_i\right)^{-1},
\]
where the \(\widehat{\Delta\boldsymbol\varepsilon_i}\)'s are consistent estimates of the \(\Delta\boldsymbol\varepsilon_i\)'s that result from a preliminary estimation. In this sense, this estimator is a two-step GMM one.

If the disturbances are homoskedastic, then it can be shown that an asymptotically equivalent (efficient) GMM estimator can be obtained by using:
\[
W_{1,n} = \left(\frac{1}{n}Z_i'HZ_i\right)^{-1},
\]
where \(H\) is is \((T-2) \times (T-2)\) matrix of the form:
\[
H = \left[\begin{array}{ccccccc}
2 & -1 & 0 & \dots &0 \\
-1 & 2 & -1 &  & \vdots \\
0 & \ddots& \ddots & \ddots & 0 \\
\vdots &  & -1 & 2&-1\\
0&\dots & 0 & -1 & 2
\end{array}\right].
\]

It is straightforward to extend these GMM methods to cases where there is more than one lag of the dependent variable on the right-hand side of the equation or in cases where disturbances feature limited moving-average serial correlation.

The \texttt{pdynmc} package allows to run these GMM approaches (see \citet{Fritsch_et_al_2019}). The following lines of code allow to replicate the results of \citet{Arellano_Bond_1991}:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(pdynmc)}
\FunctionTok{data}\NormalTok{(EmplUK, }\AttributeTok{package =} \StringTok{"plm"}\NormalTok{)}
\NormalTok{dat }\OtherTok{\textless{}{-}}\NormalTok{ EmplUK}
\NormalTok{dat[,}\FunctionTok{c}\NormalTok{(}\DecValTok{4}\SpecialCharTok{:}\DecValTok{7}\NormalTok{)]         }\OtherTok{\textless{}{-}} \FunctionTok{log}\NormalTok{(dat[,}\FunctionTok{c}\NormalTok{(}\DecValTok{4}\SpecialCharTok{:}\DecValTok{7}\NormalTok{)])}
\NormalTok{m1 }\OtherTok{\textless{}{-}} \FunctionTok{pdynmc}\NormalTok{(}\AttributeTok{dat =}\NormalTok{ dat, }\CommentTok{\# name of the dataset}
             \AttributeTok{varname.i =} \StringTok{"firm"}\NormalTok{, }\CommentTok{\# name of the cross{-}section identifier}
             \AttributeTok{varname.t =} \StringTok{"year"}\NormalTok{, }\CommentTok{\# name of the time{-}series identifiers}
             \AttributeTok{use.mc.diff =} \ConstantTok{TRUE}\NormalTok{, }\CommentTok{\# use moment conditions from equations in differences? (i.e. instruments in levels) }
             \AttributeTok{use.mc.lev =} \ConstantTok{FALSE}\NormalTok{, }\CommentTok{\# use moment conditions from equations in levels? (i.e. instruments in differences)}
             \AttributeTok{use.mc.nonlin =} \ConstantTok{FALSE}\NormalTok{, }\CommentTok{\# use nonlinear (quadratic) moment conditions?}
             \AttributeTok{include.y =} \ConstantTok{TRUE}\NormalTok{, }\CommentTok{\# instruments should be derived from the lags of the dependent variable?}
             \AttributeTok{varname.y =} \StringTok{"emp"}\NormalTok{, }\CommentTok{\# name of the dependent variable in the dataset}
             \AttributeTok{lagTerms.y =} \DecValTok{2}\NormalTok{, }\CommentTok{\# number of lags of the dependent variable}
             \AttributeTok{fur.con =} \ConstantTok{TRUE}\NormalTok{, }\CommentTok{\# further control variables (covariates) are included?}
             \AttributeTok{fur.con.diff =} \ConstantTok{TRUE}\NormalTok{, }\CommentTok{\# include further control variables in equations from differences ?}
             \AttributeTok{fur.con.lev =} \ConstantTok{FALSE}\NormalTok{, }\CommentTok{\# include further control variables in equations from level?}
             \AttributeTok{varname.reg.fur =} \FunctionTok{c}\NormalTok{(}\StringTok{"wage"}\NormalTok{, }\StringTok{"capital"}\NormalTok{, }\StringTok{"output"}\NormalTok{), }\CommentTok{\# covariate(s) {-}in the dataset{-} to treat as further controls}
             \AttributeTok{lagTerms.reg.fur =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{), }\CommentTok{\# number of lags of the further controls}
             \AttributeTok{include.dum =} \ConstantTok{TRUE}\NormalTok{, }\CommentTok{\# A logical variable indicating whether dummy variables for the time periods are included (defaults to \textquotesingle{}FALSE\textquotesingle{}).}
             \AttributeTok{dum.diff =} \ConstantTok{TRUE}\NormalTok{, }\CommentTok{\# A logical variable indicating whether dummy variables are included in the equations in first differences (defaults to \textquotesingle{}NULL\textquotesingle{}).}
             \AttributeTok{dum.lev =} \ConstantTok{FALSE}\NormalTok{, }\CommentTok{\# A logical variable indicating whether dummy variables are included in the equations in levels (defaults to \textquotesingle{}NULL\textquotesingle{}).}
             \AttributeTok{varname.dum =} \StringTok{"year"}\NormalTok{,}
             \AttributeTok{w.mat =} \StringTok{"iid.err"}\NormalTok{, }\CommentTok{\# One of the character strings c(\textquotesingle{}"iid.err"\textquotesingle{}, \textquotesingle{}"identity"\textquotesingle{}, \textquotesingle{}"zero.cov"\textquotesingle{}) indicating the type of weighting matrix to use (defaults to \textquotesingle{}"iid.err"\textquotesingle{})}
             \AttributeTok{std.err =} \StringTok{"corrected"}\NormalTok{,}
             \AttributeTok{estimation =} \StringTok{"onestep"}\NormalTok{, }\CommentTok{\# One of the character strings c(\textquotesingle{}"onestep"\textquotesingle{}, \textquotesingle{}"twostep"\textquotesingle{}, \textquotesingle{}"iterative"\textquotesingle{}). Denotes the number of iterations of the parameter procedure (defaults to \textquotesingle{}"twostep"\textquotesingle{}).}
             \AttributeTok{opt.meth =} \StringTok{"none"} \CommentTok{\# numerical optimization procedure. When no nonlinear moment conditions are employed in estimation, closed form estimates can be computed by setting the argument to \textquotesingle{}"none"}
\NormalTok{)}
\FunctionTok{summary}\NormalTok{(m1,}\AttributeTok{digits=}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Dynamic linear panel estimation (onestep)
## Estimation steps: 1
## 
## Coefficients:
##             Estimate Std.Err.rob z-value.rob Pr(>|z.rob|)    
## L1.emp      0.686226    0.144594       4.746      < 2e-16 ***
## L2.emp     -0.085358    0.056016      -1.524      0.12751    
## L0.wage    -0.607821    0.178205      -3.411      0.00065 ***
## L1.wage     0.392623    0.167993       2.337      0.01944 *  
## L0.capital  0.356846    0.059020       6.046      < 2e-16 ***
## L1.capital -0.058001    0.073180      -0.793      0.42778    
## L2.capital -0.019948    0.032713      -0.610      0.54186    
## L0.output   0.608506    0.172531       3.527      0.00042 ***
## L1.output  -0.711164    0.231716      -3.069      0.00215 ** 
## L2.output   0.105798    0.141202       0.749      0.45386    
## 1979        0.009554    0.010290       0.929      0.35289    
## 1980        0.022015    0.017710       1.243      0.21387    
## 1981       -0.011775    0.029508      -0.399      0.68989    
## 1982       -0.027059    0.029275      -0.924      0.35549    
## 1983       -0.021321    0.030460      -0.700      0.48393    
## 1976       -0.007703    0.031411      -0.245      0.80646    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
##  41 total instruments are employed to estimate 16 parameters
##  27 linear (DIF) 
##  8 further controls (DIF) 
##  6 time dummies (DIF) 
##  
## J-Test (overid restrictions):  70.82 with 25 DF, pvalue: <0.001
## F-Statistic (slope coeff):  528.06 with 10 DF, pvalue: <0.001
## F-Statistic (time dummies):  14.98 with 6 DF, pvalue: 0.0204
\end{verbatim}

We generate novel results (\texttt{m2}) by replacing ``\texttt{onestep}'' with ``\texttt{twostep}'' (in the \texttt{estimation} field). The resulting estimated coefficients are:

\begin{verbatim}
##      L1.emp      L2.emp     L0.wage     L1.wage  L0.capital  L1.capital 
##  0.62870890 -0.06518800 -0.52575951  0.31128961  0.27836190  0.01409950 
##  L2.capital   L0.output   L1.output   L2.output        1979        1980 
## -0.04024847  0.59192286 -0.56598515  0.10054264  0.01121551  0.02306871 
##        1981        1982        1983        1976 
## -0.02135806 -0.03111604 -0.01799335 -0.02336762
\end{verbatim}

\citet{Arellano_Bond_1991} have proposed a specification test. If the model is correctly specified, then the errors of Eq. \eqref{eq:paneldynFisrtDiff} ---that is the first-difference equation--- should feature non-zero first-order auto-correlations, but zero higher-order autocorrelations.

Function \texttt{mtest.fct} of package \texttt{pdynmc} implements this test. Here is its result in the present case:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{mtest.fct}\NormalTok{(m1,}\AttributeTok{order=}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  Arellano and Bond (1991) serial correlation test of degree 3
## 
## data:  1step GMM Estimation
## normal = 0.045945, p-value = 0.9634
## alternative hypothesis: serial correlation of order 3 in the error terms
\end{verbatim}

One can also implement the Hansen J-test of the over-identifying restrictions (see Section \ref{overidentif}):

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{jtest.fct}\NormalTok{(m1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  J-Test of Hansen
## 
## data:  1step GMM Estimation
## chisq = 70.82, df = 25, p-value = 2.905e-06
## alternative hypothesis: overidentifying restrictions invalid
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{jtest.fct}\NormalTok{(m2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  J-Test of Hansen
## 
## data:  2step GMM Estimation
## chisq = 31.381, df = 25, p-value = 0.1767
## alternative hypothesis: overidentifying restrictions invalid
\end{verbatim}

\hypertarget{introduction-to-program-evaluation}{%
\section{Introduction to program evaluation}\label{introduction-to-program-evaluation}}

This section brielfy introduces the econometrics of program evaluation. Program evaluation refer to the analysis of the causal effects of some ``treatments'' in a broad sense. These treatment can, e.g., correspond to the implementation (or announcement) of policy measures. A comprehensive review is proposed by \citet{Abadie_Cattaneo_2018}. A seminal book on the subject is that of \citet{angrist_mostly_2008}.

\hypertarget{presentation-of-the-problem}{%
\subsection{Presentation of the problem}\label{presentation-of-the-problem}}

To begin with, let us consider a single entity. To simplify notations, we drop the entity index (\(i\)). Let us denote by \(Y\) the outcome variable (for the variable of interest), by \(W\) is a binary variable indicating whether the considered entity has received treatment (\(W=1\)) or not (\(W=0\)), and by \(X\) a vector of covariates, assumed to be predetermined relative to the treatment. That is, \(W\) and \(X\) could be correlated, but the values of \(X\) have been determined before that of \(W\) (in such a way that the realization of \(W\) does not affect \(X\)). Typcally, \(X\) contains characteristics of the considered entity.

We are interested in the effect of the treatment, that is:
\[
Y_1 - Y_0,
\]
where \(Y_1\) correspond to the outcome obtained under treatment, while \(Y_0\) is the outcome obtained without it. Notice that we have:
\[
Y = (1-W) Y_0 + W Y_1.
\]

The problem is that observing \((Y,W,X)\) is not sufficient to observe the treatment effect \(Y_1 - Y_0\). Additional assumptions are needed to estimate it, or, more precisely, its expectations (\emph{average treatment effect}):
\[
ATE = \mathbb{E}(Y_1 - Y_0).
\]

Importantly, \(ATE\) is different from the following quantity:
\[
\alpha = \underbrace{\mathbb{E}(Y|W=1)}_{=\mathbb{E}(Y_1|W=1)} - \underbrace{\mathbb{E}(Y|W=0)}_{=\mathbb{E}(Y_0|W=0)},
\]
that is easier to estimate. Indeed, a consistent estimate of \(\alpha\) is the difference between the means of the outcome variables in two sub-samples: one containing only the treated entities (this gives an estimate of \(\mathbb{E}(Y_1|W=1)\)) and the other containing only the non-treated entities (this gives an estimate of \(\mathbb{E}(Y_0|W=0)\)). Coming back to \(ATE\), the problem is that we won't have direct information regarding \(\mathbb{E}(Y_0|W=1)\) and \(\mathbb{E}(Y_1|W=0)\). However, these two conditional expectations are part of \(ATE\). Indeed, \(ATE = \mathbb{E}(Y_1) - \mathbb{E}(Y_0)\), and:
\begin{eqnarray}
\mathbb{E}(Y_1) &=& \mathbb{E}(Y_1|W=0)\mathbb{P}(W=0)+\mathbb{E}(Y_1|W=1)\mathbb{P}(W=1) \label{eq:EY1a} \\
\mathbb{E}(Y_0) &=& \mathbb{E}(Y_0|W=0)\mathbb{P}(W=0)+\mathbb{E}(Y_0|W=1)\mathbb{P}(W=1). \label{eq:EY0a}
\end{eqnarray}

\hypertarget{randomized-controlled-trials-rcts}{%
\subsection{Randomized controlled trials (RCTs)}\label{randomized-controlled-trials-rcts}}

In the context of Randomized controlled trials (RCTs), entities are randomly assigned to receive the treatment. As a result, we have \(\mathbb{E}(Y_1) = \mathbb{E}(Y_1|W=0) = \mathbb{E}(Y_1|W=1)\) and \(\mathbb{E}(Y_0) = \mathbb{E}(Y_0|W=0) = \mathbb{E}(Y_0|W=1)\). Using this into Eqs. \eqref{eq:EY1a} and \eqref{eq:EY0a} yields \(ATE = \alpha\).

Therefore, in this context, estimating \(\mathbb{E}(Y_1-Y_0)\) amounts to computing the difference between two sample means, namely (a) the sample mean of the subset of \(Y_i\)'s corresponding to the entities for which \(W_i=1\), and (b) the one for which \(W_i=0\).

More accurate estimates can be obtained through regressions. Assume that the model reads:
\[
Y_{i} = W_{i} \beta_{1} + X_i'\boldsymbol\beta_z + \varepsilon_i,
\]
where \(\mathbb{E}(\varepsilon_i|X_i) = 0\) (and \(W_i\) is independent from \(X_i\) and \(\varepsilon_i\)). In this case, we obtain a consistent estimate of \(\beta_1\) by regressing \(\mathbf{y}\) on \(\mathbf{Z} = [\mathbf{w},\mathbf{X}]\).

\hypertarget{difference-in-difference-did-approach}{%
\subsection{Difference-in-Difference (DiD) approach}\label{difference-in-difference-did-approach}}

The DiD approach is a popular methodology implemented in cases where \(W\) cannot be considered as an independent variable. It exploits two dimensions: entities (\(i\)), and time (\(t\)). To simplify the exposition, we consider only two periods here (\(t=0\) and \(t=1\)).

Consider the following model:
\begin{equation}
Y_{i,t} = W_{i,t} \beta_1 + \mu_i + \delta_t + \varepsilon_{i,t}\label{eq:DiD}
\end{equation}

The parameter of interest is \(\beta_{1}\), which is the treatment effect (recall that \(W_{i,t} \in \{0,1\}\)). Usually, for all entities \(i\), we have \(W_{i,t=0}=0\). But only some of them are treated on date 1, i.e., \(W_{i,1} \in \{0,1\}\).

The disturbance \(\varepsilon_{i,t}\) affects the outcome, but we assume that it does not relate to the selection for treatment; therefore, \(\mathbb{E}(\varepsilon_{i,t}|W_{i,t})=0\). By contrast, we do not exclude some correlation between \(W_{i,t}\) (for \(t=1\)) and \(\mu_i\); hence, \(\mu_i\) may constitute a \emph{confounder}. Finally, we suppose that the micro-variables \(W_i\) do not affect the time fixed effects \(\delta_t\), such that \(\mathbb{E}(\delta_t|W_{i,t})=\mathbb{E}(\delta_t)\).

We have:

\[
\begin{array}{cccccccccc}
\mathbb{E}(Y_{i,1}|W_{i,1}=1) &=& \beta_1 &+& \mathbb{E}(\mu_i|W_{i,1}=1) &+&\mathbb{E}(\delta_1|W_{i,1}=1) &+& \mathbb{E}(\varepsilon_{i,1}) \\
\mathbb{E}(Y_{i,0}|W_{i,1}=1) &=&  && \mathbb{E}(\mu_i|W_{i,1}=1) &+&\mathbb{E}(\delta_0|W_{i,1}=1) &+& \mathbb{E}(\varepsilon_{i,0}) \\
\mathbb{E}(Y_{i,1}|W_{i,1}=0) &=& && \mathbb{E}(\mu_i|W_{i,1}=0) &+&\mathbb{E}(\delta_1|W_{i,1}=0) &+& \mathbb{E}(\varepsilon_{i,1}) \\
\mathbb{E}(Y_{i,0}|W_{i,1}=0) &=&  && \mathbb{E}(\mu_i|W_{i,1}=0) &+&\mathbb{E}(\delta_0|W_{i,1}=0) &+& \mathbb{E}(\varepsilon_{i,0}).
\end{array}
\]
and, under our assumptions, it can be checked that:

\[
\beta_1 = \mathbb{E}(\Delta Y_{i,1}|W_{i,1}=1) - \mathbb{E}(\Delta Y_{i,1}|W_{i,1}=0),
\]
where \(\Delta Y_{i,1}=Y_{i,1}-Y_{i,0}\). Therefore, in this context, the treatment effect appears to be a difference (of two conditionnal expectations) of difference (of the outcome variable, through time).

This is illustrated by Figure \ref{fig:figAbadie}, which represents the generic DiD framework.

\begin{figure}

{\centering \includegraphics[width=0.95\linewidth]{images/Abadie_et_al_2018} 

}

\caption{Source: Abadie et al., (1998).}\label{fig:figAbadie}
\end{figure}

In practice, implementing this approach consists in running a linear regression of the type of Eq. \eqref{eq:DiD}. These regressions also usually involve controls on top of the fixed effects \(\mu_i\). As illustrated in the next subsection, the parameter of interest (\(\beta_1\)) is often associated with an interaction term.

\hypertarget{application-of-the-did-approach}{%
\subsection{Application of the DiD approach}\label{application-of-the-did-approach}}

This example is based on the data used in \citet{Meyer_Viscusi_Durbin_1995}. This dataset is part of the \texttt{wooldridge} package. This paper examines the effect of workers' compensation for injury on time out of work. It exploits a \textbf{natural experiment} approach of comparing individuals injured before and after increases in the maximum weekly benefit amount. Specifically, in 1980, the cap on weekly earnings covered by worker's compensation was increased in Kentucky and Michigan. Let us check whether this new policy was followed by an increase in the amount of time workers spent unemployed (for example, higher compensation may reduce workers' incentives to avoid injury).

As shown in Figure \ref{fig:figMeyer}, the measure has only affected high-earning workers. The idea exploited by \citet{Meyer_Viscusi_Durbin_1995} was to compare the increase in time out of work before-after 1980 for higher-earnings workers on the one hand (entities who received the treatment) and low-earnings workers on the other hand (control group).

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{images/Figure_Meyer_et_al} 

}

\caption{Source: Meyer et al., (1995).}\label{fig:figMeyer}
\end{figure}

The next lines of codes replicate some of their results. The dependent variable is the logarithm of the duration of benefits. For more information use \texttt{?injury}, after having loaded the \texttt{wooldridge} library.

In the table of results below, the parameter of interest is the one associated with the interaction term \texttt{afchnge:highearn}. Columns 2 and 3 correspond to the first two column of Table 6 in \citet{Meyer_Viscusi_Durbin_1995}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(wooldridge)}
\FunctionTok{data}\NormalTok{(injury)}
\NormalTok{injury }\OtherTok{\textless{}{-}} \FunctionTok{subset}\NormalTok{(injury,ky}\SpecialCharTok{==}\DecValTok{1}\NormalTok{)}
\NormalTok{injury}\SpecialCharTok{$}\NormalTok{indust }\OtherTok{\textless{}{-}} \FunctionTok{as.factor}\NormalTok{(injury}\SpecialCharTok{$}\NormalTok{indust)}
\NormalTok{injury}\SpecialCharTok{$}\NormalTok{injtype }\OtherTok{\textless{}{-}} \FunctionTok{as.factor}\NormalTok{(injury}\SpecialCharTok{$}\NormalTok{injtype)}
\CommentTok{\#names(injury)}
\NormalTok{eq1 }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(}\FunctionTok{log}\NormalTok{(durat) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ afchnge }\SpecialCharTok{+}\NormalTok{ highearn }\SpecialCharTok{+}\NormalTok{ afchnge}\SpecialCharTok{*}\NormalTok{highearn,}\AttributeTok{data=}\NormalTok{injury)}
\NormalTok{eq2 }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(}\FunctionTok{log}\NormalTok{(durat) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ afchnge }\SpecialCharTok{+}\NormalTok{ highearn }\SpecialCharTok{+}\NormalTok{ afchnge}\SpecialCharTok{*}\NormalTok{highearn }\SpecialCharTok{+}
\NormalTok{            lprewage}\SpecialCharTok{*}\NormalTok{highearn }\SpecialCharTok{+}\NormalTok{ male }\SpecialCharTok{+}\NormalTok{ married }\SpecialCharTok{+}\NormalTok{ lage }\SpecialCharTok{+}\NormalTok{ ltotmed }\SpecialCharTok{+}\NormalTok{ hosp }\SpecialCharTok{+}
\NormalTok{            indust }\SpecialCharTok{+}\NormalTok{ injtype,}\AttributeTok{data=}\NormalTok{injury)}
\NormalTok{eq3 }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(}\FunctionTok{log}\NormalTok{(durat) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ afchnge }\SpecialCharTok{+}\NormalTok{ highearn }\SpecialCharTok{+}\NormalTok{ afchnge}\SpecialCharTok{*}\NormalTok{highearn }\SpecialCharTok{+}
\NormalTok{            lprewage}\SpecialCharTok{*}\NormalTok{highearn }\SpecialCharTok{+}\NormalTok{ male }\SpecialCharTok{+}\NormalTok{ married }\SpecialCharTok{+}\NormalTok{ lage }\SpecialCharTok{+}\NormalTok{ indust }\SpecialCharTok{+}
\NormalTok{            injtype,}\AttributeTok{data=}\NormalTok{injury)}
\NormalTok{stargazer}\SpecialCharTok{::}\FunctionTok{stargazer}\NormalTok{(eq1,eq2,eq3,}\AttributeTok{type=}\StringTok{"text"}\NormalTok{,}
                     \AttributeTok{omit=}\FunctionTok{c}\NormalTok{(}\StringTok{"indust"}\NormalTok{,}\StringTok{"injtype"}\NormalTok{,}\StringTok{"Constant"}\NormalTok{),}\AttributeTok{no.space =} \ConstantTok{TRUE}\NormalTok{,}
                     \AttributeTok{add.lines =} \FunctionTok{list}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\StringTok{"industry dummy"}\NormalTok{,}\StringTok{"no"}\NormalTok{,}\StringTok{"yes"}\NormalTok{,}\StringTok{"yes"}\NormalTok{),}
                                      \FunctionTok{c}\NormalTok{(}\StringTok{"injury dummy"}\NormalTok{,}\StringTok{"no"}\NormalTok{,}\StringTok{"yes"}\NormalTok{,}\StringTok{"yes"}\NormalTok{)),}
                     \AttributeTok{order =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{18}\NormalTok{,}\DecValTok{3}\SpecialCharTok{:}\DecValTok{17}\NormalTok{,}\DecValTok{19}\NormalTok{,}\DecValTok{20}\NormalTok{),}\AttributeTok{omit.stat =} \FunctionTok{c}\NormalTok{(}\StringTok{"f"}\NormalTok{,}\StringTok{"ser"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## ===============================================
##                        Dependent variable:     
##                   -----------------------------
##                            log(durat)          
##                      (1)       (2)       (3)   
## -----------------------------------------------
## afchnge             0.008    -0.004     0.016  
##                    (0.045)   (0.038)   (0.045) 
## highearn          0.256***   -0.595    -1.522  
##                    (0.047)   (0.930)   (1.099) 
## afchnge:highearn  0.191***  0.162***  0.215*** 
##                    (0.069)   (0.059)   (0.069) 
## lprewage                     0.207**   0.258** 
##                              (0.088)   (0.104) 
## male                         -0.070*   -0.072  
##                              (0.039)   (0.046) 
## married                       0.055     0.051  
##                              (0.035)   (0.041) 
## lage                        0.244***  0.252*** 
##                              (0.044)   (0.052) 
## ltotmed                     0.361***           
##                              (0.011)           
## hosp                        0.252***           
##                              (0.044)           
## highearn:lprewage             0.065     0.232  
##                              (0.158)   (0.187) 
## -----------------------------------------------
## industry dummy       no        yes       yes   
## injury dummy         no        yes       yes   
## Observations        5,626     5,347     5,347  
## R2                  0.021     0.319     0.049  
## Adjusted R2         0.020     0.316     0.046  
## ===============================================
## Note:               *p<0.1; **p<0.05; ***p<0.01
\end{verbatim}

\hypertarget{estimation-methods}{%
\chapter{Estimation Methods}\label{estimation-methods}}

This chapter presents three approaches to estimate parametric models: the General Method of Moments (GMM), the Maximum Likelihood approach (ML), and the Bayesian approach. The general context is the following: You observe a sample \(\mathbf{y}=\{y_1,\dots,y_n\}\), you assume that these data have been generated by a model parameterized by \({\boldsymbol\theta} \in \mathbb{R}^K\), and you want to estimate this vector \({\boldsymbol\theta}_0\).

\hypertarget{secGMM}{%
\section{Generalized Method of Moments (GMM)}\label{secGMM}}

\hypertarget{definition-of-the-gmm-estimator}{%
\subsection{Definition of the GMM estimator}\label{definition-of-the-gmm-estimator}}

We denote by \(y_i\) a \(p \times 1\) vector of variables; by \(\boldsymbol\theta\) an \(K \times 1\) vector of parameters, and by \(h(y_i;\boldsymbol\theta)\) a continuous \(r \times 1\) vector-valued function.

We denote by \(\boldsymbol\theta_0\) the true value of \(\boldsymbol\theta\) and we assume that \(\boldsymbol\theta_0\) satisfies:
\[
\mathbb{E}[h(y_i;\boldsymbol\theta_0)] = \mathbf{0}.
\]

We denote by \(\underline{y_i}\) the information contained in the current and past observations of \(y_i\), that is: \(\underline{y_i} = \{y_i,y_{i-1},\dots,y_1\}\). We denote by \(g(\underline{y_n};\boldsymbol\theta)\) the sample average of the \(h(y_i;\boldsymbol\theta)\) vectors, i.e.:
\[
g(\underline{y_n};\boldsymbol\theta) = \frac{1}{n} \sum_{i=1}^{n} h(y_i;\boldsymbol\theta).
\]

The intuition behind the GMM estimator is the following: choose \(\boldsymbol\theta\) so as to make the sample moment as close as possible to their population values, that is 0.

\begin{definition}
\protect\hypertarget{def:GMM}{}\label{def:GMM}A GMM estimator of \(\boldsymbol\theta_0\) is given by:
\[
\hat{\boldsymbol\theta}_n = \mbox{argmin}_{\boldsymbol\theta} \quad g(\underline{y_n};\boldsymbol\theta)'\, W_n \, g(\underline{y_n};\boldsymbol\theta),
\]
where \(W_n\) is a positive definite matrix (that may depend on \(\underline{y_n}\)).
\end{definition}

In the specific case where \(K = r\) (the dimension of \(\boldsymbol\theta\) is the same as that of \(h(y_i;\boldsymbol\theta)\) ---or of \(g(\underline{y_n};\boldsymbol\theta)\)--- then \(\hat{\boldsymbol\theta}_n\) satisfies:
\[
g(\underline{y_n};\hat{\boldsymbol\theta}_n) = \mathbf{0}.
\]
Under regularity and identification conditions, this estimator is consistent, that is \(\hat{\boldsymbol\theta}_{n}\) converges towards \(\boldsymbol\theta_0\) in probability, which we denote by:
\begin{equation}
\mbox{plim}_n\;\hat{\boldsymbol\theta}_{n}= \boldsymbol\theta_0,\quad \mbox{or} \quad\hat{\boldsymbol\theta}_{n} \overset{p}{\rightarrow} \boldsymbol\theta_0,\label{eq:consistGMM}
\end{equation}
i.e.~\(\forall \varepsilon>0\), \(\lim_{n \rightarrow \infty} \mathbb{P}(|\hat{\boldsymbol\theta}_{n} - \boldsymbol\theta_0|>\varepsilon) = 0\) (this is Definition \ref{def:convergenceproba}).

Definition \ref{def:GMM} involves a positive definite matrix \(W_n\). While one can take any positive definite matrix to have consistency (Eq. \eqref{eq:consistGMM}), it can be shown that he GMM estimator achieves the minimum asymptotic variance when \(W_n\) is the inverse of matrix \(S\), the latter being defined by:
\[
S = Asy.\mathbb{V}ar\left(\sqrt{n}g(\underline{y_n};\hat{\boldsymbol\theta}_n)\right).
\]
In this case, \(W_n\) is said to be the \emph{optimal weighting matrix}.

The intuition behind this result is the same that underlies Generalized Least Squares (see Section \ref{GLS}), that is: it is beneficial to use a criterion in which the weights are inversely proportional to the variances of the moments.

If \(h(x_i;\boldsymbol\theta_0)\) is not correlated to \(h(x_j;\boldsymbol\theta_0)\), for \(i \ne j\), then we have:
\[
S = \mathbb{V}ar(h(x_i;\boldsymbol\theta_0)),
\]
which can be approximated by
\[
\hat{\Gamma}_{0,n}=\frac{1}{n}\sum_{i=1}^{n} h(x_i;\hat{\boldsymbol\theta}_n)h(x_{i};\hat{\boldsymbol\theta}_n)'.
\]

In a time series context, we often have correlation between \(x_i\) and \(x_{i+k}\), especially for small \(k\)'s. In this case, and if the time series \(\{y_i\}\) is covariance stationary (see Def. \ref{def:covstat}), then we have:
\[
S := \sum_{\nu = -\infty}^{\infty} \Gamma_\nu,
\]
where \(\Gamma_\nu := \mathbb{E}[h(x_i;\boldsymbol\theta_0) h(x_{i-\nu};\boldsymbol\theta_0)']\). Matrix \(S\) is called the \textbf{long-run variance} of process \(\{y_i\}\) (see Def. \ref{def:LRV}).

For \(\nu \ge 0\), let us define \(\hat{\Gamma}_{\nu,n}\) by:
\[
\hat{\Gamma}_{\nu,n} = \frac{1}{n} \sum_{i=\nu + 1}^{n} h(x_i;\hat{\boldsymbol\theta}_n)h(x_{i-\nu};\hat{\boldsymbol\theta}_n)',
\]
then \(S\) can be approximated by the \citet{Newey_West_1987} formula (similar to Eq. \eqref{eq:NWest}):
\begin{equation}
\hat{\Gamma}_{0,n} + \sum_{\nu=1}^{q}\left[1-\frac{\nu}{q+1}\right](\hat{\Gamma}_{\nu,n}+\hat{\Gamma}_{\nu,n}').    \label{eq:Shat}
\end{equation}

\hypertarget{asymptotic-distribution-of-the-gmm-estimator}{%
\subsection{Asymptotic distribution of the GMM estimator}\label{asymptotic-distribution-of-the-gmm-estimator}}

We have:
\begin{equation}
\boxed{\sqrt{n}(\hat{\boldsymbol\theta}_n - \boldsymbol\theta_0) \overset{d}{\rightarrow} \mathcal{N}(0,V),}\label{eq:asymptGMM}
\end{equation}
where \(V = (DS^{-1}D')^{-1}\), with
\[
D := \left.\mathbb{E}\left(\frac{\partial h(x_i;\boldsymbol\theta)}{\partial \boldsymbol\theta'}\right)\right|_{\boldsymbol\theta = \boldsymbol\theta_0}.
\]

Matrix \(V\) can be approximated by
\begin{equation}
\hat{V}_n = (\hat{D}_n\hat{S}_n^{-1}\hat{D}_n')^{-1},\label{eq:VGMM}
\end{equation}
where \(\hat{S}_n\) is given by Eq. \eqref{eq:Shat} and
\[
\hat{D}'_n := \left.\frac{\partial g(\underline{y_n};\boldsymbol\theta)}{\partial \boldsymbol\theta'}\right|_{\boldsymbol\theta = \hat{\boldsymbol\theta}_n}.
\]
In practice, the previous matrix is computed numerically.

\hypertarget{overidentif}{%
\subsection{Testing hypotheses in the GMM framework}\label{overidentif}}

A first important test is the one concerning the validity of the moment restrictions (Sargan-Hansen test; \citet{Sargan_1958} and \citet{Hansen_1982}). Assume that the number of restrictions imposed is larger than the number of parameters to estimate (\(r>K\)). In this case, the restrictions are said to be over-identifiying.

Under correct specification, we asymptotically have:
\[
\sqrt{n}g(\underline{y_n};{\boldsymbol\theta}_0)  \sim \mathcal{N}(0,S).
\]
As a result, it comes that:
\begin{equation}
J_n = \left(\sqrt{n}g(\underline{y_n};{\boldsymbol\theta}_0)\right)'S^{-1}\left(\sqrt{n}g(\underline{y_n};{\boldsymbol\theta}_0)\right) \label{eq:HansenSargan}
\end{equation}
asymptotically follows a \(\chi^2\) distribution. The number of degrees of freedom is equal to \(r-K\). (Note that, for \(r=K\), we have, as expected, \(J=0\).) That is, asymptotically:
\[
J_n \sim \chi^2(r-K).
\]
The GMM framework also allows to easily test linear restrictions on the parameters. First, given Eq. \eqref{eq:asymptGMM}, Wald tests (see Eq. \eqref{eq:W1} in Section \ref{Ftest}) are readily available. Second, one can also resort to a test equivalent to the \emph{likelihood ratio tests} (see Definition \ref{def:LR}). More precisely, consider an unconstrained model and a constrained version of this model, the number of restrictions being equal to \(k\). If the two models are estimated by considering the same moment constraints, and the same weighting matrix ---using Eq. \eqref{eq:VGMM}, based on the unrestricted model---, then we have that:
\[
n \left[(g(\underline{y_n};\hat{{\boldsymbol\theta}}^*_n)-g(\underline{y_n};\hat{{\boldsymbol\theta}}_n)\right] \sim \chi^2(k),
\]
where \(\hat{{\boldsymbol\theta}}^*_n\) is the constrained estimate of \({\boldsymbol\theta}_0\).

\hypertarget{example-estimation-of-the-stochastic-discount-factor-s.d.f.}{%
\subsection{Example: Estimation of the Stochastic Discount Factor (s.d.f.)}\label{example-estimation-of-the-stochastic-discount-factor-s.d.f.}}

Under the no-arbitrage assumption, there exists a random variable \(\mathcal{M}_{t,t+1}\) (a s.d.f.) such that
\[
\mathbb{E}_t(\mathcal{M}_{t,t+1}R_{t+1})=1
\]
for any (gross) asset return \(R_t\). In the following, \(R_t\) denotes a \(n_r\)-dimensional vector of gross returns.

We consider the following specification of the s.d.f.:
\begin{equation}
\mathcal{M}_{t,t+1} = 1 - \textbf{b}_M'(F_{t+1} - \mathbb{E}_t(F_{t+1})), \label{eq:sdf}
\end{equation}
where \(F_t\) is a vector of factors. Eq. \eqref{eq:sdf} then reads:
\[
\mathbb{E}_t([1 - \textbf{b}_M'(F_{t+1} - \mathbb{E}_t(F_{t+1}))]R_{t+1})=1.
\]

Assume that the date-\(t\) information set is \(\mathcal{I}_t=\{\textbf{z}_t,\mathcal{I}_{t-1}\}\), where \(\textbf{z}_t\) is a vector of variables observed on date \(t\). (We then have \(\mathbb{E}_t(\bullet) \equiv \mathbb{E}(\bullet|\mathcal{I}_t)\).)

We can use \(\textbf{z}_t\) as an instrument. Indeed, we have:
\begin{eqnarray}
&&\mathbb{E}(z_{i,t} [\textbf{b}_M'\{F_{t+1} - \mathbb{E}_t(F_{t+1})\}R_{t+1}-R_{t+1}+1]) \nonumber \\
&=&\mathbb{E}(\mathbb{E}_t\{z_{i,t} [\textbf{b}_M'\{F_{t+1} - \mathbb{E}_t(F_{t+1})\}R_{t+1}-R_{t+1}+1]\})\nonumber\\
&=&\mathbb{E}(z_{i,t} \underbrace{\mathbb{E}_t\{\textbf{b}_M'\{F_{t+1} - \mathbb{E}_t(F_{t+1})\}R_{t+1}-R_{t+1}+1\}}_{1 - \mathbb{E}_t(\mathcal{M}_{t,t+1}R_{t+1})=0})=0.\label{eq:momF}
\end{eqnarray}
We have then converted a conditional moment condition into a unconditional one (which we need to implement the GMM approach described above). However, at that stage, we can still not directly use the GMM formulas because of the conditional expectation \(\mathbb{E}_t(F_{t+1})\) that appears in \(\mathbb{E}(z_{i,t} [\textbf{b}_M'\{F_{t+1} - \mathbb{E}_t(F_{t+1})\}R_{t+1}-R_{t+1}+1])=0\).

To go further, let us assume that:
\[
\mathbb{E}_t(F_{t+1}) = \textbf{b}_F \textbf{z}_t.
\]
We can then easily estimate matrix \(\textbf{b}_F\) (of dimension \(n_F \times n_z\)) by OLS. Note here that these OLS can be seen as a special GMM case. Indeed, as was done in Eq. \eqref{eq:momF}, we can show that, for the \(j^{th}\) component of \(F_t\), we have:
\[
\mathbb{E}( [F_{j,t+1} - \textbf{b}_{F,j} \textbf{z}_t]\textbf{z}_{t})=0,
\]
where \(\textbf{b}_{F,j}\) denotes the \(j^{th}\) row of \(\textbf{b}_{F}\). This yields the OLS formula.

Equipped with \(\textbf{b}_F\), we rely on the following moment restrictions to estimate \(\textbf{b}_M\):
\[
\mathbb{E}(z_{i,t} [\textbf{b}_M'\{F_{t+1} - \textbf{b}_F \textbf{z}_t\}R_{t+1}-R_{t+1}+1])=0.
\]
Specifically, the number of restrictions is \(n_R \times n_z\). Let us implement this approach in the U.S. context, using data extracted from the \href{https://fred.stlouisfed.org}{FRED database}. In factor \(F_t\), we use the changes in the VIX and in the personal consumption expenditures. The returns (\(R_t\)) are based on the Wilshire 5000 Price Index (a stock price index) and on the ICE BofA BBB US Corporate Index Total Return Index (a bond return index).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(fredr)}
\FunctionTok{fredr\_set\_key}\NormalTok{(}\StringTok{"df65e14c054697a52b4511e77fcfa1f3"}\NormalTok{)}
\NormalTok{start\_date }\OtherTok{\textless{}{-}} \FunctionTok{as.Date}\NormalTok{(}\StringTok{"1990{-}01{-}01"}\NormalTok{); end\_date }\OtherTok{\textless{}{-}} \FunctionTok{as.Date}\NormalTok{(}\StringTok{"2022{-}01{-}01"}\NormalTok{)}
\NormalTok{f }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(ticker)\{}
  \FunctionTok{fredr}\NormalTok{(}\AttributeTok{series\_id =}\NormalTok{ ticker,}
        \AttributeTok{observation\_start =}\NormalTok{ start\_date,}\AttributeTok{observation\_end =}\NormalTok{ end\_date,}
        \AttributeTok{frequency =} \StringTok{"m"}\NormalTok{,}\AttributeTok{aggregation\_method =} \StringTok{"avg"}\NormalTok{)}
\NormalTok{\}}
\NormalTok{vix }\OtherTok{\textless{}{-}} \FunctionTok{f}\NormalTok{(}\StringTok{"VIXCLS"}\NormalTok{) }\CommentTok{\# VIX}
\NormalTok{pce }\OtherTok{\textless{}{-}} \FunctionTok{f}\NormalTok{(}\StringTok{"PCE"}\NormalTok{) }\CommentTok{\# Personal consumption expenditures}
\NormalTok{sto }\OtherTok{\textless{}{-}} \FunctionTok{f}\NormalTok{(}\StringTok{"WILL5000PRFC"}\NormalTok{) }\CommentTok{\# Wilshire 5000 Full Cap Price Index}
\NormalTok{bdr }\OtherTok{\textless{}{-}} \FunctionTok{f}\NormalTok{(}\StringTok{"BAMLCC0A4BBBTRIV"}\NormalTok{) }\CommentTok{\# ICE BofA BBB US Corp. Index Tot. Return}
\NormalTok{T }\OtherTok{\textless{}{-}} \FunctionTok{dim}\NormalTok{(vix)[}\DecValTok{1}\NormalTok{]}
\NormalTok{dvix }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(vix}\SpecialCharTok{$}\NormalTok{value[}\DecValTok{3}\SpecialCharTok{:}\NormalTok{T]}\SpecialCharTok{/}\NormalTok{vix}\SpecialCharTok{$}\NormalTok{value[}\DecValTok{2}\SpecialCharTok{:}\NormalTok{(T}\DecValTok{{-}1}\NormalTok{)]) }\CommentTok{\# change in VIX t+1}
\NormalTok{dpce }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(pce}\SpecialCharTok{$}\NormalTok{value[}\DecValTok{3}\SpecialCharTok{:}\NormalTok{T]}\SpecialCharTok{/}\NormalTok{pce}\SpecialCharTok{$}\NormalTok{value[}\DecValTok{2}\SpecialCharTok{:}\NormalTok{(T}\DecValTok{{-}1}\NormalTok{)]) }\CommentTok{\# change in PCE t+1}
\NormalTok{dsto }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(sto}\SpecialCharTok{$}\NormalTok{value[}\DecValTok{3}\SpecialCharTok{:}\NormalTok{T]}\SpecialCharTok{/}\NormalTok{sto}\SpecialCharTok{$}\NormalTok{value[}\DecValTok{2}\SpecialCharTok{:}\NormalTok{(T}\DecValTok{{-}1}\NormalTok{)]) }\CommentTok{\# return t+1}
\NormalTok{dbdr }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(bdr}\SpecialCharTok{$}\NormalTok{value[}\DecValTok{3}\SpecialCharTok{:}\NormalTok{T]}\SpecialCharTok{/}\NormalTok{bdr}\SpecialCharTok{$}\NormalTok{value[}\DecValTok{2}\SpecialCharTok{:}\NormalTok{(T}\DecValTok{{-}1}\NormalTok{)]) }\CommentTok{\# return t+1}
\NormalTok{dvix\_1 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(vix}\SpecialCharTok{$}\NormalTok{value[}\DecValTok{2}\SpecialCharTok{:}\NormalTok{(T}\DecValTok{{-}1}\NormalTok{)]}\SpecialCharTok{/}\NormalTok{vix}\SpecialCharTok{$}\NormalTok{value[}\DecValTok{1}\SpecialCharTok{:}\NormalTok{(T}\DecValTok{{-}2}\NormalTok{)]) }\CommentTok{\# change in VIX t}
\NormalTok{dpce\_1 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(pce}\SpecialCharTok{$}\NormalTok{value[}\DecValTok{2}\SpecialCharTok{:}\NormalTok{(T}\DecValTok{{-}1}\NormalTok{)]}\SpecialCharTok{/}\NormalTok{pce}\SpecialCharTok{$}\NormalTok{value[}\DecValTok{1}\SpecialCharTok{:}\NormalTok{(T}\DecValTok{{-}2}\NormalTok{)]) }\CommentTok{\# change in PCE t}
\NormalTok{dsto\_1 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(sto}\SpecialCharTok{$}\NormalTok{value[}\DecValTok{2}\SpecialCharTok{:}\NormalTok{(T}\DecValTok{{-}1}\NormalTok{)]}\SpecialCharTok{/}\NormalTok{sto}\SpecialCharTok{$}\NormalTok{value[}\DecValTok{1}\SpecialCharTok{:}\NormalTok{(T}\DecValTok{{-}2}\NormalTok{)]) }\CommentTok{\# return t}
\NormalTok{dbdr\_1 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(bdr}\SpecialCharTok{$}\NormalTok{value[}\DecValTok{2}\SpecialCharTok{:}\NormalTok{(T}\DecValTok{{-}1}\NormalTok{)]}\SpecialCharTok{/}\NormalTok{bdr}\SpecialCharTok{$}\NormalTok{value[}\DecValTok{1}\SpecialCharTok{:}\NormalTok{(T}\DecValTok{{-}2}\NormalTok{)]) }\CommentTok{\# return t}
\end{Highlighting}
\end{Shaded}

Define the matrices containing the \(F_{t+1}\), \(\textbf{z}_t\), and \(R_{t+1}\) vectors:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{F\_tp1 }\OtherTok{\textless{}{-}} \FunctionTok{cbind}\NormalTok{(dvix,dpce)}
\NormalTok{Z     }\OtherTok{\textless{}{-}} \FunctionTok{cbind}\NormalTok{(}\DecValTok{1}\NormalTok{,dvix\_1,dpce\_1,dsto\_1,dbdr\_1)}
\NormalTok{b\_F }\OtherTok{\textless{}{-}} \FunctionTok{t}\NormalTok{(}\FunctionTok{solve}\NormalTok{(}\FunctionTok{t}\NormalTok{(Z) }\SpecialCharTok{\%*\%}\NormalTok{ Z) }\SpecialCharTok{\%*\%} \FunctionTok{t}\NormalTok{(Z) }\SpecialCharTok{\%*\%}\NormalTok{ F\_tp1)}
\NormalTok{F\_innov }\OtherTok{\textless{}{-}}\NormalTok{ F\_tp1 }\SpecialCharTok{{-}}\NormalTok{ Z }\SpecialCharTok{\%*\%} \FunctionTok{t}\NormalTok{(b\_F)}
\NormalTok{R\_tp1 }\OtherTok{\textless{}{-}} \FunctionTok{cbind}\NormalTok{(dsto,dbdr)}
\NormalTok{n\_F }\OtherTok{\textless{}{-}} \FunctionTok{dim}\NormalTok{(F\_tp1)[}\DecValTok{2}\NormalTok{]; n\_R }\OtherTok{\textless{}{-}} \FunctionTok{dim}\NormalTok{(R\_tp1)[}\DecValTok{2}\NormalTok{]; n\_z }\OtherTok{\textless{}{-}} \FunctionTok{dim}\NormalTok{(Z)[}\DecValTok{2}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

Function \texttt{f\_aux} compute the \(h(x_t;{\boldsymbol\theta})\) and the \(g(\underline{y_T};{\boldsymbol\theta})\); function \texttt{f2beMin} is the function to be minimized.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{f\_aux }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(theta)\{}
\NormalTok{  b\_M }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(theta[}\DecValTok{1}\SpecialCharTok{:}\NormalTok{n\_F],}\AttributeTok{ncol=}\DecValTok{1}\NormalTok{)}
\NormalTok{  R\_aux }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(F\_innov }\SpecialCharTok{\%*\%}\NormalTok{ b\_M,T}\DecValTok{{-}2}\NormalTok{,n\_R) }\SpecialCharTok{*}\NormalTok{ R\_tp1 }\SpecialCharTok{{-}}\NormalTok{ R\_tp1 }\SpecialCharTok{+} \DecValTok{1}
\NormalTok{  H }\OtherTok{\textless{}{-}}\NormalTok{ (R\_aux }\SpecialCharTok{\%x\%} \FunctionTok{matrix}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,n\_z)) }\SpecialCharTok{*}\NormalTok{ (}\FunctionTok{matrix}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,n\_R) }\SpecialCharTok{\%x\%}\NormalTok{ Z)}
\NormalTok{  g }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\FunctionTok{apply}\NormalTok{(H,}\DecValTok{2}\NormalTok{,mean),}\AttributeTok{ncol=}\DecValTok{1}\NormalTok{)}
  \FunctionTok{return}\NormalTok{(}\FunctionTok{list}\NormalTok{(}\AttributeTok{g=}\NormalTok{g,}\AttributeTok{H=}\NormalTok{H))}
\NormalTok{\}}
\NormalTok{f2beMin }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(theta,W)\{}\CommentTok{\# function to be minimized}
\NormalTok{  res }\OtherTok{\textless{}{-}} \FunctionTok{f\_aux}\NormalTok{(theta)}
  \FunctionTok{return}\NormalTok{(}\FunctionTok{t}\NormalTok{(res}\SpecialCharTok{$}\NormalTok{g) }\SpecialCharTok{\%*\%}\NormalTok{ W }\SpecialCharTok{\%*\%}\NormalTok{ res}\SpecialCharTok{$}\NormalTok{g)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Now, let's minimize this function, using use the BFGS numerical algorithm (part of the \texttt{optim} wrapper). We run 5 iterations (where \(W\) is updated).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(AEC)}
\NormalTok{theta }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\FunctionTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{,n\_F)) }\CommentTok{\# inital value}
\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\DecValTok{10}\NormalTok{)\{}\CommentTok{\# recursion on W}
\NormalTok{  res }\OtherTok{\textless{}{-}} \FunctionTok{f\_aux}\NormalTok{(theta)}
\NormalTok{  W }\OtherTok{\textless{}{-}}  \FunctionTok{solve}\NormalTok{(}\FunctionTok{NW.LongRunVariance}\NormalTok{(res}\SpecialCharTok{$}\NormalTok{H,}\AttributeTok{q=}\DecValTok{6}\NormalTok{))}
\NormalTok{  res.optim }\OtherTok{\textless{}{-}} \FunctionTok{optim}\NormalTok{(theta,f2beMin,}\AttributeTok{W=}\NormalTok{W,}
                     \AttributeTok{method=}\StringTok{"BFGS"}\NormalTok{, }\CommentTok{\# could be "Nelder{-}Mead"}
                     \AttributeTok{control=}\FunctionTok{list}\NormalTok{(}\AttributeTok{trace=}\ConstantTok{FALSE}\NormalTok{,}\AttributeTok{maxit=}\DecValTok{200}\NormalTok{),}\AttributeTok{hessian=}\ConstantTok{TRUE}\NormalTok{)}
\NormalTok{  theta }\OtherTok{\textless{}{-}}\NormalTok{ res.optim}\SpecialCharTok{$}\NormalTok{par}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Finally, let's compute the standard deviation of the parameter estimates, using Eq. \eqref{eq:VGMM}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{eps }\OtherTok{\textless{}{-}}\NormalTok{ .}\DecValTok{0001}
\NormalTok{g0 }\OtherTok{\textless{}{-}} \FunctionTok{f\_aux}\NormalTok{(theta)}\SpecialCharTok{$}\NormalTok{g}
\NormalTok{D }\OtherTok{\textless{}{-}} \ConstantTok{NULL}
\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\FunctionTok{length}\NormalTok{(theta))\{}
\NormalTok{  theta.i }\OtherTok{\textless{}{-}}\NormalTok{ theta}
\NormalTok{  theta.i[i] }\OtherTok{\textless{}{-}}\NormalTok{ theta.i[i] }\SpecialCharTok{+}\NormalTok{ eps}
\NormalTok{  gi }\OtherTok{\textless{}{-}} \FunctionTok{f\_aux}\NormalTok{(theta.i)}\SpecialCharTok{$}\NormalTok{g}
\NormalTok{  D }\OtherTok{\textless{}{-}} \FunctionTok{cbind}\NormalTok{(D,(gi}\SpecialCharTok{{-}}\NormalTok{g0)}\SpecialCharTok{/}\NormalTok{eps)}
\NormalTok{\}}
\NormalTok{V }\OtherTok{\textless{}{-}} \DecValTok{1}\SpecialCharTok{/}\NormalTok{T }\SpecialCharTok{*} \FunctionTok{solve}\NormalTok{(}\FunctionTok{t}\NormalTok{(D) }\SpecialCharTok{\%*\%}\NormalTok{ W }\SpecialCharTok{\%*\%}\NormalTok{ D)}
\NormalTok{std.dev }\OtherTok{\textless{}{-}} \FunctionTok{sqrt}\NormalTok{(}\FunctionTok{diag}\NormalTok{(V));t.stud }\OtherTok{\textless{}{-}}\NormalTok{ theta}\SpecialCharTok{/}\NormalTok{std.dev}
\FunctionTok{cbind}\NormalTok{(theta,std.dev,t.stud)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##            theta    std.dev     t.stud
## [1,]  -0.7180716  0.4646617 -1.5453642
## [2,] -11.2042452 17.1039449 -0.6550679
\end{verbatim}

The Hansen statistic can be used to test the model (see Eq. \eqref{eq:HansenSargan}). If the model is correct, we have:
\[
T g(\underline{y_T};{\boldsymbol\theta})'\, S^{-1} \, g(\underline{y_T};{\boldsymbol\theta}) \sim \,i.i.d.\,\chi^2(J - K),
\]
where \(J\) is the number of moment constraints (\(n_z \times n_r\) here) and \(K\) is the number of estimated parameters (\(=n_F\) here).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{g }\OtherTok{\textless{}{-}} \FunctionTok{f\_aux}\NormalTok{(theta)}\SpecialCharTok{$}\NormalTok{g}
\NormalTok{Hansen\_stat }\OtherTok{\textless{}{-}}\NormalTok{ T }\SpecialCharTok{*} \FunctionTok{t}\NormalTok{(g) }\SpecialCharTok{\%*\%}\NormalTok{ W }\SpecialCharTok{\%*\%}\NormalTok{ g}
\NormalTok{pvalue }\OtherTok{\textless{}{-}} \FunctionTok{pchisq}\NormalTok{(}\AttributeTok{q =}\NormalTok{ Hansen\_stat,}\AttributeTok{df =}\NormalTok{ n\_R}\SpecialCharTok{*}\NormalTok{n\_z }\SpecialCharTok{{-}}\NormalTok{ n\_F)}
\NormalTok{pvalue}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##           [,1]
## [1,] 0.8789782
\end{verbatim}

\hypertarget{secMLE}{%
\section{Maximum Likelihood Estimation}\label{secMLE}}

\hypertarget{intuition}{%
\subsection{Intuition}\label{intuition}}

Intuitively, the \emph{Maximum Likelihood Estimation (MLE)} consists in looking for the value of \({\boldsymbol\theta}\) that is such that the probability of having observed \(\mathbf{y}\) (the sample at hand) is the highest possible.

To set an example, assume that the time periods between the arrivals of two customers in a shop, denoted by \(y_i\), are i.i.d. and follow an exponential distribution, i.e.~\(y_i \sim \,i.i.d.\, \mathcal{E}(\lambda)\). You have observed these arrivals for some time, thereby constituting a sample \(\mathbf{y}=\{y_1,\dots,y_n\}\). You want to estimate \(\lambda\) (i.e.~in that case, the vector of parameters is simply \({\boldsymbol\theta} = \lambda\)).

The density of \(Y\) (one observation) is \(f(y;\lambda) = \dfrac{1}{\lambda}\exp(-y/\lambda)\). Fig. \ref{fig:MLE1} represents such density functions for different values of \(\lambda\).

Your 200 observations are reported at the bottom of Fig. \ref{fig:MLE1} (red bars). You build the histogram and display it on the same chart.

\begin{figure}
\includegraphics[width=0.95\linewidth]{MicroEc_files/figure-latex/MLE1-1} \caption{The red ticks, at the bottom, indicate observations (there are 200 of them). The historgram is based on these 200 observations}\label{fig:MLE1}
\end{figure}

What is your estimate of \(\lambda\)? Intuitively, one is led to take the \(\lambda\) for which the (theoretical) distribution is the closest to the histogram (that can be seen as an ``empirical distribution''). This approach is consistent with the idea of picking the \(\lambda\) for which the probability of observing the values included in \(\mathbf{y}\) is the highest.

Let us be more formal. Assume that you have only four observations: \(y_1=1.1\), \(y_2=2.2\), \(y_3=0.7\) and \(y_4=5.0\). What was the probability of jointly observing:

\begin{itemize}
\tightlist
\item
  \(1.1-\varepsilon \le Y_1 < 1.1+\varepsilon\),
\item
  \(2.2-\varepsilon \le Y_2 < 2.2+\varepsilon\),
\item
  \(0.7-\varepsilon \le Y_3 < 0.7+\varepsilon\), and
\item
  \(5.0-\varepsilon \le Y_4 < 5.0+\varepsilon\)?
\end{itemize}

Because the \(y_i\)'s are i.i.d., this probability is \(\prod_{i=1}^4(2\varepsilon f(y_i,\lambda))\).
The next plot shows the probability (divided by \(16\varepsilon^4\), which does not depend on \(\lambda\)) as a function of \(\lambda\).

\begin{figure}
\includegraphics[width=0.95\linewidth]{MicroEc_files/figure-latex/MLE2-1} \caption{Proba. that $y_i-\varepsilon \le Y_i < y_i+\varepsilon$, $i \in \{1,2,3,4\}$. The vertical red line indicates the maximum of the function.}\label{fig:MLE2}
\end{figure}

The value of \(\lambda\) that maximizes the probability is 2.26.

Let us come back to the example with 200 observations:

\begin{figure}
\includegraphics[width=0.95\linewidth]{MicroEc_files/figure-latex/MLE3-1} \caption{Log-likelihood function associated with the 200 i.i.d. observations. The vertical red line indicates the maximum of the function.}\label{fig:MLE3}
\end{figure}

In that case, the value of \(\lambda\) that maxmimizes the probability is 3.42.

\hypertarget{definition-and-properties}{%
\subsection{Definition and properties}\label{definition-and-properties}}

\(f(y;\boldsymbol\theta)\) denotes the probability density function (p.d.f.) of a random variable \(Y\) which depends on a set of parameters \(\boldsymbol\theta\). The density of \(n\) independent and identically distributed (i.i.d.) observations of \(Y\) is given by:
\[
f(\mathbf{y};\boldsymbol\theta) = \prod_{i=1}^n f(y_i;\boldsymbol\theta),
\]
where \(\mathbf{y}\) denotes the vector of observations; \(\mathbf{y} = \{y_1,\dots,y_n\}\).

\begin{definition}[Likelihood function]
\protect\hypertarget{def:likelihood}{}\label{def:likelihood}The likelihood function is:
\[
\mathcal{L}: \boldsymbol\theta \rightarrow  \mathcal{L}(\boldsymbol\theta;\mathbf{y})=f(\mathbf{y};\boldsymbol\theta)=f(y_1,\dots,y_n;\boldsymbol\theta).
\]
\end{definition}

We often work with \(\log \mathcal{L}\), the \textbf{log-likelihood function}.

\begin{example}[Gaussian distribution]
\protect\hypertarget{exm:normal}{}\label{exm:normal}If \(y_i \sim \mathcal{N}(\mu,\sigma^2)\), then
\[
\log \mathcal{L}(\boldsymbol\theta;\mathbf{y}) = - \frac{1}{2}\sum_{i=1}^n\left( \log \sigma^2 + \log 2\pi + \frac{(y_i-\mu)^2}{\sigma^2} \right).
\]
\end{example}

\begin{definition}[Score]
\protect\hypertarget{def:score}{}\label{def:score}The score \(S(y;\boldsymbol\theta)\) is given by \(\frac{\partial \log f(y;\boldsymbol\theta)}{\partial \boldsymbol\theta}\).
\end{definition}

If \(y_i \sim \mathcal{N}(\mu,\sigma^2)\) (Example \ref{exm:normal}), then
\[
\frac{\partial \log f(y;\boldsymbol\theta)}{\partial \boldsymbol\theta} =
\left[\begin{array}{c}
\dfrac{\partial \log f(y;\boldsymbol\theta)}{\partial \mu}\\
\dfrac{\partial \log f(y;\boldsymbol\theta)}{\partial \sigma^2}
\end{array}\right] =
\left[\begin{array}{c}
\dfrac{y-\mu}{\sigma^2}\\
\frac{1}{2\sigma^2}\left(\frac{(y-\mu)^2}{\sigma^2}-1\right)
\end{array}\right].
\]

\begin{proposition}[Score expectation]
\protect\hypertarget{prp:score}{}\label{prp:score}The expectation of the score is zero.
\end{proposition}

\begin{proof}
We have:
\begin{eqnarray*}
\mathbb{E}\left(\frac{\partial \log f(Y;\boldsymbol\theta)}{\partial \boldsymbol\theta}\right) &=&
\int \frac{\partial \log f(y;\boldsymbol\theta)}{\partial \boldsymbol\theta} f(y;\boldsymbol\theta) dy \\
&=& \int \frac{\partial f(y;\boldsymbol\theta)/\partial \boldsymbol\theta}{f(y;\boldsymbol\theta)} f(y;\boldsymbol\theta) dy =
\frac{\partial}{\partial \boldsymbol\theta} \int f(y;\boldsymbol\theta) dy\\
&=&\partial 1 /\partial \boldsymbol\theta = 0,
\end{eqnarray*}
which gives the result.
\end{proof}

\begin{definition}[Fisher information matrix]
\protect\hypertarget{def:Fisher}{}\label{def:Fisher}The information matrix is (minus) the the expectation of the second derivatives of the log-likelihood function:
\[
\mathcal{I}_Y(\boldsymbol\theta) = - \mathbb{E} \left( \frac{\partial^2 \log f(Y;\boldsymbol\theta)}{\partial \boldsymbol\theta \partial \boldsymbol\theta'} \right).
\]
\end{definition}

\begin{proposition}
\protect\hypertarget{prp:Fisher}{}\label{prp:Fisher}We have
\[
\mathcal{I}_Y(\boldsymbol\theta) = \mathbb{E} \left[ \left( \frac{\partial \log f(Y;\boldsymbol\theta)}{\partial \boldsymbol\theta} \right)
\left( \frac{\partial \log f(Y;\boldsymbol\theta)}{\partial \boldsymbol\theta} \right)' \right] = \mathbb{V}ar[S(Y;\boldsymbol\theta)].
\]
\end{proposition}

\begin{proof}
We have \(\frac{\partial^2 \log f(Y;\boldsymbol\theta)}{\partial \boldsymbol\theta \partial \boldsymbol\theta'} = \frac{\partial^2 f(Y;\boldsymbol\theta)}{\partial \boldsymbol\theta \partial \boldsymbol\theta'}\frac{1}{f(Y;\boldsymbol\theta)} - \frac{\partial \log f(Y;\boldsymbol\theta)}{\partial \boldsymbol\theta}\frac{\partial \log f(Y;\boldsymbol\theta)}{\partial \boldsymbol\theta'}\). The expectation of the first right-hand side term is \(\partial^2 1 /(\partial \boldsymbol\theta \partial \boldsymbol\theta') = \mathbf{0}\), which gives the result.
\end{proof}

\begin{example}
If \(y_i \sim\,i.i.d.\, \mathcal{N}(\mu,\sigma^2)\), let \(\boldsymbol\theta = [\mu,\sigma^2]'\) then
\[
\frac{\partial \log f(y;\boldsymbol\theta)}{\partial \boldsymbol\theta} = \left[\frac{y-\mu}{\sigma^2} \quad \frac{1}{2\sigma^2}\left(\frac{(y-\mu)^2}{\sigma^2}-1\right) \right]',
\]
and
\[
\mathcal{I}_Y(\boldsymbol\theta) = \mathbb{E}\left( \frac{1}{\sigma^4}
\left[
\begin{array}{cc}
\sigma^2&y-\mu\\
y-\mu & \frac{(y-\mu)^2}{\sigma^2}-\frac{1}{2}
\end{array}\right]
\right)=
\left[
\begin{array}{cc}
1/\sigma^2&0\\
0 & 1/(2\sigma^4)
\end{array}\right].
\]
\end{example}

\begin{proposition}[Additive property of the Information matrix]
\protect\hypertarget{prp:additiv}{}\label{prp:additiv}The information matrix resulting from two independent experiments is the sum of the information matrices:
\[
\mathcal{I}_{X,Y}(\boldsymbol\theta) = \mathcal{I}_X(\boldsymbol\theta) + \mathcal{I}_Y(\boldsymbol\theta).
\]
\end{proposition}

\begin{proof}
Directly deduced from the definition of the information matrix (Def. \ref{def:Fisher}), using that the epxectation of a product of independent variables is the product of the expectations.
\end{proof}

\begin{theorem}[Frechet-Darmois-Cramer-Rao bound]
\protect\hypertarget{thm:FDCR}{}\label{thm:FDCR}Consider an unbiased estimator of \(\boldsymbol\theta\) denoted by \(\hat{\boldsymbol\theta}(Y)\). The variance of the random variable \(\boldsymbol\omega'\hat{\boldsymbol\theta}\) (which is a linear combination of the components of \(\hat{\boldsymbol\theta}\)) is larger than:
\[
(\boldsymbol\omega'\boldsymbol\omega)^2/(\boldsymbol\omega' \mathcal{I}_Y(\boldsymbol\theta) \boldsymbol\omega).
\]
\end{theorem}

\begin{proof}
The Cauchy-Schwarz inequality implies that \(\sqrt{\mathbb{V}ar(\boldsymbol\omega'\hat{\boldsymbol\theta}(Y))\mathbb{V}ar(\boldsymbol\omega'S(Y;\boldsymbol\theta))} \ge |\boldsymbol\omega'\mathbb{C}ov[\hat{\boldsymbol\theta}(Y),S(Y;\boldsymbol\theta)]\boldsymbol\omega |\). Now, \(\mathbb{C}ov[\hat{\boldsymbol\theta}(Y),S(Y;\boldsymbol\theta)] = \int_y \hat{\boldsymbol\theta}(y) \frac{\partial \log f(y;\boldsymbol\theta)}{\partial \boldsymbol\theta} f(y;\boldsymbol\theta)dy = \frac{\partial}{\partial \boldsymbol\theta}\int_y \hat{\boldsymbol\theta}(y) f(y;\boldsymbol\theta)dy = \mathbf{I}\) because \(\hat{\boldsymbol\theta}\) is unbiased. Therefore \(\mathbb{V}ar(\boldsymbol\omega'\hat{\boldsymbol\theta}(Y)) \ge \mathbb{V}ar(\boldsymbol\omega'S(Y;\boldsymbol\theta))^{-1} (\boldsymbol\omega'\boldsymbol\omega)^2\). Prop. \ref{prp:Fisher} leads to the result.
\end{proof}

\begin{definition}[Identifiability]
\protect\hypertarget{def:identif}{}\label{def:identif}The vector of parameters \(\boldsymbol\theta\) is identifiable if, for any other vector \(\boldsymbol\theta^*\):
\[
\boldsymbol\theta^* \ne \boldsymbol\theta \Rightarrow \mathcal{L}(\boldsymbol\theta^*;\mathbf{y}) \ne \mathcal{L}(\boldsymbol\theta;\mathbf{y}).
\]
\end{definition}

\begin{definition}[Maximum Likelihood Estimator (MLE)]
\protect\hypertarget{def:MLEest}{}\label{def:MLEest}The maximum likelihood estimator (MLE) is the vector \(\boldsymbol\theta\) that maximizes the likelihood function. Formally:
\begin{equation}
\boldsymbol\theta_{MLE} = \arg \max_{\boldsymbol\theta} \mathcal{L}(\boldsymbol\theta;\mathbf{y})  = \arg \max_{\boldsymbol\theta} \log \mathcal{L}(\boldsymbol\theta;\mathbf{y}).\label{eq:MLEestimator}
\end{equation}
\end{definition}

\begin{definition}[Likelihood equation]
\protect\hypertarget{def:likFunction}{}\label{def:likFunction}A necessary condition for maximizing the likelihood function (under regularity assumption, see Hypotheses \ref{hyp:MLEregularity}) is:
\begin{equation}
\dfrac{\partial \log \mathcal{L}(\boldsymbol\theta;\mathbf{y})}{\partial \boldsymbol\theta} = \mathbf{0}.
\end{equation}
\end{definition}

\begin{hypothesis}[Regularity assumptions]
\protect\hypertarget{hyp:MLEregularity}{}\label{hyp:MLEregularity}

We have:

\begin{enumerate}
\def\labelenumi{\roman{enumi}.}
\tightlist
\item
  \(\boldsymbol\theta \in \Theta\) where \(\Theta\) is compact.
\item
  \(\boldsymbol\theta_0\) is identified.
\item
  The log-likelihood function is continuous in \(\boldsymbol\theta\).
\item
  \(\mathbb{E}_{\boldsymbol\theta_0}(\log f(Y;\boldsymbol\theta))\) exists.
\item
  The log-likelihood function is such that \((1/n)\log\mathcal{L}(\boldsymbol\theta;\mathbf{y})\) converges almost surely to \(\mathbb{E}_{\boldsymbol\theta_0}(\log f(Y;\boldsymbol\theta))\), uniformly in \(\boldsymbol\theta \in \Theta\).
\item
  The log-likelihood function is twice continuously differentiable in an open neighborood of \(\boldsymbol\theta_0\).
\item
  The matrix \(\mathbf{I}(\boldsymbol\theta_0) = - \mathbb{E}_0 \left( \frac{\partial^2 \log \mathcal{L}(\boldsymbol\theta;\mathbf{y})}{\partial \boldsymbol\theta \partial \boldsymbol\theta'}\right)\) ---the Fisher Information matrix--- exists and is nonsingular.
\end{enumerate}

\end{hypothesis}

\begin{proposition}[Properties of MLE]
\protect\hypertarget{prp:MLEproperties}{}\label{prp:MLEproperties}

Under regularity conditions (Assumptions \ref{hyp:MLEregularity}), the MLE is:

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  \textbf{Consistent}: \(\mbox{plim}\; \boldsymbol\theta_{MLE} = {\boldsymbol\theta}_0\) (\({\boldsymbol\theta}_0\) is the true vector of parameters).
\item
  \textbf{Asymptotically normal}:
  \begin{equation}
  \boxed{\sqrt{n}(\boldsymbol\theta_{MLE} - \boldsymbol\theta_{0}) \overset{d}{\rightarrow} \mathcal{N}(0,\mathcal{I}_Y(\boldsymbol\theta_0)^{-1}).} \label{eq:normMLE}
  \end{equation}
\item
  \textbf{Asymptotically efficient}: \(\boldsymbol\theta_{MLE}\) is asymptotically efficient and achieves the Freechet-Darmois-Cramer-Rao lower bound for consistent estimators.
\item
  \textbf{Invariant}: The MLE of \(g(\boldsymbol\theta_0)\) is \(g(\boldsymbol\theta_{MLE})\) if \(g\) is a continuous and continuously differentiable function.
\end{enumerate}

\end{proposition}

\begin{proof}
See Appendix \ref{AppendixProof}.
\end{proof}

Since \(\mathcal{I}_Y(\boldsymbol\theta_0)=\frac{1}{n}\mathbf{I}(\boldsymbol\theta_0)\), the asymptotic covariance matrix of the MLE is \([\mathbf{I}(\boldsymbol\theta_0)]^{-1}\), that is:
\[
[\mathbf{I}(\boldsymbol\theta_0)]^{-1} = \left[- \mathbb{E}_0 \left( \frac{\partial^2 \log \mathcal{L}(\boldsymbol\theta;\mathbf{y})}{\partial \boldsymbol\theta \partial \boldsymbol\theta'}\right) \right]^{-1}.
\]
A direct (analytical) evaluation of this expectation is often out of reach. It can however be estimated by, either:
\begin{eqnarray}
\hat{\mathbf{I}}_1^{-1} &=&  \left( - \frac{\partial^2 \log \mathcal{L}({\boldsymbol\theta_{MLE}};\mathbf{y})}{\partial {\boldsymbol\theta} \partial {\boldsymbol\theta}'}\right)^{-1}, \label{eq:III1}\\
\hat{\mathbf{I}}_2^{-1} &=&  \left( \sum_{i=1}^n \frac{\partial \log \mathcal{L}({\boldsymbol\theta_{MLE}};y_i)}{\partial {\boldsymbol\theta}} \frac{\partial \log \mathcal{L}({\boldsymbol\theta_{MLE}};y_i)}{\partial {\boldsymbol\theta'}} \right)^{-1}.  \label{eq:I2}
\end{eqnarray}

Asymptotically, we have \((\hat{\mathbf{I}}_1^{-1})\hat{\mathbf{I}}_2=Id\), that is, the two formulas provide the same result.

In case of (suspected) misspecification, one can use the so-called \emph{sandwich estimator} of the covariance matrix.\footnote{For more details, see, e.g., \href{https://www.stat.umn.edu/geyer/5601/notes/sand.pdf}{Charles Geyer's lectures notes}.} This covariance matrix is given by:
\begin{equation}
\hat{\mathbf{I}}_3^{-1} = \hat{\mathbf{I}}_2^{-1} \hat{\mathbf{I}}_1 \hat{\mathbf{I}}_2^{-1}.\label{eq:III3}
\end{equation}

\hypertarget{to-sum-up-mle-in-practice}{%
\subsection{To sum up -- MLE in practice}\label{to-sum-up-mle-in-practice}}

To implement MLE, we need:

\begin{itemize}
\tightlist
\item
  A parametric model (depending on the vector of parameters \(\boldsymbol\theta\) whose ``true'' value is \(\boldsymbol\theta_0\)) is specified.
\item
  i.i.d. sources of randomness are identified.
\item
  The density associated to one observation \(y_i\) is computed analytically (as a function of \(\boldsymbol\theta\)): \(f(y;\boldsymbol\theta)\).
\item
  The log-likelihood is \(\log \mathcal{L}(\boldsymbol\theta;\mathbf{y}) = \sum_i \log f(y_i;\boldsymbol\theta)\).
\item
  The MLE estimator results from the optimization problem (this is Eq. \eqref{eq:MLEestimator}):
  \begin{equation}
  \boldsymbol\theta_{MLE} = \arg \max_{\boldsymbol\theta} \log \mathcal{L}(\boldsymbol\theta;\mathbf{y}).
  \end{equation}
\item
  We have: \(\boldsymbol\theta_{MLE} \sim \mathcal{N}({\boldsymbol\theta}_0,\mathbf{I}(\boldsymbol\theta_0)^{-1})\), where \(\mathbf{I}(\boldsymbol\theta_0)^{-1}\) is estimated by means of Eq. \eqref{eq:III1}, Eq. \eqref{eq:I2}, or Eq. \eqref{eq:III3}. Most of the time, this computation is numerical.
\end{itemize}

\hypertarget{example-mle-estimation-of-a-mixture-of-gaussian-distribution}{%
\subsection{Example: MLE estimation of a mixture of Gaussian distribution}\label{example-mle-estimation-of-a-mixture-of-gaussian-distribution}}

Consider the returns of the Swiss Market Index (SMI). Assume that these returns are independently drawn from a mixture of Gaussian distributions. The p.d.f. \(f(x;\boldsymbol\theta)\), with \(\boldsymbol\theta = [\mu_1,\sigma_1,\mu_2,\sigma_2,p]'\), is given by:
\[
p \frac{1}{\sqrt{2\pi\sigma_1^2}}\exp\left(-\frac{(x - \mu_1)^2}{2\sigma_1^2}\right) + (1-p)\frac{1}{\sqrt{2\pi\sigma_2^2}}\exp\left(-\frac{(x - \mu_2)^2}{2\sigma_2^2}\right).
\]
(See \href{https://jrenne.shinyapps.io/density/}{p.d.f. of mixtures of Gaussian distributions}.)

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(AEC);}\FunctionTok{data}\NormalTok{(smi)}
\NormalTok{T }\OtherTok{\textless{}{-}} \FunctionTok{dim}\NormalTok{(smi)[}\DecValTok{1}\NormalTok{]}
\NormalTok{h }\OtherTok{\textless{}{-}} \DecValTok{5} \CommentTok{\# holding period (one week)}
\NormalTok{smi}\SpecialCharTok{$}\NormalTok{r }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\FunctionTok{rep}\NormalTok{(}\ConstantTok{NaN}\NormalTok{,h),}
           \DecValTok{100}\SpecialCharTok{*}\FunctionTok{c}\NormalTok{(}\FunctionTok{log}\NormalTok{(smi}\SpecialCharTok{$}\NormalTok{Close[(}\DecValTok{1}\SpecialCharTok{+}\NormalTok{h)}\SpecialCharTok{:}\NormalTok{T]}\SpecialCharTok{/}\NormalTok{smi}\SpecialCharTok{$}\NormalTok{Close[}\DecValTok{1}\SpecialCharTok{:}\NormalTok{(T}\SpecialCharTok{{-}}\NormalTok{h)])))}
\NormalTok{indic.dates }\OtherTok{\textless{}{-}} \FunctionTok{seq}\NormalTok{(}\DecValTok{1}\NormalTok{,T,}\AttributeTok{by=}\DecValTok{5}\NormalTok{)  }\CommentTok{\# weekly returns}
\NormalTok{smi }\OtherTok{\textless{}{-}}\NormalTok{ smi[indic.dates,]}
\NormalTok{smi }\OtherTok{\textless{}{-}}\NormalTok{ smi[}\FunctionTok{complete.cases}\NormalTok{(smi),]}
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{));}\FunctionTok{par}\NormalTok{(}\AttributeTok{plt=}\FunctionTok{c}\NormalTok{(.}\DecValTok{15}\NormalTok{,.}\DecValTok{95}\NormalTok{,.}\DecValTok{1}\NormalTok{,.}\DecValTok{95}\NormalTok{))}
\FunctionTok{plot}\NormalTok{(smi}\SpecialCharTok{$}\NormalTok{Date,smi}\SpecialCharTok{$}\NormalTok{r,}\AttributeTok{type=}\StringTok{"l"}\NormalTok{,}\AttributeTok{xlab=}\StringTok{""}\NormalTok{,}\AttributeTok{ylab=}\StringTok{"in percent"}\NormalTok{)}
\FunctionTok{abline}\NormalTok{(}\AttributeTok{h=}\DecValTok{0}\NormalTok{,}\AttributeTok{col=}\StringTok{"blue"}\NormalTok{)}
\FunctionTok{abline}\NormalTok{(}\AttributeTok{h=}\FunctionTok{mean}\NormalTok{(smi}\SpecialCharTok{$}\NormalTok{r,}\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{)}\SpecialCharTok{+}\DecValTok{2}\SpecialCharTok{*}\FunctionTok{sd}\NormalTok{(smi}\SpecialCharTok{$}\NormalTok{r,}\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{),}\AttributeTok{lty=}\DecValTok{3}\NormalTok{,}\AttributeTok{col=}\StringTok{"blue"}\NormalTok{)}
\FunctionTok{abline}\NormalTok{(}\AttributeTok{h=}\FunctionTok{mean}\NormalTok{(smi}\SpecialCharTok{$}\NormalTok{r,}\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{)}\SpecialCharTok{{-}}\DecValTok{2}\SpecialCharTok{*}\FunctionTok{sd}\NormalTok{(smi}\SpecialCharTok{$}\NormalTok{r,}\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{),}\AttributeTok{lty=}\DecValTok{3}\NormalTok{,}\AttributeTok{col=}\StringTok{"blue"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\includegraphics[width=0.95\linewidth]{MicroEc_files/figure-latex/smiData-1} \caption{Time series of SMI weekly returns (source: Yahoo Finance).}\label{fig:smiData}
\end{figure}

Build the log-likelihood function (fucntion \texttt{log.f}), and use the numerical BFGS algorithm to maximize it (using the \texttt{optim} wrapper):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{f }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(theta,y)\{ }\CommentTok{\# Likelihood function}
\NormalTok{  mu}\FloatTok{.1} \OtherTok{\textless{}{-}}\NormalTok{ theta[}\DecValTok{1}\NormalTok{]; mu}\FloatTok{.2} \OtherTok{\textless{}{-}}\NormalTok{ theta[}\DecValTok{2}\NormalTok{]}
\NormalTok{  sigma}\FloatTok{.1} \OtherTok{\textless{}{-}}\NormalTok{ theta[}\DecValTok{3}\NormalTok{]; sigma}\FloatTok{.2} \OtherTok{\textless{}{-}}\NormalTok{ theta[}\DecValTok{4}\NormalTok{]}
\NormalTok{  p }\OtherTok{\textless{}{-}} \FunctionTok{exp}\NormalTok{(theta[}\DecValTok{5}\NormalTok{])}\SpecialCharTok{/}\NormalTok{(}\DecValTok{1}\SpecialCharTok{+}\FunctionTok{exp}\NormalTok{(theta[}\DecValTok{5}\NormalTok{]))}
\NormalTok{  res }\OtherTok{\textless{}{-}}\NormalTok{ p}\SpecialCharTok{*}\DecValTok{1}\SpecialCharTok{/}\FunctionTok{sqrt}\NormalTok{(}\DecValTok{2}\SpecialCharTok{*}\NormalTok{pi}\SpecialCharTok{*}\NormalTok{sigma}\FloatTok{.1}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}\SpecialCharTok{*}\FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{(y}\SpecialCharTok{{-}}\NormalTok{mu}\FloatTok{.1}\NormalTok{)}\SpecialCharTok{\^{}}\DecValTok{2}\SpecialCharTok{/}\NormalTok{(}\DecValTok{2}\SpecialCharTok{*}\NormalTok{sigma}\FloatTok{.1}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)) }\SpecialCharTok{+}
\NormalTok{    (}\DecValTok{1}\SpecialCharTok{{-}}\NormalTok{p)}\SpecialCharTok{*}\DecValTok{1}\SpecialCharTok{/}\FunctionTok{sqrt}\NormalTok{(}\DecValTok{2}\SpecialCharTok{*}\NormalTok{pi}\SpecialCharTok{*}\NormalTok{sigma}\FloatTok{.2}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}\SpecialCharTok{*}\FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{(y}\SpecialCharTok{{-}}\NormalTok{mu}\FloatTok{.2}\NormalTok{)}\SpecialCharTok{\^{}}\DecValTok{2}\SpecialCharTok{/}\NormalTok{(}\DecValTok{2}\SpecialCharTok{*}\NormalTok{sigma}\FloatTok{.2}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{))}
  \FunctionTok{return}\NormalTok{(res)}
\NormalTok{\}}
\NormalTok{log.f }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(theta,y)\{ }\CommentTok{\#log{-}Likelihood function}
  \FunctionTok{return}\NormalTok{(}\SpecialCharTok{{-}}\FunctionTok{sum}\NormalTok{(}\FunctionTok{log}\NormalTok{(}\FunctionTok{f}\NormalTok{(theta,y))))}
\NormalTok{\}}
\NormalTok{res.optim }\OtherTok{\textless{}{-}} \FunctionTok{optim}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{,}\FloatTok{0.5}\NormalTok{,}\FloatTok{1.5}\NormalTok{,.}\DecValTok{5}\NormalTok{),}
\NormalTok{                   log.f,}
                   \AttributeTok{y=}\NormalTok{smi}\SpecialCharTok{$}\NormalTok{r,}
                   \AttributeTok{method=}\StringTok{"BFGS"}\NormalTok{, }\CommentTok{\# could be "Nelder{-}Mead"}
                   \AttributeTok{control=}\FunctionTok{list}\NormalTok{(}\AttributeTok{trace=}\ConstantTok{FALSE}\NormalTok{,}\AttributeTok{maxit=}\DecValTok{100}\NormalTok{),}\AttributeTok{hessian=}\ConstantTok{TRUE}\NormalTok{)}
\NormalTok{theta }\OtherTok{\textless{}{-}}\NormalTok{ res.optim}\SpecialCharTok{$}\NormalTok{par}
\NormalTok{theta}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1]  0.3012379 -1.3167476  1.7715072  4.8197596  1.9454889
\end{verbatim}

Next, compute estimates of the covariance matrix of the MLE (using Eqs. \eqref{eq:III1}, \eqref{eq:I2}, and \eqref{eq:III3}), and compare the three sets of resulting standard deviations for the five estimated paramters:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Hessian approach:}
\NormalTok{I}\FloatTok{.1} \OtherTok{\textless{}{-}} \FunctionTok{solve}\NormalTok{(res.optim}\SpecialCharTok{$}\NormalTok{hessian)}
\CommentTok{\# Outer{-}product of gradient approach:}
\NormalTok{log.f}\FloatTok{.0} \OtherTok{\textless{}{-}} \FunctionTok{log}\NormalTok{(}\FunctionTok{f}\NormalTok{(theta,smi}\SpecialCharTok{$}\NormalTok{r))}
\NormalTok{epsilon }\OtherTok{\textless{}{-}}\NormalTok{ .}\DecValTok{00000001}
\NormalTok{d.log.f }\OtherTok{\textless{}{-}} \ConstantTok{NULL}
\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\FunctionTok{length}\NormalTok{(theta))\{}
\NormalTok{  theta.i }\OtherTok{\textless{}{-}}\NormalTok{ theta}
\NormalTok{  theta.i[i] }\OtherTok{\textless{}{-}}\NormalTok{ theta.i[i] }\SpecialCharTok{+}\NormalTok{ epsilon}
\NormalTok{  log.f.i }\OtherTok{\textless{}{-}} \FunctionTok{log}\NormalTok{(}\FunctionTok{f}\NormalTok{(theta.i,smi}\SpecialCharTok{$}\NormalTok{r))}
\NormalTok{  d.log.f }\OtherTok{\textless{}{-}} \FunctionTok{cbind}\NormalTok{(d.log.f,}
\NormalTok{                   (log.f.i }\SpecialCharTok{{-}}\NormalTok{ log.f}\FloatTok{.0}\NormalTok{)}\SpecialCharTok{/}\NormalTok{epsilon)}
\NormalTok{\}}
\NormalTok{I}\FloatTok{.2} \OtherTok{\textless{}{-}} \FunctionTok{solve}\NormalTok{(}\FunctionTok{t}\NormalTok{(d.log.f) }\SpecialCharTok{\%*\%}\NormalTok{ d.log.f)}
\CommentTok{\# Misspecification{-}robust approach (sandwich formula):}
\NormalTok{I}\FloatTok{.3} \OtherTok{\textless{}{-}}\NormalTok{ I}\FloatTok{.1} \SpecialCharTok{\%*\%} \FunctionTok{solve}\NormalTok{(I}\FloatTok{.2}\NormalTok{) }\SpecialCharTok{\%*\%}\NormalTok{ I}\FloatTok{.1}
\FunctionTok{cbind}\NormalTok{(}\FunctionTok{diag}\NormalTok{(I}\FloatTok{.1}\NormalTok{),}\FunctionTok{diag}\NormalTok{(I}\FloatTok{.2}\NormalTok{),}\FunctionTok{diag}\NormalTok{(I}\FloatTok{.3}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##             [,1]        [,2]       [,3]
## [1,] 0.003683422 0.003199481 0.00586160
## [2,] 0.226892824 0.194283391 0.38653389
## [3,] 0.005764271 0.002769579 0.01712255
## [4,] 0.194081311 0.047466419 0.83130838
## [5,] 0.092114437 0.040366005 0.31347858
\end{verbatim}

According to the first (respectively third) type of estimate for the covariance matrix, a 95\% confidence interval for \(\mu_1\) is {[}0.182, 0.42{]} (resp. {[}0.151, 0.451{]}).

Note that we have not directly estimated parameter \(p\) but \(\nu = \log(p/(1-p))\) (in such a way that \(p = \exp(\nu)/(1+\exp(\nu))\)). In order to get an estimate of the standard deviation of our esitmate of \(p\), we can implement the \textbf{Delta method}. This method is based on the fact that, for a function \(g\) that is continuous in the neighborhood of \(\boldsymbol\theta_0\) and for large \(n\), we have:
\begin{equation}
\mathbb{V}ar(g(\hat{\boldsymbol\theta}_n)) \approx \frac{\partial g(\hat{\boldsymbol\theta}_n)}{\partial \boldsymbol\theta'}\mathbb{V}ar(\hat{\boldsymbol\theta}_n)\frac{\partial g(\hat{\boldsymbol\theta}_n)'}{\partial \boldsymbol\theta}.\label{eq:DeltaMethod}
\end{equation}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{g }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(theta)\{}
\NormalTok{  mu}\FloatTok{.1} \OtherTok{\textless{}{-}}\NormalTok{ theta[}\DecValTok{1}\NormalTok{]; mu}\FloatTok{.2} \OtherTok{\textless{}{-}}\NormalTok{ theta[}\DecValTok{2}\NormalTok{]}
\NormalTok{  sigma}\FloatTok{.1} \OtherTok{\textless{}{-}}\NormalTok{ theta[}\DecValTok{3}\NormalTok{]; sigma}\FloatTok{.2} \OtherTok{\textless{}{-}}\NormalTok{ theta[}\DecValTok{4}\NormalTok{]}
\NormalTok{  p }\OtherTok{\textless{}{-}} \FunctionTok{exp}\NormalTok{(theta[}\DecValTok{5}\NormalTok{])}\SpecialCharTok{/}\NormalTok{(}\DecValTok{1}\SpecialCharTok{+}\FunctionTok{exp}\NormalTok{(theta[}\DecValTok{5}\NormalTok{]))}
  \FunctionTok{return}\NormalTok{(}\FunctionTok{c}\NormalTok{(mu}\FloatTok{.1}\NormalTok{,mu}\FloatTok{.2}\NormalTok{,sigma}\FloatTok{.1}\NormalTok{,sigma}\FloatTok{.2}\NormalTok{,p))}
\NormalTok{\}}
\CommentTok{\# Computation of g\textquotesingle{}s gradient around estimated theta:}
\NormalTok{eps }\OtherTok{\textless{}{-}}\NormalTok{ .}\DecValTok{00001}
\NormalTok{g.theta }\OtherTok{\textless{}{-}} \FunctionTok{g}\NormalTok{(theta)}
\NormalTok{g.gradient }\OtherTok{\textless{}{-}} \ConstantTok{NULL}
\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{)\{}
\NormalTok{  theta.perturb }\OtherTok{\textless{}{-}}\NormalTok{ theta}
\NormalTok{  theta.perturb[i] }\OtherTok{\textless{}{-}}\NormalTok{ theta[i] }\SpecialCharTok{+}\NormalTok{ eps}
\NormalTok{  g.gradient }\OtherTok{\textless{}{-}} \FunctionTok{cbind}\NormalTok{(g.gradient,(}\FunctionTok{g}\NormalTok{(theta.perturb)}\SpecialCharTok{{-}}\NormalTok{g.theta)}\SpecialCharTok{/}\NormalTok{eps)}
\NormalTok{\}}
\NormalTok{Var }\OtherTok{\textless{}{-}}\NormalTok{ g.gradient }\SpecialCharTok{\%*\%}\NormalTok{ I}\FloatTok{.3} \SpecialCharTok{\%*\%} \FunctionTok{t}\NormalTok{(g.gradient)}
\NormalTok{stdv.g.theta }\OtherTok{\textless{}{-}} \FunctionTok{sqrt}\NormalTok{(}\FunctionTok{diag}\NormalTok{(Var))}
\NormalTok{stdv.theta }\OtherTok{\textless{}{-}} \FunctionTok{sqrt}\NormalTok{(}\FunctionTok{diag}\NormalTok{(I}\FloatTok{.3}\NormalTok{))}
\FunctionTok{cbind}\NormalTok{(theta,stdv.theta,g.theta,stdv.g.theta)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##           theta stdv.theta    g.theta stdv.g.theta
## [1,]  0.3012379 0.07656108  0.3012379   0.07656108
## [2,] -1.3167476 0.62171850 -1.3167476   0.62171850
## [3,]  1.7715072 0.13085316  1.7715072   0.13085316
## [4,]  4.8197596 0.91176114  4.8197596   0.91176114
## [5,]  1.9454889 0.55989158  0.8749539   0.06125726
\end{verbatim}

The previous results show that the MLE estimate of \(p\) is 0.8749539, and its standard deviation is approximately equal to 0.0612573.

To finish with, let us draw the estimated parametric p.d.f. (the mixture of Gaussian distribution), and compare it to a non-parametric (kernel-based) estimate of this p.d.f. (using function \texttt{density}):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{seq}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{5}\NormalTok{,}\DecValTok{5}\NormalTok{,}\AttributeTok{by=}\NormalTok{.}\DecValTok{01}\NormalTok{)}
\FunctionTok{par}\NormalTok{(}\AttributeTok{plt=}\FunctionTok{c}\NormalTok{(.}\DecValTok{1}\NormalTok{,.}\DecValTok{95}\NormalTok{,.}\DecValTok{1}\NormalTok{,.}\DecValTok{95}\NormalTok{))}
\FunctionTok{plot}\NormalTok{(x,}\FunctionTok{f}\NormalTok{(theta,x),}\AttributeTok{type=}\StringTok{"l"}\NormalTok{,}\AttributeTok{lwd=}\DecValTok{2}\NormalTok{,}\AttributeTok{xlab=}\StringTok{"returns, in percent"}\NormalTok{,}\AttributeTok{ylab=}\StringTok{""}\NormalTok{,}
     \AttributeTok{ylim=}\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\FloatTok{1.4}\SpecialCharTok{*}\FunctionTok{max}\NormalTok{(}\FunctionTok{f}\NormalTok{(theta,x))))}
\FunctionTok{lines}\NormalTok{(}\FunctionTok{density}\NormalTok{(smi}\SpecialCharTok{$}\NormalTok{r),}\AttributeTok{type=}\StringTok{"l"}\NormalTok{,}\AttributeTok{lwd=}\DecValTok{2}\NormalTok{,}\AttributeTok{lty=}\DecValTok{3}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(x,}\FunctionTok{dnorm}\NormalTok{(x,}\AttributeTok{mean=}\FunctionTok{mean}\NormalTok{(smi}\SpecialCharTok{$}\NormalTok{r),}\AttributeTok{sd =} \FunctionTok{sd}\NormalTok{(smi}\SpecialCharTok{$}\NormalTok{r)),}\AttributeTok{col=}\StringTok{"red"}\NormalTok{,}\AttributeTok{lty=}\DecValTok{2}\NormalTok{,}\AttributeTok{lwd=}\DecValTok{2}\NormalTok{)}
\FunctionTok{rug}\NormalTok{(smi}\SpecialCharTok{$}\NormalTok{r,}\AttributeTok{col=}\StringTok{"blue"}\NormalTok{)}
\FunctionTok{legend}\NormalTok{(}\StringTok{"topleft"}\NormalTok{,}
       \FunctionTok{c}\NormalTok{(}\StringTok{"Kernel estimate (non{-}parametric)"}\NormalTok{,}
         \StringTok{"Estimated mixture of Gaussian distr. (MLE, parametric)"}\NormalTok{,}
         \StringTok{"Normal distribution"}\NormalTok{),}
       \AttributeTok{lty=}\FunctionTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{),}\AttributeTok{lwd=}\FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{), }\CommentTok{\# line width}
       \AttributeTok{col=}\FunctionTok{c}\NormalTok{(}\StringTok{"black"}\NormalTok{,}\StringTok{"black"}\NormalTok{,}\StringTok{"red"}\NormalTok{),}\AttributeTok{pt.bg=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{),}\AttributeTok{pt.cex =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{),}
       \AttributeTok{bg=}\StringTok{"white"}\NormalTok{,}\AttributeTok{seg.len =} \DecValTok{4}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\includegraphics[width=0.95\linewidth]{MicroEc_files/figure-latex/smidistri-1} \caption{Comparison of different estimates of the distribution of returns.}\label{fig:smidistri}
\end{figure}

\hypertarget{TestMLE}{%
\subsection{Test procedures}\label{TestMLE}}

Suppose we want to test the following parameter restrictions:
\begin{equation}
\boxed{H_0: \underbrace{h(\boldsymbol\theta)}_{r \times 1}=0.}
\end{equation}

In the context of MLE, three tests are largely used:

\begin{itemize}
\tightlist
\item
  Likelihood Ratio (LR) test,
\item
  Wald (W) test,
\item
  Lagrange Multiplier (LM) test.
\end{itemize}

Here is the rationale behind these three tests:\footnote{An interesting graphical presentation of the tests is proposed in \href{http://hedibert.org/wp-content/uploads/2014/04/LR-W-LM-Tests-Buse1982.pdf}{Buse (1982)}.}

\begin{itemize}
\tightlist
\item
  LR: If \(h(\boldsymbol\theta)=0\), then imposing this restriction during the estimation (restricted estimator) should not result in a large decrease in the likelihood function (w.r.t the unrestricted estimation).
\item
  Wald: If \(h(\boldsymbol\theta)=0\), then \(h(\hat{\boldsymbol\theta})\) should not be far from \(0\) (even if these restrictions are not imposed during the MLE).
\item
  LM: If \(h(\boldsymbol\theta)=0\), then the gradient of the likelihood function should be small when evaluated at the restricted estimator.
\end{itemize}

In terms of implementation, while the LR necessitates to estimate both restricted and unrestricted models, the Wald test requires the estimation of the unrestricted model only, and the LM tests requires the estimation of the restricted model only.

As shown below, the three test statistics associated with these three tests coincide asymptotically. (Therefore, they naturally have the same asymptotic distribution, that are \(\chi^2\).)

\begin{proposition}[Asymptotic distribution of the Wald statistic]
\protect\hypertarget{prp:Walddistri}{}\label{prp:Walddistri}Under regularity conditions (Assumptions \ref{hyp:MLEregularity}) and under \(H_0: h(\boldsymbol\theta)=0\), the Wald statistic, defined by:
\[
\boxed{\xi^W = h(\hat{\boldsymbol\theta})' \mathbb{V}ar[h(\hat{\boldsymbol\theta})]^{-1} h(\hat{\boldsymbol\theta}),}
\]
where
\begin{equation}
\mathbb{V}ar[h(\hat{\boldsymbol\theta})] = \left(\frac{\partial h(\hat{\boldsymbol\theta})}{\partial \boldsymbol\theta'} \right) \mathbb{V}ar[\hat{\boldsymbol\theta}]
\left(\frac{\partial h(\hat{\boldsymbol\theta})'}{\partial \boldsymbol\theta} \right),\label{eq:varinWald}
\end{equation}
is asymptotically \(\chi^2(r)\), where the number of degrees of freedom \(r\) corresponds to the dimension of \(h(\boldsymbol\theta)\). (Note that Eq. \eqref{eq:varinWald} is the same as the one used in the Delta method, see Eq. \eqref{eq:DeltaMethod}.)

The Wald test, defined by the critical region
\[
\{\xi^W \ge \chi^2_{1-\alpha}(r)\},
\]
where \(\chi^2_{1-\alpha}(r)\) denotes the quantile of level \(1-\alpha\) of the \(\chi^2(r)\) distribution, has asymptotic level \(\alpha\) and is consistent.\footnote{See Defs. \ref{def:asmyptlevel} and \ref{def:asmyptconsisttest} for definitions of the asymptotic levels and consistency of tests.}
\end{proposition}

\begin{proof}
See Appendix \ref{AppendixProof}.
\end{proof}

In practice, in Eq. \eqref{eq:varinWald}, \(\mathbb{V}ar[\hat{\boldsymbol\theta}]\) is replaced by an estimate given, e.g., by Eq. \eqref{eq:III1}, Eq. \eqref{eq:I2}, or Eq. \eqref{eq:III3}.

\begin{proposition}[Asymptotic distribution of the LM test statistic]
\protect\hypertarget{prp:LMdistri}{}\label{prp:LMdistri}Under regularity conditions (Assumptions \ref{hyp:MLEregularity}) and under \(H_0: h(\boldsymbol\theta)=0\), the LM statistic
\begin{equation}
\boxed{\xi^{LM} =
\left(\left.\frac{\partial \log \mathcal{L}(\boldsymbol\theta)}{\partial \boldsymbol\theta'}\right|_{\boldsymbol\theta = \hat{\boldsymbol\theta}^0}  \right)
[\mathbf{I}(\hat{\boldsymbol\theta}^0)]^{-1}
\left(\left.\frac{\partial \log \mathcal{L}(\boldsymbol\theta)}{\partial \boldsymbol\theta }\right|_{\boldsymbol\theta = \hat{\boldsymbol\theta}^0}  \right),} \label{eq:xiLM}
\end{equation}
(where \(\hat{\boldsymbol\theta}^0\) is the restricted MLE estimator) is \(\chi^2(r)\).

The test defined by the critical region:
\[
\{\xi^{LM} \ge \chi^2_{1-\alpha}(r)\}
\]
has asymptotic level \(\alpha\) and is consistent (see Defs. \ref{def:asmyptlevel} and \ref{def:asmyptconsisttest}). This test is called \emph{Score} or \emph{Lagrange Multiplier (LM)} test.
\end{proposition}

\begin{proof}
See Appendix \ref{AppendixProof}.
\end{proof}

\begin{definition}[Likelihood Ratio test statistics]
\protect\hypertarget{def:LR}{}\label{def:LR}The likelihood ratio associated to a restriction of the form \(H_0: h({\boldsymbol\theta})=0\) is given by:
\[
LR = \frac{\mathcal{L}_R(\boldsymbol\theta;\mathbf{y})}{\mathcal{L}_U(\boldsymbol\theta;\mathbf{y})} \quad (\in [0,1]),
\]
where \(\mathcal{L}_R\) (respectively \(\mathcal{L}_U\)) is the likelihood function that imposes (resp. that does not impose) the restriction. The likelihood ratio test statistic is given by \(-2\log(LR)\), that is:
\[
\boxed{\xi^{LR}= 2 (\log\mathcal{L}_U(\boldsymbol\theta;\mathbf{y})-\log\mathcal{L}_R(\boldsymbol\theta;\mathbf{y})).}
\]
\end{definition}

\begin{proposition}[Asymptotic equivalence of LR, LM, and Wald tests]
\protect\hypertarget{prp:equivLRLMW}{}\label{prp:equivLRLMW}Under the null hypothesis \(H_0\), we have, asymptotically:
\[
\xi^{LM} = \xi^{LR} = \xi^{W}.
\]
\end{proposition}

\begin{proof}
See Appendix \ref{AppendixProof}.
\end{proof}

\hypertarget{bayesian-approach}{%
\section{Bayesian approach}\label{bayesian-approach}}

\hypertarget{introduction}{%
\subsection{Introduction}\label{introduction}}

An excellent introduction to Bayesian methods is proposed by \href{http://www.columbia.edu/~mh2078/MonteCarlo/MCMC_Bayes.pdf}{Martin Haugh, 2017}.

As suggested by the name of this approach, the starting point is the Bayes formula:
\[
\mathbb{P}(A|B) = \frac{\mathbb{P}(A \& B)}{\mathbb{P}(B)},
\]
where \(A\) and \(B\) are two ``events''. For instance, \(A\) may be: parameter \(\alpha\) (conceived as something stochastic) lies in interval \([a,b]\). Assume that you are interested in the probability of occurrence of \(A\). Without any specific information (or ``unconditionally''), this probability if \(\mathbb{P}(A)\). Your evaluation of this probability can only be better if you are provided with any additional form of information. Typically, if the event \(B\) tends to occur simultaneously with \(A\), then knowledge of \(B\) can be useful. The Bayes formula says how this additional information (on \(B\)) can be used to ``update'' the probability of event \(A\).

In our case, this intuition will work as follows: assume that you know the form of the data-generating process (DGP). That is, you know the structure of the model used to draw some stochastic data; you also know the type of distributions used to generate these data. However, you do not know the numerical values of all the parameters characterizing the DGP. Let us denote by \({\boldsymbol\theta}\) the vector of unknown parameters. While these parameters are not known exactly, assume that we have --even without having observed any data-- some \textbf{priors} on their distribution. Then, as was the case in the example above (with \(A\) and \(B\)), the observation of data generated by the model can only reduce the uncertainty associated with \({\boldsymbol\theta}\). Loosely speaking, combining the priors and the observations of data generated by the model should result in ``thinner'' distributions for the components of \({\boldsymbol\theta}\). The latter distributions are called the \textbf{posterior distributions}.\footnote{The output of the Bayesian approach will be the (posterior) distribution of the vector of parameters (\(\boldsymbol\theta\)). When we speak about the \emph{distributions of the components of \({\boldsymbol\theta}\)}, we mean the marginal distributions of each component of the vector.}

Let us formalize this intuition. Define the prior by \(f_{\boldsymbol\theta}({\boldsymbol\theta})\) and the model realizations (the ``data'') by vector \(\mathbf{y}\). The joint distribution of \((\mathbf{y},{\boldsymbol\theta})\) is given by:
\[
f_{Y,{\boldsymbol\theta}}(\mathbf{y},{\boldsymbol\theta}) = f_{Y|{\boldsymbol\theta}}(\mathbf{y},{\boldsymbol\theta})f_{\boldsymbol\theta}({\boldsymbol\theta}),
\]
and, symmetrically, by
\[
f_{Y,{\boldsymbol\theta}}(\mathbf{y},{\boldsymbol\theta}) = f_{{\boldsymbol\theta}|Y}({\boldsymbol\theta},\mathbf{y})f_Y(\mathbf{y}),
\]
where \(f_{{\boldsymbol\theta}|Y}(\cdot,\mathbf{y})\), the distribution of the parameters conditional on the observations, is the \textbf{posterior} distribution.

The last two equations imply that:
\begin{equation}
f_{{\boldsymbol\theta}|Y}({\boldsymbol\theta},\mathbf{y}) = \frac{f_{Y|{\boldsymbol\theta}}(\mathbf{y},{\boldsymbol\theta})f_{{\boldsymbol\theta}}({\boldsymbol\theta})}{f_Y(\mathbf{y})}.\label{eq:post1}
\end{equation}
Note that \(f_Y\) is the marginal (or unconditional) distribution of \(\mathbf{y}\), that can be written:
\begin{equation}
f_Y(\mathbf{y}) = \int f_{Y|{\boldsymbol\theta}}(\mathbf{y},{\boldsymbol\theta})f_{\boldsymbol\theta}({\boldsymbol\theta}) d {\boldsymbol\theta}.
\end{equation}

Eq. \eqref{eq:post1} is sometimes rewritten as follows:
\begin{equation}
f_{{\boldsymbol\theta}|Y}({\boldsymbol\theta},\mathbf{y}) \propto f_{{\boldsymbol\theta},Y}({\boldsymbol\theta},\mathbf{y}) := f_{Y|{\boldsymbol\theta}}(\mathbf{y},{\boldsymbol\theta})f_{\boldsymbol\theta}({\boldsymbol\theta}), \label{eq:post2}
\end{equation}
where \(\propto\) means, loosely speaking, ``\emph{proportional to}''. In rare instances, starting from given priors, one can analytically compute the posterior distribution \(f_{\boldsymbol\theta}({\boldsymbol\theta},\mathbf{y})\). However, in most cases, this is out of reach. One then has to resort to numerical approaches to compute the posterior distribution. Monte Carlo Markov Chains (MCMC) is one of them.

According to the Bernstein-von Mises Theorem, Bayesian and MLE estimators have the same large sample properties. (In particular, the Bayesian approach also achieve the FDCR bound, see Theorem \ref{thm:FDCR}.) The intuition behind this result is that the influence of the prior diminishes with increasing sample sizes.

\hypertarget{monte-carlo-markov-chains}{%
\subsection{Monte-Carlo Markov Chains}\label{monte-carlo-markov-chains}}

MCMC techniques aim at using simulations to approach a distribution whose distribution is difficult to obtain analytically. Indeed, in some circumstances, one can draw in a distribution even if we do not know its analytical expression.

\begin{definition}[Markov Chain]
\protect\hypertarget{def:MC}{}\label{def:MC}The sequence \(\{z_i\}\) is said to be a (first-order) Markovian process is it satisfies:
\[
f(z_i|z_{i-1},z_{i-2},\dots) = f(z_i|z_{i-1}).
\]
\end{definition}

The Metropolis-Hastings (MH) algorithm is a specific MCMC approach that allows to generate samples of \({\boldsymbol\theta}\)'s whose distribution approximately corresponds to the posterior distribution of Eq. \eqref{eq:post1}.

The MH algorithm is a recursive algorithm. That is, one can draw the \(i^{th}\) value of \({\boldsymbol\theta}\), denoted by \({\boldsymbol\theta}_i\), if one has already drawn \({\boldsymbol\theta}_{i-1}\). Assume we have \({\boldsymbol\theta}_{i-1}\). We obtain a value for \({\boldsymbol\theta}_i\) by implementing the following steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Draw \(\tilde{{\boldsymbol\theta}}_i\) from the conditional distribution \(Q_{\tilde{{\boldsymbol\theta}}|{\boldsymbol\theta}}(\cdot,{\boldsymbol\theta}_{i-1})\), called \textbf{proposal distribution}.
\item
  Draw \(u\) in a uniform distribution on \([0,1]\).
\item
  Compute
  \begin{equation}
  \alpha(\tilde{{\boldsymbol\theta}}_i,{\boldsymbol\theta}_{i-1}):= \min\left(\frac{f_{{\boldsymbol\theta},Y}(\tilde{{\boldsymbol\theta}}_i,\mathbf{y})}{f_{{\boldsymbol\theta},Y}({\boldsymbol\theta}_{i-1},\mathbf{y})}\times\frac{Q_{\tilde{{\boldsymbol\theta}}|{\boldsymbol\theta}}({\boldsymbol\theta}_{i-1},\tilde{{\boldsymbol\theta}}_i)}{Q_{\tilde{{\boldsymbol\theta}}|{\boldsymbol\theta}}(\tilde{{\boldsymbol\theta}}_i,{\boldsymbol\theta}_{i-1})},1\right),\label{eq:alphaXXX}
  \end{equation}
  where \(f_{{\boldsymbol\theta},Y}\) is given in Eq. \eqref{eq:post2}.
\item
  If \(u<\alpha(\tilde{{\boldsymbol\theta}}_i,{\boldsymbol\theta}_{i-1})\), then take \({\boldsymbol\theta}_i = \tilde{{\boldsymbol\theta}}_i\), otherwise we leave \({\boldsymbol\theta}_i\) equal to \({\boldsymbol\theta}_{i-1}\).
\end{enumerate}

It can be shown that, the distribution of the draws converges to the posterior distribution. That is, after a sufficiently large number of iterations, the draws can be considered to be drawn from the posterior distribution.\footnote{The proof of this claim is based on the fact that, if \({\boldsymbol\theta}_{i-1}\) is drawn from the posterior distribution, then it is also the case for \({\boldsymbol\theta}_i\).}

To get some insights into the algorithm, consider the case of a \textbf{symmetric proposal distribution}, that is:
\begin{equation}
Q_{\tilde{{\boldsymbol\theta}}|{\boldsymbol\theta}}(\tilde{{\boldsymbol\theta}}_i,{\boldsymbol\theta}_{i-1})=Q_{\tilde{{\boldsymbol\theta}}|{\boldsymbol\theta}}({\boldsymbol\theta}_{i-1},\tilde{{\boldsymbol\theta}}_i).\label{eq:symmQ}
\end{equation}
We then have:
\begin{equation}
\alpha(\tilde{{\boldsymbol\theta}},{\boldsymbol\theta}_{i-1})= \min\left(\frac{q(\tilde{{\boldsymbol\theta}},y)}{q({\boldsymbol\theta}_{i-1},y)},1\right). \label{eq:hypoQ}
\end{equation}
Remember that, up to the marginal distribution of the data (\(f_Y(\mathbf{y})\)), \(f_{{\boldsymbol\theta},Y}(\tilde{{\boldsymbol\theta}},\mathbf{y})\) is the probability of observing \(\mathbf{y}\) conditional on having a model parameterized by \(\tilde{\boldsymbol\theta}\). Then, under Eq. \eqref{eq:hypoQ}, it appears that if this probability is larger for \(\tilde{\boldsymbol\theta}\) than for \({\boldsymbol\theta}_{i-1}\) (in which case \(\tilde{\boldsymbol\theta}\) seems ``more consistent with the observations \(\mathbf{y}\)'' than \({\boldsymbol\theta}_{i-1}\)), we accept \({\boldsymbol\theta}_i\). By contrast, if \(f_{{\boldsymbol\theta},Y}(\tilde{{\boldsymbol\theta}},\mathbf{y})<f_{{\boldsymbol\theta},Y}({\boldsymbol\theta}_{i-1},\mathbf{y})\), then we do not necessarily accept the proposed value \(\tilde{{\boldsymbol\theta}}\), especially if \(f_{{\boldsymbol\theta},Y}(\tilde{{\boldsymbol\theta}},\mathbf{y})\ll f_{{\boldsymbol\theta},Y}({\boldsymbol\theta}_{i-1},\mathbf{y})\) (in which case \(\tilde{\boldsymbol\theta}\) seems far less consistent with the observations \(\mathbf{y}\) than \({\boldsymbol\theta}_{i-1}\), and, accordingly, the acceptance probability, namely \(\alpha(\tilde{{\boldsymbol\theta}},{\boldsymbol\theta}_{i-1})\), is small).

The choice of the \textbf{proposal distribution} \(Q_{\tilde{\boldsymbol\theta}|{\boldsymbol\theta}}\) is crucial to get a rapid convergence of the algorithm. Looking at Eq. \eqref{eq:alphaXXX}, it is easily seen that the optimal choice would be \(Q_{\tilde{\boldsymbol\theta}|{\boldsymbol\theta}}(\cdot,{\boldsymbol\theta}_i)=f_{{\boldsymbol\theta}|Y}(\cdot,\mathbf{y})\). In that case, we would have \(\alpha(\tilde{{\boldsymbol\theta}}_i,{\boldsymbol\theta}_{i-1})\equiv 1\) (see Eq. \eqref{eq:alphaXXX}). We would then accept all draws from the proposal distribution, as this distribution would directly be the posterior distribution. Of course, this situation is not realistoc as the objective of the algorithm is precisely to approximate the posterior distribution.

A common choice for \(Q\) is a multivariate normal distribution. If \({\boldsymbol\theta}\) is of dimension \(K\), we can for instance use:
\[
Q(\tilde{\boldsymbol\theta},{\boldsymbol\theta})= \frac{1}{\left(\sqrt{2\pi\sigma^2}\right)^K}\exp\left(-\frac{1}{2}\sum_{j=1}^K\frac{(\tilde{\boldsymbol\theta}_j-{\boldsymbol\theta}_j)^2}{\sigma^2}\right),
\]
which is an example of symmetric proposal distribution (see Eq. \eqref{eq:symmQ}). Equivalently, we then have:
\[
\tilde{\boldsymbol\theta} = {\boldsymbol\theta} + \varepsilon,
\]
where \(\varepsilon\) is a \(K\)-dimensional vector of independent zero-mean normal disturbances of variance \(\sigma^2\).\footnote{We could also have different variances for the different components of \({\boldsymbol\theta}\). However, this may lead to complicated settings. A useful practice consists in looking for model (re)parametrization --based, e.g., on exponential and/or logistic functions-- that are such that the components of \({\boldsymbol\theta}\) are all expected to be of the order of magnitude of the unity.} One then has to determine an appropriate value for \(\sigma\). If it is too low, then \(\alpha\) will be close to 1 (as \(\tilde{{\boldsymbol\theta}}_i\) will be close to \({\boldsymbol\theta}_{i-1}\)), and we will accept very often the proposed value (\(\tilde{{\boldsymbol\theta}}_i\)). This seems to be a favourable situation. But it may not be. Indeed, it means that it will take a large number of iterations to explore the whole distribution of \({\boldsymbol\theta}\). What if \(\sigma\) is very large? In this case, it is likely that the porposed values (\(\tilde{{\boldsymbol\theta}}_i\)) will often result in poor likelihoods; The probability of acceptance will then be low and the Markov chain may be blocked at its initial value. Therefore, intermediate values of \(\sigma^2\) have to be determined. The acceptance rate (i.e., the average value of \(\alpha(\tilde{{\boldsymbol\theta}},{\boldsymbol\theta}_{i-1})\)) can be used as a guide for that. Indeed, a literature explores the optimal values for such acceptance rate (in order to obtain the best possible fit of the posterior for a minimum number of algorithm iterations). In particular, following \citet{Roberts_Gelman_Gilks_1997}, people often target acceptance rate of the order of magnitude of 20\%.

It is important to note that, to implement this approach, one only has to be able to compute the joint p.d.f. \(q({\boldsymbol\theta},\mathbf{y})=f_{Y|{\boldsymbol\theta}}(\mathbf{y},{\boldsymbol\theta})f_{\boldsymbol\theta}({\boldsymbol\theta})\) (Eq. \eqref{eq:post2}). That is, as soon as one can evaluate the likelihood (\(f_{Y|{\boldsymbol\theta}}(\mathbf{y},{\boldsymbol\theta})\)) and the prior (\(f_{\boldsymbol\theta}({\boldsymbol\theta})\)), we can employ this methodology.

\hypertarget{example-ar1-specification}{%
\subsection{Example: AR(1) specification}\label{example-ar1-specification}}

In the following example, we employ MCMC in order to estimate the posterior distributions of the three parameters defining an AR(1) model (see Section \ref{ARsection}). The specification is as follows:
\[
y_t = \mu + \rho y_{t-1} + \sigma \varepsilon_{t}, \quad \varepsilon_t \sim \,i.i.d.\,\mathcal{N}(0,1).
\]
Hence, we have \({\boldsymbol\theta} = [\mu,\rho,\sigma]\). Let us first simulate the process on \(T\) periods:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mu }\OtherTok{\textless{}{-}}\NormalTok{ .}\DecValTok{6}\NormalTok{; rho }\OtherTok{\textless{}{-}}\NormalTok{ .}\DecValTok{8}\NormalTok{; sigma }\OtherTok{\textless{}{-}}\NormalTok{ .}\DecValTok{5} \CommentTok{\# true model specification}
\NormalTok{T }\OtherTok{\textless{}{-}} \DecValTok{20} \CommentTok{\# number of observations}
\NormalTok{y0 }\OtherTok{\textless{}{-}}\NormalTok{ mu}\SpecialCharTok{/}\NormalTok{(}\DecValTok{1}\SpecialCharTok{{-}}\NormalTok{rho)}
\NormalTok{Y }\OtherTok{\textless{}{-}} \ConstantTok{NULL}
\ControlFlowTok{for}\NormalTok{(t }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{T)\{}
  \ControlFlowTok{if}\NormalTok{(t}\SpecialCharTok{==}\DecValTok{1}\NormalTok{)\{y }\OtherTok{\textless{}{-}}\NormalTok{ y0\}}
\NormalTok{  y }\OtherTok{\textless{}{-}}\NormalTok{ mu }\SpecialCharTok{+}\NormalTok{ rho}\SpecialCharTok{*}\NormalTok{y }\SpecialCharTok{+}\NormalTok{ sigma }\SpecialCharTok{*} \FunctionTok{rnorm}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{  Y }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(Y,y)\}}
\FunctionTok{plot}\NormalTok{(Y,}\AttributeTok{type=}\StringTok{"l"}\NormalTok{,}\AttributeTok{xlab=}\StringTok{"time t"}\NormalTok{,}\AttributeTok{ylab=}\FunctionTok{expression}\NormalTok{(y[t]))}
\end{Highlighting}
\end{Shaded}

\includegraphics{MicroEc_files/figure-latex/MCMC1-1.pdf}

Next, let us write the likelihood function, i.e.~\(f_{Y|{\boldsymbol\theta}}(\mathbf{y},{\boldsymbol\theta})\). For \(\rho\), which is expected to be between 0 and 1, we use a logistic transformation. For \(\sigma\), that is expected to be positive, we use an exponential transformation.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{likelihood }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(param,Y)\{}
\NormalTok{  mu  }\OtherTok{\textless{}{-}}\NormalTok{ param[}\DecValTok{1}\NormalTok{]}
\NormalTok{  rho }\OtherTok{\textless{}{-}} \FunctionTok{exp}\NormalTok{(param[}\DecValTok{2}\NormalTok{])}\SpecialCharTok{/}\NormalTok{(}\DecValTok{1}\SpecialCharTok{+}\FunctionTok{exp}\NormalTok{(param[}\DecValTok{2}\NormalTok{]))}
\NormalTok{  sigma }\OtherTok{\textless{}{-}} \FunctionTok{exp}\NormalTok{(param[}\DecValTok{3}\NormalTok{])}
\NormalTok{  MU }\OtherTok{\textless{}{-}}\NormalTok{ mu}\SpecialCharTok{/}\NormalTok{(}\DecValTok{1}\SpecialCharTok{{-}}\NormalTok{rho)}
\NormalTok{  SIGMA2 }\OtherTok{\textless{}{-}}\NormalTok{ sigma}\SpecialCharTok{\^{}}\DecValTok{2}\SpecialCharTok{/}\NormalTok{(}\DecValTok{1}\SpecialCharTok{{-}}\NormalTok{rho}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}
\NormalTok{  L }\OtherTok{\textless{}{-}} \DecValTok{1}\SpecialCharTok{/}\FunctionTok{sqrt}\NormalTok{(}\DecValTok{2}\SpecialCharTok{*}\NormalTok{pi}\SpecialCharTok{*}\NormalTok{SIGMA2)}\SpecialCharTok{*}\FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{(Y[}\DecValTok{1}\NormalTok{]}\SpecialCharTok{{-}}\NormalTok{MU)}\SpecialCharTok{\^{}}\DecValTok{2}\SpecialCharTok{/}\NormalTok{(}\DecValTok{2}\SpecialCharTok{*}\NormalTok{SIGMA2))}
\NormalTok{  Y1 }\OtherTok{\textless{}{-}}\NormalTok{ Y[}\DecValTok{2}\SpecialCharTok{:}\FunctionTok{length}\NormalTok{(Y)]}
\NormalTok{  Y0 }\OtherTok{\textless{}{-}}\NormalTok{ Y[}\DecValTok{1}\SpecialCharTok{:}\NormalTok{(}\FunctionTok{length}\NormalTok{(Y)}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{)]}
\NormalTok{  aux }\OtherTok{\textless{}{-}} \DecValTok{1}\SpecialCharTok{/}\FunctionTok{sqrt}\NormalTok{(}\DecValTok{2}\SpecialCharTok{*}\NormalTok{pi}\SpecialCharTok{*}\NormalTok{sigma}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}\SpecialCharTok{*}\FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{(Y1}\SpecialCharTok{{-}}\NormalTok{mu}\SpecialCharTok{{-}}\NormalTok{rho}\SpecialCharTok{*}\NormalTok{Y0)}\SpecialCharTok{\^{}}\DecValTok{2}\SpecialCharTok{/}\NormalTok{(}\DecValTok{2}\SpecialCharTok{*}\NormalTok{sigma}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{))}
\NormalTok{  L }\OtherTok{\textless{}{-}}\NormalTok{ L }\SpecialCharTok{*} \FunctionTok{prod}\NormalTok{(aux)}
  \FunctionTok{return}\NormalTok{(L)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Next define function \texttt{rQ} that draws from the (Gaussian) proposal distribution, as well as function \texttt{Q}, that computes \(Q_{\tilde{\boldsymbol\theta}|{\boldsymbol\theta}}(\tilde{\boldsymbol\theta},{\boldsymbol\theta})\):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rQ }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(x,a)\{}
\NormalTok{  n }\OtherTok{\textless{}{-}} \FunctionTok{length}\NormalTok{(x)}
\NormalTok{  y }\OtherTok{\textless{}{-}}\NormalTok{ x }\SpecialCharTok{+}\NormalTok{ a }\SpecialCharTok{*} \FunctionTok{rnorm}\NormalTok{(n)}
  \FunctionTok{return}\NormalTok{(y)\}}
\NormalTok{Q }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(y,x,a)\{}
\NormalTok{  q }\OtherTok{\textless{}{-}} \DecValTok{1}\SpecialCharTok{/}\FunctionTok{sqrt}\NormalTok{(}\DecValTok{2}\SpecialCharTok{*}\NormalTok{pi}\SpecialCharTok{*}\NormalTok{a}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}\SpecialCharTok{*}\FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{(y }\SpecialCharTok{{-}}\NormalTok{ x)}\SpecialCharTok{\^{}}\DecValTok{2}\SpecialCharTok{/}\NormalTok{(}\DecValTok{2}\SpecialCharTok{*}\NormalTok{a}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{))}
  \FunctionTok{return}\NormalTok{(}\FunctionTok{prod}\NormalTok{(q))\}}
\end{Highlighting}
\end{Shaded}

We consider Gaussian priors:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{prior }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(param,means\_prior,stdv\_prior)\{}
\NormalTok{  f }\OtherTok{\textless{}{-}} \DecValTok{1}\SpecialCharTok{/}\FunctionTok{sqrt}\NormalTok{(}\DecValTok{2}\SpecialCharTok{*}\NormalTok{pi}\SpecialCharTok{*}\NormalTok{stdv\_prior}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}\SpecialCharTok{*}\FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{(param }\SpecialCharTok{{-}} 
\NormalTok{                                         means\_prior)}\SpecialCharTok{\^{}}\DecValTok{2}\SpecialCharTok{/}\NormalTok{(}\DecValTok{2}\SpecialCharTok{*}\NormalTok{stdv\_prior}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{))}
  \FunctionTok{return}\NormalTok{(}\FunctionTok{prod}\NormalTok{(f))\}}
\end{Highlighting}
\end{Shaded}

Function \texttt{p\_tilde} corresponds to \(f_{{\boldsymbol\theta},Y}\):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p\_tilde }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(param,Y,means\_prior,stdv\_prior)\{}
\NormalTok{  p }\OtherTok{\textless{}{-}} \FunctionTok{likelihood}\NormalTok{(param,Y) }\SpecialCharTok{*} \FunctionTok{prior}\NormalTok{(param,means\_prior,stdv\_prior)}
  \FunctionTok{return}\NormalTok{(p)\}}
\end{Highlighting}
\end{Shaded}

We can now define function \(\alpha\) (Eq. \eqref{eq:alphaXXX}):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{alpha }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(y,x,means\_prior,stdv\_prior,a)\{}
\NormalTok{  aux }\OtherTok{\textless{}{-}} \FunctionTok{p\_tilde}\NormalTok{(y,Y,means\_prior,stdv\_prior)}\SpecialCharTok{/}
    \FunctionTok{p\_tilde}\NormalTok{(x,Y,means\_prior,stdv\_prior) }\SpecialCharTok{*} \FunctionTok{Q}\NormalTok{(y,x,a)}\SpecialCharTok{/}\FunctionTok{Q}\NormalTok{(x,y,a)}
\NormalTok{  alpha\_proba }\OtherTok{\textless{}{-}} \FunctionTok{min}\NormalTok{(aux,}\DecValTok{1}\NormalTok{)}
  \FunctionTok{return}\NormalTok{(alpha\_proba)\}}
\end{Highlighting}
\end{Shaded}

Now, all is set for us to write the MCMC function:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{MCMC }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(Y,means\_prior,stdv\_prior,a,N)\{}
\NormalTok{  x }\OtherTok{\textless{}{-}}\NormalTok{ means\_prior}
\NormalTok{  all\_theta }\OtherTok{\textless{}{-}} \ConstantTok{NULL}
\NormalTok{  count\_accept }\OtherTok{\textless{}{-}} \DecValTok{0}
  \ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{N)\{}
\NormalTok{    y }\OtherTok{\textless{}{-}} \FunctionTok{rQ}\NormalTok{(x,a)}
\NormalTok{    alph }\OtherTok{\textless{}{-}} \FunctionTok{alpha}\NormalTok{(y,x,means\_prior,stdv\_prior,a)}
    \CommentTok{\#print(alph)}
\NormalTok{    u }\OtherTok{\textless{}{-}} \FunctionTok{runif}\NormalTok{(}\DecValTok{1}\NormalTok{)}
    \ControlFlowTok{if}\NormalTok{(u }\SpecialCharTok{\textless{}}\NormalTok{ alph)\{}
\NormalTok{      count\_accept }\OtherTok{\textless{}{-}}\NormalTok{ count\_accept }\SpecialCharTok{+} \DecValTok{1}
\NormalTok{      x }\OtherTok{\textless{}{-}}\NormalTok{ y\}}
\NormalTok{    all\_theta }\OtherTok{\textless{}{-}} \FunctionTok{rbind}\NormalTok{(all\_theta,x)\}}
  \FunctionTok{print}\NormalTok{(}\FunctionTok{paste}\NormalTok{(}\StringTok{"Acceptance rate:"}\NormalTok{,}\FunctionTok{toString}\NormalTok{(}\FunctionTok{round}\NormalTok{(count\_accept}\SpecialCharTok{/}\NormalTok{N,}\DecValTok{3}\NormalTok{))))}
  \FunctionTok{return}\NormalTok{(all\_theta)\}}
\end{Highlighting}
\end{Shaded}

Specify the Gaussian priors:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{true\_values }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(mu,}\FunctionTok{log}\NormalTok{(rho}\SpecialCharTok{/}\NormalTok{(}\DecValTok{1}\SpecialCharTok{{-}}\NormalTok{rho)),}\FunctionTok{log}\NormalTok{(sigma))}
\NormalTok{means\_prior }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{) }\CommentTok{\# as if we did not know the true values}
\NormalTok{stdv\_prior }\OtherTok{\textless{}{-}} \FunctionTok{rep}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{)}
\NormalTok{resultMCMC }\OtherTok{\textless{}{-}} \FunctionTok{MCMC}\NormalTok{(Y,means\_prior,stdv\_prior,}\AttributeTok{a=}\NormalTok{.}\DecValTok{45}\NormalTok{,}\AttributeTok{N=}\DecValTok{20000}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Acceptance rate: 0.098"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{))}
\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\FunctionTok{length}\NormalTok{(means\_prior))\{}
\NormalTok{  m }\OtherTok{\textless{}{-}}\NormalTok{ means\_prior[i]}
\NormalTok{  s }\OtherTok{\textless{}{-}}\NormalTok{ stdv\_prior[i]}
\NormalTok{  x }\OtherTok{\textless{}{-}} \FunctionTok{seq}\NormalTok{(m}\DecValTok{{-}3}\SpecialCharTok{*}\NormalTok{s,m}\SpecialCharTok{+}\DecValTok{3}\SpecialCharTok{*}\NormalTok{s,}\AttributeTok{length.out =} \DecValTok{100}\NormalTok{)}
  \FunctionTok{par}\NormalTok{(}\AttributeTok{mfg=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,i))}
\NormalTok{  aux }\OtherTok{\textless{}{-}} \FunctionTok{density}\NormalTok{(resultMCMC[,i])}
  \FunctionTok{par}\NormalTok{(}\AttributeTok{plt=}\FunctionTok{c}\NormalTok{(.}\DecValTok{15}\NormalTok{,.}\DecValTok{95}\NormalTok{,.}\DecValTok{15}\NormalTok{,.}\DecValTok{85}\NormalTok{))}
  \FunctionTok{plot}\NormalTok{(x,}\FunctionTok{dnorm}\NormalTok{(x,m,s),}\AttributeTok{type=}\StringTok{"l"}\NormalTok{,}\AttributeTok{xlab=}\StringTok{""}\NormalTok{,}\AttributeTok{ylab=}\StringTok{""}\NormalTok{,}\AttributeTok{main=}\FunctionTok{paste}\NormalTok{(}\StringTok{"Parameter"}\NormalTok{,i),}
       \AttributeTok{ylim=}\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\FunctionTok{max}\NormalTok{(aux}\SpecialCharTok{$}\NormalTok{y)))}
  \FunctionTok{lines}\NormalTok{(aux}\SpecialCharTok{$}\NormalTok{x,aux}\SpecialCharTok{$}\NormalTok{y,}\AttributeTok{col=}\StringTok{"red"}\NormalTok{,}\AttributeTok{lwd=}\DecValTok{2}\NormalTok{)}
  \FunctionTok{abline}\NormalTok{(}\AttributeTok{v=}\NormalTok{true\_values[i],}\AttributeTok{lty=}\DecValTok{2}\NormalTok{,}\AttributeTok{col=}\StringTok{"blue"}\NormalTok{)}
  \FunctionTok{par}\NormalTok{(}\AttributeTok{mfg=}\FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,i))}
  \FunctionTok{plot}\NormalTok{(resultMCMC[,i],}\DecValTok{1}\SpecialCharTok{:}\FunctionTok{length}\NormalTok{(resultMCMC[,i]),}\AttributeTok{xlim=}\FunctionTok{c}\NormalTok{(}\FunctionTok{min}\NormalTok{(x),}\FunctionTok{max}\NormalTok{(x)),}
       \AttributeTok{type=}\StringTok{"l"}\NormalTok{,}\AttributeTok{xlab=}\StringTok{""}\NormalTok{,}\AttributeTok{ylab=}\StringTok{""}\NormalTok{)\}}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\includegraphics[width=1\linewidth]{MicroEc_files/figure-latex/MCMC8-1} \caption{The upper line of plot compares prior (black) and posterior (red) distributions. The vertical dashed blue lines indicate the true values of the parameters. The second row of plots show the sequence of $\boldsymbol\theta_i$'s generated by the MCMC algorithm. These sequences are the ones used to produce the posterior distributions (red lines) in the upper plots.}\label{fig:MCMC8}
\end{figure}

\hypertarget{microeconometrics}{%
\chapter{Microeconometrics}\label{microeconometrics}}

In microeconometric models, the variables of interest often feature restricted distributions ---for instance with discontinuous support---, which necessitates specific models. Typical examples are discrete-choice models (binary, multinomial, ordered outcomes), sample selection models (censored or truncated outcomes), and count-data models (integer outcomes). This chapter describes the estimation and interpretation of these models. It also shows how the discrete-choice models can emerge from (structural) random-utility frameworks.

\hypertarget{binary-choice-models}{%
\section{Binary-choice models}\label{binary-choice-models}}

In many instances, the variables to be explained (the \(y_i\)'s) have only two possible values (\(0\) and \(1\), say). That is, they are binary variables. The probability for them to be equal to either 0 or 1 may depend on independent variables, gathered in vectors \(\mathbf{x}_i\) (\(K \times 1\)).

The spectrum of applications is wide:

\begin{itemize}
\tightlist
\item
  Binary decisions (e.g.~in referendums, being owner or renter, living in the city or in the countryside, in/out of the labour force,\ldots),
\item
  Contamination (disease or default),
\item
  Success/failure (exams).
\end{itemize}

Without loss of generality, the model reads:
\begin{equation}\label{eq:binaryBenroulli}
y_i | \mathbf{X} \sim \mathcal{B}(g(\mathbf{x}_i;\boldsymbol\theta)),
\end{equation}
where \(g(\mathbf{x}_i;\boldsymbol\theta)\) is the parameter of the Bernoulli distribution. In other words, conditionally on \(\mathbf{X}\):
\begin{equation}
y_i = \left\{
\begin{array}{cl}
1 & \mbox{ with probability } g(\mathbf{x}_i;\boldsymbol\theta)\\
0 & \mbox{ with probability } 1-g(\mathbf{x}_i;\boldsymbol\theta),
\end{array}
\right.\label{eq:genericBinary}
\end{equation}
where \(\boldsymbol\theta\) is a vector of parameters to be estimated.

An estimation strategy is to assume that \(g(\mathbf{x}_i;\boldsymbol\theta)\) can be proxied by \(\tilde{\boldsymbol\theta}'\mathbf{x}_i\) and to run a linear regression to estimate \(\tilde{\boldsymbol\theta}\) (a situation called \textbf{Linear Probability Model, LPM}):
\[
y_i = \tilde{\boldsymbol\theta}'\mathbf{x}_i + \varepsilon_i.
\]
Notwithstanding the fact that this specification does not exclude negative probabilities or probabilities greater than one, it could be compatible with the \emph{assumption of zero conditional mean} (Hypothesis \ref{hyp:exogeneity}) and with the \emph{assumption of non-correlated residuals} (Hypothesis \ref{hyp:noncorrelResid}), but more difficultly with the \emph{homoskedasticity assumption} (Hypothesis \ref{hyp:homoskedasticity}). Moreover, the \(\varepsilon_i\)'s cannot be Gaussian (because \(y_i \in \{0,1\}\)). Hence, using a linear regression to study the relationship between \(\mathbf{x}_i\) and \(y_i\) can be consistent but it is inefficient.

Figure \ref{fig:LPM} illustrates the fit resulting from an application of the LPM model to binary (dependent) variables.

\begin{figure}
\includegraphics[width=0.95\linewidth]{MicroEc_files/figure-latex/LPM-1} \caption{Fitting a binary variable with a linear model (Linear Probability Model, LPM). The model is $\mathbb{P}(y_i=1|x_i)=\Phi(0.5+2x_i)$, where $\Phi$ is the c.d.f. of the normal distribution and where $x_i \sim \,i.i.d.\,\mathcal{N}(0,1)$.}\label{fig:LPM}
\end{figure}

Except for its last row (LPM case), Table \ref{tab:foo} provides examples of functions \(g\) valued in \([0,1]\), and that can therefore used in models of the type: \(\mathbb{P}(y_i=1|\mathbf{x}_i;\boldsymbol\theta) = g(\boldsymbol\theta'\mathbf{x}_i)\) (see Eq. \eqref{eq:genericBinary}). The ``linear'' case is given for comparison, but note that it does not satisfy \(g(\boldsymbol\theta'\mathbf{x}_i) \in [0,1]\) for any value of \(\boldsymbol\theta'\mathbf{x}_i\).

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2143}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3571}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.4286}}@{}}
\caption{\label{tab:foo} This table provides examples of function \(g\), s.t. \(\mathbb{P}(y_i=1|\mathbf{x}_i;\boldsymbol heta) = g(\boldsymbol\theta'\mathbf{x}_i)\). The LPM case (last row) is given for comparison but, again, it does not satisfy \(g(\boldsymbol\theta'\mathbf{x}_i) \in [0,1]\) for any value of \(\boldsymbol\theta'\mathbf{x}_i\).}\tabularnewline
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
Model
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Function \(g\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Derivative
\end{minipage} \\
\midrule()
\endfirsthead
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
Model
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Function \(g\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Derivative
\end{minipage} \\
\midrule()
\endhead
Probit & \(\Phi\) & \(\phi\) \\
Logit & \(\dfrac{\exp(x)}{1+\exp(x)}\) & \(\dfrac{\exp(x)}{(1+\exp(x))^2}\) \\
log-log & \(1 - \exp(-\exp(x))\) & \(\exp(-\exp(x))\exp(x)\) \\
linear (LPM) & \(x\) & 1 \\
\bottomrule()
\end{longtable}

Figure \ref{fig:ProbLogit} displays the first three \(g\) functions appearing in Table \ref{tab:foo}.

\begin{figure}
\includegraphics[width=0.95\linewidth]{MicroEc_files/figure-latex/ProbLogit-1} \caption{Probit, Logit, and Log-log functions.}\label{fig:ProbLogit}
\end{figure}

The \textbf{probit} and the \textbf{logit} models are popular binary-choice models. In the probit model, we have:
\begin{equation}
g(z) = \Phi(z),\label{eq:probit}
\end{equation}
where \(\Phi\) is the c.d.f. of the normal distribution. And for the logit model:
\begin{equation}
g(z) = \frac{1}{1+\exp(-z)}.\label{eq:logit}
\end{equation}

Figure \ref{fig:LPM2} shows the conditional probabilities associated with the (probit) model that had been used to generate the data of Figure \ref{fig:LPM}.

\begin{figure}
\includegraphics[width=0.95\linewidth]{MicroEc_files/figure-latex/LPM2-1} \caption{The model is $\mathbb{P}(y_i=1|x_i)=\Phi(0.5+2x_i)$, where $\Phi$ is the c.d.f. of the normal distribution and where $x_i \sim \,i.i.d.\,\mathcal{N}(0,1)$. Crosses give the model-implied probabilities of having $y_i=1$ (conditional on $x_i$).}\label{fig:LPM2}
\end{figure}

\hypertarget{latent}{%
\subsection{Interpretation in terms of latent variable, and utility-based models}\label{latent}}

The probit model has an interpretation in terms of latent variables, which, in turn, is often exploited in structural models, called \textbf{Random Utility Models (RUM)}. In such structural models, it is assumed that the agents that have to take the decision do so by selecting the outcome that provides them with the larger utility (for agent \(i\), two possible outcomes: \(y_i=0\) or \(y_i=1\)). Part of this utility is observed by the econometrician ---it depends on the covariates \(\mathbf{x}_i\)--- and part of it is latent.

In the probit model, we have:
\[
\mathbb{P}(y_i=1|\mathbf{x}_i;\boldsymbol\theta) = \Phi(\boldsymbol\theta'\mathbf{x}_i) = \mathbb{P}(-\varepsilon_{i}<\boldsymbol\theta'\mathbf{x}_i),
\]
where \(\varepsilon_{i} \sim \mathcal{N}(0,1)\). That is:
\[
\mathbb{P}(y_i=1|\mathbf{x}_i;\boldsymbol\theta) = \mathbb{P}(0< y_i^*),
\]
where \(y_i^* = \boldsymbol\theta'\mathbf{x}_i + \varepsilon_i\), with \(\varepsilon_{i} \sim \mathcal{N}(0,1)\). Variable \(y_i^*\) can be interpreted as a (latent) variable that determines \(y_i\) (since \(y_i = \mathbb{I}_{\{y_i^*>0\}}\)).

Figure \ref{fig:Latent} illustrates this situation.

\begin{figure}
\includegraphics[width=0.95\linewidth]{MicroEc_files/figure-latex/Latent-1} \caption{Distribution of $y_i^*$ conditional on $\mathbf{x}_i$.}\label{fig:Latent}
\end{figure}

Assume that agent (\(i\)) chooses \(y_i=1\) if the utility associated with this choice (\(U_{i,1}\)) is higher than the one associated with \(y_i=0\) (that is \(U_{i,0}\)). Assume further that the utility of agent \(i\), if she chooses outcome \(j\) (\(\in \{0,1\}\)), is given by
\[
U_{i,j} = V_{i,j} + \varepsilon_{i,j},
\]
where \(V_{i,j}\) is the deterministic component of the utility associated with choice and where \(\varepsilon_{i,j}\) is a random (agent-specific) component. Moreover, posit \(V_{i,j} = \boldsymbol\theta_j'\mathbf{x}_i\). We then have:
\begin{eqnarray}
\mathbb{P}(y_i = 1|\mathbf{x}_i;\boldsymbol\theta) &=& \mathbb{P}(\boldsymbol\theta_1'\mathbf{x}_i+\varepsilon_{i,1}>\boldsymbol\theta_0'\mathbf{x}_i+\varepsilon_{i,0}) \nonumber\\
&=& F(\boldsymbol\theta_1'\mathbf{x}_i-\boldsymbol\theta_0'\mathbf{x}_i) = F([\boldsymbol\theta_1-\boldsymbol\theta_0]'\mathbf{x}_i),\label{eq:utility}
\end{eqnarray}
where \(F\) is the c.d.f. of \(\varepsilon_{i,0}-\varepsilon_{i,1}\).

Note that only the difference \(\boldsymbol\theta_1-\boldsymbol\theta_0\) is identifiable (as opposed to \(\boldsymbol\theta_1\) \emph{and} \(\boldsymbol\theta_0\)). Indeed, replacing \(U\) with \(aU\) (\(a>0\)) gives the same model. This \emph{scaling} issue can be solved by fixing the variance of \(\varepsilon_{i,0}-\varepsilon_{i,1}\).

\begin{example}[Migration and income]
\protect\hypertarget{exm:migration}{}\label{exm:migration}The RUM approach has been used by \citet{Nakosteen_Zimmer_1980} to study migration choices. Their model is based on the comparison of marginal costs and benefits associated with migration. The main ingredients of their approach are as follows:

\begin{itemize}
\tightlist
\item
  Wage that can be earned at the present location: \(y_p^* = \boldsymbol\theta_p'\mathbf{x}_p + \varepsilon_p\).
\item
  Migration cost: \(C^*= \boldsymbol\theta_c'\mathbf{x}_c + \varepsilon_c\).
\item
  Wage earned elsewhere: \(y_m^* = \boldsymbol\theta_m'\mathbf{x}_m + \varepsilon_m\).
\end{itemize}

In this context, agents decision to migrate if \(y_m^* > y_p^* + C^*\), i.e.~if
\[
y^* = y_m^* -  y_p^* - C^* =  \boldsymbol\theta'\mathbf{x} + \underbrace{\varepsilon}_{=\varepsilon_m - \varepsilon_c - \varepsilon_p}>0,
\]
where \(\mathbf{x}\) is the union of the \(\mathbf{x}_i\)s, for \(i \in \{p,m,c\}\).
\end{example}

\hypertarget{Avregressors}{%
\subsection{Alternative-Varying Regressors}\label{Avregressors}}

In some cases, regressors may depend on the considered alternative (\(0\) or \(1\)). For instance:

\begin{itemize}
\tightlist
\item
  When modeling the decision to participate in the labour force (or not), the wage depends on the alternative. Typically, it is zero if the considered agent has decided not to work (and strictly positive otherwise).
\item
  In the context of the choice of transportation mode, ``time cost'' depends on the considered transportation mode.
\end{itemize}

In terms of utility, we then have:
\[
V_{i,j} = {\theta^{(u)}_{j}}'\mathbf{u}_{i,j} + {\theta^{(v)}_{j}}'\mathbf{v}_{i},
\]
where the \(\mathbf{u}_{i,j}\)'s are regressors associated with agent \(i\), but taking different values for the different choices (\(j=0\) or \(j=1\)). In that case, Eq. \eqref{eq:utility} becomes:
\begin{equation}
\mathbb{P}(y_i = 1|\mathbf{x}_i;\boldsymbol\theta)  = F\left({\theta^{(u)}_{1}}'\mathbf{u}_{i,1}-{\theta^{(u)}_{0}}'\mathbf{u}_{i,0}+[\boldsymbol\theta_1^{(v)}-\boldsymbol\theta_0^{(v)}]'\mathbf{v}_i\right),\label{eq:utility2}
\end{equation}
and, if \(\theta^{(u)}_{1}=\theta^{(u)}_{0}=\theta^{(u)}\) ---as is customary--- we get:
\begin{equation}
\mathbb{P}(y_i = 1|\mathbf{x}_i;\boldsymbol\theta)  = F\left({\theta^{(u)}_{1}}'(\mathbf{u}_{i,1}-\mathbf{u}_{i,0})+[\boldsymbol\theta_1^{(v)}-\boldsymbol\theta_0^{(v)}]'\mathbf{v}_i\right).\label{eq:utility3}
\end{equation}

\begin{example}[Fishing-mode dataset]
\protect\hypertarget{exm:FishingTable}{}\label{exm:FishingTable}

The fishing-mode dataset used in \citet{Cameron_Trivedi_2005} (Chapters 14 and 15) contains alternative-specific variables. Specifically, for each individual, the price and catch rate depend on the fishing model. In the table reported below, lines \texttt{price} and \texttt{catch} correspond to the prices and catch rates associated with the chosen alternative.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(mlogit)}
\FunctionTok{data}\NormalTok{(}\StringTok{"Fishing"}\NormalTok{,}\AttributeTok{package=}\StringTok{"mlogit"}\NormalTok{)}
\NormalTok{stargazer}\SpecialCharTok{::}\FunctionTok{stargazer}\NormalTok{(Fishing,}\AttributeTok{type=}\StringTok{"text"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## ==========================================================
## Statistic       N     Mean    St. Dev.    Min      Max    
## ----------------------------------------------------------
## price.beach   1,182  103.422   103.641   1.290   843.186  
## price.pier    1,182  103.422   103.641   1.290   843.186  
## price.boat    1,182  55.257    62.713    2.290   666.110  
## price.charter 1,182  84.379    63.545   27.290   691.110  
## catch.beach   1,182   0.241     0.191    0.068    0.533   
## catch.pier    1,182   0.162     0.160    0.001    0.452   
## catch.boat    1,182   0.171     0.210   0.0002    0.737   
## catch.charter 1,182   0.629     0.706    0.002    2.310   
## income        1,182 4,099.337 2,461.964 416.667 12,500.000
## ----------------------------------------------------------
\end{verbatim}

\end{example}

\hypertarget{estimation}{%
\subsection{Estimation}\label{estimation}}

These models can be estimated by Maximum Likelihood approaches (see Section \ref{secMLE}).

To simplify the exposition, we consider the \(\mathbf{x}_i\) vectors of covariates to be deterministic. Moreover, we assume that the r.v. are independent across entities \(i\). How to write the likelihood in that case? It is easily checked that:
\[
f(y_i|\mathbf{x}_i;\boldsymbol\theta) =   g(\boldsymbol\theta'\mathbf{x}_i)^{y_i}(1-g(\boldsymbol\theta'\mathbf{x}_i))^{1-y_i}.
\]

Therefore, if the observations \((\mathbf{x}_i,y_i)\) are independent across entities \(i\), we obtain:
\[
\log \mathcal{L}(\boldsymbol\theta;\mathbf{y},\mathbf{X}) = \sum_{i=1}^{n}y_i \log[g(\boldsymbol\theta'\mathbf{x}_i)] + (1-y_i)\log[1-g(\boldsymbol\theta'\mathbf{x}_i)].
\]

The likelihood equation reads (FOC of the optimization program, see Def. \ref{def:likFunction}):
\[
\dfrac{\partial \log \mathcal{L}(\boldsymbol\theta;\mathbf{y},\mathbf{X})}{\partial \boldsymbol\theta} = \mathbf{0},
\]
that is:
\[
\sum_{i=1}^{n} y_i \mathbf{x}_i\frac{g'(\boldsymbol\theta'\mathbf{x}_i)}{g(\boldsymbol\theta'\mathbf{x}_i)} - (1-y_i) \mathbf{x}_i \frac{g'(\boldsymbol\theta'\mathbf{x}_i)}{1-g(\boldsymbol\theta'\mathbf{x}_i)} = \mathbf{0}.
\]

This is a nonlinear (multivariate) equation that can be solved numerically. Under regularity conditions (Hypotheses \ref{hyp:MLEregularity}), we approximately have (Prop. \ref{prp:MLEproperties}):
\[
\boldsymbol\theta_{MLE} \sim \mathcal{N}(\boldsymbol\theta_0,\mathbf{I}(\boldsymbol\theta_0)^{-1}),
\]
where
\[
\mathbf{I}(\boldsymbol\theta_0) = - \mathbb{E}_0 \left( \frac{\partial^2 \log \mathcal{L}(\theta;\mathbf{y},\mathbf{X})}{\partial \boldsymbol\theta \partial \boldsymbol\theta'}\right) = n \mathcal{I}_Y(\boldsymbol\theta_0).
\]

For finite samples, we can e.g.~approximate \(\mathbf{I}(\boldsymbol\theta_0)^{-1}\) by Eq. \eqref{eq:III1}:
\[
\mathbf{I}(\boldsymbol\theta_0)^{-1} \approx -\left(\frac{\partial^2 \log \mathcal{L}(\boldsymbol\theta_{MLE};\mathbf{y},\mathbf{X})}{\partial \boldsymbol\theta \partial \boldsymbol\theta'}\right)^{-1}.
\]

In the Probit case (see Table \ref{tab:foo}), it can be shown that we have:
\begin{eqnarray*}
&&\frac{\partial^2 \log \mathcal{L}(\boldsymbol\theta;\mathbf{y},\mathbf{X})}{\partial \boldsymbol\theta \partial \boldsymbol\theta'} = - \sum_{i=1}^{n} g'(\boldsymbol\theta'\mathbf{x}_i) [\mathbf{x}_i \mathbf{x}_i'] \times \\
&&\left[y_i \frac{g'(\boldsymbol\theta'\mathbf{x}_i) + \boldsymbol\theta'\mathbf{x}_ig(\boldsymbol\theta'\mathbf{x}_i)}{g(\boldsymbol\theta'\mathbf{x}_i)^2} + (1-y_i) \frac{g'(\boldsymbol\theta'\mathbf{x}_i) - \boldsymbol\theta'\mathbf{x}_i (1 - g(\boldsymbol\theta'\mathbf{x}_i))}{(1-g(\boldsymbol\theta'\mathbf{x}_i))^2}\right].
\end{eqnarray*}

In the Logit case (see Table \ref{tab:foo}), it can be shown that we have:
\[
\frac{\partial^2 \log \mathcal{L}(\boldsymbol\theta;\mathbf{y},\mathbf{X})}{\partial \boldsymbol\theta \partial \boldsymbol\theta'} = - \sum_{i=1}^{n} g'(\boldsymbol\theta'\mathbf{x}_i) \mathbf{x}_i\mathbf{x}_i',
\]
where \(g'(x)=\dfrac{\exp(-x)}{(1 + \exp(-x))^2}\).

Remark that, since \(g'(x)>0\), \(-\partial^2 \log \mathcal{L}(\boldsymbol\theta;\mathbf{y},\mathbf{X})/\partial \boldsymbol\theta \partial \boldsymbol\theta'\) is positive definite.

\hypertarget{marginalFX}{%
\subsection{Marginal effects}\label{marginalFX}}

How to measure marginal effects, i.e.~the effect on the probability that \(y_i=1\) of a marginal increase of \(x_{i,k}\)? This object is given by:
\[
\frac{\partial \mathbb{P}(y_i=1|\mathbf{x}_i;\boldsymbol\theta)}{\partial x_{i,k}} = \underbrace{g'(\boldsymbol\theta'\mathbf{x}_i)}_{>0}\theta_k,
\]
which is of the same sign as \(\theta_k\) if function \(g\) is monotonously increasing.

For agent \(i\), this marginal effect is consistently estimated by \(g'(\boldsymbol\theta_{MLE}'\mathbf{x}_i)\theta_{MLE,k}\). It is important to see that the marginal effect depends on \(\mathbf{x}_i\): respective increases by 1 unit of \(x_{i,k}\) (entity \(i\)) and of \(x_{j,k}\) (entity \(j\)) do not necessarily have the same effect on \(\mathbb{P}(y_i=1|\mathbf{x}_i;\boldsymbol\theta)\) as on \(\mathbb{P}(y_j=1|\mathbf{x}_j;\boldsymbol\theta)\). To address this issue, one can compute some measures of ``average'' marginal effect. There are two main solutions. For each explanatory variable \(k\):

\begin{enumerate}
\def\labelenumi{\roman{enumi}.}
\tightlist
\item
  Denoting by \(\hat{\mathbf{x}}\) the sample average of the \(\mathbf{x}_i\)s, compute \(g'(\boldsymbol\theta_{MLE}'\hat{\mathbf{x}})\theta_{MLE,k}\).
\item
  Compute the average (across \(i\)) of \(g'(\boldsymbol\theta_{MLE}'\mathbf{x}_i)\theta_{MLE,k}\).
\end{enumerate}

\hypertarget{goodness-of-fit}{%
\subsection{Goodness of fit}\label{goodness-of-fit}}

There is no obvious version of ``\(R^2\)'' for binary-choice models. Existing measures are called \textbf{pseudo-\(R^2\) measures}.

Denoting by \(\log \mathcal{L}_0(\mathbf{y})\) the (maximum) log-likelihood that would be obtained for a model containing only a constant term (i.e.~with \(\mathbf{x}_i = 1\) for all \(i\)), the McFadden's pseudo-\(R^2\) is given by:
\[
R^2_{MF} = 1 - \frac{\log \mathcal{L}(\boldsymbol\theta;\mathbf{y})}{\log \mathcal{L}_0(\mathbf{y})}.
\]
Intuitively, \(R^2_{MF}=0\) if the explanatory variables do not convey any information on the outcome \(y\). Indeed, in this case, the model is not better than the reference model, that simply captures the fraction of \(y_i\)'s that are equal to 1.

\begin{example}[Credit and defaults (Lending-club dataset)]
\protect\hypertarget{exm:creditProbit}{}\label{exm:creditProbit}This example makes use of the \texttt{credit} data of package \texttt{AEC}. The objective is to model the default probabilities of borrowers.

Let us first represent the relationship between the fraction of households that have defaulted on their loan and their annual income:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(AEC)}
\NormalTok{credit}\SpecialCharTok{$}\NormalTok{Default }\OtherTok{\textless{}{-}} \DecValTok{0}
\NormalTok{credit}\SpecialCharTok{$}\NormalTok{Default[credit}\SpecialCharTok{$}\NormalTok{loan\_status }\SpecialCharTok{==} \StringTok{"Charged Off"}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1}
\NormalTok{credit}\SpecialCharTok{$}\NormalTok{Default[credit}\SpecialCharTok{$}\NormalTok{loan\_status }\SpecialCharTok{==}
                 \StringTok{"Does not meet the credit policy. Status:Charged Off"}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1}
\NormalTok{credit}\SpecialCharTok{$}\NormalTok{amt2income }\OtherTok{\textless{}{-}}\NormalTok{ credit}\SpecialCharTok{$}\NormalTok{loan\_amnt}\SpecialCharTok{/}\NormalTok{credit}\SpecialCharTok{$}\NormalTok{annual\_inc}
\FunctionTok{plot}\NormalTok{(}\FunctionTok{as.factor}\NormalTok{(credit}\SpecialCharTok{$}\NormalTok{Default)}\SpecialCharTok{\textasciitilde{}}\FunctionTok{log}\NormalTok{(credit}\SpecialCharTok{$}\NormalTok{annual\_inc),}
     \AttributeTok{ylevels=}\DecValTok{2}\SpecialCharTok{:}\DecValTok{1}\NormalTok{,}\AttributeTok{ylab=}\StringTok{"Default status"}\NormalTok{,}\AttributeTok{xlab=}\StringTok{"log(annual income)"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{MicroEc_files/figure-latex/Probitlending-1.pdf}

The previous figure suggests that the effect of annual income on the probability of default is non-monotonous. We will therefore include a quadratic term in one of our specification (namely \texttt{eq1} below).

We consider three specifications. The first one (\texttt{eq0}), with no explanatory variables, is trivial. It will just be used to compute the pseudo-\(R^2\). In the second (\texttt{eq1}), we consider a few covariates (loan amount, the ratio between the amount and annual income, The number of more-than-30 days past-due incidences of delinquency in the borrower's credit file for the past 2 years, and a quadratic function of annual income). In the third model (\texttt{eq2}), we add a credit rating.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{eq0 }\OtherTok{\textless{}{-}} \FunctionTok{glm}\NormalTok{(Default }\SpecialCharTok{\textasciitilde{}} \DecValTok{1}\NormalTok{,}\AttributeTok{data=}\NormalTok{credit,}\AttributeTok{family=}\FunctionTok{binomial}\NormalTok{(}\AttributeTok{link=}\StringTok{"probit"}\NormalTok{))}
\NormalTok{eq1 }\OtherTok{\textless{}{-}} \FunctionTok{glm}\NormalTok{(Default }\SpecialCharTok{\textasciitilde{}} \FunctionTok{log}\NormalTok{(loan\_amnt) }\SpecialCharTok{+}\NormalTok{ amt2income }\SpecialCharTok{+}\NormalTok{ delinq\_2yrs }\SpecialCharTok{+} 
             \FunctionTok{log}\NormalTok{(annual\_inc)}\SpecialCharTok{+} \FunctionTok{I}\NormalTok{(}\FunctionTok{log}\NormalTok{(annual\_inc)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{),}
           \AttributeTok{data=}\NormalTok{credit,}\AttributeTok{family=}\FunctionTok{binomial}\NormalTok{(}\AttributeTok{link=}\StringTok{"probit"}\NormalTok{))}
\NormalTok{eq2 }\OtherTok{\textless{}{-}} \FunctionTok{glm}\NormalTok{(Default }\SpecialCharTok{\textasciitilde{}}\NormalTok{ grade }\SpecialCharTok{+} \FunctionTok{log}\NormalTok{(loan\_amnt) }\SpecialCharTok{+}\NormalTok{ amt2income }\SpecialCharTok{+}\NormalTok{ delinq\_2yrs }\SpecialCharTok{+} 
             \FunctionTok{log}\NormalTok{(annual\_inc)}\SpecialCharTok{+} \FunctionTok{I}\NormalTok{(}\FunctionTok{log}\NormalTok{(annual\_inc)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{),}
           \AttributeTok{data=}\NormalTok{credit,}\AttributeTok{family=}\FunctionTok{binomial}\NormalTok{(}\AttributeTok{link=}\StringTok{"probit"}\NormalTok{))}
\NormalTok{stargazer}\SpecialCharTok{::}\FunctionTok{stargazer}\NormalTok{(eq0,eq1,eq2,}\AttributeTok{type=}\StringTok{"text"}\NormalTok{,}\AttributeTok{no.space =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## ====================================================
##                           Dependent variable:       
##                     --------------------------------
##                                 Default             
##                        (1)        (2)        (3)    
## ----------------------------------------------------
## gradeB                                     0.400*** 
##                                            (0.055)  
## gradeC                                     0.587*** 
##                                            (0.057)  
## gradeD                                     0.820*** 
##                                            (0.061)  
## gradeE                                     0.874*** 
##                                            (0.091)  
## gradeF                                     1.230*** 
##                                            (0.147)  
## gradeG                                     1.439*** 
##                                            (0.227)  
## log(loan_amnt)                  -0.149**  -0.194*** 
##                                 (0.060)    (0.061)  
## amt2income                      1.266***   1.222*** 
##                                 (0.383)    (0.393)  
## delinq_2yrs                     0.096***    0.009   
##                                 (0.034)    (0.035)  
## log(annual_inc)                 -1.444**    -0.874  
##                                 (0.569)    (0.586)  
## I(log(annual_inc)2)             0.064**     0.038   
##                                 (0.025)    (0.026)  
## Constant            -1.231***   7.937***    4.749   
##                      (0.017)    (3.060)    (3.154)  
## ----------------------------------------------------
## Observations          9,156      9,156      9,156   
## Log Likelihood      -3,157.696 -3,120.625 -2,981.343
## Akaike Inf. Crit.   6,317.392  6,253.250  5,986.686 
## ====================================================
## Note:                    *p<0.1; **p<0.05; ***p<0.01
\end{verbatim}

Let us compute the pseudo R2 for the last two models:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{logL0 }\OtherTok{\textless{}{-}} \FunctionTok{logLik}\NormalTok{(eq0);logL1 }\OtherTok{\textless{}{-}} \FunctionTok{logLik}\NormalTok{(eq1);logL2 }\OtherTok{\textless{}{-}} \FunctionTok{logLik}\NormalTok{(eq2)}
\NormalTok{pseudoR2\_eq1 }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ logL1}\SpecialCharTok{/}\NormalTok{logL0 }\CommentTok{\# pseudo R2}
\NormalTok{pseudoR2\_eq2 }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ logL2}\SpecialCharTok{/}\NormalTok{logL0 }\CommentTok{\# pseudo R2}
\FunctionTok{c}\NormalTok{(pseudoR2\_eq1,pseudoR2\_eq2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.01173993 0.05584870
\end{verbatim}

Let us now compute the (average) marginal effects, using method ii of Section \ref{marginalFX}:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{mean}\NormalTok{(}\FunctionTok{dnorm}\NormalTok{(}\FunctionTok{predict}\NormalTok{(eq2)),}\AttributeTok{na.rm=}\ConstantTok{TRUE}\NormalTok{)}\SpecialCharTok{*}\NormalTok{eq2}\SpecialCharTok{$}\NormalTok{coefficients}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##          (Intercept)               gradeB               gradeC 
##          0.840731198          0.070747353          0.103944305 
##               gradeD               gradeE               gradeF 
##          0.145089219          0.154773742          0.217702041 
##               gradeG       log(loan_amnt)           amt2income 
##          0.254722161         -0.034289921          0.216251992 
##          delinq_2yrs      log(annual_inc) I(log(annual_inc)^2) 
##          0.001574178         -0.154701321          0.006813694
\end{verbatim}

There is an issue for the \texttt{annual\_inc} variable. Indeed, the previous computation does not realize that this variable appears twice among the explanatory variables (through \texttt{log(annual\_inc)} and \texttt{I(log(annual\_inc)\^{}2)}). To address this, one can proceed as follows: (1) we construct a new counterfactual dataset where annual incomes are increased by 1\%, (2) we use the model to compute model-implied probabilities of default on this new dataset and (3), we subtract the probabilities resulting from the original dataset from these counterfactual probabilities:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{new\_credit }\OtherTok{\textless{}{-}}\NormalTok{ credit}
\NormalTok{new\_credit}\SpecialCharTok{$}\NormalTok{annual\_inc }\OtherTok{\textless{}{-}} \FloatTok{1.01} \SpecialCharTok{*}\NormalTok{ new\_credit}\SpecialCharTok{$}\NormalTok{annual\_inc}
\NormalTok{bas\_predict\_eq2  }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(eq2, }\AttributeTok{newdata =}\NormalTok{ credit, }\AttributeTok{type =} \StringTok{"response"}\NormalTok{)}
\CommentTok{\# This is equivalent to pnorm(predict(eq2, newdata = credit))}
\NormalTok{new\_predict\_eq2  }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(eq2, }\AttributeTok{newdata =}\NormalTok{ new\_credit, }\AttributeTok{type =} \StringTok{"response"}\NormalTok{)}
\FunctionTok{mean}\NormalTok{(new\_predict\_eq2 }\SpecialCharTok{{-}}\NormalTok{ bas\_predict\_eq2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] -6.562126e-05
\end{verbatim}

The negative sign means that, on average across the entities considered in the analysis, a 1\% increase in annual income results in a decrease in the default probability. This average effect is however pretty low. To get an economic sense of the size of this effect, let us compute the average effect associated with a unit increase in the number of delinquencies:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{new\_credit }\OtherTok{\textless{}{-}}\NormalTok{ credit}
\NormalTok{new\_credit}\SpecialCharTok{$}\NormalTok{delinq\_2yrs }\OtherTok{\textless{}{-}}\NormalTok{ credit}\SpecialCharTok{$}\NormalTok{delinq\_2yrs }\SpecialCharTok{+} \DecValTok{1}
\NormalTok{new\_predict\_eq2  }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(eq2, }\AttributeTok{newdata =}\NormalTok{ new\_credit, }\AttributeTok{type =} \StringTok{"response"}\NormalTok{)}
\FunctionTok{mean}\NormalTok{(new\_predict\_eq2 }\SpecialCharTok{{-}}\NormalTok{ bas\_predict\_eq2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.001582332
\end{verbatim}

We can employ a likelihood ratio test (see Def. \ref{def:LR}) to see if the two variables associated with annual income are jointly statistically significant (in the context of \texttt{eq1}):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{eq1restr }\OtherTok{\textless{}{-}} \FunctionTok{glm}\NormalTok{(Default }\SpecialCharTok{\textasciitilde{}} \FunctionTok{log}\NormalTok{(loan\_amnt) }\SpecialCharTok{+}\NormalTok{ amt2income }\SpecialCharTok{+}\NormalTok{ delinq\_2yrs,}
                \AttributeTok{data=}\NormalTok{credit,}\AttributeTok{family=}\FunctionTok{binomial}\NormalTok{(}\AttributeTok{link=}\StringTok{"probit"}\NormalTok{))}
\NormalTok{LRstat }\OtherTok{\textless{}{-}} \DecValTok{2}\SpecialCharTok{*}\NormalTok{(logL1 }\SpecialCharTok{{-}} \FunctionTok{logLik}\NormalTok{(eq1restr))}
\NormalTok{pvalue }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}} \FunctionTok{c}\NormalTok{(}\FunctionTok{pchisq}\NormalTok{(LRstat,}\AttributeTok{df=}\DecValTok{2}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

The computation gives a p-value of 0.0436.
\end{example}

\begin{example}[Replicating Table 14.2 of Cameron and Trivedi (2005)]
\protect\hypertarget{exm:Fisch142}{}\label{exm:Fisch142}

The following lines of codes replicate Table 14.2 of \citet{Cameron_Trivedi_2005} (see Example \ref{exm:FishingTable}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data.reduced }\OtherTok{\textless{}{-}} \FunctionTok{subset}\NormalTok{(Fishing,mode }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{"charter"}\NormalTok{,}\StringTok{"pier"}\NormalTok{))}
\NormalTok{data.reduced}\SpecialCharTok{$}\NormalTok{lnrelp }\OtherTok{\textless{}{-}} \FunctionTok{log}\NormalTok{(data.reduced}\SpecialCharTok{$}\NormalTok{price.charter}\SpecialCharTok{/}\NormalTok{data.reduced}\SpecialCharTok{$}\NormalTok{price.pier)}
\NormalTok{data.reduced}\SpecialCharTok{$}\NormalTok{y }\OtherTok{\textless{}{-}} \DecValTok{1}\SpecialCharTok{*}\NormalTok{(data.reduced}\SpecialCharTok{$}\NormalTok{mode}\SpecialCharTok{==}\StringTok{"charter"}\NormalTok{)}
\CommentTok{\# check first line of Table 14.1:}
\NormalTok{price.charter.y0 }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(data.reduced}\SpecialCharTok{$}\NormalTok{pcharter[data.reduced}\SpecialCharTok{$}\NormalTok{y}\SpecialCharTok{==}\DecValTok{0}\NormalTok{])}
\NormalTok{price.charter.y1 }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(data.reduced}\SpecialCharTok{$}\NormalTok{pcharter[data.reduced}\SpecialCharTok{$}\NormalTok{y}\SpecialCharTok{==}\DecValTok{1}\NormalTok{])}
\NormalTok{price.charter    }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(data.reduced}\SpecialCharTok{$}\NormalTok{pcharter)}
\CommentTok{\# Run probit regression:}
\NormalTok{reg.probit }\OtherTok{\textless{}{-}} \FunctionTok{glm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ lnrelp,}
                  \AttributeTok{data=}\NormalTok{data.reduced,}
                  \AttributeTok{family=}\FunctionTok{binomial}\NormalTok{(}\AttributeTok{link=}\StringTok{"probit"}\NormalTok{))}
\CommentTok{\# Run Logit regression:}
\NormalTok{reg.logit }\OtherTok{\textless{}{-}} \FunctionTok{glm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ lnrelp,}
                 \AttributeTok{data=}\NormalTok{data.reduced,}
                 \AttributeTok{family=}\FunctionTok{binomial}\NormalTok{(}\AttributeTok{link=}\StringTok{"logit"}\NormalTok{))}
\CommentTok{\# Run OLS regression:}
\NormalTok{reg.OLS }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ lnrelp,}
              \AttributeTok{data=}\NormalTok{data.reduced)}
\CommentTok{\# Replicates Table 14.2 of Cameron and Trivedi:}
\NormalTok{stargazer}\SpecialCharTok{::}\FunctionTok{stargazer}\NormalTok{(reg.logit, reg.probit, reg.OLS,}\AttributeTok{no.space =} \ConstantTok{TRUE}\NormalTok{,}
                     \AttributeTok{type=}\StringTok{"text"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## ================================================================
##                                 Dependent variable:             
##                     --------------------------------------------
##                                          y                      
##                     logistic   probit             OLS           
##                        (1)       (2)              (3)           
## ----------------------------------------------------------------
## lnrelp              -1.823*** -1.056***        -0.243***        
##                      (0.145)   (0.075)          (0.010)         
## Constant            2.053***  1.194***          0.784***        
##                      (0.169)   (0.088)          (0.013)         
## ----------------------------------------------------------------
## Observations           630       630              630           
## R2                                               0.463          
## Adjusted R2                                      0.462          
## Log Likelihood      -206.827  -204.411                          
## Akaike Inf. Crit.    417.654   412.822                          
## Residual Std. Error                         0.330 (df = 628)    
## F Statistic                             542.123*** (df = 1; 628)
## ================================================================
## Note:                                *p<0.1; **p<0.05; ***p<0.01
\end{verbatim}

\end{example}

\hypertarget{predictions-and-roc-curves}{%
\subsection{Predictions and ROC curves}\label{predictions-and-roc-curves}}

How to compute model-implied predicted outcomes? As is the case for \(y_i\), predicted outcomes \(\hat{y}_i\) need to be valued in \(\{0,1\}\). A natural choice consists in considering that \(\hat{y}_i=1\) if \(\mathbb{P}(y_i=1|\mathbf{x}_i;\boldsymbol\theta) > 0.5\), i.e., in taking a cutoff of \(c=0.5\). There exist, though, situations where doing so is not relevant. For instance, we may have some models where all predicted probabilities are small, but some less than others. In this context, a model-implied probability of 10\% (say) could characterize a ``high-risk'' entity. However, using a cutoff of 50\% would not identify this level of riskiness.

The \textbf{receiver operating characteristics (ROC)} curve consitutes a more general approach. The idea is to remain agnostic and to consider all possible values of the cutoff \(c\). It works as follows. For each potential cutoff \(c \in [0,1]\), compute (and plot):

\begin{itemize}
\tightlist
\item
  The fraction of \(y = 1\) values correctly classified (\emph{True Positive Rate}) against
\item
  The fraction of \(y = 0\) values incorrectly specified (\emph{False Positive Rate}).
\end{itemize}

Such a curve mechanically starts at (0,0) ---which corresponds to \(c=1\)--- and terminates at (1,1) --situation when \(c=0\).

In the case of no predictive ability (worst situation), the ROC curve is a straight line between (0,0) and (1,1).

\begin{example}[ROC with the fishing-mode dataset]
\protect\hypertarget{exm:FishingROC}{}\label{exm:FishingROC}

Figure \ref{fig:fishing3} shows the ROC curve associated with the probit model estimated in Example \ref{exm:Fisch142}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(pROC)}
\NormalTok{predict\_model }\OtherTok{\textless{}{-}} \FunctionTok{predict.glm}\NormalTok{(reg.probit,}\AttributeTok{type =} \StringTok{"response"}\NormalTok{)}
\FunctionTok{roc}\NormalTok{(data.reduced}\SpecialCharTok{$}\NormalTok{y, predict\_model, }\AttributeTok{percent=}\NormalTok{T,}
    \AttributeTok{boot.n=}\DecValTok{1000}\NormalTok{, }\AttributeTok{ci.alpha=}\FloatTok{0.9}\NormalTok{, }\AttributeTok{stratified=}\NormalTok{T, }\AttributeTok{plot=}\ConstantTok{TRUE}\NormalTok{, }\AttributeTok{grid=}\ConstantTok{TRUE}\NormalTok{,}
    \AttributeTok{show.thres=}\ConstantTok{TRUE}\NormalTok{, }\AttributeTok{legacy.axes =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{reuse.auc =} \ConstantTok{TRUE}\NormalTok{,}
    \AttributeTok{print.auc =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{print.thres.col =} \StringTok{"blue"}\NormalTok{, }\AttributeTok{ci=}\ConstantTok{TRUE}\NormalTok{,}
    \AttributeTok{ci.type=}\StringTok{"bars"}\NormalTok{, }\AttributeTok{print.thres.cex =} \FloatTok{0.7}\NormalTok{, }\AttributeTok{col =} \StringTok{\textquotesingle{}red\textquotesingle{}}\NormalTok{,}
    \AttributeTok{main =} \FunctionTok{paste}\NormalTok{(}\StringTok{"ROC curve using"}\NormalTok{,}\StringTok{"(N = "}\NormalTok{,}\FunctionTok{nrow}\NormalTok{(data.reduced),}\StringTok{")"}\NormalTok{) )}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\includegraphics[width=0.95\linewidth]{MicroEc_files/figure-latex/fishing3-1} \caption{Application of the ROC methodology on the fishing-mode dataset.}\label{fig:fishing3}
\end{figure}

\end{example}

\hypertarget{multiple-choice-models}{%
\section{Multiple Choice Models}\label{multiple-choice-models}}

We will now consider cases where the number of possible outcomes (or alternatives) is larger than two. Let us denote by \(J\) this number. We have \(y_j \in \{1,\dots,J\}\). This situation arise for instance when the outcome variable reflects:

\begin{itemize}
\tightlist
\item
  Opinions: strongly opposed / opposed / neutral / support (ranked choices),
\item
  Occupational field: lawyer / farmer / engineer / doctor / \ldots,
\item
  Alternative shopping areas,
\item
  Transportation types.
\end{itemize}

In a few cases, the values associated with the choices will themselves be meaningful, for example, number of accidents per day: \(y = 0, 1,2, \dots\) (count data). In most cases, the values are meaningless.

We assume the existence of covariates, gathered in vector \(\mathbf{x}_i\) (\(K \times 1\)), that are suspected to influence for the probabilities of obtaining the different outcomes (\(y_i=j\), \(j \in \{1,\dots,J\}\)).

In what follows, we will assume that the \(y_i\)'s are assumed to be independently distributed, with:
\begin{equation}
y_i = \left\{
\begin{array}{cl}
1 & \mbox{ with probability } g_1(\mathbf{x}_i;\boldsymbol\theta)\\
\vdots \\
J & \mbox{ with probability } g_J(\mathbf{x}_i;\boldsymbol\theta).
\end{array}
\right.\label{eq:generalMultiNom}
\end{equation}

(Of course, for all entities (\(i\)), we must have \(\sum_{j=1}^J g_j(\mathbf{x}_i;\boldsymbol\theta)=1\).) Our objective is to estimate the vector of population parameters \(\boldsymbol\theta\) given functional forms for the \(g_j\)'s.

\hypertarget{ordered-case}{%
\subsection{Ordered case}\label{ordered-case}}

Sometimes, there exists a natural order for the different alternatives. This is typically the case where respondents have to choose a level of agreement to a statement, e.g.: (1) Strongly disagree; (2) Disagree; (3) Neither agree nor disagree; (4) Agree; (5) Strongly agree. Another standard case is that of ratings (from A to F, say).

The ordered probit model consists in extending the binary case, considering the latent-variable view of the latter (see Section \ref{latent}). Formally, the model is as follows:
\begin{equation}
\mathbb{P}(y_i = j | \mathbf{x}_i) = \mathbb{P}(\alpha_{j-1} <y^*_i < \alpha_{j} |\mathbf{x}_i), \label{eq:Pordered}
\end{equation}
where
\[
y_{i}^* = \boldsymbol\theta'\mathbf{x}_i + \varepsilon_i,
\]
with \(\varepsilon_i \sim \,i.i.d.\,\mathcal{N}(0,1)\). The \(\alpha_j\)'s, \(j \in \{1,\dots,J-1\}\), are (new) parameters that have to be estimated, on top of \(\boldsymbol\theta\). Naturally, we have \(\alpha_1<\alpha_2<\dots<\alpha_{J-1}\). Moreover \(\alpha_0\) is \(- \infty\) and \(\alpha_J\) is \(+ \infty\), so that Eq. \eqref{eq:Pordered} is valid for any \(j \in \{1,\dots,J\}\) (including \(1\) and \(J\)).

We have:
\begin{eqnarray*}
g_j(\mathbf{x}_i;\boldsymbol\theta,\boldsymbol\alpha) = \mathbb{P}(y_i = j | \mathbf{x}_i) &=& \mathbb{P}(\alpha_{j-1} <y^*_i < \alpha_{j} |\mathbf{x}_i) \\
&=& \mathbb{P}(\alpha_{j-1} - \boldsymbol\theta'\mathbf{x}_i  <\varepsilon_i < \alpha_{j} - \boldsymbol\theta'\mathbf{x}_i) \\
&=& \Phi(\alpha_{j} - \boldsymbol\theta'\mathbf{x}_i) - \Phi(\alpha_{j-1} - \boldsymbol\theta'\mathbf{x}_i),
\end{eqnarray*}
where \(\Phi\) is the c.d.f. of \(\mathcal{N}(0,1)\).

If, for all \(i\), one of the components of \(\mathbf{x}_i\) is equal to 1 (which is what is done in linear regression to introduce an intercept in the specification), then one of the \(\alpha_j\) (\(j\in\{1,\dots,J-1\}\)) is not identified. One can then arbitrarily set \(\alpha_1=0\). This is what is done in the binary logit/probit cases.

This model can be estimated by maximizing the likelihood function (see Section \ref{secMLE}). This function is given by:
\begin{equation}
\log \mathcal{L}(\boldsymbol\theta,\boldsymbol\alpha;\mathbf{y},\mathbf{X}) = \sum_{i=1}^n  \sum_{j=1}^J \mathbb{I}_{\{y_i=j\}} \log \left(g_j(\mathbf{x}_i;\boldsymbol\theta,\boldsymbol\alpha)\right). \label{eq:multipleLogLik}
\end{equation}

Let us stress that we have two types of parameters to estimate: those included in vector \(\boldsymbol\theta\), and the \(\alpha_j\)'s, gathered in vector \(\boldsymbol\alpha\).

The estimated values of the \(\theta_j\)'s are slightly more complicated to interpret (at least in term of sign) than in the binary case. Indeed, we have:
\[
\mathbb{P}(y_i \le j | \mathbf{x}_i) = \Phi(\alpha_{j} - \boldsymbol\theta'\mathbf{x}_i) \Rightarrow \frac{\partial \mathbb{P}(y_i \le j | \mathbf{x}_i)}{\mathbf{x}_i} =- \underbrace{\Phi'(\alpha_{j} - \boldsymbol\theta'\mathbf{x}_i)}_{>0}\boldsymbol\theta.
\]
Hence the sign of \(\theta_k\) indicates whether \(\mathbb{P}(y_i \le j | \mathbf{x}_i)\) increases or decreases w.r.t. \(x_{i,k}\) (the \(k^{th}\) component of \(\mathbf{x}_i\)). By contrast:
\[
\frac{\partial \mathbb{P}(y_i = j | \mathbf{x}_i)}{\mathbf{x}_i} = \underbrace{\left(-F'(\alpha_{j} + \boldsymbol\theta'\mathbf{x}_i)+F'(\alpha_{j-1} + \boldsymbol\theta'\mathbf{x}_i)\right)}_{A}\boldsymbol\theta.
\]
Therefore the signs of the components of \(\boldsymbol\theta\) are not necessarily those of the marginal effects. (For the sign of \(A\) is a priori unknown.)

\begin{example}[Predicting credit ratings (Lending-club dataset)]
\protect\hypertarget{exm:orderedCredit}{}\label{exm:orderedCredit}

Let us use credit dataset again (see Example \ref{exm:creditProbit}), and let use try and model the ratings attributed by the lending-club:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(AEC)}
\FunctionTok{library}\NormalTok{(MASS)}
\NormalTok{credit}\SpecialCharTok{$}\NormalTok{emp\_length\_low5y   }\OtherTok{\textless{}{-}}\NormalTok{ credit}\SpecialCharTok{$}\NormalTok{emp\_length }\SpecialCharTok{\%in\%}
  \FunctionTok{c}\NormalTok{(}\StringTok{"\textless{} 1 year"}\NormalTok{,}\StringTok{"1 year"}\NormalTok{,}\StringTok{"2 years"}\NormalTok{,}\StringTok{"3 years"}\NormalTok{,}\StringTok{"4 years"}\NormalTok{)}
\NormalTok{credit}\SpecialCharTok{$}\NormalTok{emp\_length\_high10y }\OtherTok{\textless{}{-}}\NormalTok{ credit}\SpecialCharTok{$}\NormalTok{emp\_length}\SpecialCharTok{==}\StringTok{"10+ years"}
\NormalTok{credit}\SpecialCharTok{$}\NormalTok{annual\_inc }\OtherTok{\textless{}{-}}\NormalTok{ credit}\SpecialCharTok{$}\NormalTok{annual\_inc}\SpecialCharTok{/}\DecValTok{1000}
\NormalTok{credit}\SpecialCharTok{$}\NormalTok{loan\_amnt  }\OtherTok{\textless{}{-}}\NormalTok{ credit}\SpecialCharTok{$}\NormalTok{loan\_amnt}\SpecialCharTok{/}\DecValTok{1000}
\NormalTok{credit}\SpecialCharTok{$}\NormalTok{income2loan }\OtherTok{\textless{}{-}}\NormalTok{ credit}\SpecialCharTok{$}\NormalTok{annual\_inc}\SpecialCharTok{/}\NormalTok{credit}\SpecialCharTok{$}\NormalTok{loan\_amnt}
\NormalTok{training }\OtherTok{\textless{}{-}}\NormalTok{ credit[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{20000}\NormalTok{,] }\CommentTok{\# sample is reduced}
\NormalTok{training }\OtherTok{\textless{}{-}} \FunctionTok{subset}\NormalTok{(training,grade}\SpecialCharTok{!=}\FunctionTok{c}\NormalTok{(}\StringTok{"E"}\NormalTok{,}\StringTok{"F"}\NormalTok{,}\StringTok{"G"}\NormalTok{))}
\NormalTok{training }\OtherTok{\textless{}{-}} \FunctionTok{droplevels}\NormalTok{(training)}
\NormalTok{training}\SpecialCharTok{$}\NormalTok{grade.ordered }\OtherTok{\textless{}{-}} \FunctionTok{factor}\NormalTok{(training}\SpecialCharTok{$}\NormalTok{grade,}\AttributeTok{ordered=}\ConstantTok{TRUE}\NormalTok{,}
                                 \AttributeTok{levels =} \FunctionTok{c}\NormalTok{(}\StringTok{"D"}\NormalTok{,}\StringTok{"C"}\NormalTok{,}\StringTok{"B"}\NormalTok{,}\StringTok{"A"}\NormalTok{))}
\NormalTok{model1 }\OtherTok{\textless{}{-}} \FunctionTok{polr}\NormalTok{(grade.ordered }\SpecialCharTok{\textasciitilde{}} \FunctionTok{log}\NormalTok{(loan\_amnt) }\SpecialCharTok{+} \FunctionTok{log}\NormalTok{(income2loan) }\SpecialCharTok{+}\NormalTok{ delinq\_2yrs,}
               \AttributeTok{data=}\NormalTok{training, }\AttributeTok{Hess=}\ConstantTok{TRUE}\NormalTok{, }\AttributeTok{method=}\StringTok{"probit"}\NormalTok{)}
\NormalTok{model2 }\OtherTok{\textless{}{-}} \FunctionTok{polr}\NormalTok{(grade.ordered }\SpecialCharTok{\textasciitilde{}} \FunctionTok{log}\NormalTok{(loan\_amnt) }\SpecialCharTok{+} \FunctionTok{log}\NormalTok{(income2loan) }\SpecialCharTok{+}\NormalTok{ delinq\_2yrs }\SpecialCharTok{+}
\NormalTok{                 emp\_length\_low5y }\SpecialCharTok{+}\NormalTok{ emp\_length\_high10y,}
               \AttributeTok{data=}\NormalTok{training, }\AttributeTok{Hess=}\ConstantTok{TRUE}\NormalTok{, }\AttributeTok{method=}\StringTok{"probit"}\NormalTok{)}
\NormalTok{stargazer}\SpecialCharTok{::}\FunctionTok{stargazer}\NormalTok{(model1,model2,}\AttributeTok{ord.intercepts =} \ConstantTok{TRUE}\NormalTok{,}\AttributeTok{type=}\StringTok{"text"}\NormalTok{,}
                     \AttributeTok{no.space =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## ===============================================
##                        Dependent variable:     
##                    ----------------------------
##                           grade.ordered        
##                         (1)            (2)     
## -----------------------------------------------
## log(loan_amnt)         -0.014        -0.040*   
##                       (0.022)        (0.022)   
## log(income2loan)      0.115***      0.092***   
##                       (0.022)        (0.022)   
## delinq_2yrs          -0.399***      -0.404***  
##                       (0.025)        (0.025)   
## emp_length_low5y                    -0.096***  
##                                      (0.027)   
## emp_length_high10y                   0.088**   
##                                      (0.035)   
## D| C                 -0.937***      -1.073***  
##                       (0.082)        (0.086)   
## C| B                  -0.160**      -0.295***  
##                       (0.082)        (0.085)   
## B| A                  0.696***      0.564***   
##                       (0.082)        (0.086)   
## -----------------------------------------------
## Observations           8,695          8,695    
## ===============================================
## Note:               *p<0.1; **p<0.05; ***p<0.01
\end{verbatim}

Predicted ratings (and probabilties of being given a given rating) can be computed as follows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pred.grade }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(model1,}\AttributeTok{newdata =}\NormalTok{ training)}
\CommentTok{\# pred.grade = predicted grade, defined as the most likely according model}
\NormalTok{pred.proba }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(model1,}\AttributeTok{newdata =}\NormalTok{ training, }\AttributeTok{type=}\StringTok{"probs"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\end{example}

\hypertarget{MNL}{%
\subsection{General multinomial logit model}\label{MNL}}

This section introduces the general multinomial logit model, which is the natural extension of the binary logit model (see Table \ref{tab:foo}). Its general formulation is as follows:
\begin{equation}
g_j(\mathbf{x}_i;\boldsymbol\theta) = \frac{\exp(\theta_j'\mathbf{x}_i)}{\sum_{k=1}^J \exp(\theta_k'\mathbf{x}_i)}.\label{eq:GeneralMNL}
\end{equation}

Note that, by construction, \(g_j(\mathbf{x}_i;\boldsymbol\theta) \in [0,1]\) and \(\sum_{j}g_j(\mathbf{x}_i;\boldsymbol\theta)=1\).

The components of \(\mathbf{x}_i\) (regressors, or covariates) may be \emph{alternative-specific} or \emph{alternative invariant} (see also Section \ref{Avregressors}). We may, e.g., organize \(\mathbf{x}_i\) as follows:
\begin{equation}
\mathbf{x}_i = [\mathbf{u}_{i,1}',\dots,\mathbf{u}_{i,J}',\mathbf{v}_{i}']',\label{eq:xorganiz}
\end{equation}
where the notations are as in Section \ref{Avregressors}, that is:

\begin{itemize}
\tightlist
\item
  \(\mathbf{u}_{i,j}\) (\(j \in \{1,\dots,J\}\)): vector of variables associated with agent \(i\) and alternative \(j\) (alternative-specific regressors). Examples: Travel time per type of transportation (transportation choice), wage per type of work, cost per type of car.
\item
  \(\mathbf{v}_{i}\): vector of variables associated with agent \(i\) but alternative-invariant. Examples: age or gender of agent \(i\),
\end{itemize}

When \(\mathbf{x}_i\) is as in Eq. \eqref{eq:xorganiz}, with obvious notations, \(\theta_j\) is of the form:
\begin{equation}
\theta_j = [{\theta^{(u)}_{1,j}}',\dots,{\theta^{(u)}_{J,j}}',{\theta_j^{(v)}}']',\label{eq:thetaOrganiz}
\end{equation}
and \(\boldsymbol\theta=[\theta_1',\dots,\theta_J']'\).

The literature has considered different specific cases of the general multinomial logit model:\footnote{The labelling ``CL'' and ``MNL'' ---used in the literature--- are relatively \emph{ad hoc} (see 15.4.1 in \citet{Cameron_Trivedi_2005}).}

\begin{itemize}
\tightlist
\item
  \textbf{Conditional logit (CL)} with alternative-varying regressors:
  \begin{equation}
  \theta_j = [\mathbf{0}',\dots,\mathbf{0}',\underbrace{\boldsymbol\beta'}_{\mbox{j$^{th}$ position}},\mathbf{0}',\dots]',\label{eq:thetaOrganizCL}
  \end{equation}
  i.e., we have \(\boldsymbol\beta=\theta^{(u)}_{1,1}=\dots=\theta^{(u)}_{J,J}\) and \(\theta^{(u)}_{i,j}=\mathbf{0}\) for \(i \ne j\).
\item
  \textbf{Multinomial logit (MNL)} with alternative-invariant regressors:
  \begin{equation}
  \theta_j = \left[\mathbf{0}',\dots,\mathbf{0}',{\theta_j^{(v)}}'\right]'.\label{eq:thetaOrganizML}
  \end{equation}
\item
  \textbf{Mixed logit:}
  \begin{equation}
  \theta_j = \left[\mathbf{0}',\dots,\mathbf{0}',\boldsymbol\beta',\mathbf{0}',\dots,\mathbf{0}',{\theta_j^{(v)}}'\right]'.\label{eq:thetaOrganizCL}
  \end{equation}
\end{itemize}

\begin{example}[CL and MNL with the fishing-mode dataset]
\protect\hypertarget{exm:FishingGeneralLogit}{}\label{exm:FishingGeneralLogit}

The following lines replicate Table 15.2 in \citet{Cameron_Trivedi_2005} (see also Examples \ref{exm:FishingTable} and \ref{exm:Fisch142}):

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Specify data organization:}
\FunctionTok{library}\NormalTok{(mlogit)}
\FunctionTok{library}\NormalTok{(stargazer)}
\FunctionTok{data}\NormalTok{(}\StringTok{"Fishing"}\NormalTok{,}\AttributeTok{package=}\StringTok{"mlogit"}\NormalTok{)}
\NormalTok{Fish }\OtherTok{\textless{}{-}} \FunctionTok{mlogit.data}\NormalTok{(Fishing,}
                    \AttributeTok{varying =} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\SpecialCharTok{:}\DecValTok{9}\NormalTok{),}
                    \AttributeTok{choice =} \StringTok{"mode"}\NormalTok{,}
                    \AttributeTok{shape =} \StringTok{"wide"}\NormalTok{)}
\NormalTok{MNL1 }\OtherTok{\textless{}{-}} \FunctionTok{mlogit}\NormalTok{(mode }\SpecialCharTok{\textasciitilde{}}\NormalTok{ price }\SpecialCharTok{+}\NormalTok{ catch, }\AttributeTok{data =}\NormalTok{ Fish)}
\NormalTok{MNL2 }\OtherTok{\textless{}{-}} \FunctionTok{mlogit}\NormalTok{(mode }\SpecialCharTok{\textasciitilde{}}\NormalTok{ price }\SpecialCharTok{+}\NormalTok{ catch }\SpecialCharTok{{-}} \DecValTok{1}\NormalTok{, }\AttributeTok{data =}\NormalTok{ Fish)}
\NormalTok{MNL3 }\OtherTok{\textless{}{-}} \FunctionTok{mlogit}\NormalTok{(mode }\SpecialCharTok{\textasciitilde{}} \DecValTok{0} \SpecialCharTok{|}\NormalTok{ income, }\AttributeTok{data =}\NormalTok{ Fish)}
\NormalTok{MNL4 }\OtherTok{\textless{}{-}} \FunctionTok{mlogit}\NormalTok{(mode }\SpecialCharTok{\textasciitilde{}}\NormalTok{ price }\SpecialCharTok{+}\NormalTok{ catch }\SpecialCharTok{|}\NormalTok{ income, }\AttributeTok{data =}\NormalTok{ Fish)}
\FunctionTok{stargazer}\NormalTok{(MNL1,MNL2,MNL3,MNL4,}\AttributeTok{type=}\StringTok{"text"}\NormalTok{,}\AttributeTok{no.space =} \ConstantTok{TRUE}\NormalTok{,}
          \AttributeTok{omit.stat =} \FunctionTok{c}\NormalTok{(}\StringTok{"lr"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## ===============================================================
##                                 Dependent variable:            
##                     -------------------------------------------
##                                        mode                    
##                        (1)        (2)        (3)        (4)    
## ---------------------------------------------------------------
## (Intercept):boat     0.871***              0.739***   0.527**  
##                      (0.114)               (0.197)    (0.223)  
## (Intercept):charter  1.499***              1.341***   1.694*** 
##                      (0.133)               (0.195)    (0.224)  
## (Intercept):pier     0.307***              0.814***   0.778*** 
##                      (0.115)               (0.229)    (0.220)  
## price               -0.025***  -0.020***             -0.025*** 
##                      (0.002)    (0.001)               (0.002)  
## catch                0.377***   0.953***              0.358*** 
##                      (0.110)    (0.089)               (0.110)  
## income:boat                                0.0001**   0.0001*  
##                                           (0.00004)   (0.0001) 
## income:charter                             -0.00003   -0.00003 
##                                           (0.00004)   (0.0001) 
## income:pier                               -0.0001*** -0.0001** 
##                                            (0.0001)   (0.0001) 
## ---------------------------------------------------------------
## Observations          1,182      1,182      1,182      1,182   
## R2                    0.178      0.014      0.189              
## Log Likelihood      -1,230.784 -1,311.980 -1,477.151 -1,215.138
## ===============================================================
## Note:                               *p<0.1; **p<0.05; ***p<0.01
\end{verbatim}

\end{example}

\textbf{ML estimation}

General multinomial logit models can be estimated by Maximum Likelihood techniques (see Section \ref{secMLE}). Consider the general model described in Eq. \eqref{eq:generalMultiNom}. It can be noted that:
\[
f(y_i|\mathbf{x}_i;\boldsymbol\theta) = \prod_{j=1}^J g_j(\mathbf{x}_i;\boldsymbol\theta)^{\mathbb{I}_{\{y_i=j\}}},
\]
which leads to
\[
\log f(y_i|\mathbf{x}_i;\boldsymbol\theta) = \sum_{j=1}^J \mathbb{I}_{\{y_i=j\}} \log \left(g_j(\mathbf{x}_i;\boldsymbol\theta)\right).
\]
The log-likelihood function is therefore given by:
\begin{equation}
\log \mathcal{L}(\boldsymbol\theta;\mathbf{y},\mathbf{X}) = \sum_{i=1}^n  \sum_{j=1}^J \mathbb{I}_{\{y_i=j\}} \log \left(g_j(\mathbf{x}_i;\boldsymbol\theta)\right).\label{eq:multipleLogLik}
\end{equation}
Numerical methods have to be employed in order to find the maximum-likelihood estimate of \(\boldsymbol\theta\). (Standard packages contain fast algorithms.)

\textbf{Marginal Effects}

Let us consider the computation of marginal effects in the general multinomial logit model (Eq. \eqref{eq:GeneralMNL}). Using the notation \(p_{i,j} \equiv \mathbb{P}(y_i=j|\mathbf{x}_i;\boldsymbol\theta)\), we have:
\begin{eqnarray*}
\frac{\partial p_{i,j}}{\partial x_{i,s}} &=& \frac{\theta_{j,s}\exp(\theta_j'\mathbf{x}_i)\sum_{k=1}^J \exp(\theta_k'\mathbf{x}_i)}{(\sum_{k=1}^J \exp(\theta_k'\mathbf{x}_i))^2} \\
&& - \frac{\exp(\theta_j'\mathbf{x}_i)\sum_{k=1}^J \theta_{k,s} \exp(\theta_k'\mathbf{x}_i)}{(\sum_{k=1}^J \exp(\theta_k'\mathbf{x}_i))^2}\\
&=& \theta_{j,s} p_{i,j} - \sum_{k=1}^J \theta_{k,s} p_{i,j}p_{i,k}\\
&=&  p_{i,j} \times \Big(\theta_{j,s} - \underbrace{\sum_{k=1}^J \theta_{k,s} p_{i,k}}_{=\overline{\boldsymbol{\theta}}^{(i)}_{s}}\Big),
\end{eqnarray*}
where \(\overline{\boldsymbol\theta}^{(i)}_{s}\) does not depend on \(j\). Note that the sign of the marginal effect is not necessarily that of \(\theta_{j,s}\).

\textbf{Random Utility models}

The general multinomial logit model may arise as the natural specification arising in structural contexts where agents compare (random) utilities associated with \(J\) potential outcomes (see Section \ref{latent} for the binary situation).

Let's drop the \(i\) subscript for simplicity and assume that the utility derived form choosing \(j\) is given by \(U_j = V_j + \varepsilon_j\), where \(V_j\) is deterministic (may depend on observed covariates) and \(\varepsilon_j\) is stochastic. We have (with obvious notations):
\begin{eqnarray*}
\mathbb{P}(y=j) &=& \mathbb{P}(U_j>U_k,\,\forall k \ne j)\\
\mathbb{P}(y=j) &=& \mathbb{P}(U_k-U_j<0,\,\forall k \ne j)\\
\mathbb{P}(y=j) &=& \mathbb{P}(\underbrace{\varepsilon_k-\varepsilon_j}_{=:\tilde\varepsilon_{k,j}}<\underbrace{V_j - V_k}_{=:-\tilde{V}_{k,j}},\,\forall k \ne j).
\end{eqnarray*}

The last expression is an \((J-1)\)-variate integral. While it has, in general, no analytical solution, Prop. \ref{prp:Weibull} shows that it is the case when employing Gumbel distributions (see Def. \ref{def:Gumbel}).

\begin{definition}[Gumbel distribution]
\protect\hypertarget{def:Gumbel}{}\label{def:Gumbel}The c.d.f. of the Gumbel distribution (\(\mathcal{W}\)) is:
\[
F(u) = \exp(-\exp(-u)), \qquad f(u)=\exp(-u-\exp(u)).
\]
\end{definition}

Remark: if \(X\sim\mathcal{W}\), then \(\mathbb{E}(X)=0.577\) (Euler constant)\footnote{The Euler constant \(\gamma\) satisfies \(\gamma = \lim_{n\rightarrow \infty} \left(- \ln(n) + \sum_{k=1}^n \frac{1}{k}\right)\).} and \(\mathbb{V}ar(X)=\pi^2/6\).

\begin{figure}
\includegraphics[width=0.95\linewidth]{MicroEc_files/figure-latex/Gumbel-1} \caption{C.d.f. of the Gumbel distribution ($F(x)=\exp(-\exp(-x))$).}\label{fig:Gumbel}
\end{figure}

\begin{proposition}[Weibull]
\protect\hypertarget{prp:Weibull}{}\label{prp:Weibull}In the context of the utility model described above, if \(\varepsilon_j \sim \,i.i.d.\,\mathcal{W}\), then
\[
\mathbb{P}(y=j) = \frac{\exp(V_j)}{\sum_{k=1}^J \exp(V_k)}.
\]
\end{proposition}

\begin{proof}
We have:
\begin{eqnarray*}
\mathbb{P}(y=j) &=& \mathbb{P}(\forall\,k \ne j,\,U_k < U_j) =  \mathbb{P}(\forall\,k \ne j,\,\varepsilon_k < V_j - V_k + \varepsilon_j) \\
&=& \int \prod_{k \ne j} F(V_j - V_k + \varepsilon) f(\varepsilon)d\varepsilon.
\end{eqnarray*}
After computation, it comes that
\[
\prod_{k \ne j} F(V_j - V_k + \varepsilon) f(\varepsilon) = \exp\left[-\varepsilon-\exp(-\varepsilon+\lambda_j)\right],
\]
where \(\lambda_j = \log\left(1 + \frac{\sum_{k \ne j} \exp(V_k)}{\exp(V_j)}\right)\). We then have:
\begin{eqnarray*}
\mathbb{P}(y=j) &=& \int  \exp\left[-\varepsilon-\exp(-\varepsilon+\lambda_j)\right] d\varepsilon\\
&=& \int  \exp\left[-t - \lambda_j-\exp(-t)\right] d\varepsilon = \exp(- \lambda_j),
\end{eqnarray*}
which leads to the result.
\end{proof}

Some remarks on identification (see Def. \ref{def:identif}) are in order.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  We have:
  \begin{eqnarray*}
  \mathbb{P}(y=j) &=& \frac{\exp(V_j)}{\sum_{k=1}^J \exp(V_k)}= \frac{\exp(V^*_j)}{1 + \sum_{k=2}^J \exp(V^*_k)},
  \end{eqnarray*}
  where \(V^*_j = V_j - V_1\). We can therefore always assume that \(V_{1}=0\). In the case where \(V_{i,j} = \theta_j'\mathbf{x}_i = \boldsymbol\beta'\mathbf{u}_{i,j}+{\theta_j^{(v)}}'\mathbf{v}_i\) (see Eqs. \eqref{eq:xorganiz} and \eqref{eq:thetaOrganizCL}), we can for instance assume that:
  \begin{eqnarray*}
  &(A)& \mathbf{u}_{i,1}=0,\\
  &(B)& \theta_1^{(v)} = 0.
  \end{eqnarray*}
  If (A) does not hold, we can replace \(\mathbf{u}_{i,j}\) with \(\mathbf{u}_{i,j}-\mathbf{u}_{i,1}\).
\item
  If \(J=2\) and \(j \in \{0,1\}\) (shift by one unit), we have \(\mathbb{P}(y=1|\mathbf{x})=\dfrac{\exp(\boldsymbol\theta'\mathbf{x})}{1+\exp(\boldsymbol\theta'\mathbf{x})}\), this is the logit model (Table \ref{tab:foo}).
\end{enumerate}

\textbf{Limitations of logit models}

In a Logit model, we have:
\begin{equation}
\mathbb{P}(y=j|y \in \{k,j\}) = \frac{\exp(\theta_j'\mathbf{x})}{\exp(\theta_j'\mathbf{x}) + \exp(\theta_k'\mathbf{x})}.\label{eq:condiProba}
\end{equation}
This conditional probability does not depend on other alternatives (i.e., it does not depend on \(\theta_m\), \(m \ne j,k\)). In particular, if \(\mathbf{x} = [\mathbf{u}_1',\dots,\mathbf{u}_J',\mathbf{v}']'\), then changes in \(\mathbf{u}_m\) (\(m \ne j,\,k\)) have no impact on the object shown in Eq. \eqref{eq:condiProba}.

That is, a Multinomial Logit can be seen as a series of pairwise comparisons that are unaffected by the characteristics of alternatives. Such a model is said to satisfy the \textbf{independence from irrelevant alternatives (IIA)} property. That is, in these models, for any individual, the ratio of probabilities of choosing two alternatives is independent of the availability or attributes of any other alternatives. While this may not sound alarming, there are situations where you would like it not to be the case, this is for instance the case when you want to extrapolate the results of your estimated model to a situation where there is a novel outcome that is highly susbstitutable to one of the previous ones. This can be illustrated with the famous ``red-blue bus'' example:

\begin{example}[Red-blue bus and IIA]
\protect\hypertarget{exm:redbluebus}{}\label{exm:redbluebus}Assume one has a logit model capturing the decision to travel using either a car (\(y=1\)) or a (red) bus (\(y=2\)). Assume you want to augment this model to allow for a third choice (\(y=3\)): travel with a blue bus. If a blue bus (\(y=3\)) is exactly as a red bus, except for the color, then one would expect to have:
\[
\mathbb{P}(y=3|y \in \{2,3\}) = 0.5,
\]
i.e.~\(\theta_2 = \theta_3\).

Assume we had \(V_1=V_2\). We expect to have \(V_2=V_3\) (hence \(p_2=p_3\)). A multinomial logit model would then imply \(p_1=p_2=p_3=0.33\). It would however seem more reasonable to have \(p_1 = p_2 + p_3 = 0.5\) and \(p_2=p_3=0.25\).
\end{example}

\hypertarget{nested-logits}{%
\subsection{Nested logits}\label{nested-logits}}

Nested Logits are natural extensions of logit models when choices feature a nesting structure. This approach is relevant when it makes sense to group some choices into the same \emph{nest}, also called \emph{limbs}. Intuitively, this framework is consistent with the idea according to which, for each agent, there exist unobserved nest-specific variables.

The setup is as follows: we consider \(J\) \emph{limbs}. For each limb \(j\), we have \(K_j\) \emph{branches}. Let us denotes by \(y_1\) the limb choice (i.e., \(y_1 \in \{1,\dots,J\}\)) and by \(y_2\) the branch choice (with \(y_2 \in \{1,\dots,K_j\}\)). The utility associated with the pair of choices \((j,k)\) is given by
\[
U_{j,k} = V_{j,k} + \varepsilon_{j,k}.
\]
We have:
\[
\mathbb{P}[(y_1,y_2) = (j,k)|\mathbf{x}] = \mathbb{P}(U_{j,k}>U_{l,m},\,(l,m) \ne (j,k)|\mathbf{x}).
\]

One usually make the following two assumptions:

\begin{enumerate}
\def\labelenumi{\roman{enumi}.}
\item
  The deterministic part of the utility is given by \(V_{j,k} = \mathbf{u}_j'\boldsymbol\alpha + \mathbf{v}_{j,k}'\boldsymbol\beta_j\), where \(\boldsymbol\alpha\) is common to all nests and the \(\boldsymbol\beta_j\)'s are nest-specific.
\item
  The disturbances \(\boldsymbol\varepsilon\) follow the Generalized Extreme Value (GEV) distribution (see Def. \ref{def:GEVdistri}).
\end{enumerate}

The following figure displays simulations of pairs \((\varepsilon_1,\varepsilon_2)\) drawn from GEV distributions for different values of \(\rho\). The simulation approach is based on \href{http://www.caee.utexas.edu/prof/bhat/ABSTRACTS/Supp_material.pdf}{Bhat}. The code used to produce this chart is provided in Appendix \ref{App:GEV}.

\begin{figure}
\includegraphics[width=0.95\linewidth]{images/Figure_GEV} \caption{GEV simulations.}\label{fig:GEV}
\end{figure}

Under (i) and (ii), we have:
\begin{eqnarray}
\mathbb{P}[(y_1,y_2) = (j,k)|\mathbf{x}] &=& \underbrace{\frac{\exp(\mathbf{u}_j'\boldsymbol\alpha + \rho_j I_j)}{\sum_{m=1}^J \exp(\mathbf{u}_m'\boldsymbol\alpha + \rho_m I_m)}}_{= \mathbb{P}[y_1 = j|\mathbf{x}]} \times \nonumber\\
&& \underbrace{\frac{\exp(\mathbf{v}_{j,k}'\boldsymbol\beta_j/\rho_j)}{\sum_{l=1}^{K_j} \exp(\mathbf{v}_{j,l}'\boldsymbol\beta_j/\rho_j)}}_{= \mathbb{P}[y_2 = k|y_1=j,\mathbf{x}]}, \label{eq:Nested}
\end{eqnarray}
where \(I_j\)'s are called inclusive values (or log sums), given by:
\[
I_j = \log \left( \sum_{l=1}^{K_j} \exp(\mathbf{v}_{j,l}'\boldsymbol\beta_j/\rho_j)\right).
\]

Some remarks are in order:

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  It can be shown that \(\rho_j = \sqrt{1 - \mathbb{C}or(\varepsilon_{j,k},\varepsilon_{j,l})}\), for \(k \ne l\).
\item
  \(\rho_j=1\) implies that \(\varepsilon_{j,k}\) and \(\varepsilon_{j,l}\) are uncorrelated (we are then back to the multinomial logit case).
\item
  When \(J=1\):
  \[
  F([\varepsilon_1,\dots,\varepsilon_K]',\rho) = \exp\left(-\left(\sum_{k=1}^{K} \exp(-\varepsilon_k/\rho)\right)^{\rho}\right).
  \]
\item
  We have:
  \begin{eqnarray*}
  I_j = \mathbb{E}(\max_k(U_{j,k})) &=& \mathbb{E}(\max_k(V_{j,k} + \varepsilon_{j,k})),
  \end{eqnarray*}
  The inclusive values can therefore be seen as measures of the relative attractiveness of a nest.
\end{enumerate}

This approach allows for some level of correlation across the \(\varepsilon_{j,k}\) (for a given \(j\)). This can be interpreted as the existence of an (unobserved) \emph{common error component} for the alternatives of a same nest. This component contributes to making the alternatives of a given nest more similar. In other words, this approach can accommodate a higher sensitivity (cross-elasticity) between the alternatives of a given nest.

Note that if the common component is reduced to zero (i.e.~\(\rho_i=1\)), the model boils down to the multinomial logit model with no covariance of error terms among the alternatives.

Contrary to the general multinmial model, nested logits can solve the Red-Blue problem described in Section \ref{MNL} (see Example \ref{exm:redbluebus}). Assume you have estimated a model specifying \(U_{1} = V_{1} + \varepsilon_{1}\) (car choice) and \(U_{2} = V_{2} + \varepsilon_{2}\) (red bus choice). You can then assume that the blue-bus utility is of the form \(U_{3} = V_{2} + \varepsilon_{3}\) where \(\varepsilon_{3}\) is perfectly correlated to \(\varepsilon_{2}\). This is done by redefining the set of choices as follows:
\begin{eqnarray*}
j=1 &\Leftrightarrow& (j'=1,k=1) \\
j=2 &\Leftrightarrow& (j'=2,k=1) \\
j=3 &\Leftrightarrow& (j'=2,k=2),
\end{eqnarray*}
and by setting \(\rho_2 \rightarrow 0\).

IIA holds within a nest, but not when considering alternatives in different nests. Indeed, using Eq. \eqref{eq:Nested}:
\[
\frac{\mathbb{P}[y_1=j,y_2=k_A|\mathbf{x}] }{\mathbb{P}[y_1=j,y_2=k_B|\mathbf{x}]} = \frac{\exp(\mathbf{v}_{j,k_A}'\boldsymbol\beta_j/\rho_j)}{\exp(\mathbf{v}_{j,k_B}'\boldsymbol\beta_j/\rho_j)},
\]
i.e.~we have IIA in nest \(j\).

By contrast:
\begin{eqnarray*}
\frac{\mathbb{P}[y_1=j_A,y_2=k_A|\mathbf{x}] }{\mathbb{P}[y_1=j_B,y_2=k_B|\mathbf{x}]} &=& \frac{\exp(\mathbf{u}_{j_A}'\boldsymbol\alpha + \rho_{j_A} I_{j_A})\exp(\mathbf{v}_{{j_A},{k_A}}'\boldsymbol\beta_{j_A}/\rho_{j_A})}{\exp(\mathbf{u}_{j_B}'\boldsymbol\alpha + \rho_{j_B} I_{j_B})\exp(\mathbf{v}_{{j_B},{k_B}}'\boldsymbol\beta_{j_B}/\rho_{j_B})}\times\\
&& \frac{\sum_{l=1}^{K_{j_B}} \exp(\mathbf{v}_{{j_B},l}'\boldsymbol\beta_{j_B}/\rho_{j_B})}{\sum_{l=1}^{K_{j_A}} \exp(\mathbf{v}_{{j_A},l}'\boldsymbol\beta_{J_A}/\rho_{j_A})},
\end{eqnarray*}
which depends on the expected utilities of all alternatives in nest \(j_A\) and \(j_B\). So the IIA does not hold.

\begin{example}[Travel-mode dataset]
\protect\hypertarget{exm:nestedTravel}{}\label{exm:nestedTravel}

Let us illustrate nested logits on the travel-mode dataset used, e.g., by \citet{Hensher_Greene_2002} (see also \citet{Heiss_2002}).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(mlogit)}
\FunctionTok{library}\NormalTok{(stargazer)}
\FunctionTok{data}\NormalTok{(}\StringTok{"TravelMode"}\NormalTok{, }\AttributeTok{package =} \StringTok{"AER"}\NormalTok{)}
\NormalTok{Prepared.TravelMode }\OtherTok{\textless{}{-}} \FunctionTok{mlogit.data}\NormalTok{(TravelMode,}\AttributeTok{chid.var =} \StringTok{"individual"}\NormalTok{,}
                                   \AttributeTok{alt.var =} \StringTok{"mode"}\NormalTok{,}\AttributeTok{choice =} \StringTok{"choice"}\NormalTok{,}
                                   \AttributeTok{shape =} \StringTok{"long"}\NormalTok{)}
\CommentTok{\# Fit a multinomial model:}
\NormalTok{hl }\OtherTok{\textless{}{-}} \FunctionTok{mlogit}\NormalTok{(choice }\SpecialCharTok{\textasciitilde{}}\NormalTok{ wait }\SpecialCharTok{+}\NormalTok{ travel }\SpecialCharTok{+}\NormalTok{ vcost, Prepared.TravelMode,}
             \AttributeTok{method =} \StringTok{"bfgs"}\NormalTok{, }\AttributeTok{heterosc =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{tol =} \DecValTok{10}\NormalTok{)}
\DocumentationTok{\#\# Fit a nested logit model:}
\NormalTok{TravelMode}\SpecialCharTok{$}\NormalTok{avincome }\OtherTok{\textless{}{-}} \FunctionTok{with}\NormalTok{(TravelMode, income }\SpecialCharTok{*}\NormalTok{ (mode }\SpecialCharTok{==} \StringTok{"air"}\NormalTok{))}
\NormalTok{TravelMode}\SpecialCharTok{$}\NormalTok{time }\OtherTok{\textless{}{-}} \FunctionTok{with}\NormalTok{(TravelMode, travel }\SpecialCharTok{+}\NormalTok{ wait)}\SpecialCharTok{/}\DecValTok{60}
\NormalTok{TravelMode}\SpecialCharTok{$}\NormalTok{timeair }\OtherTok{\textless{}{-}} \FunctionTok{with}\NormalTok{(TravelMode, time }\SpecialCharTok{*} \FunctionTok{I}\NormalTok{(mode }\SpecialCharTok{==} \StringTok{"air"}\NormalTok{))}
\NormalTok{TravelMode}\SpecialCharTok{$}\NormalTok{income }\OtherTok{\textless{}{-}} \FunctionTok{with}\NormalTok{(TravelMode, income }\SpecialCharTok{/} \DecValTok{10}\NormalTok{)}
\CommentTok{\# Hensher and Greene (2002), table 1 p.8{-}9 model 5}
\NormalTok{TravelMode}\SpecialCharTok{$}\NormalTok{incomeother }\OtherTok{\textless{}{-}} \FunctionTok{with}\NormalTok{(TravelMode,}
                               \FunctionTok{ifelse}\NormalTok{(mode }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}air\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}car\textquotesingle{}}\NormalTok{), income, }\DecValTok{0}\NormalTok{))}
\NormalTok{nl1 }\OtherTok{\textless{}{-}} \FunctionTok{mlogit}\NormalTok{(choice }\SpecialCharTok{\textasciitilde{}}\NormalTok{ gcost }\SpecialCharTok{+}\NormalTok{ wait }\SpecialCharTok{+}\NormalTok{ incomeother, TravelMode,}
              \AttributeTok{shape=}\StringTok{\textquotesingle{}long\textquotesingle{}}\NormalTok{, }\CommentTok{\# Indicates how the dataset is organized}
              \AttributeTok{alt.var=}\StringTok{\textquotesingle{}mode\textquotesingle{}}\NormalTok{, }\CommentTok{\# variable that defines the alternative choices.}
              \AttributeTok{nests=}\FunctionTok{list}\NormalTok{(}\AttributeTok{public=}\FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}train\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}bus\textquotesingle{}}\NormalTok{),}
                         \AttributeTok{car=}\StringTok{\textquotesingle{}car\textquotesingle{}}\NormalTok{,}\AttributeTok{air=}\StringTok{\textquotesingle{}air\textquotesingle{}}\NormalTok{), }\CommentTok{\# defines the "limbs".}
              \AttributeTok{un.nest.el =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{nl2 }\OtherTok{\textless{}{-}} \FunctionTok{mlogit}\NormalTok{(choice }\SpecialCharTok{\textasciitilde{}}\NormalTok{ gcost }\SpecialCharTok{+}\NormalTok{ wait }\SpecialCharTok{+}\NormalTok{ time, TravelMode,}
              \AttributeTok{shape=}\StringTok{\textquotesingle{}long\textquotesingle{}}\NormalTok{, }\CommentTok{\# Inidcates how the dataset is organized}
              \AttributeTok{alt.var=}\StringTok{\textquotesingle{}mode\textquotesingle{}}\NormalTok{, }\CommentTok{\# variable that defines the alternative choices.}
              \AttributeTok{nests=}\FunctionTok{list}\NormalTok{(}\AttributeTok{public=}\FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}train\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}bus\textquotesingle{}}\NormalTok{),}
                         \AttributeTok{car=}\StringTok{\textquotesingle{}car\textquotesingle{}}\NormalTok{,}\AttributeTok{air=}\StringTok{\textquotesingle{}air\textquotesingle{}}\NormalTok{), }\CommentTok{\# defines the "limbs".}
              \AttributeTok{un.nest.el =} \ConstantTok{TRUE}\NormalTok{)}
\FunctionTok{stargazer}\NormalTok{(nl1,nl2,}\AttributeTok{type=}\StringTok{"text"}\NormalTok{,}\AttributeTok{no.space =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## ==============================================
##                       Dependent variable:     
##                   ----------------------------
##                              choice           
##                        (1)            (2)     
## ----------------------------------------------
## (Intercept):train     -0.211        -0.284    
##                      (0.562)        (0.551)   
## (Intercept):bus       -0.824        -0.712    
##                      (0.708)        (0.690)   
## (Intercept):car     -5.237***      -3.845***  
##                      (0.785)        (0.844)   
## gcost               -0.013***       -0.004    
##                      (0.004)        (0.006)   
## wait                -0.088***      -0.089***  
##                      (0.011)        (0.011)   
## incomeother          0.430***                 
##                      (0.113)                  
## time                               -0.202***  
##                                     (0.060)   
## iv                   0.835***      0.877***   
##                      (0.192)        (0.198)   
## ----------------------------------------------
## Observations           210            210     
## R2                    0.328          0.313    
## Log Likelihood       -190.779      -194.841   
## LR Test (df = 7)    185.959***    177.836***  
## ==============================================
## Note:              *p<0.1; **p<0.05; ***p<0.01
\end{verbatim}

\end{example}

\hypertarget{tobit}{%
\section{Tobit models}\label{tobit}}

In some situations, the dependent variable is incompletely observed, which may result in a non-representative sample. Typically, in some cases, observations of the dependent variable can have a lower and/or an upper limit, while the ``true'', underlying, dependent variable has not. In this case, OLS regression may lead to inconsistent parameter estimates.

Tobit models have been designed to address some of these situations. This approach has been named after James Tobin, who developed this model in the late 50s (see \citet{Tobin_1956}).

Figure \ref{fig:tobit1} illustrates the situation. The dots (white and black) represent the ``true'' observations. Now, assume that only the black are observed. If ones uses these observations in an OLS regression to estimate the relatonship between \(x\) and \(y\), then one gets the red line. It is clear that the sensitivity of \(y\) to \(x\) is then underestimated. The blue line is the line one would obtain if white dots were also observed; the grey line represents the model used to genrate the data (\(y_i=x_i+\varepsilon_i\)).

\begin{figure}
\includegraphics{MicroEc_files/figure-latex/tobit1-1} \caption{Bias in the case of sample selection. The grey line represents the population regression line. The model is $y_i = x_i + \varepsilon_i$, with $\varepsilon_{i,t} \sim \mathcal{N}(0,1)$. The red line is the OLS regression line based on black dots only.}\label{fig:tobit1}
\end{figure}

Assume that the (partially) observed dependent variable follows:
\[
y^* = \boldsymbol\beta'\mathbf{x} + \varepsilon,
\]
with \(\varepsilon\) is drawn from a distribution characterized by a p.d.f. denoted by \(f_{\boldsymbol\gamma}^*\) and a c.d.f. denoted by \(F_{\boldsymbol\gamma}^*\); these functions depend on a vector of parameters \(\boldsymbol{\gamma}\).

The observed dependent variable is:
\begin{eqnarray*}
\mbox{Censored case:}&&y = \left\{
\begin{array}{ccc}
y^* &if& y^*>L \\
L &if& y^*\le L,
\end{array}
\right.\\
\mbox{Truncated case:}&&y = \left\{
\begin{array}{ccc}
y^* &if& y^*>L \\
- &if& y^*\le L,
\end{array}
\right.
\end{eqnarray*}
where ``\(-\)'' stands for missing observations.

This formulation is easily extended to censoring from above (\(L \rightarrow U\)), or censoring from both below and above.

The model parameters are gathered in vector \(\theta = [\boldsymbol\beta',\boldsymbol\gamma']'\). Let us write the conditional p.d.f. of the observed variable:
\begin{eqnarray*}
\mbox{Censored case:}&& f(y|\mathbf{x};\theta) = \left\{
\begin{array}{ccc}
f_{\boldsymbol\gamma}^*(y -  \boldsymbol\beta'\mathbf{x}) &if& y>L \\
F_{\boldsymbol\gamma}^*(L-  \boldsymbol\beta'\mathbf{x}) &if& y = L,
\end{array}
\right.\\
\mbox{Truncated case:}&&  f(y|\mathbf{x};\theta) =
\dfrac{f_{\boldsymbol\gamma}^*(y -  \boldsymbol\beta'\mathbf{x})}{1 - F_{\boldsymbol\gamma}^*(L-  \boldsymbol\beta'\mathbf{x})} \quad \mbox{with} \quad y>L.
\end{eqnarray*}

The (conditional) log-likelihood function is then given by:
\[
\log \mathcal{L}(\theta;\mathbf{y},\mathbf{X}) = \sum_{i=1}^n \log f(y_i|\mathbf{x}_i;\theta).
\]
In the censored case, we have:
\begin{eqnarray*}
\log \mathcal{L}(\theta;\mathbf{y},\mathbf{X}) &=& \sum_{i=1}^n \left\{
\mathbb{I}_{\{y_i=L\}}\log\left[F_{\boldsymbol\gamma}^*(L-  \boldsymbol\beta'\mathbf{x}_i)\right] + \right.\\
&& \left. \mathbb{I}_{\{y_i>0\}} \log \left[f_{\boldsymbol\gamma}^*(y_i -  \boldsymbol\beta'\mathbf{x}_i)\right]\right\}.
\end{eqnarray*}

The Tobit, or censored/truncated normal regression model, corresponds to the case described above, but with Gaussian errors \(\varepsilon\). Specifically:
\[
y^* = \boldsymbol\beta'\mathbf{x} + \varepsilon,
\]
with \(\varepsilon \sim \,i.i.d.\,\mathcal{N}(0,\sigma^2)\) (\(\Rightarrow\) \(\boldsymbol\gamma = \sigma^2\)).

Without loss of generality, we can assume that \(L=0\). (One can shift observed data if necessary.)

\begin{itemize}
\item
  The censored density (with \(L=0\)) is given by:
  \[
  f(y) = \left[
  \frac{1}{\sqrt{2 \pi \sigma^2}}\exp\left(-\frac{1}{2 \sigma^2}(y - \boldsymbol\beta'\mathbf{x})^2\right)
  \right]^{\mathbb{I}_{\{y>0\}}}
  \left[
  1 - \Phi\left(\frac{\boldsymbol\beta'\mathbf{x}}{\sigma}\right)
  \right]^{\mathbb{I}_{\{y=0\}}}.
  \]
\item
  The truncated density (with \(L=0\)) is given by:
  \[
  f(y) = \frac{1}{\Phi(\boldsymbol\beta'\mathbf{x})}
  \frac{1}{\sqrt{2 \pi \sigma^2}}\exp\left(-\frac{1}{2 \sigma^2}(y - \boldsymbol\beta'\mathbf{x})^2\right).
  \]
\end{itemize}

Results usually heavily rely on distributional assumptions (more than in uncensored/untruncated case). The framework is easy to extend to an heteroskedastic case, for instance by setting \(\sigma_i^2=\exp(\alpha'\mathbf{x}_i)\). Such a situation is illustrated by Figure \ref{fig:tobit2}.

\begin{figure}
\includegraphics{MicroEc_files/figure-latex/tobit2-1} \caption{Censored dataset with heteroskedasticitiy. The model is $y_i = x_i + \varepsilon_i$, with $\varepsilon_{i,t} \sim \mathcal{N}(0,\sigma_i^2)$ where $\sigma_i = \exp(-1 + x_i)$.}\label{fig:tobit2}
\end{figure}

Let us consider the conditional means of \(y\) in the general case, i.e., for any \(\varepsilon\) distribution. Assume \(\mathbf{x}\) is observed, such that expectations are conditional on \(\mathbf{x}\).

\begin{itemize}
\item
  For data that are left-truncated at 0, we have:
  \begin{eqnarray*}
  \mathbb{E}(y) &=& \mathbb{E}(y^*|y^*>0) = \underbrace{\boldsymbol\beta'\mathbf{x}}_{=\mathbb{E}(y^*)} + \underbrace{\mathbb{E}(\varepsilon|\varepsilon>-\boldsymbol\beta'\mathbf{x})}_{>0} > \mathbb{E}(y^*).
  \end{eqnarray*}
\item
  Consider data that are left-censored at 0. By Bayes, we have:
  \[
  f_{y^*|y^*>0}(u) = \frac{f_{y^*}(u)}{\mathbb{P}(y^*>0)}\mathbb{I}_{\{u>0\}}.
  \]
  Therefore:
  \begin{eqnarray*}
  \mathbb{E}(y^*|y^*>0) &=& \frac{1}{\mathbb{P}(y^*>0)} \int_{-\infty}^\infty u\, f_{y^*}(u)\mathbb{I}_{\{u>0\}} du \\
  &=&  \frac{1}{\mathbb{P}(y^*>0)} \mathbb{E}(\underbrace{y^*\mathbb{I}_{\{y^*>0\}}}_{=y}),
  \end{eqnarray*}
  and, further:
  \begin{eqnarray*}
  \mathbb{E}(y) &=&  \mathbb{P}(y^*>0)\mathbb{E}(y^*|y^*>0)\\
  &>&  \mathbb{E}(y^*) =  \mathbb{P}(y^*>0)\mathbb{E}(y^*|y^*>0) +  \mathbb{P}(y^*<0)\underbrace{\mathbb{E}(y^*|y^*<0)}_{<0}.
  \end{eqnarray*}
\end{itemize}

Now, let us come back to the Tobit (i.e., Gaussian case) case.

\begin{itemize}
\item
  For data that are left-truncated at 0:
  \begin{eqnarray}
  \mathbb{E}(y) &=& \boldsymbol\beta'\mathbf{x} + \mathbb{E}(\varepsilon|\varepsilon>-\boldsymbol\beta'\mathbf{x}) \nonumber\\
  &=&  \boldsymbol\beta'\mathbf{x} + \sigma \underbrace{\frac{\phi(\boldsymbol\beta'\mathbf{x}/\sigma)}{\Phi(\boldsymbol\beta'\mathbf{x}/\sigma)}}_{=: \lambda(\boldsymbol\beta'\mathbf{x}/\sigma)} = \sigma \left( \frac{\boldsymbol\beta'\mathbf{x}}{\sigma} + \lambda\left(\frac{\boldsymbol\beta'\mathbf{x}}{\sigma}\right)\right). \label{eq:Econdtruncated}
  \end{eqnarray}
  where the penultimate line is obtained by using Eq. \eqref{eq:Etrunc}.
\item
  For data that are left-censored at 0:
  \begin{eqnarray*}
  \mathbb{E}(y) &=&  \mathbb{P}(y^*>0)\mathbb{E}(y^*|y^*>0)\\
  &=&  \Phi\left( \frac{\boldsymbol\beta'\mathbf{x}}{\sigma}\right) \sigma \left(
  \frac{\boldsymbol\beta'\mathbf{x}}{\sigma} +   \lambda\left(\frac{\boldsymbol\beta'\mathbf{x}}{\sigma}\right)
  \right).
  \end{eqnarray*}
\end{itemize}

\begin{figure}
\includegraphics{MicroEc_files/figure-latex/tobit3-1} \caption{Conditional means of $y$ in Tobit models. The model is $y_i = x_i + \varepsilon_i$, with $\varepsilon_i \sim \mathcal{N}(0,1)$.}\label{fig:tobit3}
\end{figure}

\textbf{Heckit regression}

The previous formula (Eq. \eqref{eq:Econdtruncated}) can in particular be used in an alternative estimation approach, namely the Heckman two-step estimation. This approach is based on two steps:\footnote{See 16.10.2 of \citet{Cameron_Trivedi_2005} for the derivation of asymptotic standard errors of \(\boldsymbol\beta\).}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Using the complete sample, fit a Probit model of \(\mathbb{I}_{\{y_i>0\}}\) on \(\mathbf{x}\). This provides a consistent estimate of \(\frac{\boldsymbol\beta}{\sigma}\), and therefore of \(\lambda(\boldsymbol\beta'\mathbf{x}/\sigma)\). (Indeed, if \(z_i \equiv \mathbb{I}_{\{y_i>0\}}\), then \(\mathbb{P}(z_i=1|\mathbf{x}_i;\boldsymbol\beta/\sigma)=\Phi(\boldsymbol\beta'\mathbf{x}_i/\sigma)\).)
\item
  Using the truncated sample only: run an OLS regression of \(\mathbf{y}\) on \(\left\{\mathbf{x},\lambda(\boldsymbol\beta'\mathbf{x}/\sigma)\right\}\) (having Eq. \eqref{eq:Econdtruncated} in mind). This provides a consistent estimate of \((\boldsymbol\beta,\sigma)\).
\end{enumerate}

The underlying specification is of the form:
\[
\mbox{Conditional mean} + \mbox{disturbance}.
\]
where ``Conditional mean'' comes from Eq. \eqref{eq:Econdtruncated} and ``disturbance'' is an error with zero conditional mean.

This approach is also applied to the case of \textbf{sample selection models} (Section \ref{SSM}).

\begin{example}[Wage prediction]
\protect\hypertarget{exm:WageMroz1}{}\label{exm:WageMroz1}

The present example is based on the dataset used in \citet{Mroz_1987} (whicht is part of the \texttt{sampleSelection} package).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(sampleSelection)}
\FunctionTok{library}\NormalTok{(AER)}
\FunctionTok{data}\NormalTok{(}\StringTok{"Mroz87"}\NormalTok{)}
\NormalTok{Mroz87}\SpecialCharTok{$}\NormalTok{lfp.yesno }\OtherTok{\textless{}{-}} \ConstantTok{NaN}
\NormalTok{Mroz87}\SpecialCharTok{$}\NormalTok{lfp.yesno[Mroz87}\SpecialCharTok{$}\NormalTok{lfp}\SpecialCharTok{==}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \StringTok{"yes"}
\NormalTok{Mroz87}\SpecialCharTok{$}\NormalTok{lfp.yesno[Mroz87}\SpecialCharTok{$}\NormalTok{lfp}\SpecialCharTok{==}\DecValTok{0}\NormalTok{] }\OtherTok{\textless{}{-}} \StringTok{"no"}
\NormalTok{Mroz87}\SpecialCharTok{$}\NormalTok{lfp.yesno }\OtherTok{\textless{}{-}} \FunctionTok{as.factor}\NormalTok{(Mroz87}\SpecialCharTok{$}\NormalTok{lfp.yesno)}
\NormalTok{ols }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(wage }\SpecialCharTok{\textasciitilde{}}\NormalTok{ educ }\SpecialCharTok{+}\NormalTok{ exper }\SpecialCharTok{+} \FunctionTok{I}\NormalTok{( exper}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{ ) }\SpecialCharTok{+}\NormalTok{ city,}\AttributeTok{data=}\FunctionTok{subset}\NormalTok{(Mroz87,lfp}\SpecialCharTok{==}\DecValTok{1}\NormalTok{))}
\NormalTok{tobit }\OtherTok{\textless{}{-}} \FunctionTok{tobit}\NormalTok{(wage }\SpecialCharTok{\textasciitilde{}}\NormalTok{ educ }\SpecialCharTok{+}\NormalTok{ exper }\SpecialCharTok{+} \FunctionTok{I}\NormalTok{( exper}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{ ) }\SpecialCharTok{+}\NormalTok{ city,}
               \AttributeTok{left =} \DecValTok{0}\NormalTok{, }\AttributeTok{right =} \ConstantTok{Inf}\NormalTok{,}
               \AttributeTok{data=}\NormalTok{Mroz87)}
\NormalTok{Heckit }\OtherTok{\textless{}{-}} \FunctionTok{heckit}\NormalTok{(lfp }\SpecialCharTok{\textasciitilde{}}\NormalTok{ educ }\SpecialCharTok{+}\NormalTok{ exper }\SpecialCharTok{+} \FunctionTok{I}\NormalTok{( exper}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{ ) }\SpecialCharTok{+}\NormalTok{ city, }\CommentTok{\# selection equation}
\NormalTok{                 wage }\SpecialCharTok{\textasciitilde{}}\NormalTok{ educ }\SpecialCharTok{+}\NormalTok{ exper }\SpecialCharTok{+} \FunctionTok{I}\NormalTok{( exper}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{ ) }\SpecialCharTok{+}\NormalTok{ city, }\CommentTok{\# outcome equation}
                 \AttributeTok{data=}\NormalTok{Mroz87 )}

\FunctionTok{stargazer}\NormalTok{(ols,Heckit,tobit,}\AttributeTok{no.space =} \ConstantTok{TRUE}\NormalTok{,}\AttributeTok{type=}\StringTok{"text"}\NormalTok{,}\AttributeTok{omit.stat =} \StringTok{"f"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## ======================================================================
##                                    Dependent variable:                
##                     --------------------------------------------------
##                                            wage                       
##                           OLS           Heckman           Tobit       
##                                        selection                      
##                           (1)             (2)              (3)        
## ----------------------------------------------------------------------
## educ                    0.481***       0.759***         0.642***      
##                         (0.067)         (0.270)          (0.081)      
## exper                    0.032           0.430          0.461***      
##                         (0.062)         (0.369)          (0.068)      
## I(exper2)               -0.0003         -0.008          -0.009***     
##                         (0.002)         (0.008)          (0.002)      
## city                     0.449           0.113           -0.087       
##                         (0.318)         (0.522)          (0.378)      
## Constant               -2.561***        -12.251        -10.395***     
##                         (0.929)         (8.853)          (1.095)      
## ----------------------------------------------------------------------
## Observations              428             753              753        
## R2                       0.125           0.128                        
## Adjusted R2              0.117           0.117                        
## Log Likelihood                                         -1,462.700     
## rho                                      1.063                        
## Inverse Mills Ratio                  5.165 (4.594)                    
## Residual Std. Error 3.111 (df = 423)                                  
## Wald Test                                          153.892*** (df = 4)
## ======================================================================
## Note:                                      *p<0.1; **p<0.05; ***p<0.01
\end{verbatim}

Figure \ref{fig:tobit5} shows that, low wages, the OLS model tends to over-predict wages. The slope between observed and Tobit-predicted wages is closer to one (the adjustment line is closer to the 45-degree line.)

\begin{figure}
\includegraphics{MicroEc_files/figure-latex/tobit5-1} \caption{Predicted versus observed wages.}\label{fig:tobit5}
\end{figure}

\end{example}

\textbf{Two-part model}

In the standard Tobit framework, the model determining censored ---or truncated--- data \emph{censoring mechanism} is the same as the one determining non-censored ---or observed--- data \emph{outcome mechanism}. A two-part model adds flexibility by permitting the zeros and non-zeros to be generated by different densities. The second model characterizes the outcome \emph{conditional on} the outcome being observed.

In a seminal paper, \citet{Duan_et_al_1983} employ this methodology to account for individual annual hospital expenses. The two models are then as follows:

\begin{itemize}
\tightlist
\item
  \(1^{st}\) model: \(\mathbb{P}(hosp=1|\mathbf{x}) = \Phi(\mathbf{x}_1'\boldsymbol\beta_1)\),
\item
  \(2^{nd}\) model: \(Expense = \exp(\mathbf{x}_2'\boldsymbol\beta_2 + \eta)\), with \(\eta \sim\,i.i.d.\, \mathcal{N}(0,\sigma_2^2)\).
\end{itemize}

Specifically:
\[
\mathbb{E}(Expense|\mathbf{x}_1,\mathbf{x}_2) = \Phi(\mathbf{x}_1'\boldsymbol\beta_1)\exp\left(\mathbf{x}_2'\boldsymbol\beta_2+ \frac{\sigma_2^2}{2}\right).
\]

In sample-selection models, studied in the next section, one specifies the joint distribution for the censoring and outcome mechanisms (while the two parts are independent here).

\hypertarget{SSM}{%
\section{Sample Selection Models}\label{SSM}}

The situation tackled by sample-selection models is the following. The dependent variable of interest, denoted by \(y_2\), depends on observed variables \(\mathbf{x}_2\). Observing \(y_2\), or not, depends on the value of a latent variable (\(y_1^*\)) that is correlated to observed variables \(\mathbf{x}_1\). The difference w.r.t. the two-part model skethed above is that, even conditionally on \((\mathbf{x}_1,\mathbf{x}_2)\), \(y_1^*\) and \(y_2\) may be correlated.

As in the Tobit case, even in the simplest case of population conditional mean linear in regressors (i.e.~\(y_2 = \mathbf{x}_2'\boldsymbol\beta_2 + \varepsilon_2\)), OLS regression leads to inconsistent parameter estimates because the sample is not representative of the population.

There are two latent variables: \(y_1^*\) and \(y_2^*\). We observe \(y_1\) and, if the considered entity ``participates'', we also observe \(y_2\). More specifically:
\begin{eqnarray*}
y_1 &=& \left\{
\begin{array}{ccc}
1 &\mbox{if}& y_1^* > 0 \\
0 &\mbox{if}& y_1^* \le 0
\end{array}
\right. \quad \mbox{(participation equation)}\\
y_2 &=& \left\{
\begin{array}{ccc}
y_2^* &\mbox{if}& y_1 = 1 \\
- &\mbox{if}& y_1 = 0
\end{array}
\right. \quad \mbox{(outcome equation).}
\end{eqnarray*}

Moreover:
\begin{eqnarray*}
y_1^* &=& \mathbf{x}_1'\boldsymbol\beta_1 + \varepsilon_1 \\
y_2^* &=& \mathbf{x}_2'\boldsymbol\beta_2 + \varepsilon_2.
\end{eqnarray*}

Note that the Tobit model (Section \ref{tobit}) is the special case where \(y_1^*=y_2^*\).

Usually:
\[
\left[\begin{array}{c}\varepsilon_1\\\varepsilon_2\end{array}\right] \sim \mathcal{N}\left(\mathbf{0},
\left[
\begin{array}{cc}
1 & \rho  \sigma_2 \\
\rho  \sigma_2 & \sigma_2^2
\end{array}
\right]
\right).
\]
Let us derive the likelihood associated with this model. We have:
\begin{eqnarray}
f(\underbrace{0}_{=y_1},\underbrace{-}_{=y_2}|\mathbf{x};\theta) &=& \mathbb{P}(y_1^*\le 0) = \Phi(-\mathbf{x}_1'\boldsymbol\beta_1) \label{eq:probaPP1}\\
f(1,y_2|\mathbf{x};\theta) &=& f(y_2^*|\mathbf{x};\theta) \mathbb{P}(y_1^*>0|y_2^*,\mathbf{x};\theta) \nonumber \\
&=& \frac{1}{\sigma}\phi\left(\frac{y_2 - \mathbf{x}_2'\boldsymbol\beta_2}{\sigma}\right)  \mathbb{P}(y_1^*>0|y_2,\mathbf{x};\theta).\label{eq:probaPP2}
\end{eqnarray}

Let us compute \(\mathbb{P}(y_1^*>0|y_2,\mathbf{x};\theta)\). By Prop. \ref{prp:update} (in Appendix \ref{GaussianVar}), applied to (\(\varepsilon_1,\varepsilon_2\)), we have:
\[
y_1^*|y_2 \sim \mathcal{N}\left(\mathbf{x}_1'\boldsymbol\beta_1 + \frac{\rho}{\sigma_2}(y_2-\mathbf{x}_2'\boldsymbol\beta_2),1-\rho^2\right).
\]
which leads to
\begin{equation}
\mathbb{P}(y_1^*>0|y_2,\mathbf{x};\theta) = \Phi\left( \frac{\mathbf{x}_1'\boldsymbol\beta_1 + \dfrac{\rho}{\sigma_2}(y_2-\mathbf{x}_2'\boldsymbol\beta_2)}{\sqrt{1-\rho^2}}\right).\label{eq:probaPP3}
\end{equation}

Figure \ref{fig:SampleSelec} displays \(\mathbb{P}(y_1^*>0|y_2,\mathbf{x};\theta)\) for different values of \(y_2\) and of \(\rho\), in the case where \(\boldsymbol\beta_1=\boldsymbol\beta_2=0\).

\begin{figure}
\includegraphics{MicroEc_files/figure-latex/SampleSelec-1} \caption{Probability of observing $y_2$ depending on its value, for different values of conditional correlation between $y_2$ and $y_1^*$.}\label{fig:SampleSelec}
\end{figure}

Using Eqs. \eqref{eq:probaPP1}, \eqref{eq:probaPP2} and \eqref{eq:probaPP3}, one gets the log-likelihood function:
\begin{eqnarray*}
\log \mathcal{L}(\theta;\mathbf{y},\mathbf{X}) &=& \sum_{i=1}^n  (1 - y_{1,i})\log \Phi(-\mathbf{x}_{1,i}'\boldsymbol\beta_1) + \\
&&  \sum_{i=1}^n y_{1,i} \log \left(  \frac{1}{\sigma}\phi\left(\frac{y_{2,i} - \mathbf{x}_{2,i}'\boldsymbol\beta_2}{\sigma}\right)\right) + \\
&&  \sum_{i=1}^n y_{1,i} \log \left(\Phi\left( \frac{\mathbf{x}_{1,i}'\boldsymbol\beta_1 + \dfrac{\rho}{\sigma_2}(y_{2,i}-\mathbf{x}_2'\boldsymbol\beta_2)}{\sqrt{1-\rho^2}}\right)\right).
\end{eqnarray*}

We can also compute conditional expectations:
\begin{eqnarray}
\mathbb{E}(y_2^*|y_1=1,\mathbf{x}) &=& \mathbb{E}(\mathbb{E}(y_2^*|y_1^*,\mathbf{x})|y_1=1,\mathbf{x})\nonumber\\
&=& \mathbb{E}(\mathbf{x}_2'\boldsymbol\beta_2 + \rho\sigma_2(y_1^*-\mathbf{x}_1'\boldsymbol\beta_1)|y_1=1,\mathbf{x})\nonumber\\
&=& \mathbf{x}_2'\boldsymbol\beta_2 + \rho\sigma_2\mathbb{E}( \underbrace{y_1^*-\mathbf{x}_1'\boldsymbol\beta_1}_{=\varepsilon_1 \sim\mathcal{N}(0,1)}|\varepsilon_1>-\mathbf{x}_1'\boldsymbol\beta_1,\mathbf{x})\nonumber\\
&=& \mathbf{x}_2'\boldsymbol\beta_2 + \rho\sigma_2\frac{\phi(-\mathbf{x}_1'\boldsymbol\beta_1)}{1 - \Phi(-\mathbf{x}_1'\boldsymbol\beta_1)}\nonumber\\
&=& \mathbf{x}_2'\boldsymbol\beta_2 + \rho\sigma_2\frac{\phi(\mathbf{x}_1'\boldsymbol\beta_1)}{\Phi(\mathbf{x}_1'\boldsymbol\beta_1)}=\mathbf{x}_2'\boldsymbol\beta_2 + \rho\sigma_2\lambda(\mathbf{x}_1'\boldsymbol\beta_1),\label{eq:y2y11}
\end{eqnarray}
and:
\begin{eqnarray*}
\mathbb{E}(y_2^*|y_1=0,\mathbf{x}) &=&  \mathbf{x}_2'\boldsymbol\beta_2 + \rho\sigma_2\mathbb{E}(y_1^*-\mathbf{x}_1'\boldsymbol\beta_1|\varepsilon_1\le-\mathbf{x}_1'\boldsymbol\beta_1,\mathbf{x})\\
&=& \mathbf{x}_2'\boldsymbol\beta_2 + \rho\sigma_2\frac{\phi(-\mathbf{x}_1'\boldsymbol\beta_1)}{1 - \Phi(-\mathbf{x}_1'\boldsymbol\beta_1)}\\
&=& \mathbf{x}_2'\boldsymbol\beta_2 - \rho\sigma_2\frac{\phi(-\mathbf{x}_1'\boldsymbol\beta_1)}{\Phi(-\mathbf{x}_1'\boldsymbol\beta_1)}=\mathbf{x}_2'\boldsymbol\beta_2 - \rho\sigma_2\lambda(-\mathbf{x}_1'\boldsymbol\beta_1).
\end{eqnarray*}

\textbf{Heckman procedure}

As for tobit models (Section \ref{tobit}), we can use the Heckman procedure to estimate this model. Eq. \eqref{eq:y2y11} shows that \(\mathbb{E}(y_2^*|y_1=1,\mathbf{x}) \ne \mathbf{x}_2'\boldsymbol\beta_2\) when \(\rho \ne 0\). Therefore, the OLS approach yields biased estimates based when it is employed only on the sub-sample where \(y_1=1\).

The Heckman two-step procedure (or ``Heckit'') consists in replacing \(\lambda(\mathbf{x}_1'\boldsymbol\beta_1)\) appearing in Eq. \eqref{eq:y2y11} with a consistent estimate of it. More precisely:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Get an estimate \(\widehat{\boldsymbol\beta_1}\) of \(\boldsymbol\beta_1\) (probit regression of \(y_1\) on \(\mathbf{x}_1\)).
\item
  Run the OLS regression (using only data associated with \(y_1=1\)):
  \begin{equation}\label{eq:OLSregHeckit}
  y_2  = \mathbf{x}_2'\boldsymbol\beta_2 + \rho \sigma_2 \lambda(\mathbf{x}_1'\widehat{\boldsymbol\beta_1}) + \varepsilon_2,
  \end{equation}
  considering \(\lambda(\mathbf{x}_1'\widehat{\boldsymbol\beta_1})\) as a regressor.
\end{enumerate}

How to estimate \(\sigma_2^2\)? By Eq. \eqref{eq:Vtrunc}, we have:
\[
\mathbb{V}ar(y_2|y_1^*>0,\mathbf{x}) = \mathbb{V}ar(\varepsilon_2|\varepsilon_1>-\mathbf{x}_1'\boldsymbol\beta_1,\mathbf{x}).
\]
Using that \(\varepsilon_2\) can be decomposed as \(\rho\sigma_2\varepsilon_1 + \xi\), where \(\xi \sim \mathcal{N}(0,\sigma_2^2(1-\rho^2))\) is independent from \(\varepsilon_1\), we get:
\[
\mathbb{V}ar(y_2|y_1^*>0,\mathbf{x}) = \sigma_2^2(1-\rho^2) + \rho^2\sigma_2^2 \mathbb{V}ar(\varepsilon_1|\varepsilon_1>-\mathbf{x}_1'\boldsymbol\beta_1,\mathbf{x}).
\]
Using Eq. \eqref{eq:Vtrunc2}, we get:
\[
\mathbb{V}ar(\varepsilon_1|\varepsilon_1>-\mathbf{x}_1'\boldsymbol\beta_1,\mathbf{x}) = 1 - \mathbf{x}_1'\boldsymbol\beta_1 \lambda(\mathbf{x}_1'\boldsymbol\beta_1) - \lambda(\mathbf{x}_1'\boldsymbol\beta_1)^2,
\]
which gives
\begin{eqnarray*}
\mathbb{V}ar(y_2|y_1^*>0,\mathbf{x}) &=& \sigma_2^2(1-\rho^2) + \rho^2\sigma_2^2 (1 - \mathbf{x}_1'\boldsymbol\beta_1 \lambda(\mathbf{x}_1'\boldsymbol\beta_1) - \lambda(\mathbf{x}_1'\boldsymbol\beta_1)^2)\\
&=& \sigma_2^2 - \rho^2\sigma_2^2 \left(\mathbf{x}_1'\boldsymbol\beta_1 \lambda(\mathbf{x}_1'\boldsymbol\beta_1) + \lambda(\mathbf{x}_1'\boldsymbol\beta_1)^2\right),
\end{eqnarray*}
and, finally:
\[
\sigma_2^2 \approx \widehat{\mathbb{V}ar}(y_2|y_1^*>0,\mathbf{x}) + \widehat{\rho \sigma_2}^2 \left(\mathbf{x}_1'\widehat{\boldsymbol\beta_1} \lambda(\mathbf{x}_1'\widehat{\boldsymbol\beta_1}) + \lambda(\mathbf{x}_1'\widehat{\boldsymbol\beta_1})^2\right).
\]

The Heckman procedure is computationally simple. Although computational costs are no longer an issue, the two-step solution allows certain generalisations more easily than ML, and is more robust in certain circumstances. The computation of parameter standard errors is fairly complicated because of the two steps (see \citet{Cameron_Trivedi_2005}, Subsection 16.10.2). Bootstrap can be resorted to.

\begin{example}[Wage prediction]
\protect\hypertarget{exm:WageSample}{}\label{exm:WageSample}

As in Example \ref{exm:WageMroz1}, let us use the \citet{Mroz_1987} dataset again, with the objective of explaining wage setting.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(sampleSelection)}
\FunctionTok{library}\NormalTok{(AER)}
\FunctionTok{data}\NormalTok{(}\StringTok{"Mroz87"}\NormalTok{)}
\NormalTok{Mroz87}\SpecialCharTok{$}\NormalTok{lfp.yesno }\OtherTok{\textless{}{-}} \ConstantTok{NaN}
\NormalTok{Mroz87}\SpecialCharTok{$}\NormalTok{lfp.yesno[Mroz87}\SpecialCharTok{$}\NormalTok{lfp}\SpecialCharTok{==}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \StringTok{"yes"}
\NormalTok{Mroz87}\SpecialCharTok{$}\NormalTok{lfp.yesno[Mroz87}\SpecialCharTok{$}\NormalTok{lfp}\SpecialCharTok{==}\DecValTok{0}\NormalTok{] }\OtherTok{\textless{}{-}} \StringTok{"no"}
\NormalTok{Mroz87}\SpecialCharTok{$}\NormalTok{lfp.yesno }\OtherTok{\textless{}{-}} \FunctionTok{as.factor}\NormalTok{(Mroz87}\SpecialCharTok{$}\NormalTok{lfp.yesno)}
\CommentTok{\#Logit \& Probit (selection equation)}
\NormalTok{logitW }\OtherTok{\textless{}{-}} \FunctionTok{glm}\NormalTok{(lfp }\SpecialCharTok{\textasciitilde{}}\NormalTok{ age }\SpecialCharTok{+} \FunctionTok{I}\NormalTok{( age}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{ ) }\SpecialCharTok{+}\NormalTok{ kids5 }\SpecialCharTok{+}\NormalTok{ huswage }\SpecialCharTok{+}\NormalTok{ educ,}
              \AttributeTok{family =} \FunctionTok{binomial}\NormalTok{(}\AttributeTok{link =} \StringTok{"logit"}\NormalTok{), }\AttributeTok{data =}\NormalTok{ Mroz87) }
\NormalTok{probitW }\OtherTok{\textless{}{-}} \FunctionTok{glm}\NormalTok{(lfp }\SpecialCharTok{\textasciitilde{}}\NormalTok{ age }\SpecialCharTok{+} \FunctionTok{I}\NormalTok{( age}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{ ) }\SpecialCharTok{+}\NormalTok{ kids5 }\SpecialCharTok{+}\NormalTok{ huswage }\SpecialCharTok{+}\NormalTok{ educ,}
               \AttributeTok{family =} \FunctionTok{binomial}\NormalTok{(}\AttributeTok{link =} \StringTok{"probit"}\NormalTok{), }\AttributeTok{data =}\NormalTok{ Mroz87) }
\CommentTok{\# OLS for outcome:}
\NormalTok{ols1 }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(}\FunctionTok{log}\NormalTok{(wage) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ educ}\SpecialCharTok{+}\NormalTok{exper}\SpecialCharTok{+}\FunctionTok{I}\NormalTok{( exper}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{ )}\SpecialCharTok{+}\NormalTok{city,}\AttributeTok{data=}\FunctionTok{subset}\NormalTok{(Mroz87,lfp}\SpecialCharTok{==}\DecValTok{1}\NormalTok{))}
\CommentTok{\# Two{-}step Heckman estimation}
\NormalTok{heckvan }\OtherTok{\textless{}{-}} 
  \FunctionTok{heckit}\NormalTok{( lfp }\SpecialCharTok{\textasciitilde{}}\NormalTok{ age }\SpecialCharTok{+} \FunctionTok{I}\NormalTok{( age}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{ ) }\SpecialCharTok{+}\NormalTok{ kids5 }\SpecialCharTok{+}\NormalTok{ huswage }\SpecialCharTok{+}\NormalTok{ educ, }\CommentTok{\# selection equation}
          \FunctionTok{log}\NormalTok{(wage) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ educ }\SpecialCharTok{+}\NormalTok{ exper }\SpecialCharTok{+} \FunctionTok{I}\NormalTok{( exper}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{ ) }\SpecialCharTok{+}\NormalTok{ city, }\CommentTok{\# outcome equation}
          \AttributeTok{data=}\NormalTok{Mroz87 )}
\CommentTok{\# Maximun likelihood estimation of selection model:}
\NormalTok{ml }\OtherTok{\textless{}{-}} \FunctionTok{selection}\NormalTok{(lfp}\SpecialCharTok{\textasciitilde{}}\NormalTok{age}\SpecialCharTok{+}\FunctionTok{I}\NormalTok{(age}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}\SpecialCharTok{+}\NormalTok{kids5}\SpecialCharTok{+}\NormalTok{huswage}\SpecialCharTok{+}\NormalTok{educ, }
                \FunctionTok{log}\NormalTok{(wage)}\SpecialCharTok{\textasciitilde{}}\NormalTok{educ}\SpecialCharTok{+}\NormalTok{exper}\SpecialCharTok{+}\FunctionTok{I}\NormalTok{(exper}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}\SpecialCharTok{+}\NormalTok{city, }\AttributeTok{data =}\NormalTok{ Mroz87)}
\CommentTok{\# Print selection{-}equation estimates:}
\FunctionTok{stargazer}\NormalTok{(logitW,probitW,heckvan,ml,}\AttributeTok{type =} \StringTok{"text"}\NormalTok{,}\AttributeTok{no.space =} \ConstantTok{TRUE}\NormalTok{,}
          \AttributeTok{selection.equation =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## ===================================================================
##                                   Dependent variable:              
##                     -----------------------------------------------
##                                           lfp                      
##                     logistic   probit      Heckman      selection  
##                                           selection                
##                        (1)       (2)         (3)           (4)     
## -------------------------------------------------------------------
## age                   0.012     0.010       0.010         0.010    
##                      (0.114)   (0.069)     (0.069)       (0.069)   
## I(age2)              -0.001    -0.0005     -0.0005       -0.0005   
##                      (0.001)   (0.001)     (0.001)       (0.001)   
## kids5               -1.409*** -0.855***   -0.855***     -0.854***  
##                      (0.198)   (0.116)     (0.115)       (0.116)   
## huswage             -0.069*** -0.042***   -0.042***     -0.042***  
##                      (0.020)   (0.012)     (0.012)       (0.013)   
## educ                0.244***  0.148***    0.148***      0.148***   
##                      (0.040)   (0.024)     (0.023)       (0.024)   
## Constant             -0.938    -0.620      -0.620        -0.615    
##                      (2.508)   (1.506)     (1.516)       (1.518)   
## -------------------------------------------------------------------
## Observations           753       753         753           753     
## R2                                          0.158                  
## Adjusted R2                                 0.148                  
## Log Likelihood      -459.955  -459.901                  -891.177   
## Akaike Inf. Crit.    931.910   931.802                             
## rho                                         0.018     0.014 (0.203)
## Inverse Mills Ratio                     0.012 (0.152)              
## ===================================================================
## Note:                                   *p<0.1; **p<0.05; ***p<0.01
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Print outcome{-}equation estimates:}
\FunctionTok{stargazer}\NormalTok{(ols1,heckvan,ml,}\AttributeTok{type =} \StringTok{"text"}\NormalTok{,}\AttributeTok{no.space =} \ConstantTok{TRUE}\NormalTok{,}\AttributeTok{omit.stat =} \StringTok{"f"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## ================================================================
##                                 Dependent variable:             
##                     --------------------------------------------
##                                      log(wage)                  
##                           OLS           Heckman      selection  
##                                        selection                
##                           (1)             (2)           (3)     
## ----------------------------------------------------------------
## educ                    0.106***       0.106***      0.106***   
##                         (0.014)         (0.017)       (0.017)   
## exper                   0.041***       0.041***      0.041***   
##                         (0.013)         (0.013)       (0.013)   
## I(exper2)               -0.001**       -0.001**      -0.001**   
##                         (0.0004)       (0.0004)      (0.0004)   
## city                     0.054           0.053         0.053    
##                         (0.068)         (0.069)       (0.069)   
## Constant               -0.531***        -0.547*      -0.544**   
##                         (0.199)         (0.289)       (0.272)   
## ----------------------------------------------------------------
## Observations              428             753           753     
## R2                       0.158           0.158                  
## Adjusted R2              0.150           0.148                  
## Log Likelihood                                       -891.177   
## rho                                      0.018     0.014 (0.203)
## Inverse Mills Ratio                  0.012 (0.152)              
## Residual Std. Error 0.667 (df = 423)                            
## ================================================================
## Note:                                *p<0.1; **p<0.05; ***p<0.01
\end{verbatim}

\end{example}

\hypertarget{models-of-count-data}{%
\section{Models of Count Data}\label{models-of-count-data}}

Count-data models aim at explaining dependent variables \(y_i\) that take integer values. Typically, one may want to account for the number of doctor visits, of customers, of hospital stays, of borrowers' defaults, of recreational trips, of accidents. Quite often, these data feature large proportion of zeros (see, e.g., Table 20.1 in \citet{Cameron_Trivedi_2005}), and/or are skewed to the right.

\hypertarget{poisson-model}{%
\subsection{Poisson model}\label{poisson-model}}

The most basic count-data model is the Poisson model. In this model, we have \(y \sim \mathcal{P}(\mu)\), i.e.
\[
\mathbb{P}(y=k) = \frac{\mu^k e^{-\mu}}{k!},
\]
implying \(\mathbb{E}(y) = \mathbb{V}ar(y) = \mu\).

the Poisson parameter, \(\mu\), is then assumed to depend on some observed variables, gathered in vector \(\mathbf{x}_i\) for entity \(i\). To ensure that \(\mu_i \ge 0\), it is common to take \(\mu_i = \exp(\boldsymbol\beta'\mathbf{x}_i)\), which gives:
\[
y_i \sim \mathcal{P}(\exp[\boldsymbol\beta'\mathbf{x}_i]).
\]

The Poisson regression is intrinsically heteroskedastic (since \(\mathbb{V}ar(y_i) = \mu_i = \exp(\boldsymbol\beta'\mathbf{x}_i)\)).

Under the assumption of independence across entities, the log-likelihood is given by:
\[
\log \mathcal{L}(\boldsymbol\beta;\mathbf{y},\mathbf{X}) = \sum_{i=1}^n (y_i \boldsymbol\beta'\mathbf{x}_i - \exp[\boldsymbol\beta'\mathbf{x}_i] - \ln[y_i!]).
\]
The first-order condition to get the MLE is:
\begin{equation}
\sum_{i=1}^n (y_i - \exp[\boldsymbol\beta'\mathbf{x}_i])\mathbf{x}_i = \underbrace{\mathbf{0}}_{K \times 1}. \label{eq:FOCPoisson}
\end{equation}

Eq. \eqref{eq:FOCPoisson} is equivalent to what would define the \textbf{Pseudo Maximum-Likelihood} estimator of \(\boldsymbol\beta\) in the (misspecified) model
\[
y_i \sim i.i.d.\,\mathcal{N}(\exp[\boldsymbol\beta'\mathbf{x}_i],\sigma^2).
\]
That is, Eq. \eqref{eq:FOCPoisson} also characterizes the (true) ML estimator of \(\boldsymbol\beta\) in the previous model.

Since \(\mathbb{E}(y_i|\mathbf{x}_i) = \exp(\boldsymbol\beta'\mathbf{x}_i)\), we have:
\[
y_i = \exp(\boldsymbol\beta'\mathbf{x}_i) + \varepsilon_i,
\]
with \(\mathbb{E}(\varepsilon_i|\mathbf{x}_i) = 0\). This notably implies that the (N)LS estimator of \(\boldsymbol\beta\) is consistent.

How to interpret regression coefficients (the components of \(\boldsymbol\beta\))? We have:
\[
\frac{\partial \mathbb{E}(y_i|\mathbf{x}_i)}{\partial x_{i,j}} = \beta_j \exp(\boldsymbol\beta'\mathbf{x}_i),
\]
which depends on the considered individual.

The average estimated response is:
\[
\widehat{\beta}_j \frac{1}{n}\sum_{i=1}^n  \exp(\widehat{\boldsymbol\beta}'\mathbf{x}_i),
\]
which is equal to \(\widehat{\beta}_j \overline{y}\) if the model includes a constant (e.g., if \(x_{1,i}=1\) for all entities \(i\)).

The limitation of the standard Poisson model is that the distribution of \(y_i\) conditional on \(\mathbf{x}_i\) depends on a single parameter (\(\mu_i\)). Besides, there is often a tension between fitting the fraction of zeros, i.e.~\(\mathbb{P}(y_i=0|\mathbf{x}_i)=\exp[-\exp(\boldsymbol\beta'\mathbf{x}_i)]\), and the distribution of \(y_i|\mathbf{x}_i,y_i>0\). The following models (negative binomial, or NB model, the Hurdle model, and the Zero-Inflated model) have been designed to address these points.

\hypertarget{negative-binomial-model}{%
\subsection{Negative binomial model}\label{negative-binomial-model}}

In the negative binomial model, we have:
\[
y_i|\lambda_i \sim \mathcal{P}(\lambda_i),
\]
but \(\lambda_i\) is now random. Specifically, it takes the form:
\[
\lambda_i = \nu_i \times \exp(\boldsymbol\beta'\mathbf{x}_i),
\]
where \(\nu_i \sim \,i.i.d.\,\Gamma(\underbrace{\delta}_{\mbox{shape}},\underbrace{1/\delta}_{\mbox{scale}})\). That is, the p.d.f. of \(\nu_i\) is:
\[
g(\nu) = \frac{\nu^{\delta - 1}e^{-\nu\delta}\delta^\delta}{\Gamma(\delta)},
\]
where \(\Gamma:\,z \mapsto \int_0^{+\infty}t^{z-1}e^{-t}dt\) (and \(\Gamma(k+1)=k!\)).

This notably implies that:
\[
\mathbb{E}(\nu_i) = 1 \quad \mbox{and} \quad \mathbb{V}ar(\nu) = \frac{1}{\delta}.
\]

Hence, the p.d.f. of \(y_i\) conditional on \(\mu\) and \(\delta\) (with \(\mu=\exp(\boldsymbol\beta'\mathbf{x}_i)\)) is obtained as a mixture of densities:
\[
\mathbb{P}(y_i=k|\exp(\boldsymbol\beta'\mathbf{x}_i)=\mu;\delta)=\int_0^\infty \frac{e^{-\mu \nu}(\mu \nu)^k}{k!} \frac{\nu^{\delta - 1}e^{-\nu\delta}\delta^\delta}{\Gamma(\delta)} d \nu.
\]

It can be shown that:
\[
\mathbb{E}(y|\mathbf{x}) = \mu \quad \mbox{and}\quad \mathbb{V}ar(y|\mathbf{x}) = \mu\left(1+\alpha \mu\right),
\]
where \(\exp(\boldsymbol\beta'\mathbf{x}_i)=\mu\) and \(\alpha = 1/\delta\).

We have one additional degree of freedom w.r.t. the Poisson model (\(\alpha\)).

Note that \(\mathbb{V}ar(y|\mathbf{x}) > \mathbb{E}(y|\mathbf{x})\) (which is often called for by the data). Moreover, the conditional variance is quadratic in the mean:
\[
\mathbb{V}ar(y|\mathbf{x}) = \mu+\alpha \mu^2.
\]
The previous expression is the basis of the so-called \textbf{NB2} specification. If \(\delta\) is replaced with \(\mu/\gamma\), then we get the \textbf{NB1} model:
\[
\mathbb{V}ar(y|\mathbf{x}) = \mu(1+\gamma).
\]

\begin{example}[Number of doctor visits]
\protect\hypertarget{exm:Doctorvisits}{}\label{exm:Doctorvisits}The following example compares different specifications, namely a linear regression model, a Poisson model, and a NB model, to account for the number of doctor visits. The dataset (`\texttt{randdata}) is the one used in Chapter 20 of \citet{Cameron_Trivedi_2005} (available on \href{http://cameron.econ.ucdavis.edu/mmabook/mmadata.html}{that page}).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(AEC)}
\FunctionTok{library}\NormalTok{(COUNT)}
\FunctionTok{library}\NormalTok{(pscl) }\CommentTok{\# for predprob function and hurdle model}
\FunctionTok{par}\NormalTok{(}\AttributeTok{plt=}\FunctionTok{c}\NormalTok{(.}\DecValTok{15}\NormalTok{,.}\DecValTok{95}\NormalTok{,.}\DecValTok{1}\NormalTok{,.}\DecValTok{95}\NormalTok{))}
\FunctionTok{plot}\NormalTok{(}\FunctionTok{table}\NormalTok{(randdata}\SpecialCharTok{$}\NormalTok{mdvis))}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\includegraphics[width=0.95\linewidth]{MicroEc_files/figure-latex/countdata1-1} \caption{Distribution of the number of doctor visits.}\label{fig:countdata1}
\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{randdata}\SpecialCharTok{$}\NormalTok{LC }\OtherTok{\textless{}{-}} \FunctionTok{log}\NormalTok{(}\DecValTok{1} \SpecialCharTok{+}\NormalTok{ randdata}\SpecialCharTok{$}\NormalTok{coins)}
\NormalTok{model.OLS }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(mdvis }\SpecialCharTok{\textasciitilde{}}\NormalTok{ LC }\SpecialCharTok{+}\NormalTok{ idp }\SpecialCharTok{+}\NormalTok{ lpi }\SpecialCharTok{+}\NormalTok{ fmde }\SpecialCharTok{+}\NormalTok{ physlm }\SpecialCharTok{+}\NormalTok{ disea }\SpecialCharTok{+}\NormalTok{ hlthg }\SpecialCharTok{+} 
\NormalTok{                  hlthf }\SpecialCharTok{+}\NormalTok{ hlthp }\SpecialCharTok{{-}} \DecValTok{1}\NormalTok{,}\AttributeTok{data=}\NormalTok{randdata)}
\NormalTok{model.poisson }\OtherTok{\textless{}{-}} \FunctionTok{glm}\NormalTok{(mdvis }\SpecialCharTok{\textasciitilde{}}\NormalTok{ LC }\SpecialCharTok{+}\NormalTok{ idp }\SpecialCharTok{+}\NormalTok{ lpi }\SpecialCharTok{+}\NormalTok{ fmde }\SpecialCharTok{+}\NormalTok{ physlm }\SpecialCharTok{+}\NormalTok{ disea }\SpecialCharTok{+} 
\NormalTok{                       hlthg }\SpecialCharTok{+}\NormalTok{ hlthf }\SpecialCharTok{+}\NormalTok{ hlthp }\SpecialCharTok{{-}} \DecValTok{1}\NormalTok{,}\AttributeTok{data=}\NormalTok{randdata,}\AttributeTok{family =}\NormalTok{ poisson)}
\NormalTok{model.neg.bin }\OtherTok{\textless{}{-}} \FunctionTok{glm.nb}\NormalTok{(mdvis }\SpecialCharTok{\textasciitilde{}}\NormalTok{ LC }\SpecialCharTok{+}\NormalTok{ idp }\SpecialCharTok{+}\NormalTok{ lpi }\SpecialCharTok{+}\NormalTok{ fmde }\SpecialCharTok{+}\NormalTok{ physlm }\SpecialCharTok{+}\NormalTok{ disea }\SpecialCharTok{+}
\NormalTok{                          hlthg }\SpecialCharTok{+}\NormalTok{ hlthf }\SpecialCharTok{+}\NormalTok{ hlthp }\SpecialCharTok{{-}} \DecValTok{1}\NormalTok{,}\AttributeTok{data=}\NormalTok{randdata)}
\NormalTok{model.neg.bin.with.intercept }\OtherTok{\textless{}{-}} 
  \FunctionTok{glm.nb}\NormalTok{(mdvis }\SpecialCharTok{\textasciitilde{}}\NormalTok{ LC }\SpecialCharTok{+}\NormalTok{ idp }\SpecialCharTok{+}\NormalTok{ lpi }\SpecialCharTok{+}\NormalTok{ fmde }\SpecialCharTok{+}\NormalTok{ physlm }\SpecialCharTok{+}\NormalTok{ disea }\SpecialCharTok{+}\NormalTok{ hlthg }\SpecialCharTok{+} 
\NormalTok{           hlthf }\SpecialCharTok{+}\NormalTok{ hlthp,}\AttributeTok{data=}\NormalTok{randdata)}
\NormalTok{stargazer}\SpecialCharTok{::}\FunctionTok{stargazer}\NormalTok{(model.OLS,model.poisson,model.neg.bin,}
\NormalTok{                     model.neg.bin.with.intercept,}\AttributeTok{type=}\StringTok{"text"}\NormalTok{,}
                     \AttributeTok{no.space =} \ConstantTok{TRUE}\NormalTok{,}\AttributeTok{omit.stat =} \FunctionTok{c}\NormalTok{(}\StringTok{"f"}\NormalTok{,}\StringTok{"ser"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## =========================================================================
##                                     Dependent variable:                  
##                   -------------------------------------------------------
##                                            mdvis                         
##                      OLS      Poisson               negative             
##                                                     binomial             
##                      (1)        (2)           (3)              (4)       
## -------------------------------------------------------------------------
## LC                -0.155***  -0.051***     -0.057***        -0.058***    
##                    (0.020)    (0.003)       (0.006)          (0.006)     
## idp               -0.546***  -0.183***     -0.212***        -0.268***    
##                    (0.075)    (0.011)       (0.023)          (0.023)     
## lpi               0.230***   0.095***       0.088***         0.041***    
##                    (0.012)    (0.002)       (0.004)          (0.004)     
## fmde              -0.073***  -0.029***     -0.030***        -0.038***    
##                    (0.012)    (0.002)       (0.004)          (0.003)     
## physlm            0.945***   0.217***       0.229***         0.269***    
##                    (0.104)    (0.013)       (0.031)          (0.030)     
## disea             0.177***   0.050***       0.062***         0.038***    
##                    (0.004)   (0.0005)       (0.001)          (0.001)     
## hlthg             0.270***   0.126***       0.068***         -0.044**    
##                    (0.066)    (0.009)       (0.020)          (0.020)     
## hlthf             0.455***   0.149***       0.084**           0.017      
##                    (0.123)    (0.016)       (0.037)          (0.036)     
## hlthp             1.537***   0.197***       0.185**          0.178**     
##                    (0.263)    (0.027)       (0.076)          (0.074)     
## Constant                                                     0.664***    
##                                                              (0.025)     
## -------------------------------------------------------------------------
## Observations       20,190     20,190         20,190           20,190     
## R2                  0.322                                                
## Adjusted R2         0.322                                                
## Log Likelihood              -64,221.340   -43,745.860      -43,384.660   
## theta                                   0.732*** (0.010) 0.773*** (0.011)
## Akaike Inf. Crit.           128,460.700    87,509.710       86,789.320   
## =========================================================================
## Note:                                         *p<0.1; **p<0.05; ***p<0.01
\end{verbatim}

Models' predictions can be obtained as follows:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# prediction of beta\textquotesingle{}x, equivalent to "model.poisson$fitted.values":}
\NormalTok{predict\_poisson.beta.x }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(model.poisson)}
\CommentTok{\# prediction of the number of events (exp(beta\textquotesingle{}x)):}
\NormalTok{predict\_poisson }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(model.poisson,}\AttributeTok{type=}\StringTok{"response"}\NormalTok{)}
\NormalTok{predict\_NB }\OtherTok{\textless{}{-}}\NormalTok{ model.neg.bin}\SpecialCharTok{$}\NormalTok{fitted.values}
\end{Highlighting}
\end{Shaded}

Let us now compute the model-implied probabilities, and let's compare them with the frequencies observed in the data.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{prop.table.data  }\OtherTok{\textless{}{-}} \FunctionTok{prop.table}\NormalTok{(}\FunctionTok{table}\NormalTok{(randdata}\SpecialCharTok{$}\NormalTok{mdvis))}
\NormalTok{predprob.poisson }\OtherTok{\textless{}{-}} \FunctionTok{predprob}\NormalTok{(model.poisson) }\CommentTok{\# part of pscl package}
\NormalTok{predprob.nb      }\OtherTok{\textless{}{-}} \FunctionTok{predprob}\NormalTok{(model.neg.bin)}
\FunctionTok{print}\NormalTok{(}\FunctionTok{rbind}\NormalTok{(prop.table.data[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{6}\NormalTok{],}
            \FunctionTok{apply}\NormalTok{(predprob.poisson[,}\DecValTok{1}\SpecialCharTok{:}\DecValTok{6}\NormalTok{],}\DecValTok{2}\NormalTok{,mean),}
            \FunctionTok{apply}\NormalTok{(predprob.nb[,}\DecValTok{1}\SpecialCharTok{:}\DecValTok{6}\NormalTok{],}\DecValTok{2}\NormalTok{,mean)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##              0         1         2          3          4          5
## [1,] 0.3124319 0.1890540 0.1385339 0.09331352 0.06661714 0.04794453
## [2,] 0.1220592 0.2230328 0.2271621 0.17173478 0.10888940 0.06244353
## [3,] 0.3486824 0.1884640 0.1219340 0.08385899 0.05968603 0.04350209
\end{verbatim}

It appears that the NB model is better at capturing the relatively large number of zeros than the Poisson model. This will also be the case for the Hurdle and Zero-Inflation models:
\end{example}

\hypertarget{hurdle-model}{%
\subsection{Hurdle model}\label{hurdle-model}}

The main objective of this model, w.r.t. the Poisson model, is to generate more zeros in the data than predicted by the previous count models. The idea is to separate the modeling of the number of zeros and that of the number of non-zero counts. Specifically, the frequency of zeros is determined by \(f_1\), the (relative) frequencies of non-zero counts are determined by \(f_2\):
\[
f(y) = \left\{
\begin{array}{lll}
f_1(0) & \mbox{if $y=0$},\\
\dfrac{1-f_1(0)}{1-f_2(0)}f_2(y) & \mbox{if $y>0$}.
\end{array}
\right.
\]

Note that we are back to the standard Poisson model if \(f_1 \equiv f_2\). This model is straightforwardly estimated by ML.

\hypertarget{zero-inflated-model}{%
\subsection{Zero-inflated model}\label{zero-inflated-model}}

The objective is the same as for the Hurdle model, the modeling is slightly different. It is based on a mixture of a binary process \(B\) (p.d.f. \(f_1\)) and a process \(Z\) (p.d.f. \(f_2\)). \(B\) and \(Z\) are independent. Formally:
\[
y = B Z,
\]
implying:
\[
f(y) = \left\{
\begin{array}{lll}
f_1(0) + (1-f_1(0))f_2(0) & \mbox{if $y=0$},\\
(1-f_1(0))f_2(y) & \mbox{if $y>0$}.
\end{array}
\right.
\]
Typically, \(f_1\) corresponds to a logit model and \(f_2\) is Poisson or negative binomial density. This model is easily estimated by ML techniques.

\begin{example}[Number of doctor visits]
\protect\hypertarget{exm:Doctorvisits2}{}\label{exm:Doctorvisits2}

Let us come back to the data used in Example \ref{exm:Doctorvisits}, and estimate Hurdle and a zero-inflation models:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model.hurdle }\OtherTok{\textless{}{-}} 
  \FunctionTok{hurdle}\NormalTok{(mdvis }\SpecialCharTok{\textasciitilde{}}\NormalTok{ LC }\SpecialCharTok{+}\NormalTok{ idp }\SpecialCharTok{+}\NormalTok{ lpi }\SpecialCharTok{+}\NormalTok{ fmde }\SpecialCharTok{+}\NormalTok{ physlm }\SpecialCharTok{+}\NormalTok{ disea }\SpecialCharTok{+}\NormalTok{ hlthg }\SpecialCharTok{+}\NormalTok{ hlthf }\SpecialCharTok{+} 
\NormalTok{           hlthp, }\AttributeTok{data=}\NormalTok{randdata,}
         \AttributeTok{dist =} \StringTok{"poisson"}\NormalTok{, }\AttributeTok{zero.dist =} \StringTok{"binomial"}\NormalTok{, }\AttributeTok{link =} \StringTok{"logit"}\NormalTok{)}
\NormalTok{model.zeroinfl }\OtherTok{\textless{}{-}} \FunctionTok{zeroinfl}\NormalTok{(mdvis }\SpecialCharTok{\textasciitilde{}}\NormalTok{ LC }\SpecialCharTok{+}\NormalTok{ idp }\SpecialCharTok{+}\NormalTok{ lpi }\SpecialCharTok{+}\NormalTok{ fmde }\SpecialCharTok{+}\NormalTok{ physlm }\SpecialCharTok{+}
\NormalTok{                             disea }\SpecialCharTok{+}\NormalTok{ hlthg }\SpecialCharTok{+}\NormalTok{ hlthf }\SpecialCharTok{+}\NormalTok{ hlthp, }\AttributeTok{data=}\NormalTok{randdata,}
                           \AttributeTok{dist =} \StringTok{"poisson"}\NormalTok{, }\AttributeTok{link =} \StringTok{"logit"}\NormalTok{)}
\FunctionTok{stargazer}\NormalTok{(model.hurdle,model.zeroinfl,}\AttributeTok{zero.component=}\ConstantTok{FALSE}\NormalTok{,}
          \AttributeTok{no.space=}\ConstantTok{TRUE}\NormalTok{,}\AttributeTok{type=}\StringTok{"text"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## ===========================================
##                    Dependent variable:     
##                ----------------------------
##                           mdvis            
##                    hurdle     zero-inflated
##                                count data  
##                     (1)            (2)     
## -------------------------------------------
## LC               -0.015***      -0.015***  
##                   (0.003)        (0.003)   
## idp              -0.085***      -0.086***  
##                   (0.011)        (0.011)   
## lpi               0.010***      0.010***   
##                   (0.002)        (0.002)   
## fmde             -0.021***      -0.021***  
##                   (0.002)        (0.002)   
## physlm            0.231***      0.231***   
##                   (0.012)        (0.012)   
## disea             0.022***      0.022***   
##                   (0.001)        (0.001)   
## hlthg             0.027***      0.026***   
##                   (0.010)        (0.010)   
## hlthf             0.147***      0.146***   
##                   (0.016)        (0.016)   
## hlthp             0.304***      0.303***   
##                   (0.026)        (0.026)   
## Constant          1.133***      1.133***   
##                   (0.012)        (0.012)   
## -------------------------------------------
## Observations       20,190        20,190    
## Log Likelihood  -54,772.100    -54,772.550 
## ===========================================
## Note:           *p<0.1; **p<0.05; ***p<0.01
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{stargazer}\NormalTok{(model.hurdle,model.zeroinfl,}\AttributeTok{zero.component=}\ConstantTok{TRUE}\NormalTok{,}
          \AttributeTok{no.space=}\ConstantTok{TRUE}\NormalTok{,}\AttributeTok{type=}\StringTok{"text"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## ===========================================
##                    Dependent variable:     
##                ----------------------------
##                           mdvis            
##                    hurdle     zero-inflated
##                                count data  
##                     (1)            (2)     
## -------------------------------------------
## LC               -0.150***      0.154***   
##                   (0.010)        (0.011)   
## idp              -0.631***      0.637***   
##                   (0.038)        (0.040)   
## lpi               0.102***      -0.105***  
##                   (0.007)        (0.007)   
## fmde             -0.062***      0.060***   
##                   (0.006)        (0.006)   
## physlm            0.239***      -0.203***  
##                   (0.056)        (0.058)   
## disea             0.062***      -0.059***  
##                   (0.003)        (0.003)   
## hlthg            -0.142***      0.158***   
##                   (0.034)        (0.036)   
## hlthf            -0.352***      0.396***   
##                   (0.062)        (0.064)   
## hlthp              -0.181         0.233    
##                   (0.149)        (0.151)   
## Constant          0.411***      -0.528***  
##                   (0.044)        (0.047)   
## -------------------------------------------
## Observations       20,190        20,190    
## Log Likelihood  -54,772.100    -54,772.550 
## ===========================================
## Note:           *p<0.1; **p<0.05; ***p<0.01
\end{verbatim}

Let us test the importance of \texttt{LC} in the model using a Wald test:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Test whether LC is important in the model:}
\NormalTok{model.hurdle.reduced }\OtherTok{\textless{}{-}} \FunctionTok{update}\NormalTok{(model.hurdle,.}\SpecialCharTok{\textasciitilde{}}\NormalTok{.}\SpecialCharTok{{-}}\NormalTok{LC)}
\NormalTok{lmtest}\SpecialCharTok{::}\FunctionTok{waldtest}\NormalTok{(model.hurdle, model.hurdle.reduced)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Wald test
## 
## Model 1: mdvis ~ LC + idp + lpi + fmde + physlm + disea + hlthg + hlthf + 
##     hlthp
## Model 2: mdvis ~ idp + lpi + fmde + physlm + disea + hlthg + hlthf + hlthp
##   Res.Df Df  Chisq Pr(>Chisq)    
## 1  20170                         
## 2  20172 -2 247.64  < 2.2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

Finally, we compare average model-implied probabilities with the frequencies observed in the data:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{predprob.hurdle   }\OtherTok{\textless{}{-}} \FunctionTok{predprob}\NormalTok{(model.hurdle)}
\NormalTok{predprob.zeroinfl }\OtherTok{\textless{}{-}} \FunctionTok{predprob}\NormalTok{(model.zeroinfl)}
\FunctionTok{print}\NormalTok{(}\FunctionTok{rbind}\NormalTok{(prop.table.data[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{6}\NormalTok{],}
  \FunctionTok{apply}\NormalTok{(predprob.poisson[,}\DecValTok{1}\SpecialCharTok{:}\DecValTok{6}\NormalTok{],}\DecValTok{2}\NormalTok{,mean),}
  \FunctionTok{apply}\NormalTok{(predprob.nb[,}\DecValTok{1}\SpecialCharTok{:}\DecValTok{6}\NormalTok{],}\DecValTok{2}\NormalTok{,mean),}
  \FunctionTok{apply}\NormalTok{(predprob.hurdle[,}\DecValTok{1}\SpecialCharTok{:}\DecValTok{6}\NormalTok{],}\DecValTok{2}\NormalTok{,mean),}
  \FunctionTok{apply}\NormalTok{(predprob.zeroinfl[,}\DecValTok{1}\SpecialCharTok{:}\DecValTok{6}\NormalTok{],}\DecValTok{2}\NormalTok{,mean)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##              0          1         2          3          4          5
## [1,] 0.3124319 0.18905399 0.1385339 0.09331352 0.06661714 0.04794453
## [2,] 0.1220592 0.22303277 0.2271621 0.17173478 0.10888940 0.06244353
## [3,] 0.3486824 0.18846395 0.1219340 0.08385899 0.05968603 0.04350209
## [4,] 0.3124319 0.06056959 0.1083120 0.13262624 0.12553899 0.09847017
## [5,] 0.3124684 0.06053026 0.1082799 0.13262562 0.12556531 0.09850218
\end{verbatim}

\end{example}

\hypertarget{append}{%
\chapter{Appendix}\label{append}}

\hypertarget{PCAapp}{%
\section{Principal component analysis (PCA)}\label{PCAapp}}

\textbf{Principal component analysis (PCA)} is a classical and easy-to-use statistical method to reduce the dimension of large datasets containing variables that are linearly driven by a relatively small number of factors. This approach is widely used in data analysis and image compression.

Suppose that we have \(T\) observations of a \(n\)-dimensional random vector \(x\), denoted by \(x_{1},x_{2},\ldots,x_{T}\). We suppose that each component of \(x\) is of mean zero.

Denote with \(X\) the matrix given by \(\left[\begin{array}{cccc} x_{1} & x_{2} & \ldots & x_{T}\end{array}\right]'\). Denote the \(j^{th}\) column of \(X\) by \(X_{j}\).

We want to find the linear combination of the \(x_{i}\)'s (\(x.u\)), with \(\left\Vert u\right\Vert =1\), with ``maximum variance.'' That is, we want to solve:
\begin{equation}
\begin{array}{clll}
\underset{u}{\arg\max} & u'X'Xu. \\
\mbox{s.t. } & \left| u\right| =1
\end{array}\label{eq:PCA11}
\end{equation}

Since \(X'X\) is a positive definite matrix, it admits the following decomposition:
\begin{eqnarray*}
X'X & = & PDP'\\
& = & P\left[\begin{array}{ccc}
\lambda_{1}\\
& \ddots\\
&  & \lambda_{n}
\end{array}\right]P',
\end{eqnarray*}
where \(P\) is an orthogonal matrix whose columns are the eigenvectors of \(X'X\).

We can order the eigenvalues such that \(\lambda_{1}\geq\ldots\geq\lambda_{n}\). (Since \(X'X\) is positive definite, all these eigenvalues are positive.)

Since \(P\) is orthogonal, we have \(u'X'Xu=u'PDP'u=y'Dy\) where \(\left\Vert y\right\Vert =1\). Therefore, we have \(y_{i}^{2}\leq 1\) for any \(i\leq n\).

As a consequence:
\[
y'Dy=\sum_{i=1}^{n}y_{i}^{2}\lambda_{i}\leq\lambda_{1}\sum_{i=1}^{n}y_{i}^{2}=\lambda_{1}.
\]

It is easily seen that the maximum is reached for \(y=\left[1,0,\cdots,0\right]'\). Therefore, the maximum of the optimization program (Eq. \eqref{eq:PCA11}) is obtained for \(u=P\left[1,0,\cdots,0\right]'\). That is, \(u\) is the eigenvector of \(X'X\) that is associated with its larger eigenvalue (first column of \(P\)).

Let us denote with \(F\) the vector that is given by the matrix product \(XP\) (note that its last column is equal to \(Xu\)). The columns of \(F\), denoted by \(F_{j}\), are called \textbf{factors}. We have:
\[
F'F=P'X'XP=D.
\]
Therefore, in particular, the \(F_{j}\)'s are orthogonal.

Since \(X=FP'\), the \(X_{j}\)'s are linear combinations of the factors. Let us then denote with \(\hat{X}_{i,j}\) the part of \(X_{i}\) that is explained by factor \(F_{j}\), we have:
\begin{eqnarray*}
\hat{X}_{i,j} & = & p_{ij}F_{j}\\
X_{i} & = & \sum_{j}\hat{X}_{i,j}=\sum_{j}p_{ij}F_{j}.
\end{eqnarray*}

Consider the share of variance that is explained --through the \(n\) variables (\(X_{1},\ldots,X_{n}\))-- by the first factor \(F_{1}\):
\begin{eqnarray*}
\frac{\sum_{i}\hat{X}_{i,1}\hat{X}'_{i,1}}{\sum_{i}X_{i}X'_{i}} & = & \frac{\sum_{i}p_{i1}F_{1}F'_{1}p_{i1}}{tr(X'X)} = \frac{\sum_{i}p_{i1}^{2}\lambda_{1}}{tr(X'X)} = \frac{\lambda_{1}}{\sum_{i}\lambda_{i}}.
\end{eqnarray*}

Intuitively, if the first eigenvalue is large, it means that the first factor embed a large share of the fluctutaions of the \(n\) \(X_{i}\)'s.

Let us illustrate PCA on the term structure of yields. The term strucutre of yields (or yield curve) is know to be driven by only a small number of factors (e.g., \citet{Litterman_Scheinkman_1991}). One can typically employ PCA to recover such factors. The data used in the example below are taken from the \href{https://fred.stlouisfed.org}{Fred database} (tickers: ``DGS6MO'',``DGS1'', \ldots). The second plot shows the factor loardings, that indicate that the first factor is a level factor (loadings = black line), the second factor is a slope factor (loadings = blue line), the third factor is a curvature factor (loadings = red line).

To run a PCA, one simply has to apply function \texttt{prcomp} to a matrix of data:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(AEC)}
\NormalTok{USyields }\OtherTok{\textless{}{-}}\NormalTok{ USyields[}\FunctionTok{complete.cases}\NormalTok{(USyields),]}
\NormalTok{yds }\OtherTok{\textless{}{-}}\NormalTok{ USyields[}\FunctionTok{c}\NormalTok{(}\StringTok{"Y1"}\NormalTok{,}\StringTok{"Y2"}\NormalTok{,}\StringTok{"Y3"}\NormalTok{,}\StringTok{"Y5"}\NormalTok{,}\StringTok{"Y7"}\NormalTok{,}\StringTok{"Y10"}\NormalTok{,}\StringTok{"Y20"}\NormalTok{,}\StringTok{"Y30"}\NormalTok{)]}
\NormalTok{PCA.yds }\OtherTok{\textless{}{-}} \FunctionTok{prcomp}\NormalTok{(yds,}\AttributeTok{center=}\ConstantTok{TRUE}\NormalTok{,}\AttributeTok{scale. =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Let us know visualize some results. The first plot of Figure \ref{fig:USydsPCA1} shows the share of total variance explained by the different principal components (PCs). The second plot shows the facotr loadings. The two bottom plots show how yields (in black) are fitted by linear combinations of the first two PCs only.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\FunctionTok{par}\NormalTok{(}\AttributeTok{plt=}\FunctionTok{c}\NormalTok{(.}\DecValTok{1}\NormalTok{,.}\DecValTok{95}\NormalTok{,.}\DecValTok{2}\NormalTok{,.}\DecValTok{8}\NormalTok{))}
\FunctionTok{barplot}\NormalTok{(PCA.yds}\SpecialCharTok{$}\NormalTok{sdev}\SpecialCharTok{\^{}}\DecValTok{2}\SpecialCharTok{/}\FunctionTok{sum}\NormalTok{(PCA.yds}\SpecialCharTok{$}\NormalTok{sdev}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{),}
        \AttributeTok{main=}\StringTok{"Share of variance expl. by PC\textquotesingle{}s"}\NormalTok{)}
\FunctionTok{axis}\NormalTok{(}\DecValTok{1}\NormalTok{, }\AttributeTok{at=}\DecValTok{1}\SpecialCharTok{:}\FunctionTok{dim}\NormalTok{(yds)[}\DecValTok{2}\NormalTok{], }\AttributeTok{labels=}\FunctionTok{colnames}\NormalTok{(PCA.yds}\SpecialCharTok{$}\NormalTok{x))}
\NormalTok{nb.PC }\OtherTok{\textless{}{-}} \DecValTok{2}
\FunctionTok{plot}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{PCA.yds}\SpecialCharTok{$}\NormalTok{rotation[,}\DecValTok{1}\NormalTok{],}\AttributeTok{type=}\StringTok{"l"}\NormalTok{,}\AttributeTok{lwd=}\DecValTok{2}\NormalTok{,}\AttributeTok{ylim=}\FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{),}
     \AttributeTok{main=}\StringTok{"Factor loadings (1st 3 PCs)"}\NormalTok{,}\AttributeTok{xaxt=}\StringTok{"n"}\NormalTok{,}\AttributeTok{xlab=}\StringTok{""}\NormalTok{)}
\FunctionTok{axis}\NormalTok{(}\DecValTok{1}\NormalTok{, }\AttributeTok{at=}\DecValTok{1}\SpecialCharTok{:}\FunctionTok{dim}\NormalTok{(yds)[}\DecValTok{2}\NormalTok{], }\AttributeTok{labels=}\FunctionTok{colnames}\NormalTok{(yds))}
\FunctionTok{lines}\NormalTok{(PCA.yds}\SpecialCharTok{$}\NormalTok{rotation[,}\DecValTok{2}\NormalTok{],}\AttributeTok{type=}\StringTok{"l"}\NormalTok{,}\AttributeTok{lwd=}\DecValTok{2}\NormalTok{,}\AttributeTok{col=}\StringTok{"blue"}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(PCA.yds}\SpecialCharTok{$}\NormalTok{rotation[,}\DecValTok{3}\NormalTok{],}\AttributeTok{type=}\StringTok{"l"}\NormalTok{,}\AttributeTok{lwd=}\DecValTok{2}\NormalTok{,}\AttributeTok{col=}\StringTok{"red"}\NormalTok{)}
\NormalTok{Y1.hat }\OtherTok{\textless{}{-}}\NormalTok{ PCA.yds}\SpecialCharTok{$}\NormalTok{x[,}\DecValTok{1}\SpecialCharTok{:}\NormalTok{nb.PC] }\SpecialCharTok{\%*\%}\NormalTok{ PCA.yds}\SpecialCharTok{$}\NormalTok{rotation[}\StringTok{"Y1"}\NormalTok{,}\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{]}
\NormalTok{Y1.hat }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(USyields}\SpecialCharTok{$}\NormalTok{Y1) }\SpecialCharTok{+} \FunctionTok{sd}\NormalTok{(USyields}\SpecialCharTok{$}\NormalTok{Y1) }\SpecialCharTok{*}\NormalTok{ Y1.hat}
\FunctionTok{plot}\NormalTok{(USyields}\SpecialCharTok{$}\NormalTok{date,USyields}\SpecialCharTok{$}\NormalTok{Y1,}\AttributeTok{type=}\StringTok{"l"}\NormalTok{,}\AttributeTok{lwd=}\DecValTok{2}\NormalTok{,}
     \AttributeTok{main=}\StringTok{"Fit of 1{-}year yields (2 PCs)"}\NormalTok{,}
     \AttributeTok{ylab=}\StringTok{"Obs (black) / Fitted by 2PCs (dashed blue)"}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(USyields}\SpecialCharTok{$}\NormalTok{date,Y1.hat,}\AttributeTok{col=}\StringTok{"blue"}\NormalTok{,}\AttributeTok{lty=}\DecValTok{2}\NormalTok{,}\AttributeTok{lwd=}\DecValTok{2}\NormalTok{)}
\NormalTok{Y10.hat }\OtherTok{\textless{}{-}}\NormalTok{ PCA.yds}\SpecialCharTok{$}\NormalTok{x[,}\DecValTok{1}\SpecialCharTok{:}\NormalTok{nb.PC] }\SpecialCharTok{\%*\%}\NormalTok{ PCA.yds}\SpecialCharTok{$}\NormalTok{rotation[}\StringTok{"Y10"}\NormalTok{,}\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{]}
\NormalTok{Y10.hat }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(USyields}\SpecialCharTok{$}\NormalTok{Y10) }\SpecialCharTok{+} \FunctionTok{sd}\NormalTok{(USyields}\SpecialCharTok{$}\NormalTok{Y10) }\SpecialCharTok{*}\NormalTok{ Y10.hat}
\FunctionTok{plot}\NormalTok{(USyields}\SpecialCharTok{$}\NormalTok{date,USyields}\SpecialCharTok{$}\NormalTok{Y10,}\AttributeTok{type=}\StringTok{"l"}\NormalTok{,}\AttributeTok{lwd=}\DecValTok{2}\NormalTok{,}
     \AttributeTok{main=}\StringTok{"Fit of 10{-}year yields (2 PCs)"}\NormalTok{,}
     \AttributeTok{ylab=}\StringTok{"Obs (black) / Fitted by 2PCs (dashed blue)"}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(USyields}\SpecialCharTok{$}\NormalTok{date,Y10.hat,}\AttributeTok{col=}\StringTok{"blue"}\NormalTok{,}\AttributeTok{lty=}\DecValTok{2}\NormalTok{,}\AttributeTok{lwd=}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\includegraphics{MicroEc_files/figure-latex/USydsPCA1-1} \caption{Some PCA results. The dataset contains 8 time series of U.S. interest rates of different maturities.}\label{fig:USydsPCA1}
\end{figure}

\hypertarget{LinAlgebra}{%
\section{Linear algebra: definitions and results}\label{LinAlgebra}}

\begin{definition}[Eigenvalues]
\protect\hypertarget{def:determinant}{}\label{def:determinant}The eigenvalues of of a matrix \(M\) are the numbers \(\lambda\) for which:
\[
|M - \lambda I| = 0,
\]
where \(| \bullet |\) is the determinant operator.
\end{definition}

\begin{proposition}[Properties of the determinant]
\protect\hypertarget{prp:determinant}{}\label{prp:determinant}

We have:

\begin{itemize}
\tightlist
\item
  \(|MN|=|M|\times|N|\).
\item
  \(|M^{-1}|=|M|^{-1}\).
\item
  If \(M\) admits the diagonal representation \(M=TDT^{-1}\), where \(D\) is a diagonal matrix whose diagonal entries are \(\{\lambda_i\}_{i=1,\dots,n}\), then:
  \[
  |M - \lambda I |=\prod_{i=1}^n (\lambda_i - \lambda).
  \]
\end{itemize}

\end{proposition}

\begin{definition}[Moore-Penrose inverse]
\protect\hypertarget{def:MoorPenrose}{}\label{def:MoorPenrose}

If \(M \in \mathbb{R}^{m \times n}\), then its Moore-Penrose pseudo inverse (exists and) is the unique matrix \(M^* \in \mathbb{R}^{n \times m}\) that satisfies:

\begin{enumerate}
\def\labelenumi{\roman{enumi}.}
\tightlist
\item
  \(M M^* M = M\)
\item
  \(M^* M M^* = M^*\)
\item
  \((M M^*)'=M M^*\)
  .iv \((M^* M)'=M^* M\).
\end{enumerate}

\end{definition}

\begin{proposition}[Properties of the Moore-Penrose inverse]
\protect\hypertarget{prp:MoorPenrose}{}\label{prp:MoorPenrose}

\begin{itemize}
\tightlist
\item
  If \(M\) is invertible then \(M^* = M^{-1}\).
\item
  The pseudo-inverse of a zero matrix is its transpose.
  *

  \item*

  The pseudo-inverse of the pseudo-inverse is the original matrix.
\end{itemize}

\end{proposition}

\begin{definition}[Idempotent matrix]
\protect\hypertarget{def:idempotent}{}\label{def:idempotent}Matrix \(M\) is idempotent if \(M^2=M\).

If \(M\) is a symmetric idempotent matrix, then \(M'M=M\).
\end{definition}

\begin{proposition}[Roots of an idempotent matrix]
\protect\hypertarget{prp:rootsidempotent}{}\label{prp:rootsidempotent}The eigenvalues of an idempotent matrix are either 1 or 0.
\end{proposition}

\begin{proof}
If \(\lambda\) is an eigenvalue of an idempotent matrix \(M\) then \(\exists x \ne 0\) s.t. \(Mx=\lambda x\). Hence \(M^2x=\lambda M x \Rightarrow (1-\lambda)Mx=0\). Either all element of \(Mx\) are zero, in which case \(\lambda=0\) or at least one element of \(Mx\) is nonzero, in which case \(\lambda=1\).
\end{proof}

\begin{proposition}[Idempotent matrix and chi-square distribution]
\protect\hypertarget{prp:chi2idempotent}{}\label{prp:chi2idempotent}The rank of a symmetric idempotent matrix is equal to its trace.
\end{proposition}

\begin{proof}
The result follows from Prop. \ref{prp:rootsidempotent}, combined with the fact that the rank of a symmetric matrix is equal to the number of its nonzero eigenvalues.
\end{proof}

\begin{proposition}[Constrained least squares]
\protect\hypertarget{prp:constrainedLS}{}\label{prp:constrainedLS}The solution of the following optimisation problem:
\begin{eqnarray*}
\underset{\boldsymbol\beta}{\min} && || \mathbf{y} - \mathbf{X}\boldsymbol\beta ||^2 \\
&& \mbox{subject to } \mathbf{R}\boldsymbol\beta = \mathbf{q}
\end{eqnarray*}
is given by:
\[
\boxed{\boldsymbol\beta^r = \boldsymbol\beta_0 - (\mathbf{X}'\mathbf{X})^{-1} \mathbf{R}'\{\mathbf{R}(\mathbf{X}'\mathbf{X})^{-1}\mathbf{R}'\}^{-1}(\mathbf{R}\boldsymbol\beta_0 - \mathbf{q}),}
\]
where \(\boldsymbol\beta_0=(\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\mathbf{y}\).
\end{proposition}

\begin{proof}
See for instance \href{http://jackman.stanford.edu/classes/350B/07/ftestforWeb.pdf}{Jackman, 2007}.
\end{proof}

\begin{proposition}[Inverse of a partitioned matrix]
\protect\hypertarget{prp:inversepartitioned}{}\label{prp:inversepartitioned}We have:
\begin{eqnarray*}
&&\left[ \begin{array}{cc} \mathbf{A}_{11} & \mathbf{A}_{12} \\ \mathbf{A}_{21} & \mathbf{A}_{22} \end{array}\right]^{-1} = \\
&&\left[ \begin{array}{cc} (\mathbf{A}_{11} - \mathbf{A}_{12}\mathbf{A}_{22}^{-1}\mathbf{A}_{21})^{-1} & - \mathbf{A}_{11}^{-1}\mathbf{A}_{12}(\mathbf{A}_{22} - \mathbf{A}_{21}\mathbf{A}_{11}^{-1}\mathbf{A}_{12})^{-1} \\
-(\mathbf{A}_{22} - \mathbf{A}_{21}\mathbf{A}_{11}^{-1}\mathbf{A}_{12})^{-1}\mathbf{A}_{21}\mathbf{A}_{11}^{-1} & (\mathbf{A}_{22} - \mathbf{A}_{21}\mathbf{A}_{11}^{-1}\mathbf{A}_{12})^{-1} \end{array} \right].
\end{eqnarray*}
\end{proposition}

\begin{definition}[Matrix derivatives]
\protect\hypertarget{def:FOD}{}\label{def:FOD}Consider a fonction \(f: \mathbb{R}^K \rightarrow \mathbb{R}\). Its first-order derivative is:
\[
\frac{\partial f}{\partial \mathbf{b}}(\mathbf{b}) =
\left[\begin{array}{c}
\frac{\partial f}{\partial b_1}(\mathbf{b})\\
\vdots\\
\frac{\partial f}{\partial b_K}(\mathbf{b})
\end{array}
\right].
\]
We use the notation:
\[
\frac{\partial f}{\partial \mathbf{b}'}(\mathbf{b}) = \left(\frac{\partial f}{\partial \mathbf{b}}(\mathbf{b})\right)'.
\]
\end{definition}

\begin{proposition}
\protect\hypertarget{prp:partial}{}\label{prp:partial}

We have:

\begin{itemize}
\tightlist
\item
  If \(f(\mathbf{b}) = A' \mathbf{b}\) where \(A\) is a \(K \times 1\) vector then \(\frac{\partial f}{\partial \mathbf{b}}(\mathbf{b}) = A\).
\item
  If \(f(\mathbf{b}) = \mathbf{b}'A\mathbf{b}\) where \(A\) is a \(K \times K\) matrix, then \(\frac{\partial f}{\partial \mathbf{b}}(\mathbf{b}) = 2A\mathbf{b}\).
\end{itemize}

\end{proposition}

\begin{proposition}[Square and absolute summability]
\protect\hypertarget{prp:absMs}{}\label{prp:absMs}We have:
\[
\underbrace{\sum_{i=0}^{\infty}|\theta_i| < + \infty}_{\mbox{Absolute summability}} \Rightarrow \underbrace{\sum_{i=0}^{\infty} \theta_i^2 < + \infty}_{\mbox{Square summability}}.
\]
\end{proposition}

\begin{proof}
See Appendix 3.A in Hamilton. Idea: Absolute summability implies that there exist \(N\) such that, for \(j>N\), \(|\theta_j| < 1\) (deduced from Cauchy criterion, Theorem \ref{thm:cauchycritstatic} and therefore \(\theta_j^2 < |\theta_j|\).
\end{proof}

\hypertarget{variousResults}{%
\section{Statistical analysis: definitions and results}\label{variousResults}}

\hypertarget{moments-and-statistics}{%
\subsection{Moments and statistics}\label{moments-and-statistics}}

\begin{definition}[Partial correlation]
\protect\hypertarget{def:partialcorrel}{}\label{def:partialcorrel}The \textbf{partial correlation} between \(y\) and \(z\), controlling for some variables \(\mathbf{X}\) is the sample correlation between \(y^*\) and \(z^*\), where the latter two variables are the residuals in regressions of \(y\) on \(\mathbf{X}\) and of \(z\) on \(\mathbf{X}\), respectively.

This correlation is denoted by \(r_{yz}^\mathbf{X}\). By definition, we have:
\begin{equation}
r_{yz}^\mathbf{X} = \frac{\mathbf{z^*}'\mathbf{y^*}}{\sqrt{(\mathbf{z^*}'\mathbf{z^*})(\mathbf{y^*}'\mathbf{y^*})}}.\label{eq:pc}
\end{equation}
\end{definition}

\begin{definition}[Skewness and kurtosis]
\protect\hypertarget{def:skewnesskurtosis}{}\label{def:skewnesskurtosis}

Let \(Y\) be a random variable whose fourth moment exists. The expectation of \(Y\) is denoted by \(\mu\).

\begin{itemize}
\tightlist
\item
  The skewness of \(Y\) is given by:
  \[
  \frac{\mathbb{E}[(Y-\mu)^3]}{\{\mathbb{E}[(Y-\mu)^2]\}^{3/2}}.
  \]
\item
  The kurtosis of \(Y\) is given by:
  \[
  \frac{\mathbb{E}[(Y-\mu)^4]}{\{\mathbb{E}[(Y-\mu)^2]\}^{2}}.
  \]
\end{itemize}

\end{definition}

\begin{theorem}[Cauchy-Schwarz inequality]
\protect\hypertarget{thm:CauchySchwarz}{}\label{thm:CauchySchwarz}We have:
\[
|\mathbb{C}ov(X,Y)| \le \sqrt{\mathbb{V}ar(X)\mathbb{V}ar(Y)}
\]
and, if \(X \ne =\) and \(Y \ne 0\), the equality holds iff \(X\) and \(Y\) are the same up to an affine transformation.
\end{theorem}

\begin{proof}
If \(\mathbb{V}ar(X)=0\), this is trivial. If this is not the case, then let's define \(Z\) as \(Z = Y - \frac{\mathbb{C}ov(X,Y)}{\mathbb{V}ar(X)}X\). It is easily seen that \(\mathbb{C}ov(X,Z)=0\). Then, the variance of \(Y=Z+\frac{\mathbb{C}ov(X,Y)}{\mathbb{V}ar(X)}X\) is equal to the sum of the variance of \(Z\) and of the variance of \(\frac{\mathbb{C}ov(X,Y)}{\mathbb{V}ar(X)}X\), that is:
\[
\mathbb{V}ar(Y) = \mathbb{V}ar(Z) + \left(\frac{\mathbb{C}ov(X,Y)}{\mathbb{V}ar(X)}\right)^2\mathbb{V}ar(X) \ge \left(\frac{\mathbb{C}ov(X,Y)}{\mathbb{V}ar(X)}\right)^2\mathbb{V}ar(X).
\]
The equality holds iff \(\mathbb{V}ar(Z)=0\), i.e.~iff \(Y = \frac{\mathbb{C}ov(X,Y)}{\mathbb{V}ar(X)}X+cst\).
\end{proof}

\begin{definition}[Asymptotic level]
\protect\hypertarget{def:asmyptlevel}{}\label{def:asmyptlevel}An asymptotic test with critical region \(\Omega_n\) has an asymptotic level equal to \(\alpha\) if:
\[
\underset{\theta \in \Theta}{\mbox{sup}} \quad \underset{n \rightarrow \infty}{\mbox{lim}} \mathbb{P}_\theta (S_n \in \Omega_n) = \alpha,
\]
where \(S_n\) is the test statistic and \(\Theta\) is such that the null hypothesis \(H_0\) is equivalent to \(\theta \in \Theta\).
\end{definition}

\begin{definition}[Asymptotically consistent test]
\protect\hypertarget{def:asmyptconsisttest}{}\label{def:asmyptconsisttest}An asymptotic test with critical region \(\Omega_n\) is consistent if:
\[
\forall \theta \in \Theta^c, \quad \mathbb{P}_\theta (S_n \in \Omega_n) \rightarrow 1,
\]
where \(S_n\) is the test statistic and \(\Theta^c\) is such that the null hypothesis \(H_0\) is equivalent to \(\theta \notin \Theta^c\).
\end{definition}

\begin{definition}[Kullback discrepancy]
\protect\hypertarget{def:Kullback}{}\label{def:Kullback}Given two p.d.f. \(f\) and \(f^*\), the Kullback discrepancy is defined by:
\[
I(f,f^*) = \mathbb{E}^* \left( \log \frac{f^*(Y)}{f(Y)} \right) = \int \log \frac{f^*(y)}{f(y)} f^*(y) dy.
\]
\end{definition}

\begin{proposition}[Properties of the Kullback discrepancy]
\protect\hypertarget{prp:Kullback}{}\label{prp:Kullback}

We have:

\begin{enumerate}
\def\labelenumi{\roman{enumi}.}
\tightlist
\item
  \(I(f,f^*) \ge 0\)
\item
  \(I(f,f^*) = 0\) iff \(f \equiv f^*\).
\end{enumerate}

\end{proposition}

\begin{proof}
\(x \rightarrow -\log(x)\) is a convex function. Therefore \(\mathbb{E}^*(-\log f(Y)/f^*(Y)) \ge -\log \mathbb{E}^*(f(Y)/f^*(Y)) = 0\) (proves (i)). Since \(x \rightarrow -\log(x)\) is strictly convex, equality in (i) holds if and only if \(f(Y)/f^*(Y)\) is constant (proves (ii)).
\end{proof}

\begin{definition}[Characteristic function]
\protect\hypertarget{def:characteristic}{}\label{def:characteristic}For any real-valued random variable \(X\), the characteristic function is defined by:
\[
\phi_X: u \rightarrow \mathbb{E}[\exp(iuX)].
\]
\end{definition}

\hypertarget{standard-distributions}{%
\subsection{Standard distributions}\label{standard-distributions}}

\begin{definition}[F distribution]
\protect\hypertarget{def:fstatistics}{}\label{def:fstatistics}Consider \(n=n_1+n_2\) i.i.d. \(\mathcal{N}(0,1)\) r.v. \(X_i\). If the r.v. \(F\) is defined by:
\[
F = \frac{\sum_{i=1}^{n_1} X_i^2}{\sum_{j=n_1+1}^{n_1+n_2} X_j^2}\frac{n_2}{n_1}
\]
then \(F \sim \mathcal{F}(n_1,n_2)\). (See Table \ref{tab:Fstat} for quantiles.)
\end{definition}

\begin{definition}[Student-t distribution]
\protect\hypertarget{def:tStudent}{}\label{def:tStudent}\(Z\) follows a Student-t (or \(t\)) distribution with \(\nu\) degrees of freedom (d.f.) if:
\[
Z = X_0 \bigg/ \sqrt{\frac{\sum_{i=1}^{\nu}X_i^2}{\nu}}, \quad X_i \sim i.i.d. \mathcal{N}(0,1).
\]
We have \(\mathbb{E}(Z)=0\), and \(\mathbb{V}ar(Z)=\frac{\nu}{\nu-2}\) if \(\nu>2\). (See Table \ref{tab:Student} for quantiles.)
\end{definition}

\begin{definition}[Chi-square distribution]
\protect\hypertarget{def:chi2}{}\label{def:chi2}\(Z\) follows a \(\chi^2\) distribution with \(\nu\) d.f. if \(Z = \sum_{i=1}^{\nu}X_i^2\) where \(X_i \sim i.i.d. \mathcal{N}(0,1)\).
We have \(\mathbb{E}(Z)=\nu\). (See Table \ref{tab:Chi2} for quantiles.)
\end{definition}

\begin{definition}[Cauchy distribution]
\protect\hypertarget{def:Cauchy}{}\label{def:Cauchy}

The probability distribution function of the Cauchy distribution defined by a location parameter \(\mu\) and a scale parameter \(\gamma\) is:
\[
f(x) = \frac{1}{\pi \gamma \left(1 + \left[\frac{x-\mu}{\gamma}\right]^2\right)}.
\]
The mean and variance of this distribution are undefined.

\begin{figure}
\includegraphics[width=0.95\linewidth]{MicroEc_files/figure-latex/Cauchy-1} \caption{Pdf of the Cauchy distribution ($\mu=0$, $\gamma=1$).}\label{fig:Cauchy}
\end{figure}

\end{definition}

\begin{proposition}[Inner product of a multivariate Gaussian variable]
\protect\hypertarget{prp:waldtypeproduct}{}\label{prp:waldtypeproduct}Let \(X\) be a \(n\)-dimensional multivariate Gaussian variable: \(X \sim \mathcal{N}(0,\Sigma)\). We have:
\[
X' \Sigma^{-1}X \sim \chi^2(n).
\]
\end{proposition}

\begin{proof}
Because \(\Sigma\) is a symmetrical definite positive matrix, it admits the spectral decomposition \(PDP'\) where \(P\) is an orthogonal matrix (i.e.~\(PP'=Id\)) and D is a diagonal matrix with non-negative entries. Denoting by \(\sqrt{D^{-1}}\) the diagonal matrix whose diagonal entries are the inverse of those of \(D\), it is easily checked that the covariance matrix of \(Y:=\sqrt{D^{-1}}P'X\) is \(Id\). Therefore \(Y\) is a vector of uncorrelated Gaussian variables. The properties of Gaussian variables imply that the components of \(Y\) are then also independent. Hence \(Y'Y=\sum_i Y_i^2 \sim \chi^2(n)\).

It remains to note that \(Y'Y=X'PD^{-1}P'X=X'\mathbb{V}ar(X)^{-1}X\) to conclude.
\end{proof}

\begin{definition}[Generalized Extreme Value (GEV) distribution]
\protect\hypertarget{def:GEVdistri}{}\label{def:GEVdistri}The vector of disturbances \(\boldsymbol\varepsilon=[\varepsilon_{1,1},\dots,\varepsilon_{1,K_1},\dots,\varepsilon_{J,1},\dots,\varepsilon_{J,K_J}]'\) follows the Generalized Extreme Value (GEV) distribution if its c.d.f. is:
\[
F(\boldsymbol\varepsilon,\boldsymbol\rho) = \exp(-G(e^{-\varepsilon_{1,1}},\dots,e^{-\varepsilon_{J,K_J}};\boldsymbol\rho))
\]
with
\begin{eqnarray*}
G(\mathbf{Y};\boldsymbol\rho) &\equiv&  G(Y_{1,1},\dots,Y_{1,K_1},\dots,Y_{J,1},\dots,Y_{J,K_J};\boldsymbol\rho) \\
&=& \sum_{j=1}^J\left(\sum_{k=1}^{K_j} Y_{jk}^{1/\rho_j}
\right)^{\rho_j}
\end{eqnarray*}
\end{definition}

\hypertarget{StochConvergences}{%
\subsection{Stochastic convergences}\label{StochConvergences}}

\begin{proposition}[Chebychev's inequality]
\protect\hypertarget{prp:chebychev}{}\label{prp:chebychev}If \(\mathbb{E}(|X|^r)\) is finite for some \(r>0\) then:
\[
\forall \varepsilon > 0, \quad \mathbb{P}(|X - c|>\varepsilon) \le \frac{\mathbb{E}[|X - c|^r]}{\varepsilon^r}.
\]
In particular, for \(r=2\):
\[
\forall \varepsilon > 0, \quad \mathbb{P}(|X - c|>\varepsilon) \le \frac{\mathbb{E}[(X - c)^2]}{\varepsilon^2}.
\]
\end{proposition}

\begin{proof}
Remark that \(\varepsilon^r \mathbb{I}_{\{|X| \ge \varepsilon\}} \le |X|^r\) and take the expectation of both sides.
\end{proof}

\begin{definition}[Convergence in probability]
\protect\hypertarget{def:convergenceproba}{}\label{def:convergenceproba}The random variable sequence \(x_n\) converges in probability to a constant \(c\) if \(\forall \varepsilon\), \(\lim_{n \rightarrow \infty} \mathbb{P}(|x_n - c|>\varepsilon) = 0\).

It is denoted as: \(\mbox{plim } x_n = c\).
\end{definition}

\begin{definition}[Convergence in the Lr norm]
\protect\hypertarget{def:convergenceLr}{}\label{def:convergenceLr}\(x_n\) converges in the \(r\)-th mean (or in the \(L^r\)-norm) towards \(x\), if \(\mathbb{E}(|x_n|^r)\) and \(\mathbb{E}(|x|^r)\) exist and if
\[
\lim_{n \rightarrow \infty} \mathbb{E}(|x_n - x|^r) = 0.
\]
It is denoted as: \(x_n \overset{L^r}{\rightarrow} c\).

For \(r=2\), this convergence is called \textbf{mean square convergence}.
\end{definition}

\begin{definition}[Almost sure convergence]
\protect\hypertarget{def:convergenceAlmost}{}\label{def:convergenceAlmost}The random variable sequence \(x_n\) converges almost surely to \(c\) if \(\mathbb{P}(\lim_{n \rightarrow \infty} x_n = c) = 1\).

It is denoted as: \(x_n \overset{a.s.}{\rightarrow} c\).
\end{definition}

\begin{definition}[Convergence in distribution]
\protect\hypertarget{def:cvgceDistri}{}\label{def:cvgceDistri}\(x_n\) is said to converge in distribution (or in law) to \(x\) if
\[
\lim_{n \rightarrow \infty} F_{x_n}(s) = F_{x}(s)
\]
for all \(s\) at which \(F_X\) --the cumulative distribution of \(X\)-- is continuous.

It is denoted as: \(x_n \overset{d}{\rightarrow} x\).
\end{definition}

\begin{proposition}[Rules for limiting distributions (Slutsky)]
\protect\hypertarget{prp:Slutsky}{}\label{prp:Slutsky}

We have:

\begin{enumerate}
\def\labelenumi{\roman{enumi}.}
\item
  \textbf{Slutsky's theorem:} If \(x_n \overset{d}{\rightarrow} x\) and \(y_n \overset{p}{\rightarrow} c\) then
  \begin{eqnarray*}
  x_n y_n &\overset{d}{\rightarrow}& x c \\
  x_n + y_n &\overset{d}{\rightarrow}& x + c \\
  x_n/y_n &\overset{d}{\rightarrow}& x / c \quad (\mbox{if }c \ne 0)
  \end{eqnarray*}
\item
  \textbf{Continuous mapping theorem:} If \(x_n \overset{d}{\rightarrow} x\) and \(g\) is a continuous function then \(g(x_n) \overset{d}{\rightarrow} g(x).\)
\end{enumerate}

\end{proposition}

\begin{proposition}[Implications of stochastic convergences]
\protect\hypertarget{prp:implicationsconv}{}\label{prp:implicationsconv}We have:
\begin{align*}
&\boxed{\overset{L^s}{\rightarrow}}& &\underset{1 \le r \le s}{\Rightarrow}& &\boxed{\overset{L^r}{\rightarrow}}&\\
&& && &\Downarrow&\\
&\boxed{\overset{a.s.}{\rightarrow}}& &\Rightarrow& &\boxed{\overset{p}{\rightarrow}}& \Rightarrow \qquad \boxed{\overset{d}{\rightarrow}}.
\end{align*}
\end{proposition}

\begin{proof}
(of the fact that \(\left(\overset{p}{\rightarrow}\right) \Rightarrow \left( \overset{d}{\rightarrow}\right)\)). Assume that \(X_n \overset{p}{\rightarrow} X\). Denoting by \(F\) and \(F_n\) the c.d.f. of \(X\) and \(X_n\), respectively:
\begin{eqnarray*}
F_n(x) &=& \mathbb{P}(X_n \le x,X\le x+\varepsilon) + \mathbb{P}(X_n \le x,X > x+\varepsilon)\\
&\le& F(x+\varepsilon) + \mathbb{P}(|X_n - X|>\varepsilon).\label{eq:convgce1}
\end{eqnarray*}
Besides,
\begin{eqnarray*}
F(x-\varepsilon) &=& \mathbb{P}(X \le x-\varepsilon,X_n \le x) + \mathbb{P}(X \le x-\varepsilon,X_n > x)\\
&\le& F_n(x) + \mathbb{P}(|X_n - X|>\varepsilon),
\end{eqnarray*}
which implies:
\begin{equation}
F(x-\varepsilon) - \mathbb{P}(|X_n - X|>\varepsilon) \le F_n(x).\label{eq:convgce2}
\end{equation}
Eqs. \eqref{eq:convgce1} and \eqref{eq:convgce2} imply:
\[
F(x-\varepsilon) - \mathbb{P}(|X_n - X|>\varepsilon) \le F_n(x)  \le F(x+\varepsilon) + \mathbb{P}(|X_n - X|>\varepsilon).
\]
Taking limits as \(n \rightarrow \infty\) yields
\[
F(x-\varepsilon) \le \underset{n \rightarrow \infty}{\mbox{lim inf}}\; F_n(x) \le \underset{n \rightarrow \infty}{\mbox{lim sup}}\; F_n(x)  \le F(x+\varepsilon).
\]
The result is then obtained by taking limits as \(\varepsilon \rightarrow 0\) (if \(F\) is continuous at \(x\)).
\end{proof}

\begin{proposition}[Convergence in distribution to a constant]
\protect\hypertarget{prp:cvgce11}{}\label{prp:cvgce11}If \(X_n\) converges in distribution to a constant \(c\), then \(X_n\) converges in probability to \(c\).
\end{proposition}

\begin{proof}
If \(\varepsilon>0\), we have \(\mathbb{P}(X_n < c - \varepsilon) \underset{n \rightarrow \infty}{\rightarrow} 0\) i.e.~\(\mathbb{P}(X_n \ge c - \varepsilon) \underset{n \rightarrow \infty}{\rightarrow} 1\) and \(\mathbb{P}(X_n < c + \varepsilon) \underset{n \rightarrow \infty}{\rightarrow} 1\). Therefore \(\mathbb{P}(c - \varepsilon \le X_n < c + \varepsilon) \underset{n \rightarrow \infty}{\rightarrow} 1\),
which gives the result.
\end{proof}

\begin{example}[Convergence in probability but not $L^r$]
\protect\hypertarget{exm:plimButNotLr}{}\label{exm:plimButNotLr}Let \(\{x_n\}_{n \in \mathbb{N}}\) be a series of random variables defined by:
\[
x_n = n u_n,
\]
where \(u_n\) are independent random variables s.t. \(u_n \sim \mathcal{B}(1/n)\).

We have \(x_n \overset{p}{\rightarrow} 0\) but \(x_n \overset{L^r}{\nrightarrow} 0\) because \(\mathbb{E}(|X_n-0|)=\mathbb{E}(X_n)=1\).
\end{example}

\begin{theorem}[Cauchy criterion (non-stochastic case)]
\protect\hypertarget{thm:cauchycritstatic}{}\label{thm:cauchycritstatic}We have that \(\sum_{i=0}^{T} a_i\) converges (\(T \rightarrow \infty\)) iff, for any \(\eta > 0\), there exists an integer \(N\) such that, for all \(M\ge N\),
\[
\left|\sum_{i=N+1}^{M} a_i\right| < \eta.
\]
\end{theorem}

\begin{theorem}[Cauchy criterion (stochastic case)]
\protect\hypertarget{thm:cauchycritstochastic}{}\label{thm:cauchycritstochastic}We have that \(\sum_{i=0}^{T} \theta_i \varepsilon_{t-i}\) converges in mean square (\(T \rightarrow \infty\)) to a random variable iff, for any \(\eta > 0\), there exists an integer \(N\) such that, for all \(M\ge N\),
\[
\mathbb{E}\left[\left(\sum_{i=N+1}^{M} \theta_i \varepsilon_{t-i}\right)^2\right] < \eta.
\]
\end{theorem}

\hypertarget{CLTappend}{%
\subsection{Central limit theorem}\label{CLTappend}}

\begin{theorem}[Law of large numbers]
\protect\hypertarget{thm:LLNappendix}{}\label{thm:LLNappendix}The sample mean is a consistent estimator of the population mean.
\end{theorem}

\begin{proof}
Let's denote by \(\phi_{X_i}\) the characteristic function of a r.v. \(X_i\). If the mean of \(X_i\) is \(\mu\) then the Talyor expansion of the characteristic function is:
\[
\phi_{X_i}(u) = \mathbb{E}(\exp(iuX)) = 1 + iu\mu + o(u).
\]
The properties of the characteristic function (see Def. \ref{def:characteristic}) imply that:
\[
\phi_{\frac{1}{n}(X_1+\dots+X_n)}(u) = \prod_{i=1}^{n} \left(1 + i\frac{u}{n}\mu + o\left(\frac{u}{n}\right) \right) \rightarrow e^{iu\mu}.
\]
The facts that (a) \(e^{iu\mu}\) is the characteristic function of the constant \(\mu\) and (b) that a characteristic function uniquely characterises a distribution imply that the sample mean converges in distribution to the constant \(\mu\), which further implies that it converges in probability to \(\mu\).
\end{proof}

\begin{theorem}[Lindberg-Levy Central limit theorem, CLT]
\protect\hypertarget{thm:LindbergLevyCLT}{}\label{thm:LindbergLevyCLT}If \(x_n\) is an i.i.d. sequence of random variables with mean \(\mu\) and variance \(\sigma^2\) (\(\in ]0,+\infty[\)), then:
\[
\boxed{\sqrt{n} (\bar{x}_n - \mu) \overset{d}{\rightarrow} \mathcal{N}(0,\sigma^2), \quad \mbox{where} \quad \bar{x}_n = \frac{1}{n} \sum_{i=1}^{n} x_i.}
\]
\end{theorem}

\begin{proof}
Let us introduce the r.v. \(Y_n:= \sqrt{n}(\bar{X}_n - \mu)\). We have \(\phi_{Y_n}(u) = \left[ \mathbb{E}\left( \exp(i \frac{1}{\sqrt{n}} u (X_1 - \mu)) \right) \right]^n\). We have:
\begin{eqnarray*}
&&\left[ \mathbb{E}\left( \exp\left(i \frac{1}{\sqrt{n}} u (X_1 - \mu)\right) \right) \right]^n\\
&=& \left[ \mathbb{E}\left( 1 + i \frac{1}{\sqrt{n}} u (X_1 - \mu) - \frac{1}{2n} u^2 (X_1 - \mu)^2 + o(u^2) \right) \right]^n \\
&=& \left( 1 - \frac{1}{2n}u^2\sigma^2 + o(u^2)\right)^n.
\end{eqnarray*}
Therefore \(\phi_{Y_n}(u) \underset{n \rightarrow \infty}{\rightarrow} \exp \left( - \frac{1}{2}u^2\sigma^2 \right)\), which is the characteristic function of \(\mathcal{N}(0,\sigma^2)\).
\end{proof}

\hypertarget{GaussianVar}{%
\section{Some properties of Gaussian variables}\label{GaussianVar}}

\begin{proposition}
\protect\hypertarget{prp:bandsindependent}{}\label{prp:bandsindependent}If \(\mathbf{A}\) is idempotent and if \(\mathbf{x}\) is Gaussian, \(\mathbf{L}\mathbf{x}\) and \(\mathbf{x}'\mathbf{A}\mathbf{x}\) are independent if \(\mathbf{L}\mathbf{A}=\mathbf{0}\).
\end{proposition}

\begin{proof}
If \(\mathbf{L}\mathbf{A}=\mathbf{0}\), then the two Gaussian vectors \(\mathbf{L}\mathbf{x}\) and \(\mathbf{A}\mathbf{x}\) are independent. This implies the independence of any function of \(\mathbf{L}\mathbf{x}\) and any function of \(\mathbf{A}\mathbf{x}\). The results then follows from the observation that \(\mathbf{x}'\mathbf{A}\mathbf{x}=(\mathbf{A}\mathbf{x})'(\mathbf{A}\mathbf{x})\), which is a function of \(\mathbf{A}\mathbf{x}\).
\end{proof}

\begin{proposition}[Bayesian update in a vector of Gaussian variables]
\protect\hypertarget{prp:update}{}\label{prp:update}If
\[
\left[
\begin{array}{c}
Y_1\\
Y_2
\end{array}
\right]
\sim \mathcal{N}
\left(0,
\left[\begin{array}{cc}
\Omega_{11} & \Omega_{12}\\
\Omega_{21} & \Omega_{22}
\end{array}\right]
\right),
\]
then
\[
Y_{2}|Y_{1} \sim \mathcal{N}
\left(
\Omega_{21}\Omega_{11}^{-1}Y_{1},\Omega_{22}-\Omega_{21}\Omega_{11}^{-1}\Omega_{12}
\right).
\]
\[
Y_{1}|Y_{2} \sim \mathcal{N}
\left(
\Omega_{12}\Omega_{22}^{-1}Y_{2},\Omega_{11}-\Omega_{12}\Omega_{22}^{-1}\Omega_{21}
\right).
\]
\end{proposition}

\begin{proposition}[Truncated distributions]
\protect\hypertarget{prp:truncated}{}\label{prp:truncated}If \(X\) is a random variable distributed according to some p.d.f. \(f\), with c.d.f. \(F\), with infinite support. Then the p.d.f. of \(X|a \le X < b\) is
\[
g(x) = \frac{f(x)}{F(b)-F(a)}\mathbb{I}_{\{a \le x < b\}},
\]
for any \(a<b\).

In partiucular, for a Gaussian variable \(X \sim \mathcal{N}(\mu,\sigma^2)\), we have
\[
f(X=x|a\le X<b) = \dfrac{\dfrac{1}{\sigma}\phi\left(\dfrac{x - \mu}{\sigma}\right)}{Z}.
\]
with \(Z = \Phi(\beta)-\Phi(\alpha)\), where \(\alpha = \dfrac{a - \mu}{\sigma}\) and \(\beta = \dfrac{b - \mu}{\sigma}\).

Moreover:
\begin{eqnarray}
\mathbb{E}(X|a\le X<b) &=& \mu - \frac{\phi\left(\beta\right)-\phi\left(\alpha\right)}{Z}\sigma. \label{eq:Etrunc}
\end{eqnarray}

We also have:
\begin{eqnarray}
&& \mathbb{V}ar(X|a\le X<b) \nonumber\\
&=& \sigma^2\left[
1 -  \frac{\beta\phi\left(\beta\right)-\alpha\phi\left(\alpha\right)}{Z} -  \left(\frac{\phi\left(\beta\right)-\phi\left(\alpha\right)}{Z}\right)^2 \right] \label{eq:Vtrunc}
\end{eqnarray}

In particular, for \(b \rightarrow \infty\), we get:
\begin{equation}
\mathbb{V}ar(X|a < X) = \sigma^2\left[1 + \alpha\lambda(-\alpha) - \lambda(-\alpha)^2 \right], \label{eq:Vtrunc2}
\end{equation}
with \(\lambda(x)=\dfrac{\phi(x)}{\Phi(x)}\) is called the \textbf{inverse Mills ratio}.
\end{proposition}

Consider the case where \(a \rightarrow - \infty\) (i.e.~the conditioning set is \(X<b\)) and \(\mu=0\), \(\sigma=1\). Then Eq. \eqref{eq:Etrunc} gives \(\mathbb{E}(X|X<b) = - \lambda(b) = - \dfrac{\phi(b)}{\Phi(b)}\), where \(\lambda\) is the function computing the inverse Mills ratio.

\begin{figure}
\includegraphics{MicroEc_files/figure-latex/inverseMills-1} \caption{$\mathbb{E}(X|X<b)$ as a function of $b$ when $X\sim \mathcal{N}(0,1)$ (in black).}\label{fig:inverseMills}
\end{figure}

\begin{proposition}[p.d.f. of a multivariate Gaussian variable]
\protect\hypertarget{prp:pdfMultivarGaussian}{}\label{prp:pdfMultivarGaussian}If \(Y \sim \mathcal{N}(\mu,\Omega)\) and if \(Y\) is a \(n\)-dimensional vector, then the density function of \(Y\) is:
\[
\frac{1}{(2 \pi)^{n/2}|\Omega|^{1/2}}\exp\left[-\frac{1}{2}\left(Y-\mu\right)'\Omega^{-1}\left(Y-\mu\right)\right].
\]
\end{proposition}

\hypertarget{AppendixProof}{%
\section{Proofs}\label{AppendixProof}}

\textbf{Proof of Proposition \ref{prp:MLEproperties}}

\begin{proof}
Assumptions (i) and (ii) (in the set of Assumptions \ref{hyp:MLEregularity}) imply that \(\boldsymbol\theta_{MLE}\) exists (\(=\mbox{argmax}_\theta (1/n)\log \mathcal{L}(\boldsymbol\theta;\mathbf{y})\)).

\((1/n)\log \mathcal{L}(\boldsymbol\theta;\mathbf{y})\) can be interpreted as the sample mean of the r.v. \(\log f(Y_i;\boldsymbol\theta)\) that are i.i.d. Therefore \((1/n)\log \mathcal{L}(\boldsymbol\theta;\mathbf{y})\) converges to \(\mathbb{E}_{\boldsymbol\theta_0}(\log f(Y;\boldsymbol\theta))\) -- which exists (Assumption iv).

Because the latter convergence is uniform (Assumption v), the solution \(\boldsymbol\theta_{MLE}\) almost surely converges to the solution to the limit problem:
\[
\mbox{argmax}_\theta \mathbb{E}_{\boldsymbol\theta_0}(\log f(Y;\boldsymbol\theta)) = \mbox{argmax}_\theta \int_{\mathcal{Y}} \log f(y;\boldsymbol\theta)f(y;\boldsymbol\theta_0) dy.
\]

Properties of the Kullback information measure (see Prop. \ref{prp:Kullback}), together with the identifiability assumption (ii) implies that the solution to the limit problem is unique and equal to \(\boldsymbol\theta_0\).

Consider a r.v. sequence \(\boldsymbol\theta\) that converges to \(\boldsymbol\theta_0\). The Taylor expansion of the score in a neighborood of \(\boldsymbol\theta_0\) yields to:
\[
\frac{\partial \log \mathcal{L}(\boldsymbol\theta;\mathbf{y})}{\partial \boldsymbol\theta} = \frac{\partial \log \mathcal{L}(\boldsymbol\theta_0;\mathbf{y})}{\partial \boldsymbol\theta} + \frac{\partial^2 \log \mathcal{L}(\boldsymbol\theta_0;\mathbf{y})}{\partial \boldsymbol\theta \partial \boldsymbol\theta'}(\boldsymbol\theta - \boldsymbol\theta_0) + o_p(\boldsymbol\theta - \boldsymbol\theta_0)
\]

\(\boldsymbol\theta_{MLE}\) converges to \(\boldsymbol\theta_0\) and satisfies the likelihood equation \(\frac{\partial \log \mathcal{L}(\boldsymbol\theta;\mathbf{y})}{\partial \boldsymbol\theta} = \mathbf{0}\). Therefore:
\[
\frac{\partial \log \mathcal{L}(\boldsymbol\theta_0;\mathbf{y})}{\partial \boldsymbol\theta} \approx - \frac{\partial^2 \log \mathcal{L}(\boldsymbol\theta_0;\mathbf{y})}{\partial \boldsymbol\theta \partial \boldsymbol\theta'}(\boldsymbol\theta_{MLE} - \boldsymbol\theta_0),
\]
or equivalently:
\[
\frac{1}{\sqrt{n}} \frac{\partial \log \mathcal{L}(\boldsymbol\theta_0;\mathbf{y})}{\partial \boldsymbol\theta} \approx
\left(- \frac{1}{n} \sum_{i=1}^n \frac{\partial^2 \log f(y_i;\boldsymbol\theta_0)}{\partial \boldsymbol\theta \partial \boldsymbol\theta'} \right)\sqrt{n}(\boldsymbol\theta_{MLE} - \boldsymbol\theta_0),
\]

By the law of large numbers, we have: \(\left(- \frac{1}{n} \sum_{i=1}^n \frac{\partial^2 \log f(y_i;\boldsymbol\theta_0)}{\partial \boldsymbol\theta \partial \boldsymbol\theta'} \right) \overset{}\rightarrow \frac{1}{n} \mathbf{I}(\boldsymbol\theta_0) = \mathcal{I}_Y(\boldsymbol\theta_0)\).

Besides, we have:
\begin{eqnarray*}
\frac{1}{\sqrt{n}} \frac{\partial \log \mathcal{L}(\boldsymbol\theta_0;\mathbf{y})}{\partial \boldsymbol\theta} &=& \sqrt{n} \left( \frac{1}{n} \sum_i \frac{\partial \log f(y_i;\boldsymbol\theta_0)}{\partial \boldsymbol\theta} \right) \\
&=& \sqrt{n} \left( \frac{1}{n} \sum_i \left\{ \frac{\partial \log f(y_i;\boldsymbol\theta_0)}{\partial \boldsymbol\theta} - \mathbb{E}_{\boldsymbol\theta_0} \frac{\partial \log f(Y_i;\boldsymbol\theta_0)}{\partial \boldsymbol\theta} \right\} \right)
\end{eqnarray*}
which converges to \(\mathcal{N}(0,\mathcal{I}_Y(\boldsymbol\theta_0))\) by the CLT.

Collecting the preceding results leads to (b). The fact that \(\boldsymbol\theta_{MLE}\) achieves the FDCR bound proves (c).
\end{proof}

\textbf{Proof of Proposition \ref{prp:Walddistri}}

\begin{proof}
We have \(\sqrt{n}(\hat{\boldsymbol\theta}_{n} - \boldsymbol\theta_{0}) \overset{d}{\rightarrow} \mathcal{N}(0,\mathcal{I}(\boldsymbol\theta_0)^{-1})\) (Eq. \eqref{eq:normMLE}). A Taylor expansion around \(\boldsymbol\theta_0\) yields to:
\begin{equation}
\sqrt{n}(h(\hat{\boldsymbol\theta}_{n}) - h(\boldsymbol\theta_{0})) \overset{d}{\rightarrow} \mathcal{N}\left(0,\frac{\partial h(\boldsymbol\theta_{0})}{\partial \boldsymbol\theta'}\mathcal{I}(\boldsymbol\theta_0)^{-1}\frac{\partial h(\boldsymbol\theta_{0})'}{\partial \boldsymbol\theta}\right). \label{eq:XXX}
\end{equation}
Under \(H_0\), \(h(\boldsymbol\theta_{0})=0\) therefore:
\begin{equation}
\sqrt{n} h(\hat{\boldsymbol\theta}_{n}) \overset{d}{\rightarrow} \mathcal{N}\left(0,\frac{\partial h(\boldsymbol\theta_{0})}{\partial \boldsymbol\theta'}\mathcal{I}(\boldsymbol\theta_0)^{-1}\frac{\partial h(\boldsymbol\theta_{0})'}{\partial \boldsymbol\theta}\right). \label{eq:lm10}
\end{equation}
Hence
\[
\sqrt{n} \left(
\frac{\partial h(\boldsymbol\theta_{0})}{\partial \boldsymbol\theta'}\mathcal{I}(\boldsymbol\theta_0)^{-1}\frac{\partial h(\boldsymbol\theta_{0})'}{\partial \boldsymbol\theta}
\right)^{-1/2} h(\hat{\boldsymbol\theta}_{n}) \overset{d}{\rightarrow} \mathcal{N}\left(0,Id\right).
\]
Taking the quadratic form, we obtain:
\[
n h(\hat{\boldsymbol\theta}_{n})'  \left(
\frac{\partial h(\boldsymbol\theta_{0})}{\partial \boldsymbol\theta'}\mathcal{I}(\boldsymbol\theta_0)^{-1}\frac{\partial h(\boldsymbol\theta_{0})'}{\partial \boldsymbol\theta}
\right)^{-1} h(\hat{\boldsymbol\theta}_{n}) \overset{d}{\rightarrow} \chi^2(r).
\]

The fact that the test has asymptotic level \(\alpha\) directly stems from what precedes. \textbf{Consistency of the test}: Consider \(\theta_0 \in \Theta\). Because the MLE is consistent, \(h(\hat{\boldsymbol\theta}_{n})\) converges to \(h(\boldsymbol\theta_0) \ne 0\). Eq. \eqref{eq:XXX} is still valid. It implies that \(\xi^W_n\) converges to \(+\infty\) and therefore that \(\mathbb{P}_{\boldsymbol\theta}(\xi^W_n \ge \chi^2_{1-\alpha}(r)) \rightarrow 1\).
\end{proof}

\textbf{Proof of Proposition \ref{prp:LMdistri}}

\begin{proof}
Notations: ``\(\approx\)'' means ``equal up to a term that converges to 0 in probability''. We are under \(H_0\). \(\hat{\boldsymbol\theta}^0\) is the constrained ML estimator; \(\hat{\boldsymbol\theta}\) denotes the unconstrained one.

We combine the two Taylor expansion: \(h(\hat{\boldsymbol\theta}_n) \approx \dfrac{\partial h(\boldsymbol\theta_0)}{\partial \boldsymbol\theta'}(\hat{\boldsymbol\theta}_n - \boldsymbol\theta_0)\) and \(h(\hat{\boldsymbol\theta}_n^0) \approx \dfrac{\partial h(\boldsymbol\theta_0)}{\partial \boldsymbol\theta'}(\hat{\boldsymbol\theta}_n^0 - \boldsymbol\theta_0)\) and we use \(h(\hat{\boldsymbol\theta}_n^0)=0\) (by definition) to get:
\begin{equation}
\sqrt{n}h(\hat{\boldsymbol\theta}_n) \approx \dfrac{\partial h(\boldsymbol\theta_0)}{\partial \boldsymbol\theta'}\sqrt{n}(\hat{\boldsymbol\theta}_n - \hat{\boldsymbol\theta}^0_n). \label{eq:lm1}
\end{equation}
Besides, we have (using the definition of the information matrix):
\begin{equation}
\frac{1}{\sqrt{n}}\frac{\partial \log \mathcal{L}(\hat{\boldsymbol\theta}^0_n;\mathbf{y})}{\partial \boldsymbol\theta} \approx
\frac{1}{\sqrt{n}}\frac{\partial \log \mathcal{L}(\boldsymbol\theta_0;\mathbf{y})}{\partial \boldsymbol\theta} - \mathcal{I}(\boldsymbol\theta_0)\sqrt{n}(\hat{\boldsymbol\theta}^0_n-\boldsymbol\theta_0) \label{eq:lm29}
\end{equation}
and:
\begin{equation}
0=\frac{1}{\sqrt{n}}\frac{\partial \log \mathcal{L}(\hat{\boldsymbol\theta}_n;\mathbf{y})}{\partial \boldsymbol\theta} \approx
\frac{1}{\sqrt{n}}\frac{\partial \log \mathcal{L}(\boldsymbol\theta_0;\mathbf{y})}{\partial \boldsymbol\theta} - \mathcal{I}(\boldsymbol\theta_0)\sqrt{n}(\hat{\boldsymbol\theta}_n-\boldsymbol\theta_0).\label{eq:lm30}
\end{equation}
Taking the difference and multiplying by \(\mathcal{I}(\boldsymbol\theta_0)^{-1}\):
\begin{equation}
\sqrt{n}(\hat{\boldsymbol\theta}_n-\hat{\boldsymbol\theta}_n^0) \approx
\mathcal{I}(\boldsymbol\theta_0)^{-1}\frac{1}{\sqrt{n}}\frac{\partial \log \mathcal{L}(\hat{\boldsymbol\theta}^0_n;\mathbf{y})}{\partial \boldsymbol\theta}
\mathcal{I}(\boldsymbol\theta_0).\label{eq:lm2}
\end{equation}
Eqs. \eqref{eq:lm1} and \eqref{eq:lm2} yield to:
\begin{equation}
\sqrt{n}h(\hat{\boldsymbol\theta}_n) \approx \dfrac{\partial h(\boldsymbol\theta_0)}{\partial \boldsymbol\theta'} \mathcal{I}(\boldsymbol\theta_0)^{-1}\frac{1}{\sqrt{n}}\frac{\partial \log \mathcal{L}(\hat{\boldsymbol\theta}^0_n;\mathbf{y})}{\partial \boldsymbol\theta}.\label{eq:lm3}
\end{equation}

Recall that \(\hat{\boldsymbol\theta}^0_n\) is the MLE of \(\boldsymbol\theta_0\) under the constraint \(h(\boldsymbol\theta)=0\). The vector of Lagrange multipliers \(\hat\lambda_n\) associated to this program satisfies:
\begin{equation}
\frac{\partial \log \mathcal{L}(\hat{\boldsymbol\theta}^0_n;\mathbf{y})}{\partial \boldsymbol\theta}+ \frac{\partial h'(\hat{\boldsymbol\theta}^0_n;\mathbf{y})}{\partial \boldsymbol\theta}\hat\lambda_n = 0.\label{eq:multiplier}
\end{equation}
Substituting the latter equation in Eq. \eqref{eq:lm3} gives:
\begin{eqnarray*}
\sqrt{n}h(\hat{\boldsymbol\theta}_n) &\approx&
- \dfrac{\partial h(\boldsymbol\theta_0)}{\partial \boldsymbol\theta'} \mathcal{I}(\boldsymbol\theta_0)^{-1}
\frac{\partial h'(\hat{\boldsymbol\theta}^0_n;\mathbf{y})}{\partial \boldsymbol\theta} \frac{\hat\lambda_n}{\sqrt{n}} \\
&\approx&
- \dfrac{\partial h(\boldsymbol\theta_0)}{\partial \boldsymbol\theta'} \mathcal{I}(\boldsymbol\theta_0)^{-1}
\frac{\partial h'(\boldsymbol\theta_0;\mathbf{y})}{\partial \boldsymbol\theta} \frac{\hat\lambda_n}{\sqrt{n}},
\end{eqnarray*}
which yields:
\begin{equation}
\frac{\hat\lambda_n}{\sqrt{n}} \approx - \left(
\dfrac{\partial h(\boldsymbol\theta_0)}{\partial \boldsymbol\theta'} \mathcal{I}(\boldsymbol\theta_0)^{-1}
\frac{\partial h'(\boldsymbol\theta_0;\mathbf{y})}{\partial \boldsymbol\theta}
\right)^{-1}
\sqrt{n}h(\hat{\boldsymbol\theta}_n).\label{eq:lm20}
\end{equation}
It follows, from Eq. \eqref{eq:lm10}, that:
\[
\frac{\hat\lambda_n}{\sqrt{n}} \overset{d}{\rightarrow} \mathcal{N}\left(0,\left(
\dfrac{\partial h(\boldsymbol\theta_0)}{\partial \boldsymbol\theta'} \mathcal{I}(\boldsymbol\theta_0)^{-1}
\frac{\partial h'(\boldsymbol\theta_0;\mathbf{y})}{\partial \boldsymbol\theta}
\right)^{-1}\right).
\]
Taking the quadratic form of the last equation gives:
\[
\frac{1}{n}\hat\lambda_n' \dfrac{\partial h(\hat{\boldsymbol\theta}^0_n)}{\partial \boldsymbol\theta'} \mathcal{I}(\hat{\boldsymbol\theta}^0_n)^{-1}
\frac{\partial h'(\hat{\boldsymbol\theta}^0_n;\mathbf{y})}{\partial \boldsymbol\theta} \hat\lambda_n \overset{d}{\rightarrow} \chi^2(r).
\]
Using Eq. \eqref{eq:multiplier}, it appears that the left-hand side term of the last equation is \(\xi^{LM}\) as defined in Eq. \eqref{eq:xiLM}. Consistency: see Remark 17.3 in \citet{gourieroux_monfort_1995}.
\end{proof}

\textbf{Proof of Proposition \ref{prp:equivLRLMW}}

\begin{proof}
Let us first demonstrate the asymptotic equivalence of \(\xi^{LM}\) and \(\xi^{LR}\).

The second-order Taylor expansions of \(\log \mathcal{L}(\hat{\boldsymbol\theta}^0_n,\mathbf{y})\) and \(\log \mathcal{L}(\hat{\boldsymbol\theta}_n,\mathbf{y})\) are:
\begin{eqnarray*}
\log \mathcal{L}(\hat{\boldsymbol\theta}_n,\mathbf{y}) &\approx& \log \mathcal{L}(\boldsymbol\theta_0,\mathbf{y})
+ \frac{\partial \log \mathcal{L}(\boldsymbol\theta_0,\mathbf{y})}{\partial \boldsymbol\theta'}(\hat{\boldsymbol\theta}_n-\boldsymbol\theta_0) \\
&& - \frac{n}{2} (\hat{\boldsymbol\theta}_n-\boldsymbol\theta_0)' \mathcal{I}(\boldsymbol\theta_0) (\hat{\boldsymbol\theta}_n-\boldsymbol\theta_0)\\
\log \mathcal{L}(\hat{\boldsymbol\theta}^0_n,\mathbf{y}) &\approx& \log \mathcal{L}(\boldsymbol\theta_0,\mathbf{y})
+ \frac{\partial \log \mathcal{L}(\boldsymbol\theta_0,\mathbf{y})}{\partial \boldsymbol\theta'}(\hat{\boldsymbol\theta}^0_n-\boldsymbol\theta_0) \\
&& - \frac{n}{2} (\hat{\boldsymbol\theta}^0_n-\boldsymbol\theta_0)' \mathcal{I}(\boldsymbol\theta_0) (\hat{\boldsymbol\theta}^0_n-\boldsymbol\theta_0).
\end{eqnarray*}
Taking the difference, we obtain:
\begin{eqnarray*}
\xi_n^{LR} &\approx& 2\frac{\partial \log \mathcal{L}(\boldsymbol\theta_0,\mathbf{y})}{\partial \boldsymbol\theta'}
(\hat{\boldsymbol\theta}_n-\hat{\boldsymbol\theta}^0_n) + n (\hat{\boldsymbol\theta}^0_n-\boldsymbol\theta_0)' \mathcal{I}(\boldsymbol\theta_0) (\hat{\boldsymbol\theta}^0_n-\boldsymbol\theta_0)\\
&& - n (\hat{\boldsymbol\theta}_n-\boldsymbol\theta_0)' \mathcal{I}(\boldsymbol\theta_0) (\hat{\boldsymbol\theta}_n-\boldsymbol\theta_0).
\end{eqnarray*}
Using \(\dfrac{1}{\sqrt{n}}\frac{\partial \log \mathcal{L}(\boldsymbol\theta_0;\mathbf{y})}{\partial \boldsymbol\theta} \approx \mathcal{I}(\boldsymbol\theta_0)\sqrt{n}(\hat{\boldsymbol\theta}_n-\boldsymbol\theta_0)\) (Eq. \eqref{eq:lm30}), we have:
\begin{eqnarray*}
\xi_n^{LR} &\approx&
2n(\hat{\boldsymbol\theta}_n-\boldsymbol\theta_0)'\mathcal{I}(\boldsymbol\theta_0)
(\hat{\boldsymbol\theta}_n-\hat{\boldsymbol\theta}^0_n)
+ n (\hat{\boldsymbol\theta}^0_n-\boldsymbol\theta_0)' \mathcal{I}(\boldsymbol\theta_0) (\hat{\boldsymbol\theta}^0_n-\boldsymbol\theta_0) \\
&& - n (\hat{\boldsymbol\theta}_n-\boldsymbol\theta_0)' \mathcal{I}(\boldsymbol\theta_0) (\hat{\boldsymbol\theta}_n-\boldsymbol\theta_0).
\end{eqnarray*}
In the second of the three terms in the sum, we replace \((\hat{\boldsymbol\theta}^0_n-\boldsymbol\theta_0)\) by \((\hat{\boldsymbol\theta}^0_n-\hat{\boldsymbol\theta}_n+\hat{\boldsymbol\theta}_n-\boldsymbol\theta_0)\) and we develop the associated product. This leads to:
\begin{equation}
\xi_n^{LR} \approx n (\hat{\boldsymbol\theta}^0_n-\hat{\boldsymbol\theta}_n)' \mathcal{I}(\boldsymbol\theta_0)^{-1} (\hat{\boldsymbol\theta}^0_n-\hat{\boldsymbol\theta}_n). \label{eq:lr10}
\end{equation}
The difference between Eqs. \eqref{eq:lm29} and \eqref{eq:lm30} implies:
\[
\frac{1}{\sqrt{n}}\frac{\partial \log \mathcal{L}(\hat{\boldsymbol\theta}^0_n;\mathbf{y})}{\partial \boldsymbol\theta} \approx
\mathcal{I}(\boldsymbol\theta_0)\sqrt{n}(\hat{\boldsymbol\theta}_n-\hat{\boldsymbol\theta}^0_n),
\]
which, associated to Eq. @\ref(eq:lr10), gives:
\[
\xi_n^{LR} \approx \frac{1}{n} \frac{\partial \log \mathcal{L}(\hat{\boldsymbol\theta}^0_n;\mathbf{y})}{\partial \boldsymbol\theta'} \mathcal{I}(\boldsymbol\theta_0)^{-1} \frac{\partial \log \mathcal{L}(\hat{\boldsymbol\theta}^0_n;\mathbf{y})}{\partial \boldsymbol\theta} \approx \xi_n^{LM}.
\]
Hence \(\xi_n^{LR}\) has the same asymptotic distribution as \(\xi_n^{LM}\).

Let's show that the LR test is consistent. For this, note that:
\begin{eqnarray*}
\frac{\log \mathcal{L}(\hat{\boldsymbol\theta},\mathbf{y}) - \log \mathcal{L}(\hat{\boldsymbol\theta}^0,\mathbf{y})}{n} &=& \frac{1}{n} \sum_{i=1}^n[\log f(y_i;\hat{\boldsymbol\theta}_n) - \log f(y_i;\hat{\boldsymbol\theta}_n^0)]\\
&\rightarrow& \mathbb{E}_0[\log f(Y;\boldsymbol\theta_0) - \log f(Y;\boldsymbol\theta_\infty)],
\end{eqnarray*}
where \(\boldsymbol\theta_\infty\), the pseudo true value, is such that \(h(\boldsymbol\theta_\infty) \ne 0\) (by definition of \(H_1\)). From the Kullback inequality and the asymptotic identifiability of \(\boldsymbol\theta_0\), it follows that \(\mathbb{E}_0[\log f(Y;\boldsymbol\theta_0) - \log f(Y;\boldsymbol\theta_\infty)] >0\). Therefore \(\xi_n^{LR} \rightarrow + \infty\) under \(H_1\).

Let us now demonstrate the equivalence of \(\xi^{LM} and \xi^{W}\).

We have (using Eq. \ref(eq:multiplier)):
\[
\xi^{LM}_n = \frac{1}{n}\hat\lambda_n' \dfrac{\partial h(\hat{\boldsymbol\theta}^0_n)}{\partial \boldsymbol\theta'} \mathcal{I}(\hat{\boldsymbol\theta}^0_n)^{-1}
\frac{\partial h'(\hat{\boldsymbol\theta}^0_n;\mathbf{y})}{\partial \boldsymbol\theta} \hat\lambda_n.
\]
Since, under \(H_0\), \(\hat{\boldsymbol\theta}_n^0\approx\hat{\boldsymbol\theta}_n \approx {\boldsymbol\theta}_0\), Eq. \eqref{eq:lm20} therefore implies that:
\[
\xi^{LM} \approx n h(\hat{\boldsymbol\theta}_n)' \left(
\dfrac{\partial h(\hat{\boldsymbol\theta}_n)}{\partial \boldsymbol\theta'} \mathcal{I}(\hat{\boldsymbol\theta}_n)^{-1}
\frac{\partial h'(\hat{\boldsymbol\theta}_n;\mathbf{y})}{\partial \boldsymbol\theta}
\right)^{-1}
h(\hat{\boldsymbol\theta}_n) = \xi^{W},
\]
which gives the result.
\end{proof}

\textbf{Proof of Eq. \eqref{eq:TCL2}}

\begin{proof}
We have:
\begin{eqnarray*}
&&T\mathbb{E}\left[(\bar{y}_T - \mu)^2\right]\\
&=& T\mathbb{E}\left[\left(\frac{1}{T}\sum_{t=1}^T(y_t - \mu)\right)^2\right] = \frac{1}{T} \mathbb{E}\left[\sum_{t=1}^T(y_t - \mu)^2+2\sum_{s<t\le T}(y_t - \mu)(y_s - \mu)\right]\\
&=& \gamma_0 +\frac{2}{T}\left(\sum_{t=2}^{T}\mathbb{E}\left[(y_t - \mu)(y_{t-1} - \mu)\right]\right) +\frac{2}{T}\left(\sum_{t=3}^{T}\mathbb{E}\left[(y_t - \mu)(y_{t-2} - \mu)\right]\right) + \dots \\
&&+ \frac{2}{T}\left(\sum_{t=T-1}^{T}\mathbb{E}\left[(y_t - \mu)(y_{t-(T-2)} - \mu)\right]\right) + \frac{2}{T}\mathbb{E}\left[(y_t - \mu)(y_{t-(T-1)} - \mu)\right]\\
&=&  \gamma_0 + 2 \frac{T-1}{T}\gamma_1 + \dots + 2 \frac{1}{T}\gamma_{T-1} .
\end{eqnarray*}
Therefore:
\begin{eqnarray*}
&& T\mathbb{E}\left[(\bar{y}_T - \mu)^2\right] - \sum_{j=-\infty}^{+\infty} \gamma_j \\
&=& - 2\frac{1}{T}\gamma_1 - 2\frac{2}{T}\gamma_2 - \dots - 2\frac{T-1}{T}\gamma_{T-1} - 2\gamma_T - 2 \gamma_{T+1} + \dots
\end{eqnarray*}
And then:
\begin{eqnarray*}
&& \left|T\mathbb{E}\left[(\bar{y}_T - \mu)^2\right] - \sum_{j=-\infty}^{+\infty} \gamma_j\right|\\
&\le& 2\frac{1}{T}|\gamma_1| + 2\frac{2}{T}|\gamma_2| + \dots + 2\frac{T-1}{T}|\gamma_{T-1}| + 2|\gamma_T| + 2 |\gamma_{T+1}| + \dots
\end{eqnarray*}

For any \(q \le T\), we have:
\begin{eqnarray*}
\left|T\mathbb{E}\left[(\bar{y}_T - \mu)^2\right] - \sum_{j=-\infty}^{+\infty} \gamma_j\right| &\le& 2\frac{1}{T}|\gamma_1| + 2\frac{2}{T}|\gamma_2| + \dots + 2\frac{q-1}{T}|\gamma_{q-1}| +2\frac{q}{T}|\gamma_q| +\\
&&2\frac{q+1}{T}|\gamma_{q+1}| + \dots  + 2\frac{T-1}{T}|\gamma_{T-1}| + 2|\gamma_T| + 2 |\gamma_{T+1}| + \dots\\
&\le& \frac{2}{T}\left(|\gamma_1| + 2|\gamma_2| + \dots + (q-1)|\gamma_{q-1}| +q|\gamma_q|\right) +\\
&&2|\gamma_{q+1}| + \dots  + 2|\gamma_{T-1}| + 2|\gamma_T| + 2 |\gamma_{T+1}| + \dots
\end{eqnarray*}

Consider \(\varepsilon > 0\). The fact that the autocovariances are absolutely summable implies that there exists \(q_0\) such that (Cauchy criterion, Theorem \ref{thm:cauchycritstatic}):
\[
2|\gamma_{q_0+1}|+2|\gamma_{q_0+2}|+2|\gamma_{q_0+3}|+\dots < \varepsilon/2.
\]
Then, if \(T > q_0\), it comes that:
\[
\left|T \mathbb{E}\left[(\bar{y}_T - \mu)^2\right] - \sum_{j=-\infty}^{+\infty} \gamma_j\right|\le \frac{2}{T}\left(|\gamma_1| + 2|\gamma_2| + \dots + (q_0-1)|\gamma_{q_0-1}| +q_0|\gamma_{q_0}|\right) + \varepsilon/2.
\]
If \(T \ge 2\left(|\gamma_1| + 2|\gamma_2| + \dots + (q_0-1)|\gamma_{q_0-1}| +q_0|\gamma_{q_0}|\right)/(\varepsilon/2)\) (\(= f(q_0)\), say) then
\[
\frac{2}{T}\left(|\gamma_1| + 2|\gamma_2| + \dots + (q_0-1)|\gamma_{q_0-1}| +q_0|\gamma_{q_0}|\right) \le \varepsilon/2.
\]
Then, if \(T>f(q_0)\) and \(T>q_0\), i.e.~if \(T>\max(f(q_0),q_0)\), we have:
\[
\left|T \mathbb{E}\left[(\bar{y}_T - \mu)^2\right] - \sum_{j=-\infty}^{+\infty} \gamma_j\right|\le \varepsilon.
\]
\end{proof}

\textbf{Proof of Proposition \ref{prp:smallestMSE}}

\begin{proof}
We have:
\begin{eqnarray}
\mathbb{E}([y_{t+1} - y^*_{t+1}]^2) &=& \mathbb{E}\left([\color{blue}{\{y_{t+1} - \mathbb{E}(y_{t+1}|x_t)\}} + \color{red}{\{\mathbb{E}(y_{t+1}|x_t)  - y^*_{t+1}\}}]^2\right)\nonumber\\
&=&  \mathbb{E}\left(\color{blue}{[y_{t+1} - \mathbb{E}(y_{t+1}|x_t)]}^2\right) + \mathbb{E}\left(\color{red}{[\mathbb{E}(y_{t+1}|x_t)  - y^*_{t+1}]}^2\right)\nonumber\\
&& + 2\mathbb{E}\left( \color{blue}{[y_{t+1} - \mathbb{E}(y_{t+1}|x_t)]}\color{red}{ [\mathbb{E}(y_{t+1}|x_t)  - y^*_{t+1}]}\right). \label{eq:1}
\end{eqnarray}
Let us focus on the last term. We have:
\begin{eqnarray*}
&&\mathbb{E}\left(\color{blue}{[y_{t+1} - \mathbb{E}(y_{t+1}|x_t)]}\color{red}{ [\mathbb{E}(y_{t+1}|x_t)  - y^*_{t+1}]}\right)\\
&=& \mathbb{E}( \mathbb{E}( \color{blue}{[y_{t+1} - \mathbb{E}(y_{t+1}|x_t)]}\color{red}{ \underbrace{[\mathbb{E}(y_{t+1}|x_t)  - y^*_{t+1}]}_{\mbox{function of $x_t$}}}|x_t))\\
&=& \mathbb{E}( \color{red}{ [\mathbb{E}(y_{t+1}|x_t)  - y^*_{t+1}]} \mathbb{E}( \color{blue}{[y_{t+1} - \mathbb{E}(y_{t+1}|x_t)]}|x_t))\\
&=& \mathbb{E}( \color{red}{ [\mathbb{E}(y_{t+1}|x_t)  - y^*_{t+1}]} \color{blue}{\underbrace{[\mathbb{E}(y_{t+1}|x_t) - \mathbb{E}(y_{t+1}|x_t)]}_{=0}})=0.
\end{eqnarray*}

Therefore, Eq. \eqref{eq:1} becomes:
\begin{eqnarray*}
&&\mathbb{E}([y_{t+1} - y^*_{t+1}]^2) \\
&=&  \underbrace{\mathbb{E}\left(\color{blue}{[y_{t+1} - \mathbb{E}(y_{t+1}|x_t)]}^2\right)}_{\mbox{$\ge 0$ and does not depend on $y^*_{t+1}$}} + \underbrace{\mathbb{E}\left(\color{red}{[\mathbb{E}(y_{t+1}|x_t)  - y^*_{t+1}]}^2\right)}_{\mbox{$\ge 0$ and depends on $y^*_{t+1}$}}.
\end{eqnarray*}
This implies that \(\mathbb{E}([y_{t+1} - y^*_{t+1}]^2)\) is always larger than \(\color{blue}{\mathbb{E}([y_{t+1} - \mathbb{E}(y_{t+1}|x_t)]^2)}\), and is therefore minimized if the second term is equal to zero, that is if \(\mathbb{E}(y_{t+1}|x_t) = y^*_{t+1}\).
\end{proof}

\textbf{Proof of Proposition \ref{prp:estimVARGaussian}}

\begin{proof}
Using Proposition \ref{prp:pdfMultivarGaussian}, we obtain that, conditionally on \(x_1\), the log-likelihood is given by
\begin{eqnarray*}
\log\mathcal{L}(Y_{T};\theta) & = & -(Tn/2)\log(2\pi)+(T/2)\log\left|\Omega^{-1}\right|\\
&  & -\frac{1}{2}\sum_{t=1}^{T}\left[\left(y_{t}-\Pi'x_{t}\right)'\Omega^{-1}\left(y_{t}-\Pi'x_{t}\right)\right].
\end{eqnarray*}
Let's rewrite the last term of the log-likelihood:
\begin{eqnarray*}
\sum_{t=1}^{T}\left[\left(y_{t}-\Pi'x_{t}\right)'\Omega^{-1}\left(y_{t}-\Pi'x_{t}\right)\right] & =\\
\sum_{t=1}^{T}\left[\left(y_{t}-\hat{\Pi}'x_{t}+\hat{\Pi}'x_{t}-\Pi'x_{t}\right)'\Omega^{-1}\left(y_{t}-\hat{\Pi}'x_{t}+\hat{\Pi}'x_{t}-\Pi'x_{t}\right)\right] & =\\
\sum_{t=1}^{T}\left[\left(\hat{\varepsilon}_{t}+(\hat{\Pi}-\Pi)'x_{t}\right)'\Omega^{-1}\left(\hat{\varepsilon}_{t}+(\hat{\Pi}-\Pi)'x_{t}\right)\right],
\end{eqnarray*}
where the \(j^{th}\) element of the \((n\times1)\) vector \(\hat{\varepsilon}_{t}\) is the sample residual, for observation \(t\), from an OLS regression of \(y_{j,t}\) on \(x_{t}\). Expanding the previous equation, we get:
\begin{eqnarray*}
&&\sum_{t=1}^{T}\left[\left(y_{t}-\Pi'x_{t}\right)'\Omega^{-1}\left(y_{t}-\Pi'x_{t}\right)\right]  = \sum_{t=1}^{T}\hat{\varepsilon}_{t}'\Omega^{-1}\hat{\varepsilon}_{t}\\
&&+2\sum_{t=1}^{T}\hat{\varepsilon}_{t}'\Omega^{-1}(\hat{\Pi}-\Pi)'x_{t}+\sum_{t=1}^{T}x'_{t}(\hat{\Pi}-\Pi)\Omega^{-1}(\hat{\Pi}-\Pi)'x_{t}.
\end{eqnarray*}
Let's apply the trace operator on the second term (that is a scalar):
\begin{eqnarray*}
\sum_{t=1}^{T}\hat{\varepsilon}_{t}'\Omega^{-1}(\hat{\Pi}-\Pi)'x_{t} & = & Tr\left(\sum_{t=1}^{T}\hat{\varepsilon}_{t}'\Omega^{-1}(\hat{\Pi}-\Pi)'x_{t}\right)\\
=  Tr\left(\sum_{t=1}^{T}\Omega^{-1}(\hat{\Pi}-\Pi)'x_{t}\hat{\varepsilon}_{t}'\right) & = & Tr\left(\Omega^{-1}(\hat{\Pi}-\Pi)'\sum_{t=1}^{T}x_{t}\hat{\varepsilon}_{t}'\right).
\end{eqnarray*}
Given that, by construction (property of OLS estimates), the sample residuals are orthogonal to the explanatory variables, this term is zero. Introducing \(\tilde{x}_{t}=(\hat{\Pi}-\Pi)'x_{t}\), we have
\begin{eqnarray*}
\sum_{t=1}^{T}\left[\left(y_{t}-\Pi'x_{t}\right)'\Omega^{-1}\left(y_{t}-\Pi'x_{t}\right)\right] =\sum_{t=1}^{T}\hat{\varepsilon}_{t}'\Omega^{-1}\hat{\varepsilon}_{t}+\sum_{t=1}^{T}\tilde{x}'_{t}\Omega^{-1}\tilde{x}_{t}.
\end{eqnarray*}
Since \(\Omega\) is a positive definite matrix, \(\Omega^{-1}\) is as well. Consequently, the smallest value that the last term can take is obtained for \(\tilde{x}_{t}=0\), i.e.~when \(\Pi=\hat{\Pi}.\)

The MLE of \(\Omega\) is the matrix \(\hat{\Omega}\) that maximizes \(\Omega\overset{\ell}{\rightarrow}L(Y_{T};\hat{\Pi},\Omega)\). We have:
\begin{eqnarray*}
\log\mathcal{L}(Y_{T};\hat{\Pi},\Omega) & = & -(Tn/2)\log(2\pi)+(T/2)\log\left|\Omega^{-1}\right| -\frac{1}{2}\sum_{t=1}^{T}\left[\hat{\varepsilon}_{t}'\Omega^{-1}\hat{\varepsilon}_{t}\right].
\end{eqnarray*}

Matrix \(\hat{\Omega}\) is a symmetric positive definite. It is easily checked that the (unrestricted) matrix that maximizes the latter expression is symmetric positive definite matrix. Indeed:
\[
\frac{\partial \log\mathcal{L}(Y_{T};\hat{\Pi},\Omega)}{\partial\Omega}=\frac{T}{2}\Omega'-\frac{1}{2}\sum_{t=1}^{T}\hat{\varepsilon}_{t}\hat{\varepsilon}'_{t}\Rightarrow\hat{\Omega}'=\frac{1}{T}\sum_{t=1}^{T}\hat{\varepsilon}_{t}\hat{\varepsilon}'_{t},
\]
which leads to the result.
\end{proof}

\textbf{Proof of Proposition \ref{prp:OLSVAR}}

\begin{proof}
Let us drop the \(i\) subscript. Rearranging Eq. \eqref{eq:olsar1}, we have:
\[
\sqrt{T}(\mathbf{b}-\boldsymbol{\beta}) =  (X'X/T)^{-1}\sqrt{T}(X'\boldsymbol\varepsilon/T).
\]
Let us consider the autocovariances of \(\mathbf{v}_t = x_t \varepsilon_t\), denoted by \(\gamma^v_j\). Using the fact that \(x_t\) is a linear combination of past \(\varepsilon_t\)s and that \(\varepsilon_t\) is a white noise, we get that \(\mathbb{E}(\varepsilon_t x_t)=0\). Therefore
\[
\gamma^v_j = \mathbb{E}(\varepsilon_t\varepsilon_{t-j}x_tx_{t-j}').
\]
If \(j>0\), we have \(\mathbb{E}(\varepsilon_t\varepsilon_{t-j}x_tx_{t-j}')=\mathbb{E}(\mathbb{E}[\varepsilon_t\varepsilon_{t-j}x_tx_{t-j}'|\varepsilon_{t-j},x_t,x_{t-j}])=\) \(\mathbb{E}(\varepsilon_{t-j}x_tx_{t-j}'\mathbb{E}[\varepsilon_t|\varepsilon_{t-j},x_t,x_{t-j}])=0\). Note that we have \(\mathbb{E}[\varepsilon_t|\varepsilon_{t-j},x_t,x_{t-j}]=0\) because \(\{\varepsilon_t\}\) is an i.i.d. white noise sequence. If \(j=0\), we have:
\[
\gamma^v_0 = \mathbb{E}(\varepsilon_t^2x_tx_{t}')= \mathbb{E}(\varepsilon_t^2) \mathbb{E}(x_tx_{t}')=\sigma^2\mathbf{Q}.
\]
The convergence in distribution of \(\sqrt{T}(X'\boldsymbol\varepsilon/T)=\sqrt{T}\frac{1}{T}\sum_{t=1}^Tv_t\) results from the Central Limit Theorem for covariance-stationary processes, using the \(\gamma_j^v\) computed above.
\end{proof}

\hypertarget{additional-codes}{%
\section{Additional codes}\label{additional-codes}}

\hypertarget{App:GEV}{%
\subsection{Simulating GEV distributions}\label{App:GEV}}

The following lines of code have been used to generate Figure \ref{fig:GEV}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n.sim }\OtherTok{\textless{}{-}} \DecValTok{4000}
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{),}
    \AttributeTok{plt=}\FunctionTok{c}\NormalTok{(.}\DecValTok{2}\NormalTok{,.}\DecValTok{95}\NormalTok{,.}\DecValTok{2}\NormalTok{,.}\DecValTok{85}\NormalTok{))}
\NormalTok{all.rhos }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(.}\DecValTok{3}\NormalTok{,.}\DecValTok{6}\NormalTok{,.}\DecValTok{95}\NormalTok{)}
\ControlFlowTok{for}\NormalTok{(j }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\FunctionTok{length}\NormalTok{(all.rhos))\{}
\NormalTok{  theta }\OtherTok{\textless{}{-}} \DecValTok{1}\SpecialCharTok{/}\NormalTok{all.rhos[j]}
\NormalTok{  v1 }\OtherTok{\textless{}{-}} \FunctionTok{runif}\NormalTok{(n.sim)}
\NormalTok{  v2 }\OtherTok{\textless{}{-}} \FunctionTok{runif}\NormalTok{(n.sim)}
\NormalTok{  w }\OtherTok{\textless{}{-}} \FunctionTok{rep}\NormalTok{(.}\DecValTok{000001}\NormalTok{,n.sim)}
  \CommentTok{\# solve for f(w) = w*(1 {-} log(w)/theta) {-} v2 = 0}
  \ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\DecValTok{20}\NormalTok{)\{}
\NormalTok{    f.i }\OtherTok{\textless{}{-}}\NormalTok{ w }\SpecialCharTok{*}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{{-}} \FunctionTok{log}\NormalTok{(w)}\SpecialCharTok{/}\NormalTok{theta) }\SpecialCharTok{{-}}\NormalTok{ v2}
\NormalTok{    f.prime }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}} \FunctionTok{log}\NormalTok{(w)}\SpecialCharTok{/}\NormalTok{theta }\SpecialCharTok{{-}} \DecValTok{1}\SpecialCharTok{/}\NormalTok{theta}
\NormalTok{    w }\OtherTok{\textless{}{-}}\NormalTok{ w }\SpecialCharTok{{-}}\NormalTok{ f.i}\SpecialCharTok{/}\NormalTok{f.prime}
\NormalTok{  \}}
\NormalTok{  u1 }\OtherTok{\textless{}{-}} \FunctionTok{exp}\NormalTok{(v1}\SpecialCharTok{\^{}}\NormalTok{(}\DecValTok{1}\SpecialCharTok{/}\NormalTok{theta) }\SpecialCharTok{*} \FunctionTok{log}\NormalTok{(w))}
\NormalTok{  u2 }\OtherTok{\textless{}{-}} \FunctionTok{exp}\NormalTok{((}\DecValTok{1}\SpecialCharTok{{-}}\NormalTok{v1)}\SpecialCharTok{\^{}}\NormalTok{(}\DecValTok{1}\SpecialCharTok{/}\NormalTok{theta) }\SpecialCharTok{*} \FunctionTok{log}\NormalTok{(w))}

  \CommentTok{\# Get eps1 and eps2 using the inverse of}
  \CommentTok{\# the Gumbel distribution\textquotesingle{}s cdf:}
\NormalTok{  eps1 }\OtherTok{\textless{}{-}} \SpecialCharTok{{-}}\FunctionTok{log}\NormalTok{(}\SpecialCharTok{{-}}\FunctionTok{log}\NormalTok{(u1))}
\NormalTok{  eps2 }\OtherTok{\textless{}{-}} \SpecialCharTok{{-}}\FunctionTok{log}\NormalTok{(}\SpecialCharTok{{-}}\FunctionTok{log}\NormalTok{(u2))}
  \FunctionTok{cbind}\NormalTok{(}\FunctionTok{cor}\NormalTok{(eps1,eps2),}\DecValTok{1}\SpecialCharTok{{-}}\NormalTok{all.rhos[j]}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}
  \FunctionTok{plot}\NormalTok{(eps1,eps2,}\AttributeTok{pch=}\DecValTok{19}\NormalTok{,}\AttributeTok{col=}\StringTok{"\#FF000044"}\NormalTok{,}
       \AttributeTok{main=}\FunctionTok{paste}\NormalTok{(}\StringTok{"rho = "}\NormalTok{,}\FunctionTok{toString}\NormalTok{(all.rhos[j]),}\AttributeTok{sep=}\StringTok{""}\NormalTok{),}
       \AttributeTok{xlab=}\FunctionTok{expression}\NormalTok{(epsilon[}\DecValTok{1}\NormalTok{]),}
       \AttributeTok{ylab=}\FunctionTok{expression}\NormalTok{(epsilon[}\DecValTok{2}\NormalTok{]),}
       \AttributeTok{cex.lab=}\DecValTok{2}\NormalTok{,}\AttributeTok{cex.main=}\FloatTok{1.5}\NormalTok{)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\hypertarget{IRFDELTA}{%
\subsection{Computing the covariance matrix of IRF using the delta method}\label{IRFDELTA}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{irf.function }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(THETA)\{}
\NormalTok{  c }\OtherTok{\textless{}{-}}\NormalTok{ THETA[}\DecValTok{1}\NormalTok{]}
\NormalTok{  phi }\OtherTok{\textless{}{-}}\NormalTok{ THETA[}\DecValTok{2}\SpecialCharTok{:}\NormalTok{(p}\SpecialCharTok{+}\DecValTok{1}\NormalTok{)]}
  \ControlFlowTok{if}\NormalTok{(q}\SpecialCharTok{\textgreater{}}\DecValTok{0}\NormalTok{)\{}
\NormalTok{    theta }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,THETA[(}\DecValTok{1}\SpecialCharTok{+}\NormalTok{p}\SpecialCharTok{+}\DecValTok{1}\NormalTok{)}\SpecialCharTok{:}\NormalTok{(}\DecValTok{1}\SpecialCharTok{+}\NormalTok{p}\SpecialCharTok{+}\NormalTok{q)])}
\NormalTok{  \}}\ControlFlowTok{else}\NormalTok{\{}
\NormalTok{    theta }\OtherTok{\textless{}{-}} \DecValTok{1}
\NormalTok{  \}}
\NormalTok{  sigma }\OtherTok{\textless{}{-}}\NormalTok{ THETA[}\DecValTok{1}\SpecialCharTok{+}\NormalTok{p}\SpecialCharTok{+}\NormalTok{q}\SpecialCharTok{+}\DecValTok{1}\NormalTok{]}
\NormalTok{  r }\OtherTok{\textless{}{-}} \FunctionTok{dim}\NormalTok{(Matrix.of.Exog)[}\DecValTok{2}\NormalTok{] }\SpecialCharTok{{-}} \DecValTok{1}
\NormalTok{  beta }\OtherTok{\textless{}{-}}\NormalTok{ THETA[(}\DecValTok{1}\SpecialCharTok{+}\NormalTok{p}\SpecialCharTok{+}\NormalTok{q}\SpecialCharTok{+}\DecValTok{1}\SpecialCharTok{+}\DecValTok{1}\NormalTok{)}\SpecialCharTok{:}\NormalTok{(}\DecValTok{1}\SpecialCharTok{+}\NormalTok{p}\SpecialCharTok{+}\NormalTok{q}\SpecialCharTok{+}\DecValTok{1}\SpecialCharTok{+}\NormalTok{(r}\SpecialCharTok{+}\DecValTok{1}\NormalTok{))]}
  
\NormalTok{  irf }\OtherTok{\textless{}{-}} \FunctionTok{sim.arma}\NormalTok{(}\DecValTok{0}\NormalTok{,phi,beta,}\AttributeTok{sigma=}\FunctionTok{sd}\NormalTok{(Ramey}\SpecialCharTok{$}\NormalTok{ED3\_TC,}\AttributeTok{na.rm=}\ConstantTok{TRUE}\NormalTok{),}\AttributeTok{T=}\DecValTok{60}\NormalTok{,}
                  \AttributeTok{y.0=}\FunctionTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{,}\FunctionTok{length}\NormalTok{(x}\SpecialCharTok{$}\NormalTok{phi)),}\AttributeTok{nb.sim=}\DecValTok{1}\NormalTok{,}\AttributeTok{make.IRF=}\DecValTok{1}\NormalTok{,}
                  \AttributeTok{X=}\ConstantTok{NaN}\NormalTok{,}\AttributeTok{beta=}\ConstantTok{NaN}\NormalTok{)}
  \FunctionTok{return}\NormalTok{(irf)}
\NormalTok{\}}

\NormalTok{IRF}\FloatTok{.0} \OtherTok{\textless{}{-}} \DecValTok{100}\SpecialCharTok{*}\FunctionTok{irf.function}\NormalTok{(x}\SpecialCharTok{$}\NormalTok{THETA)}
\NormalTok{eps }\OtherTok{\textless{}{-}}\NormalTok{ .}\DecValTok{00000001}
\NormalTok{d.IRF }\OtherTok{\textless{}{-}} \ConstantTok{NULL}
\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\FunctionTok{length}\NormalTok{(x}\SpecialCharTok{$}\NormalTok{THETA))\{}
\NormalTok{  THETA.i }\OtherTok{\textless{}{-}}\NormalTok{ x}\SpecialCharTok{$}\NormalTok{THETA}
\NormalTok{  THETA.i[i] }\OtherTok{\textless{}{-}}\NormalTok{ THETA.i[i] }\SpecialCharTok{+}\NormalTok{ eps}
\NormalTok{  IRF.i }\OtherTok{\textless{}{-}} \DecValTok{100}\SpecialCharTok{*}\FunctionTok{irf.function}\NormalTok{(THETA.i)}
\NormalTok{  d.IRF }\OtherTok{\textless{}{-}} \FunctionTok{cbind}\NormalTok{(d.IRF,}
\NormalTok{                 (IRF.i }\SpecialCharTok{{-}}\NormalTok{ IRF}\FloatTok{.0}\NormalTok{)}\SpecialCharTok{/}\NormalTok{eps}
\NormalTok{                 )}
\NormalTok{\}}
\NormalTok{mat.var.cov.IRF }\OtherTok{\textless{}{-}}\NormalTok{ d.IRF }\SpecialCharTok{\%*\%}\NormalTok{ x}\SpecialCharTok{$}\NormalTok{I }\SpecialCharTok{\%*\%} \FunctionTok{t}\NormalTok{(d.IRF)}
\end{Highlighting}
\end{Shaded}

\hypertarget{statistical-tables}{%
\section{Statistical Tables}\label{statistical-tables}}

\begin{table}

\caption{\label{tab:Normal}Quantiles of the $\mathcal{N}(0,1)$ distribution. If $a$ and $b$ are respectively the row and column number; then the corresponding cell gives $\mathbb{P}(0<X\le a+b)$, where $X \sim \mathcal{N}(0,1)$.}
\centering
\begin{tabular}[t]{l|r|r|r|r|r|r|r|r|r|r}
\hline
  & 0 & 0.01 & 0.02 & 0.03 & 0.04 & 0.05 & 0.06 & 0.07 & 0.08 & 0.09\\
\hline
0 & 0.5000 & 0.6179 & 0.7257 & 0.8159 & 0.8849 & 0.9332 & 0.9641 & 0.9821 & 0.9918 & 0.9965\\
\hline
0.1 & 0.5040 & 0.6217 & 0.7291 & 0.8186 & 0.8869 & 0.9345 & 0.9649 & 0.9826 & 0.9920 & 0.9966\\
\hline
0.2 & 0.5080 & 0.6255 & 0.7324 & 0.8212 & 0.8888 & 0.9357 & 0.9656 & 0.9830 & 0.9922 & 0.9967\\
\hline
0.3 & 0.5120 & 0.6293 & 0.7357 & 0.8238 & 0.8907 & 0.9370 & 0.9664 & 0.9834 & 0.9925 & 0.9968\\
\hline
0.4 & 0.5160 & 0.6331 & 0.7389 & 0.8264 & 0.8925 & 0.9382 & 0.9671 & 0.9838 & 0.9927 & 0.9969\\
\hline
0.5 & 0.5199 & 0.6368 & 0.7422 & 0.8289 & 0.8944 & 0.9394 & 0.9678 & 0.9842 & 0.9929 & 0.9970\\
\hline
0.6 & 0.5239 & 0.6406 & 0.7454 & 0.8315 & 0.8962 & 0.9406 & 0.9686 & 0.9846 & 0.9931 & 0.9971\\
\hline
0.7 & 0.5279 & 0.6443 & 0.7486 & 0.8340 & 0.8980 & 0.9418 & 0.9693 & 0.9850 & 0.9932 & 0.9972\\
\hline
0.8 & 0.5319 & 0.6480 & 0.7517 & 0.8365 & 0.8997 & 0.9429 & 0.9699 & 0.9854 & 0.9934 & 0.9973\\
\hline
0.9 & 0.5359 & 0.6517 & 0.7549 & 0.8389 & 0.9015 & 0.9441 & 0.9706 & 0.9857 & 0.9936 & 0.9974\\
\hline
1 & 0.5398 & 0.6554 & 0.7580 & 0.8413 & 0.9032 & 0.9452 & 0.9713 & 0.9861 & 0.9938 & 0.9974\\
\hline
1.1 & 0.5438 & 0.6591 & 0.7611 & 0.8438 & 0.9049 & 0.9463 & 0.9719 & 0.9864 & 0.9940 & 0.9975\\
\hline
1.2 & 0.5478 & 0.6628 & 0.7642 & 0.8461 & 0.9066 & 0.9474 & 0.9726 & 0.9868 & 0.9941 & 0.9976\\
\hline
1.3 & 0.5517 & 0.6664 & 0.7673 & 0.8485 & 0.9082 & 0.9484 & 0.9732 & 0.9871 & 0.9943 & 0.9977\\
\hline
1.4 & 0.5557 & 0.6700 & 0.7704 & 0.8508 & 0.9099 & 0.9495 & 0.9738 & 0.9875 & 0.9945 & 0.9977\\
\hline
1.5 & 0.5596 & 0.6736 & 0.7734 & 0.8531 & 0.9115 & 0.9505 & 0.9744 & 0.9878 & 0.9946 & 0.9978\\
\hline
1.6 & 0.5636 & 0.6772 & 0.7764 & 0.8554 & 0.9131 & 0.9515 & 0.9750 & 0.9881 & 0.9948 & 0.9979\\
\hline
1.7 & 0.5675 & 0.6808 & 0.7794 & 0.8577 & 0.9147 & 0.9525 & 0.9756 & 0.9884 & 0.9949 & 0.9979\\
\hline
1.8 & 0.5714 & 0.6844 & 0.7823 & 0.8599 & 0.9162 & 0.9535 & 0.9761 & 0.9887 & 0.9951 & 0.9980\\
\hline
1.9 & 0.5753 & 0.6879 & 0.7852 & 0.8621 & 0.9177 & 0.9545 & 0.9767 & 0.9890 & 0.9952 & 0.9981\\
\hline
2 & 0.5793 & 0.6915 & 0.7881 & 0.8643 & 0.9192 & 0.9554 & 0.9772 & 0.9893 & 0.9953 & 0.9981\\
\hline
2.1 & 0.5832 & 0.6950 & 0.7910 & 0.8665 & 0.9207 & 0.9564 & 0.9778 & 0.9896 & 0.9955 & 0.9982\\
\hline
2.2 & 0.5871 & 0.6985 & 0.7939 & 0.8686 & 0.9222 & 0.9573 & 0.9783 & 0.9898 & 0.9956 & 0.9982\\
\hline
2.3 & 0.5910 & 0.7019 & 0.7967 & 0.8708 & 0.9236 & 0.9582 & 0.9788 & 0.9901 & 0.9957 & 0.9983\\
\hline
2.4 & 0.5948 & 0.7054 & 0.7995 & 0.8729 & 0.9251 & 0.9591 & 0.9793 & 0.9904 & 0.9959 & 0.9984\\
\hline
2.5 & 0.5987 & 0.7088 & 0.8023 & 0.8749 & 0.9265 & 0.9599 & 0.9798 & 0.9906 & 0.9960 & 0.9984\\
\hline
2.6 & 0.6026 & 0.7123 & 0.8051 & 0.8770 & 0.9279 & 0.9608 & 0.9803 & 0.9909 & 0.9961 & 0.9985\\
\hline
2.7 & 0.6064 & 0.7157 & 0.8078 & 0.8790 & 0.9292 & 0.9616 & 0.9808 & 0.9911 & 0.9962 & 0.9985\\
\hline
2.8 & 0.6103 & 0.7190 & 0.8106 & 0.8810 & 0.9306 & 0.9625 & 0.9812 & 0.9913 & 0.9963 & 0.9986\\
\hline
2.9 & 0.6141 & 0.7224 & 0.8133 & 0.8830 & 0.9319 & 0.9633 & 0.9817 & 0.9916 & 0.9964 & 0.9986\\
\hline
\end{tabular}
\end{table}

\begin{table}

\caption{\label{tab:Student}Quantiles of the Student-$t$ distribution. The rows correspond to different degrees of freedom ($\nu$, say); the columns correspond to different probabilities ($z$, say). The cell gives $q$ that is s.t. $\mathbb{P}(-q<X<q)=z$, with $X \sim t(\nu)$.}
\centering
\begin{tabular}[t]{l|r|r|r|r|r|r|r|r}
\hline
  & 0.05 & 0.1 & 0.75 & 0.9 & 0.95 & 0.975 & 0.99 & 0.999\\
\hline
1 & 0.079 & 0.158 & 2.414 & 6.314 & 12.706 & 25.452 & 63.657 & 636.619\\
\hline
2 & 0.071 & 0.142 & 1.604 & 2.920 & 4.303 & 6.205 & 9.925 & 31.599\\
\hline
3 & 0.068 & 0.137 & 1.423 & 2.353 & 3.182 & 4.177 & 5.841 & 12.924\\
\hline
4 & 0.067 & 0.134 & 1.344 & 2.132 & 2.776 & 3.495 & 4.604 & 8.610\\
\hline
5 & 0.066 & 0.132 & 1.301 & 2.015 & 2.571 & 3.163 & 4.032 & 6.869\\
\hline
6 & 0.065 & 0.131 & 1.273 & 1.943 & 2.447 & 2.969 & 3.707 & 5.959\\
\hline
7 & 0.065 & 0.130 & 1.254 & 1.895 & 2.365 & 2.841 & 3.499 & 5.408\\
\hline
8 & 0.065 & 0.130 & 1.240 & 1.860 & 2.306 & 2.752 & 3.355 & 5.041\\
\hline
9 & 0.064 & 0.129 & 1.230 & 1.833 & 2.262 & 2.685 & 3.250 & 4.781\\
\hline
10 & 0.064 & 0.129 & 1.221 & 1.812 & 2.228 & 2.634 & 3.169 & 4.587\\
\hline
20 & 0.063 & 0.127 & 1.185 & 1.725 & 2.086 & 2.423 & 2.845 & 3.850\\
\hline
30 & 0.063 & 0.127 & 1.173 & 1.697 & 2.042 & 2.360 & 2.750 & 3.646\\
\hline
40 & 0.063 & 0.126 & 1.167 & 1.684 & 2.021 & 2.329 & 2.704 & 3.551\\
\hline
50 & 0.063 & 0.126 & 1.164 & 1.676 & 2.009 & 2.311 & 2.678 & 3.496\\
\hline
60 & 0.063 & 0.126 & 1.162 & 1.671 & 2.000 & 2.299 & 2.660 & 3.460\\
\hline
70 & 0.063 & 0.126 & 1.160 & 1.667 & 1.994 & 2.291 & 2.648 & 3.435\\
\hline
80 & 0.063 & 0.126 & 1.159 & 1.664 & 1.990 & 2.284 & 2.639 & 3.416\\
\hline
90 & 0.063 & 0.126 & 1.158 & 1.662 & 1.987 & 2.280 & 2.632 & 3.402\\
\hline
100 & 0.063 & 0.126 & 1.157 & 1.660 & 1.984 & 2.276 & 2.626 & 3.390\\
\hline
200 & 0.063 & 0.126 & 1.154 & 1.653 & 1.972 & 2.258 & 2.601 & 3.340\\
\hline
500 & 0.063 & 0.126 & 1.152 & 1.648 & 1.965 & 2.248 & 2.586 & 3.310\\
\hline
\end{tabular}
\end{table}

\begin{table}

\caption{\label{tab:Chi2}Quantiles of the $\chi^2$ distribution. The rows correspond to different degrees of freedom; the columns correspond to different probabilities.}
\centering
\begin{tabular}[t]{l|r|r|r|r|r|r|r|r}
\hline
  & 0.05 & 0.1 & 0.75 & 0.9 & 0.95 & 0.975 & 0.99 & 0.999\\
\hline
1 & 0.004 & 0.016 & 1.323 & 2.706 & 3.841 & 5.024 & 6.635 & 10.828\\
\hline
2 & 0.103 & 0.211 & 2.773 & 4.605 & 5.991 & 7.378 & 9.210 & 13.816\\
\hline
3 & 0.352 & 0.584 & 4.108 & 6.251 & 7.815 & 9.348 & 11.345 & 16.266\\
\hline
4 & 0.711 & 1.064 & 5.385 & 7.779 & 9.488 & 11.143 & 13.277 & 18.467\\
\hline
5 & 1.145 & 1.610 & 6.626 & 9.236 & 11.070 & 12.833 & 15.086 & 20.515\\
\hline
6 & 1.635 & 2.204 & 7.841 & 10.645 & 12.592 & 14.449 & 16.812 & 22.458\\
\hline
7 & 2.167 & 2.833 & 9.037 & 12.017 & 14.067 & 16.013 & 18.475 & 24.322\\
\hline
8 & 2.733 & 3.490 & 10.219 & 13.362 & 15.507 & 17.535 & 20.090 & 26.124\\
\hline
9 & 3.325 & 4.168 & 11.389 & 14.684 & 16.919 & 19.023 & 21.666 & 27.877\\
\hline
10 & 3.940 & 4.865 & 12.549 & 15.987 & 18.307 & 20.483 & 23.209 & 29.588\\
\hline
20 & 10.851 & 12.443 & 23.828 & 28.412 & 31.410 & 34.170 & 37.566 & 45.315\\
\hline
30 & 18.493 & 20.599 & 34.800 & 40.256 & 43.773 & 46.979 & 50.892 & 59.703\\
\hline
40 & 26.509 & 29.051 & 45.616 & 51.805 & 55.758 & 59.342 & 63.691 & 73.402\\
\hline
50 & 34.764 & 37.689 & 56.334 & 63.167 & 67.505 & 71.420 & 76.154 & 86.661\\
\hline
60 & 43.188 & 46.459 & 66.981 & 74.397 & 79.082 & 83.298 & 88.379 & 99.607\\
\hline
70 & 51.739 & 55.329 & 77.577 & 85.527 & 90.531 & 95.023 & 100.425 & 112.317\\
\hline
80 & 60.391 & 64.278 & 88.130 & 96.578 & 101.879 & 106.629 & 112.329 & 124.839\\
\hline
90 & 69.126 & 73.291 & 98.650 & 107.565 & 113.145 & 118.136 & 124.116 & 137.208\\
\hline
100 & 77.929 & 82.358 & 109.141 & 118.498 & 124.342 & 129.561 & 135.807 & 149.449\\
\hline
200 & 168.279 & 174.835 & 213.102 & 226.021 & 233.994 & 241.058 & 249.445 & 267.541\\
\hline
500 & 449.147 & 459.926 & 520.950 & 540.930 & 553.127 & 563.852 & 576.493 & 603.446\\
\hline
\end{tabular}
\end{table}

\begin{table}

\caption{\label{tab:Fstat}Quantiles of the $\mathcal{F}$ distribution. The columns and rows correspond to different degrees of freedom (resp. $n_1$ and $n_2$). The different panels correspond to different probabilities ($\alpha$) The corresponding cell gives $z$ that is s.t. $\mathbb{P}(X \le z)=\alpha$, with $X \sim \mathcal{F}(n_1,n_2)$.}
\centering
\begin{tabular}[t]{l|r|r|r|r|r|r|r|r|r|r}
\hline
  & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10\\
\hline
alpha = 0.9 &  &  &  &  &  &  &  &  &  & \\
\hline
5 & 4.060 & 3.780 & 3.619 & 3.520 & 3.453 & 3.405 & 3.368 & 3.339 & 3.316 & 3.297\\
\hline
10 & 3.285 & 2.924 & 2.728 & 2.605 & 2.522 & 2.461 & 2.414 & 2.377 & 2.347 & 2.323\\
\hline
15 & 3.073 & 2.695 & 2.490 & 2.361 & 2.273 & 2.208 & 2.158 & 2.119 & 2.086 & 2.059\\
\hline
20 & 2.975 & 2.589 & 2.380 & 2.249 & 2.158 & 2.091 & 2.040 & 1.999 & 1.965 & 1.937\\
\hline
50 & 2.809 & 2.412 & 2.197 & 2.061 & 1.966 & 1.895 & 1.840 & 1.796 & 1.760 & 1.729\\
\hline
100 & 2.756 & 2.356 & 2.139 & 2.002 & 1.906 & 1.834 & 1.778 & 1.732 & 1.695 & 1.663\\
\hline
500 & 2.716 & 2.313 & 2.095 & 1.956 & 1.859 & 1.786 & 1.729 & 1.683 & 1.644 & 1.612\\
\hline
alpha = 0.95 &  &  &  &  &  &  &  &  &  & \\
\hline
5 & 6.608 & 5.786 & 5.409 & 5.192 & 5.050 & 4.950 & 4.876 & 4.818 & 4.772 & 4.735\\
\hline
10 & 4.965 & 4.103 & 3.708 & 3.478 & 3.326 & 3.217 & 3.135 & 3.072 & 3.020 & 2.978\\
\hline
15 & 4.543 & 3.682 & 3.287 & 3.056 & 2.901 & 2.790 & 2.707 & 2.641 & 2.588 & 2.544\\
\hline
20 & 4.351 & 3.493 & 3.098 & 2.866 & 2.711 & 2.599 & 2.514 & 2.447 & 2.393 & 2.348\\
\hline
50 & 4.034 & 3.183 & 2.790 & 2.557 & 2.400 & 2.286 & 2.199 & 2.130 & 2.073 & 2.026\\
\hline
100 & 3.936 & 3.087 & 2.696 & 2.463 & 2.305 & 2.191 & 2.103 & 2.032 & 1.975 & 1.927\\
\hline
500 & 3.860 & 3.014 & 2.623 & 2.390 & 2.232 & 2.117 & 2.028 & 1.957 & 1.899 & 1.850\\
\hline
alpha = 0.99 &  &  &  &  &  &  &  &  &  & \\
\hline
5 & 16.258 & 13.274 & 12.060 & 11.392 & 10.967 & 10.672 & 10.456 & 10.289 & 10.158 & 10.051\\
\hline
10 & 10.044 & 7.559 & 6.552 & 5.994 & 5.636 & 5.386 & 5.200 & 5.057 & 4.942 & 4.849\\
\hline
15 & 8.683 & 6.359 & 5.417 & 4.893 & 4.556 & 4.318 & 4.142 & 4.004 & 3.895 & 3.805\\
\hline
20 & 8.096 & 5.849 & 4.938 & 4.431 & 4.103 & 3.871 & 3.699 & 3.564 & 3.457 & 3.368\\
\hline
50 & 7.171 & 5.057 & 4.199 & 3.720 & 3.408 & 3.186 & 3.020 & 2.890 & 2.785 & 2.698\\
\hline
100 & 6.895 & 4.824 & 3.984 & 3.513 & 3.206 & 2.988 & 2.823 & 2.694 & 2.590 & 2.503\\
\hline
500 & 6.686 & 4.648 & 3.821 & 3.357 & 3.054 & 2.838 & 2.675 & 2.547 & 2.443 & 2.356\\
\hline
\end{tabular}
\end{table}

  \bibliography{book.bib,packages.bib}

\end{document}
