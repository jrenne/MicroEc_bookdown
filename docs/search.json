[{"path":"index.html","id":"intro","chapter":"Micro-Econometrics","heading":"Micro-Econometrics","text":"microeconometric models, variables interest often feature restricted distributions—instance discontinuous support—, necessitates specific models. Typical examples discrete-choice models (binary, multinomial, ordered outcomes), sample selection models (censored truncated outcomes), count-data models (integer outcomes). course describes estimation interpretation models. also shows discrete-choice models can emerge (structural) random-utility frameworks.R codes use various packages can obtained CRAN. AEC package available GitHub. install , one need employ devtools library:Useful (R) links:Download R:\nR software: https://cran.r-project.org (basic R software)\nRStudio: https://www.rstudio.com (convenient R editor)\nDownload R:R software: https://cran.r-project.org (basic R software)RStudio: https://www.rstudio.com (convenient R editor)Tutorials:\nRstudio: https://dss.princeton.edu/training/RStudio101.pdf (Oscar Torres-Reyna)\nR: https://cran.r-project.org/doc/contrib/Paradis-rdebuts_en.pdf (Emmanuel Paradis)\ntutorial: https://jrenne.shinyapps.io/Rtuto_publiShiny/\nTutorials:Rstudio: https://dss.princeton.edu/training/RStudio101.pdf (Oscar Torres-Reyna)R: https://cran.r-project.org/doc/contrib/Paradis-rdebuts_en.pdf (Emmanuel Paradis)tutorial: https://jrenne.shinyapps.io/Rtuto_publiShiny/","code":"\ninstall.packages(\"devtools\") # in case this library has not been loaded yet\nlibrary(devtools)\ninstall_github(\"jrenne/AEC\")\nlibrary(AEC)"},{"path":"Panel.html","id":"Panel","chapter":"1 Panel regressions","heading":"1 Panel regressions","text":"","code":""},{"path":"Panel.html","id":"specification-and-notations","chapter":"1 Panel regressions","heading":"1.1 Specification and notations","text":"standard panel situation follows: sample covers lot “entities”, indexed \\(\\\\{1,\\dots,n\\}\\), \\(n\\) large, , entity, observe different variables small number periods \\(t \\\\{1,\\dots,T\\}\\). longitudinal dataset.linear panel regression model :\n\\[\\begin{equation}\ny_{,t} = \\mathbf{x}'_{,t}\\underbrace{\\boldsymbol\\beta}_{K \\times 1} + \\underbrace{\\mathbf{z}'_{}\\boldsymbol\\alpha}_{\\mbox{Individual effects}} + \\varepsilon_{,t}.\\tag{1.1}\n\\end{equation}\\]running panel regressions, usual objective estimate \\(\\boldsymbol\\beta\\).Figure 1.1 illustrates panel-data situation. model \\(y_i = \\alpha_i + \\beta x_{,t} + \\varepsilon_{,t}\\), \\(t \\\\{1,2\\}\\). Panel (b), blue dots \\(t=1\\), red dots \\(t=2\\). lines relate dots associated entity \\(\\). remarkable simulated model , unconditional correlation \\(y\\) \\(x\\) negative, conditional correlation (conditional \\(\\alpha_i\\)) positive. Indeed, sign conditional correlation sign \\(\\beta\\), positive th simulated example (\\(\\beta=5\\)). words, one know panel nature data, tempting say \\(\\beta<0\\), case, due fixed effects (\\(\\alpha_i\\)’s) negatively correlated \\(x_i\\)’s.\nFigure 1.1: data panels. Panel (b), blue dots \\(t=1\\), red dots \\(t=2\\). lines relate dots associated entity \\(\\).\nFigure 1.2 presents type plot based Cigarette Consumption Panel dataset (CigarettesSW dataset, used Stock Watson (2003)). dataset documents average consumption cigarettes 48 continental US states two dates (1985 1995).\nFigure 1.2: Cigarette consumption versus real price CigarettesSW panel dataset.\nmake use following notations:\n\\[\n\\mathbf{y}_i =\n\\underbrace{\\left[\n\\begin{array}{c}\ny_{,1}\\\\\n\\vdots\\\\\ny_{,T}\n\\end{array}\\right]}_{T \\times 1}, \\quad\n\\boldsymbol\\varepsilon_i =\n\\underbrace{\\left[\n\\begin{array}{c}\n\\varepsilon_{,1}\\\\\n\\vdots\\\\\n\\varepsilon_{,T}\n\\end{array}\\right]}_{T \\times 1}, \\quad\n\\mathbf{x}_i =\n\\underbrace{\\left[\n\\begin{array}{c}\n\\mathbf{x}_{,1}'\\\\\n\\vdots\\\\\n\\mathbf{x}_{,T}'\n\\end{array}\\right]}_{T \\times K}, \\quad\n\\mathbf{X} =\n\\underbrace{\\left[\n\\begin{array}{c}\n\\mathbf{x}_{1}\\\\\n\\vdots\\\\\n\\mathbf{x}_{n}\n\\end{array}\\right]}_{(nT) \\times K}.\n\\]\n\\[\n\\tilde{\\mathbf{y}}_i =\n\\left[\n\\begin{array}{c}\ny_{,1} - \\bar{y}_i\\\\\n\\vdots\\\\\ny_{,T} - \\bar{y}_i\n\\end{array}\\right], \\quad\n\\tilde{\\boldsymbol\\varepsilon}_i =\n\\left[\n\\begin{array}{c}\n\\varepsilon_{,1} - \\bar{\\varepsilon}_i\\\\\n\\vdots\\\\\n\\varepsilon_{,T} - \\bar{\\varepsilon}_i\n\\end{array}\\right],\n\\]\n\\[\n\\tilde{\\mathbf{x}}_i =\n\\left[\n\\begin{array}{c}\n\\mathbf{x}_{,1}' - \\bar{\\mathbf{x}}_i'\\\\\n\\vdots\\\\\n\\mathbf{x}_{,T}' - \\bar{\\mathbf{x}}_i'\n\\end{array}\\right], \\quad\n\\tilde{\\mathbf{X}} =\n\\left[\n\\begin{array}{c}\n\\tilde{\\mathbf{x}}_{1}\\\\\n\\vdots\\\\\n\\tilde{\\mathbf{x}}_{n}\n\\end{array}\\right], \\quad\n\\tilde{\\mathbf{Y}} =\n\\left[\n\\begin{array}{c}\n\\tilde{\\mathbf{y}}_{1}\\\\\n\\vdots\\\\\n\\tilde{\\mathbf{y}}_{n}\n\\end{array}\\right],\n\\]\n\n\\[\n\\bar{y}_i = \\frac{1}{T} \\sum_{t=1}^T y_{,t}, \\quad \\bar{\\varepsilon}_i = \\frac{1}{T}\\sum_{t=1}^T \\varepsilon_{,t} \\quad \\mbox{} \\quad \\bar{\\mathbf{x}}_i = \\frac{1}{T}\\sum_{t=1}^T \\mathbf{x}_{,t}.\n\\]","code":"\nT <- 2; n <- 12 # 2 periods and 12 entities\nalpha <- 5*rnorm(n) # draw fixed effects\nx.1 <- rnorm(n) - .5*alpha # note: x_i's correlate to alpha_i's\nx.2 <- rnorm(n) - .5*alpha\nbeta <- 5; sigma <- .3\ny.1 <- alpha + x.1 + sigma*rnorm(n);y.2 <- alpha + x.2 + sigma*rnorm(n)\nx <- c(x.1,x.2) # pooled x\ny <- c(y.1,y.2) # pooled y\npar(mfrow=c(1,2))\nplot(x,y,col=\"black\",pch=19,xlab=\"x\",ylab=\"y\",main=\"(a)\")\nplot(x,y,col=\"black\",pch=19,xlab=\"x\",ylab=\"y\",main=\"(b)\")\npoints(x.1,y.1,col=\"blue\",pch=19);points(x.2,y.2,col=\"red\",pch=19)\nfor(i in 1:n){lines(c(x.1[i],x.2[i]),c(y.1[i],y.2[i]))}"},{"path":"Panel.html","id":"three-standard-cases","chapter":"1 Panel regressions","heading":"1.2 Three standard cases","text":"three typical situations:Pooled regression: \\(\\mathbf{z}_i \\equiv 1\\). case amounts case studied Chapter ??.Fixed Effects (Section 1.3): \\(\\mathbf{z}_i\\) unobserved, correlates \\(\\mathbf{x}_i\\) \\(\\Rightarrow\\) \\(\\mathbf{b}\\) biased inconsistent OLS regression \\(\\mathbf{y}\\) \\(\\mathbf{X}\\) (omitted variable, see Section ??).Random Effects (Section 1.4): \\(\\mathbf{z}_i\\) unobserved, uncorrelated \\(\\mathbf{x}_i\\). model writes:\n\\[\ny_{,t} = \\mathbf{x}'_{,t}\\boldsymbol\\beta + \\alpha +  \\underbrace{{\\color{blue}u_i + \\varepsilon_{,t}}}_{\\mbox{compound error}},\n\\]\n\\(\\alpha = \\mathbb{E}(\\mathbf{z}'_{}\\boldsymbol\\alpha)\\) \\(u_i = \\mathbf{z}'_{}\\boldsymbol\\alpha - \\mathbb{E}(\\mathbf{z}'_{}\\boldsymbol\\alpha) \\perp \\mathbf{x}_i\\). case, OLS consistent, efficient. GLS can used gain efficiencies OLS (see Section ?? presentation GLS approach).","code":""},{"path":"Panel.html","id":"FixedEffect","chapter":"1 Panel regressions","heading":"1.3 Estimation of Fixed-Effects Models","text":"Hypothesis 1.1  (Fixed-effect model) assume :perfect multicollinearity among regressors.\\(\\mathbb{E}(\\varepsilon_{,t}|\\mathbf{X})=0\\), \\(,t\\).:\n\\[\n\\mathbb{E}(\\varepsilon_{,t}\\varepsilon_{j,s}|\\mathbf{X}) =\n\\left\\{\n\\begin{array}{cl}\n\\sigma^2 & \\mbox{$=j$ $s=t$},\\\\\n0 & \\mbox{otherwise.}\n\\end{array}\\right.\n\\]assumptions analogous introduced standard linear regression:\\(\\leftrightarrow\\) Hyp. ??, (ii) \\(\\leftrightarrow\\) Hyp. ??, (iii) \\(\\leftrightarrow\\) Hyp. ?? + ??.matrix form, given \\(\\), model writes:\n\\[\n\\mathbf{y}_i = \\mathbf{x}_i \\boldsymbol\\beta + \\mathbf{1}\\alpha_i + \\boldsymbol\\varepsilon_i,\n\\]\n\\(\\mathbf{1}\\) \\(T\\)-dimensional vector ones.Least Square Dummy Variable (LSDV) model:\n\\[\\begin{equation}\n\\mathbf{y} = [\\mathbf{X} \\quad \\mathbf{D}]\n\\left[\n\\begin{array}{c}\n\\boldsymbol\\beta\\\\\n\\boldsymbol\\alpha\n\\end{array}\n\\right]\n+ \\boldsymbol\\varepsilon, \\tag{1.2}\n\\end{equation}\\]\n:\n\\[\n\\mathbf{D} = \\underbrace{ \\left[\\begin{array}{cccc}\n\\mathbf{1}&\\mathbf{0}&\\dots&\\mathbf{0}\\\\\n\\mathbf{0}&\\mathbf{1}&\\dots&\\mathbf{0}\\\\\n&&\\vdots&\\\\\n\\mathbf{0}&\\mathbf{0}&\\dots&\\mathbf{1}\\\\\n\\end{array}\\right]}_{(nT \\times n)}.\n\\]linear regression (Eq. (1.2)) —dummy variables— satisfies Gauss-Markov conditions (Theorem ??). Hence, context, OLS estimator best linear unbiased estimator (BLUE).Denoting \\(\\mathbf{Z}\\) matrix \\([\\mathbf{X} \\quad \\mathbf{D}]\\), \\(\\mathbf{b}\\) \\(\\mathbf{}\\) respective OLS estimates \\(\\boldsymbol\\beta\\) \\(\\boldsymbol\\alpha\\), :\n\\[\\begin{equation}\n\\boxed{\n\\left[\n\\begin{array}{c}\n\\mathbf{b}\\\\\n\\mathbf{}\n\\end{array}\n\\right]\n= [\\mathbf{Z}'\\mathbf{Z}]^{-1}\\mathbf{Z}'\\mathbf{y}.} \\tag{1.3}\n\\end{equation}\\]asymptotical distribution \\([\\mathbf{b}',\\mathbf{}']'\\) derives standard OLS context: Prop. ?? can used replaced \\(\\mathbf{X}\\) \\(\\mathbf{Z}=[\\mathbf{X} \\quad \\mathbf{D}]\\).:\n\\[\\begin{equation}\n\\boxed{\\left[\n\\begin{array}{c}\n\\mathbf{b}\\\\\n\\mathbf{}\n\\end{array}\n\\right] \\overset{d}{\\rightarrow}\n\\mathcal{N}\\left(\n\\left[\n\\begin{array}{c}\n\\boldsymbol\\beta\\\\\n\\boldsymbol\\alpha\n\\end{array}\n\\right],\n\\sigma^2 \\frac{Q^{-1}}{nT}\n\\right),}\n\\end{equation}\\]\n\n\\[\nQ = \\mbox{plim}_{nT \\rightarrow \\infty} \\frac{1}{nT} \\mathbf{Z}'\\mathbf{Z},\n\\]\nassuming previous limit exists.practice, estimator covariance matrix \\([\\mathbf{b}',\\mathbf{}']'\\) :\n\\[\ns^2 \\left( \\mathbf{Z}'\\mathbf{Z}\\right)^{-1} \\quad \\quad s^2 = \\frac{\\mathbf{e}'\\mathbf{e}}{nT - K - n},\n\\]\n\\(\\mathbf{e}\\) \\((nT) \\times 1\\) vector OLS residuals.alternative way expressing LSDV estimators. involves residual-maker matrix matrix \\(\\mathbf{M_D}=\\mathbf{} - \\mathbf{D}(\\mathbf{D}'\\mathbf{D})^{-1}\\mathbf{D}'\\) (see Eq. (??)), acts operator removes entity-specific means, .e.:\n\\[\n\\tilde{\\mathbf{Y}} = \\mathbf{M_D}\\mathbf{Y}, \\quad \\tilde{\\mathbf{X}} = \\mathbf{M_D}\\mathbf{X} \\quad \\quad \\tilde{\\boldsymbol\\varepsilon} = \\mathbf{M_D}\\boldsymbol\\varepsilon.\n\\]notations, using Frisch-Waugh theorem (Theorem ??), get another expression estimator \\(\\mathbf{b}\\) appearing Eq. (1.3):\n\\[\\begin{equation}\n\\boxed{\\mathbf{b} = [\\mathbf{X}'\\mathbf{M_D}\\mathbf{X}]^{-1}\\mathbf{X}'\\mathbf{M_D}\\mathbf{y}.}\\tag{1.4}\n\\end{equation}\\]amounts regressing \\(\\tilde{y}_{,t}\\)’s (\\(= y_{,t} - \\bar{y}_i\\)) \\(\\tilde{\\mathbf{x}}_{,t}\\)’s (\\(=\\mathbf{x}_{,t} - \\bar{\\mathbf{x}}_i\\)).estimate \\(\\boldsymbol\\alpha\\) given :\n\\[\\begin{equation}\n\\boxed{\\mathbf{} = (\\mathbf{D}'\\mathbf{D})^{-1}\\mathbf{D}'(\\mathbf{y} - \\mathbf{X}\\mathbf{b}),} \\tag{1.5}\n\\end{equation}\\]\nobtained developing second row \n\\[\n\\left[\n\\begin{array}{cc}\n\\mathbf{X}'\\mathbf{X} & \\mathbf{X}'\\mathbf{D}\\\\\n\\mathbf{D}'\\mathbf{X} & \\mathbf{D}'\\mathbf{D}\n\\end{array}\\right]\n\\left[\n\\begin{array}{c}\n\\mathbf{b}\\\\\n\\mathbf{}\n\\end{array}\\right] =\n\\left[\n\\begin{array}{c}\n\\mathbf{X}'\\mathbf{Y}\\\\\n\\mathbf{D}'\\mathbf{Y}\n\\end{array}\\right],\n\\]\nfirst-order conditions resulting least squares problem (see Eq. (??)).One can use different types fixed effects regression. Typically, one can time entity fixed effects. case, model writes:\n\\[\ny_{,t} = \\mathbf{x}_i'\\boldsymbol\\beta + \\alpha_i + \\gamma_t + \\varepsilon_{,t}.\n\\]LSDV approach (Eq. (1.2)) can still resorted . suffices extend \\(\\mathbf{Z}\\) matrix additional columns (called time dummies):\n\\[\\begin{equation}\n\\mathbf{y} = [\\mathbf{X} \\quad \\mathbf{D} \\quad \\mathbf{C}]\n\\left[\n\\begin{array}{c}\n\\boldsymbol\\beta\\\\\n\\boldsymbol\\alpha\\\\\n\\boldsymbol\\gamma\n\\end{array}\n\\right]\n+ \\boldsymbol\\varepsilon, \\tag{1.6}\n\\end{equation}\\]\n:\n\\[\n\\mathbf{C} = \\left[\\begin{array}{cccc}\n\\boldsymbol{\\delta}_1&\\boldsymbol{\\delta}_2&\\dots&\\boldsymbol{\\delta}_{T-1}\\\\\n\\vdots&\\vdots&&\\vdots\\\\\n\\boldsymbol{\\delta}_1&\\boldsymbol{\\delta}_2&\\dots&\\boldsymbol{\\delta}_{T-1}\\\\\n\\end{array}\\right],\n\\]\n\\(T\\)-dimensional vector \\(\\boldsymbol\\delta_t\\) (time dummy) \n\\[\n[0,\\dots,0,\\underbrace{1}_{\\mbox{t$^{th}$ entry}},0,\\dots,0]'.\n\\]Using state year fixed effects CigarettesSW panel dataset yields following results:Example 1.1  (Housing prices interest rates) example, want estimate effect short long-term interest rate housing prices. data come Jordà, Schularick, Taylor (2017) dataset (see website).","code":"\nCigarettesSW$rincome <- with(CigarettesSW, income/population/cpi)\neq.pooled <- lm(log(packs)~log(rprice)+log(rincome),data=CigarettesSW)\neq.LSDV <- lm(log(packs)~log(rprice)+log(rincome)+state,\n              data=CigarettesSW)\nCigarettesSW$year <- as.factor(CigarettesSW$year)\neq.LSDV2 <- lm(log(packs)~log(rprice)+log(rincome)+state+year,\n               data=CigarettesSW)\nstargazer::stargazer(eq.pooled,eq.LSDV,eq.LSDV2,type=\"text\",no.space = TRUE,\n                     omit=c(\"state\",\"year\"),\n                     add.lines=list(c('State FE','No','Yes','Yes'),\n                                    c('Year FE','No','No','Yes')),\n                     omit.stat=c(\"f\",\"ser\"))## \n## ==========================================\n##                   Dependent variable:     \n##              -----------------------------\n##                       log(packs)          \n##                 (1)       (2)       (3)   \n## ------------------------------------------\n## log(rprice)  -1.334*** -1.210*** -1.056***\n##               (0.135)   (0.114)   (0.149) \n## log(rincome)  0.318**    0.121     0.497  \n##               (0.136)   (0.190)   (0.304) \n## Constant     10.067*** 9.954***  8.360*** \n##               (0.516)   (0.264)   (1.049) \n## ------------------------------------------\n## State FE        No        Yes       Yes   \n## Year FE         No        No        Yes   \n## Observations    96        96        96    \n## R2             0.552     0.966     0.967  \n## Adjusted R2    0.542     0.929     0.931  \n## ==========================================\n## Note:          *p<0.1; **p<0.05; ***p<0.01\nlibrary(AEC);library(sandwich)\ndata(JST); JST <- subset(JST,year>1950);N <- dim(JST)[1]\nJST$hpreal <- JST$hpnom/JST$cpi # real house price index\nJST$dhpreal <- 100*log(JST$hpreal/c(NaN,JST$hpreal[1:(N-1)]))\n# Put NA's when change in country:\nJST$dhpreal[c(0,JST$iso[2:N]!=JST$iso[1:(N-1)])] <- NaN\nJST$dhpreal[abs(JST$dhpreal)>30] <- NaN # remove extreme price change\nJST$YEAR <- as.factor(JST$year) # to have time fixed effects\neq1_noFE <- lm(dhpreal ~ stir + ltrate,data=JST)\neq1_FE   <- lm(dhpreal ~ stir + ltrate + iso + YEAR,data=JST)\neq2_noFE <- lm(dhpreal ~ I(ltrate-stir),data=JST)\neq2_FE <- lm(dhpreal ~ I(ltrate-stir) + iso + YEAR,data=JST)\nvcov_cluster1_noFE <- vcovHC(eq1_noFE, cluster = JST[, c(\"iso\",\"YEAR\")])\nvcov_cluster1_FE   <- vcovHC(eq1_FE, cluster = JST[, c(\"iso\",\"YEAR\")])\nvcov_cluster2_noFE <- vcovHC(eq2_noFE, cluster = JST[, c(\"iso\",\"YEAR\")])\nvcov_cluster2_FE   <- vcovHC(eq2_FE, cluster = JST[, c(\"iso\",\"YEAR\")])\nrobust_se_FE1_noFE <- sqrt(diag(vcov_cluster1_noFE))\nrobust_se_FE1_FE   <- sqrt(diag(vcov_cluster1_FE))\nrobust_se_FE2_noFE <- sqrt(diag(vcov_cluster2_noFE))\nrobust_se_FE2_FE   <- sqrt(diag(vcov_cluster2_FE))\nstargazer::stargazer(eq1_noFE, eq1_FE, eq2_noFE, eq2_FE, type = \"text\",\n                     column.labels = c(\"no FE\", \"with FE\", \"no FE\",\"with FE\"),\n                     omit = c(\"iso\",\"YEAR\",\"Constant\"),keep.stat = \"n\",\n                     add.lines=list(c('Country FE','No','Yes','No','Yes'),\n                                    c('Year FE','No','Yes','No','Yes')),\n                     se = list(robust_se_FE1_noFE,robust_se_FE1_FE,\n                               robust_se_FE2_noFE,robust_se_FE2_FE))## \n## =======================================================\n##                           Dependent variable:          \n##                  --------------------------------------\n##                                 dhpreal                \n##                    no FE   with FE    no FE    with FE \n##                     (1)      (2)       (3)       (4)   \n## -------------------------------------------------------\n## stir             0.485***  0.532***                    \n##                   (0.149)  (0.170)                     \n##                                                        \n## ltrate           -0.690*** -0.384**                    \n##                   (0.164)  (0.182)                     \n##                                                        \n## I(ltrate - stir)                    -0.476*** -0.475***\n##                                      (0.145)   (0.159) \n##                                                        \n## -------------------------------------------------------\n## Country FE          No       Yes       No        Yes   \n## Year FE             No       Yes       No        Yes   \n## Observations       1,141    1,141     1,141     1,141  \n## =======================================================\n## Note:                       *p<0.1; **p<0.05; ***p<0.01"},{"path":"Panel.html","id":"RandomEffect","chapter":"1 Panel regressions","heading":"1.4 Estimation of random effects models","text":", individual effects assumed correlated variables (\\(\\mathbf{x}_i\\)’s). context, OLS estimator consistent. However, efficient. GLS approach can employed gain efficiency.Random-effect models write:\n\\[\ny_{,t}=\\mathbf{x}'_{}\\boldsymbol\\beta + (\\alpha + \\underbrace{u_i}_{\\substack{\\text{Random}\\\\\\text{heterogeneity}}}) + \\varepsilon_{,t},\n\\]\n\n\\[\\begin{eqnarray*}\n\\mathbb{E}(\\varepsilon_{,t}|\\mathbf{X})&=&\\mathbb{E}(u_{}|\\mathbf{X}) =0,\\\\\n\\mathbb{E}(\\varepsilon_{,t}\\varepsilon_{j,s}|\\mathbf{X}) &=&\n\\left\\{\n\\begin{array}{cl}\n\\sigma_\\varepsilon^2 & \\mbox{ $=j$ $s=t$},\\\\\n0 & \\mbox{ otherwise.}\n\\end{array}\n\\right.\\\\\n\\mathbb{E}(u_{}u_{j}|\\mathbf{X}) &=&\n\\left\\{\n\\begin{array}{cl}\n\\sigma_u^2 & \\mbox{ $=j$},\\\\\n0 & \\mbox{otherwise.}\n\\end{array}\n\\right.\\\\\n\\mathbb{E}(\\varepsilon_{,t}u_{j}|\\mathbf{X})&=&0 \\quad \\text{$$, $j$ $t$}.\n\\end{eqnarray*}\\]Introducing notations \\(\\eta_{,t} = u_i + \\varepsilon_{,t}\\) \\(\\boldsymbol\\eta_i = [\\eta_{,1},\\dots,\\eta_{,T}]'\\), \\(\\mathbb{E}(\\boldsymbol\\eta_i |\\mathbf{X}) = \\mathbf{0}\\) \\(\\mathbb{V}ar(\\boldsymbol\\eta_i | \\mathbf{X}) = \\boldsymbol\\Gamma\\), \n\\[\n\\boldsymbol\\Gamma = \\left[  \\begin{array}{ccccc}\n\\sigma_\\varepsilon^2+\\sigma_u^2 & \\sigma_u^2 & \\sigma_u^2 & \\dots & \\sigma_u^2\\\\\n\\sigma_u^2 & \\sigma_\\varepsilon^2+\\sigma_u^2 & \\sigma_u^2 & \\dots & \\sigma_u^2\\\\\n\\vdots && \\ddots && \\vdots \\\\\n\\sigma_u^2 & \\sigma_u^2 & \\sigma_u^2 & \\dots & \\sigma_\\varepsilon^2+\\sigma_u^2\\\\\n\\end{array}\n\\right] = \\sigma_\\varepsilon^2\\mathbf{} + \\sigma_u^2\\mathbf{1}\\mathbf{1}'.\n\\]Denoting \\(\\boldsymbol\\Sigma\\) covariance matrix \\(\\boldsymbol\\eta = [\\boldsymbol\\eta_1',\\dots,\\boldsymbol\\eta_n']'\\), :\n\\[\n\\boldsymbol\\Sigma = \\mathbf{} \\otimes \\boldsymbol\\Gamma.\n\\]knew \\(\\boldsymbol\\Sigma\\), apply (feasible) GLS (Eq. (??), Section ??):\n\\[\n\\boldsymbol\\beta = (\\mathbf{X}'\\boldsymbol\\Sigma^{-1}\\mathbf{X})^{-1}\\mathbf{X}'\\boldsymbol\\Sigma^{-1}\\mathbf{y}.\n\\]\n(explained Section ??, amounts regressing \\({\\boldsymbol\\Sigma^{-1/2}}'\\mathbf{y}\\) \\({\\boldsymbol\\Sigma^{-1/2}}'\\mathbf{X}\\).)can checked \\(\\boldsymbol\\Sigma^{-1/2} = \\mathbf{} \\otimes (\\boldsymbol\\Gamma^{-1/2})\\) \n\\[\n\\boldsymbol\\Gamma^{-1/2} = \\frac{1}{\\sigma_\\varepsilon}\\left( \\mathbf{} - \\frac{\\theta}{T}\\mathbf{1}\\mathbf{1}'\\right),\\quad \\mbox{}\\quad\\theta = 1 - \\frac{\\sigma_\\varepsilon}{\\sqrt{\\sigma_\\varepsilon^2+T\\sigma_u^2}}.\n\\]Hence, knew \\(\\boldsymbol\\Sigma\\), transform data follows:\n\\[\n\\boldsymbol\\Gamma^{-1/2}\\mathbf{y}_i = \\frac{1}{\\sigma_\\varepsilon}\\left[\\begin{array}{c}y_{,1} - \\theta\\bar{y}_i\\\\y_{,2} - \\theta\\bar{y}_i\\\\\\vdots\\\\y_{,T} - \\theta\\bar{y}_i\\\\\\end{array}\\right].\n\\]\\(\\boldsymbol\\Sigma\\) unknown? One can take deviations group means remove heterogeneity:\n\\[\\begin{equation}\ny_{,t} - \\bar{y}_i = [\\mathbf{x}_{,t} - \\bar{\\mathbf{x}}_i]'\\boldsymbol\\beta + (\\varepsilon_{,t} - \\bar{\\varepsilon}_i).\\tag{1.7}\n\\end{equation}\\]\nprevious equation can consistently estimated OLS. (Although residuals correlated across \\(t\\)’s observations pertaining given entity, OLS remain consistent; see Prop. ??.)\\(\\mathbb{E}\\left[\\sum_{=1}^{T}(\\varepsilon_{,t}-\\bar{\\varepsilon}_i)^2\\right] = (T-1)\\sigma_{\\varepsilon}^2\\).\\(\\varepsilon_{,t}\\)’s observed \\(\\mathbf{b}\\), OLS estimator \\(\\boldsymbol\\beta\\) Eq. (1.7), consistent estimator \\(\\boldsymbol\\beta\\). Using adjustment degrees freedom, can approximate variance :\n\\[\n\\hat{\\sigma}_e^2 = \\frac{1}{nT-n-K}\\sum_{=1}^{n}\\sum_{t=1}^{T}(e_{,t} - \\bar{e}_i)^2.\n\\]\\(\\sigma_u^2\\)? can exploit fact OLS consistent pooled regression:\n\\[\n\\mbox{plim }s^2_{pooled} = \\mbox{plim }\\frac{\\mathbf{e}'\\mathbf{e}}{nT-K-1} = \\sigma_u^2 + \\sigma_\\varepsilon^2,\n\\]\ntherefore use \\(s^2_{pooled} - \\hat{\\sigma}_e^2\\) approximation \\(\\sigma_u^2\\).Let us come back Example 1.1 (relationship changes housing prices interest rates). following, use random effect specification; compare results obtained pooled regression fixed-effect model. , use function plm package name. (Note eq.FE similar eq1 Example 1.1.)One can run Hausman (1978) test order check whether fixed-effect model needed. Indeed, case (.e., covariates correlated disturbances), preferable use random-effect estimation latter efficient.p-value high, reject null hypothesis according covariates errors uncorrelated. therefore prefer random-effect model.Example 1.2  (Spatial data) example makes use Airbnb prices (Zürich, 22 June 2017), collected Tom Slee’s website. covariates number bedrooms number people can accommodated. consider use district fixed effects. Figure 1.3 shows price explain (size circles proportional prices). white lines delineate 12 districts city.\nFigure 1.3: Airbnb prices Zurich area, 22 June 2017. size circles proportional prices. White lines delineate 12 districts city.\nLet us regress prices covariates well district dummies:Figure 1.4 compares residuals without fixed effects. sizes circles proportional absolute values residuals, color indicates sign (blue positive).\nFigure 1.4: Regression residuals. sizes circles proportional absolute values residuals, color indicates sign (blue negative).\nfixed effects, colors better balanced within district.","code":"\nlibrary(plm);library(stargazer)\neq.RE <- plm(dhpreal ~ stir + ltrate,data=JST,index=c(\"iso\",\"YEAR\"),\n             model=\"random\",effect=\"twoways\")\neq.FE <- plm(dhpreal ~ stir + ltrate,data=JST,index=c(\"iso\",\"YEAR\"),\n             model=\"within\",effect=\"twoways\")\neq0   <- plm(dhpreal ~ stir + ltrate,data=JST,index=c(\"iso\",\"YEAR\"),\n             model=\"pooling\") \nstargazer(eq0, eq.RE, eq.FE, type = \"text\",no.space = TRUE,\n                     column.labels=c(\"Pooled\",\"Random Effect\",\"Fixed Effects\"),\n                     add.lines=list(c('State FE','No','Yes','Yes'),\n                                    c('Year FE','No','Yes','Yes')),\n                     omit.stat=c(\"f\",\"ser\"))## \n## ==================================================\n##                       Dependent variable:         \n##              -------------------------------------\n##                             dhpreal               \n##               Pooled   Random Effect Fixed Effects\n##                 (1)         (2)           (3)     \n## --------------------------------------------------\n## stir         0.485***    0.456***      0.532***   \n##               (0.114)     (0.019)       (0.134)   \n## ltrate       -0.690***   -0.541***     -0.384***  \n##               (0.127)     (0.020)       (0.145)   \n## Constant     4.103***    3.341***                 \n##               (0.421)     (0.096)                 \n## --------------------------------------------------\n## State FE        No          Yes           Yes     \n## Year FE         No          Yes           Yes     \n## Observations   1,141       1,141         1,141    \n## R2             0.027       0.024         0.015    \n## Adjusted R2    0.025       0.022        -0.067    \n## ==================================================\n## Note:                  *p<0.1; **p<0.05; ***p<0.01\nphtest(eq.FE,eq.RE)## \n##  Hausman Test\n## \n## data:  dhpreal ~ stir + ltrate\n## chisq = 3.8386, df = 2, p-value = 0.1467\n## alternative hypothesis: one model is inconsistent\neq_noFE <- lm(price~bedrooms+accommodates,data=airbnb)\neq_FE   <- lm(price~bedrooms+accommodates+neighborhood,data=airbnb)\n# Adjust standard errors:\ncov_FE          <- vcovHC(eq_FE, cluster = airbnb[, c(\"neighborhood\")])\nrobust_se_FE    <- sqrt(diag(cov_FE))\ncov_noFE        <- vcovHC(eq_noFE, cluster = airbnb[, c(\"neighborhood\")])\nrobust_se_noFE  <- sqrt(diag(cov_noFE))\nstargazer::stargazer(eq_FE, eq_noFE, eq_FE, eq_noFE, type = \"text\",\n                     column.labels = c(\"FE (no HAC)\", \"No FE (no HAC)\",\n                                       \"FE (with HAC)\", \"No FE (with HAC)\"),\n                     omit = c(\"neighborhood\"),no.space = TRUE,\n                     omit.labels = c(\"District FE\"),keep.stat = \"n\",\n                     se = list(NULL, NULL, robust_se_FE, robust_se_noFE))## \n## ======================================================================\n##                                 Dependent variable:                   \n##              ---------------------------------------------------------\n##                                        price                          \n##              FE (no HAC) No FE (no HAC) FE (with HAC) No FE (with HAC)\n##                  (1)          (2)            (3)            (4)       \n## ----------------------------------------------------------------------\n## bedrooms      7.229***      5.629**       7.229***        5.629***    \n##                (2.135)      (2.194)        (2.052)        (2.073)     \n## accommodates  16.426***    17.449***      16.426***      17.449***    \n##                (1.284)      (1.323)        (1.431)        (1.428)     \n## Constant      95.118***    68.417***      95.118***      68.417***    \n##                (5.323)      (3.223)        (5.664)        (3.527)     \n## ----------------------------------------------------------------------\n## District FE      Yes           No            Yes             No       \n## ----------------------------------------------------------------------\n## Observations    1,321        1,321          1,321          1,321      \n## ======================================================================\n## Note:                                      *p<0.1; **p<0.05; ***p<0.01"},{"path":"Panel.html","id":"DynPanel","chapter":"1 Panel regressions","heading":"1.5 Dynamic Panel Regressions","text":"precedes, assumed correlation observations indexed \\((,t)\\) indexed \\((j,s)\\) long \\(j \\ne \\) \\(t \\ne s\\). one suspects errors \\(\\varepsilon_{,t}\\) correlated (across entities \\(\\) given date \\(t\\), across dates given entity, ), one employ robust covariance matrix (see Section ??).several cases, auto-correlation variable interest may stem auto-regressive specification. , Eq. (1.1) replaced :\n\\[\\begin{equation}\ny_{,t} = \\rho y_{,t-1} + \\mathbf{x}'_{,t}\\underbrace{\\boldsymbol\\beta}_{K \\times 1} + \\underbrace{\\alpha_i}_{\\mbox{Individual effects}} + \\varepsilon_{,t}.\\tag{1.8}\n\\end{equation}\\]case, even explanatory variables \\(\\mathbf{x}_{,t}\\) uncorrelated errors \\(\\varepsilon_{,t}\\), additional explanatory variable \\(y_{,t-1}\\) correlates errors \\(\\varepsilon_{,t-1},\\varepsilon_{,t-2},\\dots,\\varepsilon_{,1}\\). result, LSDV estimate model parameters \\(\\{\\rho,\\boldsymbol\\beta\\}\\) may biased, even \\(n\\) large. see , notice LSDV regression amounts regressing \\(\\widetilde{\\mathbf{y}}\\) \\(\\widetilde{\\mathbf{X}}\\) (see Eq. (1.4)), elements \\(\\widetilde{\\mathbf{X}}\\) explanatory variables subtract within-sample means. particular, :\n\\[\n\\tilde{y}_{,t-1} = y_{,t-1} - \\frac{1}{T} \\sum_{s=1}^{T} y_{,s-1},\n\\]\ncorrelates corresponding error, :\n\\[\n\\tilde{\\varepsilon}_{,t} = \\varepsilon_{,t} - \\frac{1}{T} \\sum_{s=1}^{T} \\varepsilon_{,s}.\n\\]previous equation shows within-group estimator (LSDV) introduces realizations \\(\\varepsilon_{,t}\\) errors transformed error term (\\(\\tilde{\\varepsilon}_{,t}\\)). result, large-\\(n\\) fixed-\\(T\\) panels, consistent right-hand-side variables regression strictly exogenous (.e., correlate past, present, future errors \\(\\varepsilon_{,t}\\)).1 case lags \\(y_{,t}\\) right-hand side regression formula.following simulation illustrate bias. \\(x\\)-coordinates dots fixed effects \\(\\alpha_i\\)’s, \\(y\\)-coordinates LSDV estimates. blue line 45-degree line.\nFigure 1.5: illustration bias pertianing LSDV estimation approach presence auto-correlation depend variable.\nprevious example, estimate \\(\\rho\\) (whose true value 0.8) 0.531.address , one can resort instrumental-variable regressions. Anderson Hsiao (1982) , particular, proposed first-differenced Two Stage Least Squares (2SLS) estimator (see Eq. (??) Section ??). estimation based following transformation model:\n\\[\\begin{equation}\n\\Delta y_{,t} = \\rho \\Delta y_{,t-1} + (\\Delta \\mathbf{x}_{,t})'\\boldsymbol\\beta + \\Delta\\varepsilon_{,t}.\\tag{1.9}\n\\end{equation}\\]\nOLS estimates parameters biased \\(\\varepsilon_{,t-1}\\) —part error \\(\\Delta\\varepsilon_{,t}\\)— correlated \\(y_{,t-1}\\) —part “explanatory variable”, namely \\(\\Delta y_{,t-1}\\). consistent estimates can obtained using 2SLS instrumental variables correlated \\(\\Delta y_{,t}\\) orthogonal \\(\\Delta\\varepsilon_{,t}\\). One can instance use \\(\\{y_{,t-2},\\mathbf{x}_{,t-2}\\}\\) instruments. Note approach can implemented 3 time observations per entity \\(\\).explanatory variables \\(\\mathbf{x}_{,t}\\) assumed predetermined (.e., contemporaneous correlate errors \\(\\varepsilon_{,t}\\)), \\(\\mathbf{x}_{,t-1}\\) can added instruments associated \\(\\Delta y_{,t}\\). , variables (\\(\\mathbf{x}_{,t}\\)’s) exogenous (.e., contemporaneous correlate errors \\(\\varepsilon_{,s}\\), \\(\\forall s\\)), \\(\\mathbf{x}_{,t}\\) also constitute valid instrument.Using previous (simulated) example, approach consists following steps:OLS estimate \\(\\rho\\) (whose true value 0.8) 0.531, obtain rho_2SLS \\(=\\) 0.89.Let us come back general case (covariates \\(\\mathbf{x}_{,k}\\)’s). \\(t=3\\), \\(y_{,1}\\) (\\(\\mathbf{x}_{,1}\\)) possible instrument. However, \\(t=4\\), one use \\(y_{,2}\\) \\(y_{,1}\\) (well \\(\\mathbf{x}_{,2}\\) \\(\\mathbf{x}_{,1}\\)). generally, defining matrix \\(Z_i\\) follows:\n\\[\nZ_i = \\left[\n\\begin{array}{ccccccccccccccccc}\n\\mathbf{z}_{,1}' & 0 & \\dots \\\\\n0 & \\mathbf{z}_{,1}' & \\mathbf{z}_{,2}' & 0 & \\dots \\\\\n0 &0 &0 & \\mathbf{z}_{,1} & \\mathbf{z}_{,2}' & \\mathbf{z}_{,3}' & 0 & \\dots \\\\\n\\vdots \\\\\n0 & \\dots &&&&&& 0 & \\mathbf{z}_{,1}' &  \\dots &   \\mathbf{z}_{,T-2}'\n\\end{array}\n\\right],\n\\]\n\\(\\mathbf{z}_{,t} = [y_{,t},\\mathbf{x}_{,t}']'\\), moment conditions:2\n\\[\n\\mathbb{E}(Z_i'\\Delta  {\\boldsymbol\\varepsilon}_i)=0,\n\\]\\(\\Delta{\\boldsymbol\\varepsilon}_i = [ \\Delta \\varepsilon_{,3},\\dots,\\Delta \\varepsilon_{,T}]'\\).restrictions used GMM approach employed Arellano Bond (1991). Specifically, GMM estimator model parameters given :\n\\[\n\\mbox{argmin}\\;\\left(\\frac{1}{n} \\sum_{=1}^n Z_i' \\Delta \\boldsymbol\\varepsilon_i\\right)'W_n\\left(\\frac{1}{n} \\sum_{=1}^n Z_i' \\Delta \\boldsymbol\\varepsilon_i\\right),\n\\]\nusing weighting matrix\n\\[\nW_n = \\left(\\frac{1}{n}\\sum_{=1}^n Z_i'\\widehat{\\Delta\\boldsymbol\\varepsilon_i}\\widehat{\\Delta\\boldsymbol\\varepsilon_i}'Z_i\\right)^{-1},\n\\]\n\\(\\widehat{\\Delta\\boldsymbol\\varepsilon_i}\\)’s consistent estimates \\(\\Delta\\boldsymbol\\varepsilon_i\\)’s result preliminary estimation. sense, estimator two-step GMM one.disturbances homoskedastic, can shown asymptotically equivalent (efficient) GMM estimator can obtained using:\n\\[\nW_{1,n} = \\left(\\frac{1}{n}Z_i'HZ_i\\right)^{-1},\n\\]\n\\(H\\) \\((T-2) \\times (T-2)\\) matrix form:\n\\[\nH = \\left[\\begin{array}{ccccccc}\n2 & -1 & 0 & \\dots &0 \\\\\n-1 & 2 & -1 &  & \\vdots \\\\\n0 & \\ddots& \\ddots & \\ddots & 0 \\\\\n\\vdots &  & -1 & 2&-1\\\\\n0&\\dots & 0 & -1 & 2\n\\end{array}\\right].\n\\]straightforward extend GMM methods cases one lag dependent variable right-hand side equation cases disturbances feature limited moving-average serial correlation.pdynmc package allows run GMM approaches (see Fritsch, Pua, Schnurbus (2019)). following lines code allow replicate results Arellano Bond (1991):generate novel results (m2) replacing “onestep” “twostep” (estimation field). resulting estimated coefficients :Arellano Bond (1991) proposed specification test. model correctly specified, errors Eq. (1.9) —first-difference equation— feature non-zero first-order auto-correlations, zero higher-order autocorrelations.Function mtest.fct package pdynmc implements test. result present case:One can also implement Hansen J-test -identifying restrictions (see Section 2.1.3):","code":"\nn <- 400;T <- 10\nrho <- 0.8;sigma <- .5\nalpha <- rnorm(n)\ny <- alpha /(1-rho) + sigma^2/(1 - rho^2) * rnorm(n)\nall_y <- y\nfor(t in 2:T){\n  y <- rho * y + alpha + sigma * rnorm(n)\n  all_y <- rbind(all_y,y)\n}\ny   <- c(all_y[2:T,]);y_1 <- c(all_y[1:(T-1),])\nD <- diag(n) %x% rep(1,T-1)\nZ <- cbind(c(y_1),D)\nb <- solve(t(Z) %*% Z) %*% t(Z) %*% y\na <- b[2:(n+1)]\nplot(alpha,a)\nlines(c(-10,10),c(-10,10),col=\"blue\")\nDy   <- c(all_y[3:T,]) - c(all_y[2:(T-1),])\nDy_1 <- c(all_y[2:(T-1),]) - c(all_y[1:(T-2),])\ny_2  <- c(all_y[1:(T-2),])\nZ <- matrix(y_2,ncol=1)\nPz <- Z %*% solve(t(Z) %*% Z) %*% t(Z)\nDy_1hat <- Pz %*% Dy_1\nrho_2SLS <- solve(t(Dy_1hat) %*% Dy_1hat) %*% t(Dy_1hat) %*% Dy\nlibrary(pdynmc)\ndata(EmplUK, package = \"plm\")\ndat <- EmplUK\ndat[,c(4:7)]         <- log(dat[,c(4:7)])\nm1 <- pdynmc(dat = dat, # name of the dataset\n             varname.i = \"firm\", # name of the cross-section identifier\n             varname.t = \"year\", # name of the time-series identifiers\n             use.mc.diff = TRUE, # use moment conditions from equations in differences? (i.e. instruments in levels) \n             use.mc.lev = FALSE, # use moment conditions from equations in levels? (i.e. instruments in differences)\n             use.mc.nonlin = FALSE, # use nonlinear (quadratic) moment conditions?\n             include.y = TRUE, # instruments should be derived from the lags of the dependent variable?\n             varname.y = \"emp\", # name of the dependent variable in the dataset\n             lagTerms.y = 2, # number of lags of the dependent variable\n             fur.con = TRUE, # further control variables (covariates) are included?\n             fur.con.diff = TRUE, # include further control variables in equations from differences ?\n             fur.con.lev = FALSE, # include further control variables in equations from level?\n             varname.reg.fur = c(\"wage\", \"capital\", \"output\"), # covariate(s) -in the dataset- to treat as further controls\n             lagTerms.reg.fur = c(1,2,2), # number of lags of the further controls\n             include.dum = TRUE, # A logical variable indicating whether dummy variables for the time periods are included (defaults to 'FALSE').\n             dum.diff = TRUE, # A logical variable indicating whether dummy variables are included in the equations in first differences (defaults to 'NULL').\n             dum.lev = FALSE, # A logical variable indicating whether dummy variables are included in the equations in levels (defaults to 'NULL').\n             varname.dum = \"year\",\n             w.mat = \"iid.err\", # One of the character strings c('\"iid.err\"', '\"identity\"', '\"zero.cov\"') indicating the type of weighting matrix to use (defaults to '\"iid.err\"')\n             std.err = \"corrected\",\n             estimation = \"onestep\", # One of the character strings c('\"onestep\"', '\"twostep\"', '\"iterative\"'). Denotes the number of iterations of the parameter procedure (defaults to '\"twostep\"').\n             opt.meth = \"none\" # numerical optimization procedure. When no nonlinear moment conditions are employed in estimation, closed form estimates can be computed by setting the argument to '\"none\"\n)\nsummary(m1,digits=3)## \n## Dynamic linear panel estimation (onestep)\n## Estimation steps: 1\n## \n## Coefficients:\n##             Estimate Std.Err.rob z-value.rob Pr(>|z.rob|)    \n## L1.emp      0.686226    0.144594       4.746      < 2e-16 ***\n## L2.emp     -0.085358    0.056016      -1.524      0.12751    \n## L0.wage    -0.607821    0.178205      -3.411      0.00065 ***\n## L1.wage     0.392623    0.167993       2.337      0.01944 *  \n## L0.capital  0.356846    0.059020       6.046      < 2e-16 ***\n## L1.capital -0.058001    0.073180      -0.793      0.42778    \n## L2.capital -0.019948    0.032713      -0.610      0.54186    \n## L0.output   0.608506    0.172531       3.527      0.00042 ***\n## L1.output  -0.711164    0.231716      -3.069      0.00215 ** \n## L2.output   0.105798    0.141202       0.749      0.45386    \n## 1979        0.009554    0.010290       0.929      0.35289    \n## 1980        0.022015    0.017710       1.243      0.21387    \n## 1981       -0.011775    0.029508      -0.399      0.68989    \n## 1982       -0.027059    0.029275      -0.924      0.35549    \n## 1983       -0.021321    0.030460      -0.700      0.48393    \n## 1976       -0.007703    0.031411      -0.245      0.80646    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n##  41 total instruments are employed to estimate 16 parameters\n##  27 linear (DIF) \n##  8 further controls (DIF) \n##  6 time dummies (DIF) \n##  \n## J-Test (overid restrictions):  70.82 with 25 DF, pvalue: <0.001\n## F-Statistic (slope coeff):  528.06 with 10 DF, pvalue: <0.001\n## F-Statistic (time dummies):  14.98 with 6 DF, pvalue: 0.0204##      L1.emp      L2.emp     L0.wage     L1.wage  L0.capital  L1.capital \n##  0.62870890 -0.06518800 -0.52575951  0.31128961  0.27836190  0.01409950 \n##  L2.capital   L0.output   L1.output   L2.output        1979        1980 \n## -0.04024847  0.59192286 -0.56598515  0.10054264  0.01121551  0.02306871 \n##        1981        1982        1983        1976 \n## -0.02135806 -0.03111604 -0.01799335 -0.02336762\nmtest.fct(m1,order=3)## \n##  Arellano and Bond (1991) serial correlation test of degree 3\n## \n## data:  1step GMM Estimation\n## normal = 0.045945, p-value = 0.9634\n## alternative hypothesis: serial correlation of order 3 in the error terms\njtest.fct(m1)## \n##  J-Test of Hansen\n## \n## data:  1step GMM Estimation\n## chisq = 70.82, df = 25, p-value = 2.905e-06\n## alternative hypothesis: overidentifying restrictions invalid\njtest.fct(m2)## \n##  J-Test of Hansen\n## \n## data:  2step GMM Estimation\n## chisq = 31.381, df = 25, p-value = 0.1767\n## alternative hypothesis: overidentifying restrictions invalid"},{"path":"Panel.html","id":"introduction-to-program-evaluation","chapter":"1 Panel regressions","heading":"1.6 Introduction to program evaluation","text":"section brielfy introduces econometrics program evaluation. Program evaluation refer analysis causal effects “treatments” broad sense. treatment can, e.g., correspond implementation (announcement) policy measures. comprehensive review proposed Abadie Cattaneo (2018). seminal book subject Angrist Pischke (2008).","code":""},{"path":"Panel.html","id":"presentation-of-the-problem","chapter":"1 Panel regressions","heading":"1.6.1 Presentation of the problem","text":"begin , let us consider single entity. simplify notations, drop entity index (\\(\\)). Let us denote \\(Y\\) outcome variable (variable interest), \\(W\\) binary variable indicating whether considered entity received treatment (\\(W=1\\)) (\\(W=0\\)), \\(X\\) vector covariates, assumed predetermined relative treatment. , \\(W\\) \\(X\\) correlated, values \\(X\\) determined \\(W\\) (way realization \\(W\\) affect \\(X\\)). Typcally, \\(X\\) contains characteristics considered entity.interested effect treatment, :\n\\[\nY_1 - Y_0,\n\\]\n\\(Y_1\\) correspond outcome obtained treatment, \\(Y_0\\) outcome obtained without . Notice :\n\\[\nY = (1-W) Y_0 + W Y_1.\n\\]problem observing \\((Y,W,X)\\) sufficient observe treatment effect \\(Y_1 - Y_0\\). Additional assumptions needed estimate , , precisely, expectations (average treatment effect):\n\\[\nATE = \\mathbb{E}(Y_1 - Y_0).\n\\]Importantly, \\(ATE\\) different following quantity:\n\\[\n\\alpha = \\underbrace{\\mathbb{E}(Y|W=1)}_{=\\mathbb{E}(Y_1|W=1)} - \\underbrace{\\mathbb{E}(Y|W=0)}_{=\\mathbb{E}(Y_0|W=0)},\n\\]\neasier estimate. Indeed, consistent estimate \\(\\alpha\\) difference means outcome variables two sub-samples: one containing treated entities (gives estimate \\(\\mathbb{E}(Y_1|W=1)\\)) containing non-treated entities (gives estimate \\(\\mathbb{E}(Y_0|W=0)\\)). Coming back \\(ATE\\), problem won’t direct information regarding \\(\\mathbb{E}(Y_0|W=1)\\) \\(\\mathbb{E}(Y_1|W=0)\\). However, two conditional expectations part \\(ATE\\). Indeed, \\(ATE = \\mathbb{E}(Y_1) - \\mathbb{E}(Y_0)\\), :\n\\[\\begin{eqnarray}\n\\mathbb{E}(Y_1) &=& \\mathbb{E}(Y_1|W=0)\\mathbb{P}(W=0)+\\mathbb{E}(Y_1|W=1)\\mathbb{P}(W=1) \\tag{1.10} \\\\\n\\mathbb{E}(Y_0) &=& \\mathbb{E}(Y_0|W=0)\\mathbb{P}(W=0)+\\mathbb{E}(Y_0|W=1)\\mathbb{P}(W=1). \\tag{1.11}\n\\end{eqnarray}\\]","code":""},{"path":"Panel.html","id":"randomized-controlled-trials-rcts","chapter":"1 Panel regressions","heading":"1.6.2 Randomized controlled trials (RCTs)","text":"context Randomized controlled trials (RCTs), entities randomly assigned receive treatment. result, \\(\\mathbb{E}(Y_1) = \\mathbb{E}(Y_1|W=0) = \\mathbb{E}(Y_1|W=1)\\) \\(\\mathbb{E}(Y_0) = \\mathbb{E}(Y_0|W=0) = \\mathbb{E}(Y_0|W=1)\\). Using Eqs. (1.10) (1.11) yields \\(ATE = \\alpha\\).Therefore, context, estimating \\(\\mathbb{E}(Y_1-Y_0)\\) amounts computing difference two sample means, namely () sample mean subset \\(Y_i\\)’s corresponding entities \\(W_i=1\\), (b) one \\(W_i=0\\).accurate estimates can obtained regressions. Assume model reads:\n\\[\nY_{} = W_{} \\beta_{1} + X_i'\\boldsymbol\\beta_z + \\varepsilon_i,\n\\]\n\\(\\mathbb{E}(\\varepsilon_i|X_i) = 0\\) (\\(W_i\\) independent \\(X_i\\) \\(\\varepsilon_i\\)). case, obtain consistent estimate \\(\\beta_1\\) regressing \\(\\mathbf{y}\\) \\(\\mathbf{Z} = [\\mathbf{w},\\mathbf{X}]\\).","code":""},{"path":"Panel.html","id":"difference-in-difference-did-approach","chapter":"1 Panel regressions","heading":"1.6.3 Difference-in-Difference (DiD) approach","text":"approach popular methodology implemented cases \\(W\\) considered independent variable. exploits two dimensions: entities (\\(\\)), time (\\(t\\)). simplify exposition, consider two periods (\\(t=0\\) \\(t=1\\)).Consider following model:\n\\[\\begin{equation}\nY_{,t} = W_{,t} \\beta_1 + \\mu_i + \\delta_t + \\varepsilon_{,t}\\tag{1.12}\n\\end{equation}\\]parameter interest \\(\\beta_{1}\\), treatment effect (recall \\(W_{,t} \\\\{0,1\\}\\)). Usually, entities \\(\\), \\(W_{,t=0}=0\\). treated date 1, .e., \\(W_{,1} \\\\{0,1\\}\\).disturbance \\(\\varepsilon_{,t}\\) affects outcome, assume relate selection treatment; therefore, \\(\\mathbb{E}(\\varepsilon_{,t}|W_{,t})=0\\). contrast, exclude correlation \\(W_{,t}\\) (\\(t=1\\)) \\(\\mu_i\\); hence, \\(\\mu_i\\) may constitute confounder. Finally, suppose micro-variables \\(W_i\\) affect time fixed effects \\(\\delta_t\\), \\(\\mathbb{E}(\\delta_t|W_{,t})=\\mathbb{E}(\\delta_t)\\).:\\[\n\\begin{array}{cccccccccc}\n\\mathbb{E}(Y_{,1}|W_{,1}=1) &=& \\beta_1 &+& \\mathbb{E}(\\mu_i|W_{,1}=1) &+&\\mathbb{E}(\\delta_1|W_{,1}=1) &+& \\mathbb{E}(\\varepsilon_{,1}) \\\\\n\\mathbb{E}(Y_{,0}|W_{,1}=1) &=&  && \\mathbb{E}(\\mu_i|W_{,1}=1) &+&\\mathbb{E}(\\delta_0|W_{,1}=1) &+& \\mathbb{E}(\\varepsilon_{,0}) \\\\\n\\mathbb{E}(Y_{,1}|W_{,1}=0) &=& && \\mathbb{E}(\\mu_i|W_{,1}=0) &+&\\mathbb{E}(\\delta_1|W_{,1}=0) &+& \\mathbb{E}(\\varepsilon_{,1}) \\\\\n\\mathbb{E}(Y_{,0}|W_{,1}=0) &=&  && \\mathbb{E}(\\mu_i|W_{,1}=0) &+&\\mathbb{E}(\\delta_0|W_{,1}=0) &+& \\mathbb{E}(\\varepsilon_{,0}).\n\\end{array}\n\\]\n, assumptions, can checked :\\[\n\\beta_1 = \\mathbb{E}(\\Delta Y_{,1}|W_{,1}=1) - \\mathbb{E}(\\Delta Y_{,1}|W_{,1}=0),\n\\]\n\\(\\Delta Y_{,1}=Y_{,1}-Y_{,0}\\). Therefore, context, treatment effect appears difference (two conditionnal expectations) difference (outcome variable, time).illustrated Figure 1.6, represents generic framework.\nFigure 1.6: Source: Abadie et al., (1998).\npractice, implementing approach consists running linear regression type Eq. (1.12). regressions also usually involve controls top fixed effects \\(\\mu_i\\). illustrated next subsection, parameter interest (\\(\\beta_1\\)) often associated interaction term.","code":""},{"path":"Panel.html","id":"application-of-the-did-approach","chapter":"1 Panel regressions","heading":"1.6.4 Application of the DiD approach","text":"example based data used Meyer, Viscusi, Durbin (1995). dataset part wooldridge package. paper examines effect workers’ compensation injury time work. exploits natural experiment approach comparing individuals injured increases maximum weekly benefit amount. Specifically, 1980, cap weekly earnings covered worker’s compensation increased Kentucky Michigan. Let us check whether new policy followed increase amount time workers spent unemployed (example, higher compensation may reduce workers’ incentives avoid injury).shown Figure 1.7, measure affected high-earning workers. idea exploited Meyer, Viscusi, Durbin (1995) compare increase time work -1980 higher-earnings workers one hand (entities received treatment) low-earnings workers hand (control group).\nFigure 1.7: Source: Meyer et al., (1995).\nnext lines codes replicate results. dependent variable logarithm duration benefits. information use ?injury, loaded wooldridge library.table results , parameter interest one associated interaction term afchnge:highearn. Columns 2 3 correspond first two column Table 6 Meyer, Viscusi, Durbin (1995).","code":"\nlibrary(wooldridge)\ndata(injury)\ninjury <- subset(injury,ky==1)\ninjury$indust <- as.factor(injury$indust)\ninjury$injtype <- as.factor(injury$injtype)\n#names(injury)\neq1 <- lm(log(durat) ~ afchnge + highearn + afchnge*highearn,data=injury)\neq2 <- lm(log(durat) ~ afchnge + highearn + afchnge*highearn +\n            lprewage*highearn + male + married + lage + ltotmed + hosp +\n            indust + injtype,data=injury)\neq3 <- lm(log(durat) ~ afchnge + highearn + afchnge*highearn +\n            lprewage*highearn + male + married + lage + indust +\n            injtype,data=injury)\nstargazer::stargazer(eq1,eq2,eq3,type=\"text\",\n                     omit=c(\"indust\",\"injtype\",\"Constant\"),no.space = TRUE,\n                     add.lines = list(c(\"industry dummy\",\"no\",\"yes\",\"yes\"),\n                                      c(\"injury dummy\",\"no\",\"yes\",\"yes\")),\n                     order = c(1,2,18,3:17,19,20),omit.stat = c(\"f\",\"ser\"))## \n## ===============================================\n##                        Dependent variable:     \n##                   -----------------------------\n##                            log(durat)          \n##                      (1)       (2)       (3)   \n## -----------------------------------------------\n## afchnge             0.008    -0.004     0.016  \n##                    (0.045)   (0.038)   (0.045) \n## highearn          0.256***   -0.595    -1.522  \n##                    (0.047)   (0.930)   (1.099) \n## afchnge:highearn  0.191***  0.162***  0.215*** \n##                    (0.069)   (0.059)   (0.069) \n## lprewage                     0.207**   0.258** \n##                              (0.088)   (0.104) \n## male                         -0.070*   -0.072  \n##                              (0.039)   (0.046) \n## married                       0.055     0.051  \n##                              (0.035)   (0.041) \n## lage                        0.244***  0.252*** \n##                              (0.044)   (0.052) \n## ltotmed                     0.361***           \n##                              (0.011)           \n## hosp                        0.252***           \n##                              (0.044)           \n## highearn:lprewage             0.065     0.232  \n##                              (0.158)   (0.187) \n## -----------------------------------------------\n## industry dummy       no        yes       yes   \n## injury dummy         no        yes       yes   \n## Observations        5,626     5,347     5,347  \n## R2                  0.021     0.319     0.049  \n## Adjusted R2         0.020     0.316     0.046  \n## ===============================================\n## Note:               *p<0.1; **p<0.05; ***p<0.01"},{"path":"estimation-methods.html","id":"estimation-methods","chapter":"2 Estimation Methods","heading":"2 Estimation Methods","text":"chapter presents three approaches estimate parametric models: General Method Moments (GMM), Maximum Likelihood approach (ML), Bayesian approach. general context following: observe sample \\(\\mathbf{y}=\\{y_1,\\dots,y_n\\}\\), assume data generated model parameterized \\({\\boldsymbol\\theta} \\\\mathbb{R}^K\\), want estimate vector \\({\\boldsymbol\\theta}_0\\).","code":""},{"path":"estimation-methods.html","id":"secGMM","chapter":"2 Estimation Methods","heading":"2.1 Generalized Method of Moments (GMM)","text":"","code":""},{"path":"estimation-methods.html","id":"definition-of-the-gmm-estimator","chapter":"2 Estimation Methods","heading":"2.1.1 Definition of the GMM estimator","text":"denote \\(y_i\\) \\(p \\times 1\\) vector variables; \\(\\boldsymbol\\theta\\) \\(K \\times 1\\) vector parameters, \\(h(y_i;\\boldsymbol\\theta)\\) continuous \\(r \\times 1\\) vector-valued function.denote \\(\\boldsymbol\\theta_0\\) true value \\(\\boldsymbol\\theta\\) assume \\(\\boldsymbol\\theta_0\\) satisfies:\n\\[\n\\mathbb{E}[h(y_i;\\boldsymbol\\theta_0)] = \\mathbf{0}.\n\\]denote \\(\\underline{y_i}\\) information contained current past observations \\(y_i\\), : \\(\\underline{y_i} = \\{y_i,y_{-1},\\dots,y_1\\}\\). denote \\(g(\\underline{y_n};\\boldsymbol\\theta)\\) sample average \\(h(y_i;\\boldsymbol\\theta)\\) vectors, .e.:\n\\[\ng(\\underline{y_n};\\boldsymbol\\theta) = \\frac{1}{n} \\sum_{=1}^{n} h(y_i;\\boldsymbol\\theta).\n\\]intuition behind GMM estimator following: choose \\(\\boldsymbol\\theta\\) make sample moment close possible population values, 0.Definition 2.1  GMM estimator \\(\\boldsymbol\\theta_0\\) given :\n\\[\n\\hat{\\boldsymbol\\theta}_n = \\mbox{argmin}_{\\boldsymbol\\theta} \\quad g(\\underline{y_n};\\boldsymbol\\theta)'\\, W_n \\, g(\\underline{y_n};\\boldsymbol\\theta),\n\\]\n\\(W_n\\) positive definite matrix (may depend \\(\\underline{y_n}\\)).specific case \\(K = r\\) (dimension \\(\\boldsymbol\\theta\\) \\(h(y_i;\\boldsymbol\\theta)\\) —\\(g(\\underline{y_n};\\boldsymbol\\theta)\\)— \\(\\hat{\\boldsymbol\\theta}_n\\) satisfies:\n\\[\ng(\\underline{y_n};\\hat{\\boldsymbol\\theta}_n) = \\mathbf{0}.\n\\]\nregularity identification conditions, estimator consistent, \\(\\hat{\\boldsymbol\\theta}_{n}\\) converges towards \\(\\boldsymbol\\theta_0\\) probability, denote :\n\\[\\begin{equation}\n\\mbox{plim}_n\\;\\hat{\\boldsymbol\\theta}_{n}= \\boldsymbol\\theta_0,\\quad \\mbox{} \\quad\\hat{\\boldsymbol\\theta}_{n} \\overset{p}{\\rightarrow} \\boldsymbol\\theta_0,\\tag{2.1}\n\\end{equation}\\]\n.e. \\(\\forall \\varepsilon>0\\), \\(\\lim_{n \\rightarrow \\infty} \\mathbb{P}(|\\hat{\\boldsymbol\\theta}_{n} - \\boldsymbol\\theta_0|>\\varepsilon) = 0\\) (Definition 4.16).Definition 2.1 involves positive definite matrix \\(W_n\\). one can take positive definite matrix consistency (Eq. (2.1)), can shown GMM estimator achieves minimum asymptotic variance \\(W_n\\) inverse matrix \\(S\\), latter defined :\n\\[\nS = Asy.\\mathbb{V}ar\\left(\\sqrt{n}g(\\underline{y_n};\\hat{\\boldsymbol\\theta}_n)\\right).\n\\]\ncase, \\(W_n\\) said optimal weighting matrix.intuition behind result underlies Generalized Least Squares (see Section ??), : beneficial use criterion weights inversely proportional variances moments.\\(h(x_i;\\boldsymbol\\theta_0)\\) correlated \\(h(x_j;\\boldsymbol\\theta_0)\\), \\(\\ne j\\), :\n\\[\nS = \\mathbb{V}ar(h(x_i;\\boldsymbol\\theta_0)),\n\\]\ncan approximated \n\\[\n\\hat{\\Gamma}_{0,n}=\\frac{1}{n}\\sum_{=1}^{n} h(x_i;\\hat{\\boldsymbol\\theta}_n)h(x_{};\\hat{\\boldsymbol\\theta}_n)'.\n\\]time series context, often correlation \\(x_i\\) \\(x_{+k}\\), especially small \\(k\\)’s. case, time series \\(\\{y_i\\}\\) covariance stationary (see Def. ??), :\n\\[\nS := \\sum_{\\nu = -\\infty}^{\\infty} \\Gamma_\\nu,\n\\]\n\\(\\Gamma_\\nu := \\mathbb{E}[h(x_i;\\boldsymbol\\theta_0) h(x_{-\\nu};\\boldsymbol\\theta_0)']\\). Matrix \\(S\\) called long-run variance process \\(\\{y_i\\}\\) (see Def. ??).\\(\\nu \\ge 0\\), let us define \\(\\hat{\\Gamma}_{\\nu,n}\\) :\n\\[\n\\hat{\\Gamma}_{\\nu,n} = \\frac{1}{n} \\sum_{=\\nu + 1}^{n} h(x_i;\\hat{\\boldsymbol\\theta}_n)h(x_{-\\nu};\\hat{\\boldsymbol\\theta}_n)',\n\\]\n\\(S\\) can approximated Newey West (1987) formula (similar Eq. (??)):\n\\[\\begin{equation}\n\\hat{\\Gamma}_{0,n} + \\sum_{\\nu=1}^{q}\\left[1-\\frac{\\nu}{q+1}\\right](\\hat{\\Gamma}_{\\nu,n}+\\hat{\\Gamma}_{\\nu,n}').    \\tag{2.2}\n\\end{equation}\\]","code":""},{"path":"estimation-methods.html","id":"asymptotic-distribution-of-the-gmm-estimator","chapter":"2 Estimation Methods","heading":"2.1.2 Asymptotic distribution of the GMM estimator","text":":\n\\[\\begin{equation}\n\\boxed{\\sqrt{n}(\\hat{\\boldsymbol\\theta}_n - \\boldsymbol\\theta_0) \\overset{d}{\\rightarrow} \\mathcal{N}(0,V),}\\tag{2.3}\n\\end{equation}\\]\n\\(V = (DS^{-1}D')^{-1}\\), \n\\[\nD := \\left.\\mathbb{E}\\left(\\frac{\\partial h(x_i;\\boldsymbol\\theta)}{\\partial \\boldsymbol\\theta'}\\right)\\right|_{\\boldsymbol\\theta = \\boldsymbol\\theta_0}.\n\\]Matrix \\(V\\) can approximated \n\\[\\begin{equation}\n\\hat{V}_n = (\\hat{D}_n\\hat{S}_n^{-1}\\hat{D}_n')^{-1},\\tag{2.4}\n\\end{equation}\\]\n\\(\\hat{S}_n\\) given Eq. (2.2) \n\\[\n\\hat{D}'_n := \\left.\\frac{\\partial g(\\underline{y_n};\\boldsymbol\\theta)}{\\partial \\boldsymbol\\theta'}\\right|_{\\boldsymbol\\theta = \\hat{\\boldsymbol\\theta}_n}.\n\\]\npractice, previous matrix computed numerically.","code":""},{"path":"estimation-methods.html","id":"overidentif","chapter":"2 Estimation Methods","heading":"2.1.3 Testing hypotheses in the GMM framework","text":"first important test one concerning validity moment restrictions (Sargan-Hansen test; Sargan (1958) Hansen (1982)). Assume number restrictions imposed larger number parameters estimate (\\(r>K\\)). case, restrictions said -identifiying.correct specification, asymptotically :\n\\[\n\\sqrt{n}g(\\underline{y_n};{\\boldsymbol\\theta}_0)  \\sim \\mathcal{N}(0,S).\n\\]\nresult, comes :\n\\[\\begin{equation}\nJ_n = \\left(\\sqrt{n}g(\\underline{y_n};{\\boldsymbol\\theta}_0)\\right)'S^{-1}\\left(\\sqrt{n}g(\\underline{y_n};{\\boldsymbol\\theta}_0)\\right) \\tag{2.5}\n\\end{equation}\\]\nasymptotically follows \\(\\chi^2\\) distribution. number degrees freedom equal \\(r-K\\). (Note , \\(r=K\\), , expected, \\(J=0\\).) , asymptotically:\n\\[\nJ_n \\sim \\chi^2(r-K).\n\\]\nGMM framework also allows easily test linear restrictions parameters. First, given Eq. (2.3), Wald tests (see Eq. (??) Section ??) readily available. Second, one can also resort test equivalent likelihood ratio tests (see Definition 2.8). precisely, consider unconstrained model constrained version model, number restrictions equal \\(k\\). two models estimated considering moment constraints, weighting matrix —using Eq. (2.4), based unrestricted model—, :\n\\[\nn \\left[(g(\\underline{y_n};\\hat{{\\boldsymbol\\theta}}^*_n)-g(\\underline{y_n};\\hat{{\\boldsymbol\\theta}}_n)\\right] \\sim \\chi^2(k),\n\\]\n\\(\\hat{{\\boldsymbol\\theta}}^*_n\\) constrained estimate \\({\\boldsymbol\\theta}_0\\).","code":""},{"path":"estimation-methods.html","id":"example-estimation-of-the-stochastic-discount-factor-s.d.f.","chapter":"2 Estimation Methods","heading":"2.1.4 Example: Estimation of the Stochastic Discount Factor (s.d.f.)","text":"-arbitrage assumption, exists random variable \\(\\mathcal{M}_{t,t+1}\\) (s.d.f.) \n\\[\n\\mathbb{E}_t(\\mathcal{M}_{t,t+1}R_{t+1})=1\n\\]\n(gross) asset return \\(R_t\\). following, \\(R_t\\) denotes \\(n_r\\)-dimensional vector gross returns.consider following specification s.d.f.:\n\\[\\begin{equation}\n\\mathcal{M}_{t,t+1} = 1 - \\textbf{b}_M'(F_{t+1} - \\mathbb{E}_t(F_{t+1})), \\tag{2.6}\n\\end{equation}\\]\n\\(F_t\\) vector factors. Eq. (2.6) reads:\n\\[\n\\mathbb{E}_t([1 - \\textbf{b}_M'(F_{t+1} - \\mathbb{E}_t(F_{t+1}))]R_{t+1})=1.\n\\]Assume date-\\(t\\) information set \\(\\mathcal{}_t=\\{\\textbf{z}_t,\\mathcal{}_{t-1}\\}\\), \\(\\textbf{z}_t\\) vector variables observed date \\(t\\). (\\(\\mathbb{E}_t(\\bullet) \\equiv \\mathbb{E}(\\bullet|\\mathcal{}_t)\\).)can use \\(\\textbf{z}_t\\) instrument. Indeed, :\n\\[\\begin{eqnarray}\n&&\\mathbb{E}(z_{,t} [\\textbf{b}_M'\\{F_{t+1} - \\mathbb{E}_t(F_{t+1})\\}R_{t+1}-R_{t+1}+1]) \\nonumber \\\\\n&=&\\mathbb{E}(\\mathbb{E}_t\\{z_{,t} [\\textbf{b}_M'\\{F_{t+1} - \\mathbb{E}_t(F_{t+1})\\}R_{t+1}-R_{t+1}+1]\\})\\nonumber\\\\\n&=&\\mathbb{E}(z_{,t} \\underbrace{\\mathbb{E}_t\\{\\textbf{b}_M'\\{F_{t+1} - \\mathbb{E}_t(F_{t+1})\\}R_{t+1}-R_{t+1}+1\\}}_{1 - \\mathbb{E}_t(\\mathcal{M}_{t,t+1}R_{t+1})=0})=0.\\tag{2.7}\n\\end{eqnarray}\\]\nconverted conditional moment condition unconditional one (need implement GMM approach described ). However, stage, can still directly use GMM formulas conditional expectation \\(\\mathbb{E}_t(F_{t+1})\\) appears \\(\\mathbb{E}(z_{,t} [\\textbf{b}_M'\\{F_{t+1} - \\mathbb{E}_t(F_{t+1})\\}R_{t+1}-R_{t+1}+1])=0\\).go , let us assume :\n\\[\n\\mathbb{E}_t(F_{t+1}) = \\textbf{b}_F \\textbf{z}_t.\n\\]\ncan easily estimate matrix \\(\\textbf{b}_F\\) (dimension \\(n_F \\times n_z\\)) OLS. Note OLS can seen special GMM case. Indeed, done Eq. (2.7), can show , \\(j^{th}\\) component \\(F_t\\), :\n\\[\n\\mathbb{E}( [F_{j,t+1} - \\textbf{b}_{F,j} \\textbf{z}_t]\\textbf{z}_{t})=0,\n\\]\n\\(\\textbf{b}_{F,j}\\) denotes \\(j^{th}\\) row \\(\\textbf{b}_{F}\\). yields OLS formula.Equipped \\(\\textbf{b}_F\\), rely following moment restrictions estimate \\(\\textbf{b}_M\\):\n\\[\n\\mathbb{E}(z_{,t} [\\textbf{b}_M'\\{F_{t+1} - \\textbf{b}_F \\textbf{z}_t\\}R_{t+1}-R_{t+1}+1])=0.\n\\]\nSpecifically, number restrictions \\(n_R \\times n_z\\). Let us implement approach U.S. context, using data extracted FRED database. factor \\(F_t\\), use changes VIX personal consumption expenditures. returns (\\(R_t\\)) based Wilshire 5000 Price Index (stock price index) ICE BofA BBB US Corporate Index Total Return Index (bond return index).Define matrices containing \\(F_{t+1}\\), \\(\\textbf{z}_t\\), \\(R_{t+1}\\) vectors:Function f_aux compute \\(h(x_t;{\\boldsymbol\\theta})\\) \\(g(\\underline{y_T};{\\boldsymbol\\theta})\\); function f2beMin function minimized.Now, let’s minimize function, using use BFGS numerical algorithm (part optim wrapper). run 5 iterations (\\(W\\) updated).Finally, let’s compute standard deviation parameter estimates, using Eq. (2.4):Hansen statistic can used test model (see Eq. (2.5)). model correct, :\n\\[\nT g(\\underline{y_T};{\\boldsymbol\\theta})'\\, S^{-1} \\, g(\\underline{y_T};{\\boldsymbol\\theta}) \\sim \\,..d.\\,\\chi^2(J - K),\n\\]\n\\(J\\) number moment constraints (\\(n_z \\times n_r\\) ) \\(K\\) number estimated parameters (\\(=n_F\\) ).","code":"\nlibrary(fredr)\nfredr_set_key(\"df65e14c054697a52b4511e77fcfa1f3\")\nstart_date <- as.Date(\"1990-01-01\"); end_date <- as.Date(\"2022-01-01\")\nf <- function(ticker){\n  fredr(series_id = ticker,\n        observation_start = start_date,observation_end = end_date,\n        frequency = \"m\",aggregation_method = \"avg\")\n}\nvix <- f(\"VIXCLS\") # VIX\npce <- f(\"PCE\") # Personal consumption expenditures\nsto <- f(\"WILL5000PRFC\") # Wilshire 5000 Full Cap Price Index\nbdr <- f(\"BAMLCC0A4BBBTRIV\") # ICE BofA BBB US Corp. Index Tot. Return\nT <- dim(vix)[1]\ndvix <- c(vix$value[3:T]/vix$value[2:(T-1)]) # change in VIX t+1\ndpce <- c(pce$value[3:T]/pce$value[2:(T-1)]) # change in PCE t+1\ndsto <- c(sto$value[3:T]/sto$value[2:(T-1)]) # return t+1\ndbdr <- c(bdr$value[3:T]/bdr$value[2:(T-1)]) # return t+1\ndvix_1 <- c(vix$value[2:(T-1)]/vix$value[1:(T-2)]) # change in VIX t\ndpce_1 <- c(pce$value[2:(T-1)]/pce$value[1:(T-2)]) # change in PCE t\ndsto_1 <- c(sto$value[2:(T-1)]/sto$value[1:(T-2)]) # return t\ndbdr_1 <- c(bdr$value[2:(T-1)]/bdr$value[1:(T-2)]) # return t\nF_tp1 <- cbind(dvix,dpce)\nZ     <- cbind(1,dvix_1,dpce_1,dsto_1,dbdr_1)\nb_F <- t(solve(t(Z) %*% Z) %*% t(Z) %*% F_tp1)\nF_innov <- F_tp1 - Z %*% t(b_F)\nR_tp1 <- cbind(dsto,dbdr)\nn_F <- dim(F_tp1)[2]; n_R <- dim(R_tp1)[2]; n_z <- dim(Z)[2]\nf_aux <- function(theta){\n  b_M <- matrix(theta[1:n_F],ncol=1)\n  R_aux <- matrix(F_innov %*% b_M,T-2,n_R) * R_tp1 - R_tp1 + 1\n  H <- (R_aux %x% matrix(1,1,n_z)) * (matrix(1,1,n_R) %x% Z)\n  g <- matrix(apply(H,2,mean),ncol=1)\n  return(list(g=g,H=H))\n}\nf2beMin <- function(theta,W){# function to be minimized\n  res <- f_aux(theta)\n  return(t(res$g) %*% W %*% res$g)\n}\nlibrary(AEC)\ntheta <- c(rep(0,n_F)) # inital value\nfor(i in 1:10){# recursion on W\n  res <- f_aux(theta)\n  W <-  solve(NW.LongRunVariance(res$H,q=6))\n  res.optim <- optim(theta,f2beMin,W=W,\n                     method=\"BFGS\", # could be \"Nelder-Mead\"\n                     control=list(trace=FALSE,maxit=200),hessian=TRUE)\n  theta <- res.optim$par\n}\neps <- .0001\ng0 <- f_aux(theta)$g\nD <- NULL\nfor(i in 1:length(theta)){\n  theta.i <- theta\n  theta.i[i] <- theta.i[i] + eps\n  gi <- f_aux(theta.i)$g\n  D <- cbind(D,(gi-g0)/eps)\n}\nV <- 1/T * solve(t(D) %*% W %*% D)\nstd.dev <- sqrt(diag(V));t.stud <- theta/std.dev\ncbind(theta,std.dev,t.stud)##            theta    std.dev     t.stud\n## [1,]  -0.7180716  0.4646617 -1.5453642\n## [2,] -11.2042452 17.1039449 -0.6550679\ng <- f_aux(theta)$g\nHansen_stat <- T * t(g) %*% W %*% g\npvalue <- pchisq(q = Hansen_stat,df = n_R*n_z - n_F)\npvalue##           [,1]\n## [1,] 0.8789782"},{"path":"estimation-methods.html","id":"secMLE","chapter":"2 Estimation Methods","heading":"2.2 Maximum Likelihood Estimation","text":"","code":""},{"path":"estimation-methods.html","id":"intuition","chapter":"2 Estimation Methods","heading":"2.2.1 Intuition","text":"Intuitively, Maximum Likelihood Estimation (MLE) consists looking value \\({\\boldsymbol\\theta}\\) probability observed \\(\\mathbf{y}\\) (sample hand) highest possible.set example, assume time periods arrivals two customers shop, denoted \\(y_i\\), ..d. follow exponential distribution, .e. \\(y_i \\sim \\,..d.\\, \\mathcal{E}(\\lambda)\\). observed arrivals time, thereby constituting sample \\(\\mathbf{y}=\\{y_1,\\dots,y_n\\}\\). want estimate \\(\\lambda\\) (.e. case, vector parameters simply \\({\\boldsymbol\\theta} = \\lambda\\)).density \\(Y\\) (one observation) \\(f(y;\\lambda) = \\dfrac{1}{\\lambda}\\exp(-y/\\lambda)\\). Fig. 2.1 represents density functions different values \\(\\lambda\\).200 observations reported bottom Fig. 2.1 (red bars). build histogram display chart.\nFigure 2.1: red ticks, bottom, indicate observations (200 ). historgram based 200 observations\nestimate \\(\\lambda\\)? Intuitively, one led take \\(\\lambda\\) (theoretical) distribution closest histogram (can seen “empirical distribution”). approach consistent idea picking \\(\\lambda\\) probability observing values included \\(\\mathbf{y}\\) highest.Let us formal. Assume four observations: \\(y_1=1.1\\), \\(y_2=2.2\\), \\(y_3=0.7\\) \\(y_4=5.0\\). probability jointly observing:\\(1.1-\\varepsilon \\le Y_1 < 1.1+\\varepsilon\\),\\(2.2-\\varepsilon \\le Y_2 < 2.2+\\varepsilon\\),\\(0.7-\\varepsilon \\le Y_3 < 0.7+\\varepsilon\\), \\(5.0-\\varepsilon \\le Y_4 < 5.0+\\varepsilon\\)?\\(y_i\\)’s ..d., probability \\(\\prod_{=1}^4(2\\varepsilon f(y_i,\\lambda))\\).\nnext plot shows probability (divided \\(16\\varepsilon^4\\), depend \\(\\lambda\\)) function \\(\\lambda\\).\nFigure 2.2: Proba. \\(y_i-\\varepsilon \\le Y_i < y_i+\\varepsilon\\), \\(\\\\{1,2,3,4\\}\\). vertical red line indicates maximum function.\nvalue \\(\\lambda\\) maximizes probability 2.26.Let us come back example 200 observations:\nFigure 2.3: Log-likelihood function associated 200 ..d. observations. vertical red line indicates maximum function.\ncase, value \\(\\lambda\\) maxmimizes probability 3.42.","code":""},{"path":"estimation-methods.html","id":"definition-and-properties","chapter":"2 Estimation Methods","heading":"2.2.2 Definition and properties","text":"\\(f(y;\\boldsymbol\\theta)\\) denotes probability density function (p.d.f.) random variable \\(Y\\) depends set parameters \\(\\boldsymbol\\theta\\). density \\(n\\) independent identically distributed (..d.) observations \\(Y\\) given :\n\\[\nf(\\mathbf{y};\\boldsymbol\\theta) = \\prod_{=1}^n f(y_i;\\boldsymbol\\theta),\n\\]\n\\(\\mathbf{y}\\) denotes vector observations; \\(\\mathbf{y} = \\{y_1,\\dots,y_n\\}\\).Definition 2.2  (Likelihood function) likelihood function :\n\\[\n\\mathcal{L}: \\boldsymbol\\theta \\rightarrow  \\mathcal{L}(\\boldsymbol\\theta;\\mathbf{y})=f(\\mathbf{y};\\boldsymbol\\theta)=f(y_1,\\dots,y_n;\\boldsymbol\\theta).\n\\]often work \\(\\log \\mathcal{L}\\), log-likelihood function.Example 2.1  (Gaussian distribution) \\(y_i \\sim \\mathcal{N}(\\mu,\\sigma^2)\\), \n\\[\n\\log \\mathcal{L}(\\boldsymbol\\theta;\\mathbf{y}) = - \\frac{1}{2}\\sum_{=1}^n\\left( \\log \\sigma^2 + \\log 2\\pi + \\frac{(y_i-\\mu)^2}{\\sigma^2} \\right).\n\\]Definition 2.3  (Score) score \\(S(y;\\boldsymbol\\theta)\\) given \\(\\frac{\\partial \\log f(y;\\boldsymbol\\theta)}{\\partial \\boldsymbol\\theta}\\).\\(y_i \\sim \\mathcal{N}(\\mu,\\sigma^2)\\) (Example 2.1), \n\\[\n\\frac{\\partial \\log f(y;\\boldsymbol\\theta)}{\\partial \\boldsymbol\\theta} =\n\\left[\\begin{array}{c}\n\\dfrac{\\partial \\log f(y;\\boldsymbol\\theta)}{\\partial \\mu}\\\\\n\\dfrac{\\partial \\log f(y;\\boldsymbol\\theta)}{\\partial \\sigma^2}\n\\end{array}\\right] =\n\\left[\\begin{array}{c}\n\\dfrac{y-\\mu}{\\sigma^2}\\\\\n\\frac{1}{2\\sigma^2}\\left(\\frac{(y-\\mu)^2}{\\sigma^2}-1\\right)\n\\end{array}\\right].\n\\]Proposition 2.1  (Score expectation) expectation score zero.Proof. :\n\\[\\begin{eqnarray*}\n\\mathbb{E}\\left(\\frac{\\partial \\log f(Y;\\boldsymbol\\theta)}{\\partial \\boldsymbol\\theta}\\right) &=&\n\\int \\frac{\\partial \\log f(y;\\boldsymbol\\theta)}{\\partial \\boldsymbol\\theta} f(y;\\boldsymbol\\theta) dy \\\\\n&=& \\int \\frac{\\partial f(y;\\boldsymbol\\theta)/\\partial \\boldsymbol\\theta}{f(y;\\boldsymbol\\theta)} f(y;\\boldsymbol\\theta) dy =\n\\frac{\\partial}{\\partial \\boldsymbol\\theta} \\int f(y;\\boldsymbol\\theta) dy\\\\\n&=&\\partial 1 /\\partial \\boldsymbol\\theta = 0,\n\\end{eqnarray*}\\]\ngives result.Definition 2.4  (Fisher information matrix) information matrix (minus) expectation second derivatives log-likelihood function:\n\\[\n\\mathcal{}_Y(\\boldsymbol\\theta) = - \\mathbb{E} \\left( \\frac{\\partial^2 \\log f(Y;\\boldsymbol\\theta)}{\\partial \\boldsymbol\\theta \\partial \\boldsymbol\\theta'} \\right).\n\\]Proposition 2.2  \n\\[\n\\mathcal{}_Y(\\boldsymbol\\theta) = \\mathbb{E} \\left[ \\left( \\frac{\\partial \\log f(Y;\\boldsymbol\\theta)}{\\partial \\boldsymbol\\theta} \\right)\n\\left( \\frac{\\partial \\log f(Y;\\boldsymbol\\theta)}{\\partial \\boldsymbol\\theta} \\right)' \\right] = \\mathbb{V}ar[S(Y;\\boldsymbol\\theta)].\n\\]Proof. \\(\\frac{\\partial^2 \\log f(Y;\\boldsymbol\\theta)}{\\partial \\boldsymbol\\theta \\partial \\boldsymbol\\theta'} = \\frac{\\partial^2 f(Y;\\boldsymbol\\theta)}{\\partial \\boldsymbol\\theta \\partial \\boldsymbol\\theta'}\\frac{1}{f(Y;\\boldsymbol\\theta)} - \\frac{\\partial \\log f(Y;\\boldsymbol\\theta)}{\\partial \\boldsymbol\\theta}\\frac{\\partial \\log f(Y;\\boldsymbol\\theta)}{\\partial \\boldsymbol\\theta'}\\). expectation first right-hand side term \\(\\partial^2 1 /(\\partial \\boldsymbol\\theta \\partial \\boldsymbol\\theta') = \\mathbf{0}\\), gives result.Example 2.2  \\(y_i \\sim\\,..d.\\, \\mathcal{N}(\\mu,\\sigma^2)\\), let \\(\\boldsymbol\\theta = [\\mu,\\sigma^2]'\\) \n\\[\n\\frac{\\partial \\log f(y;\\boldsymbol\\theta)}{\\partial \\boldsymbol\\theta} = \\left[\\frac{y-\\mu}{\\sigma^2} \\quad \\frac{1}{2\\sigma^2}\\left(\\frac{(y-\\mu)^2}{\\sigma^2}-1\\right) \\right]',\n\\]\n\n\\[\n\\mathcal{}_Y(\\boldsymbol\\theta) = \\mathbb{E}\\left( \\frac{1}{\\sigma^4}\n\\left[\n\\begin{array}{cc}\n\\sigma^2&y-\\mu\\\\\ny-\\mu & \\frac{(y-\\mu)^2}{\\sigma^2}-\\frac{1}{2}\n\\end{array}\\right]\n\\right)=\n\\left[\n\\begin{array}{cc}\n1/\\sigma^2&0\\\\\n0 & 1/(2\\sigma^4)\n\\end{array}\\right].\n\\]Proposition 2.3  (Additive property Information matrix) information matrix resulting two independent experiments sum information matrices:\n\\[\n\\mathcal{}_{X,Y}(\\boldsymbol\\theta) = \\mathcal{}_X(\\boldsymbol\\theta) + \\mathcal{}_Y(\\boldsymbol\\theta).\n\\]Proof. Directly deduced definition information matrix (Def. 2.4), using epxectation product independent variables product expectations.Theorem 2.1  (Frechet-Darmois-Cramer-Rao bound) Consider unbiased estimator \\(\\boldsymbol\\theta\\) denoted \\(\\hat{\\boldsymbol\\theta}(Y)\\). variance random variable \\(\\boldsymbol\\omega'\\hat{\\boldsymbol\\theta}\\) (linear combination components \\(\\hat{\\boldsymbol\\theta}\\)) larger :\n\\[\n(\\boldsymbol\\omega'\\boldsymbol\\omega)^2/(\\boldsymbol\\omega' \\mathcal{}_Y(\\boldsymbol\\theta) \\boldsymbol\\omega).\n\\]Proof. Cauchy-Schwarz inequality implies \\(\\sqrt{\\mathbb{V}ar(\\boldsymbol\\omega'\\hat{\\boldsymbol\\theta}(Y))\\mathbb{V}ar(\\boldsymbol\\omega'S(Y;\\boldsymbol\\theta))} \\ge |\\boldsymbol\\omega'\\mathbb{C}ov[\\hat{\\boldsymbol\\theta}(Y),S(Y;\\boldsymbol\\theta)]\\boldsymbol\\omega |\\). Now, \\(\\mathbb{C}ov[\\hat{\\boldsymbol\\theta}(Y),S(Y;\\boldsymbol\\theta)] = \\int_y \\hat{\\boldsymbol\\theta}(y) \\frac{\\partial \\log f(y;\\boldsymbol\\theta)}{\\partial \\boldsymbol\\theta} f(y;\\boldsymbol\\theta)dy = \\frac{\\partial}{\\partial \\boldsymbol\\theta}\\int_y \\hat{\\boldsymbol\\theta}(y) f(y;\\boldsymbol\\theta)dy = \\mathbf{}\\) \\(\\hat{\\boldsymbol\\theta}\\) unbiased. Therefore \\(\\mathbb{V}ar(\\boldsymbol\\omega'\\hat{\\boldsymbol\\theta}(Y)) \\ge \\mathbb{V}ar(\\boldsymbol\\omega'S(Y;\\boldsymbol\\theta))^{-1} (\\boldsymbol\\omega'\\boldsymbol\\omega)^2\\). Prop. 2.2 leads result.Definition 2.5  (Identifiability) vector parameters \\(\\boldsymbol\\theta\\) identifiable , vector \\(\\boldsymbol\\theta^*\\):\n\\[\n\\boldsymbol\\theta^* \\ne \\boldsymbol\\theta \\Rightarrow \\mathcal{L}(\\boldsymbol\\theta^*;\\mathbf{y}) \\ne \\mathcal{L}(\\boldsymbol\\theta;\\mathbf{y}).\n\\]Definition 2.6  (Maximum Likelihood Estimator (MLE)) maximum likelihood estimator (MLE) vector \\(\\boldsymbol\\theta\\) maximizes likelihood function. Formally:\n\\[\\begin{equation}\n\\boldsymbol\\theta_{MLE} = \\arg \\max_{\\boldsymbol\\theta} \\mathcal{L}(\\boldsymbol\\theta;\\mathbf{y})  = \\arg \\max_{\\boldsymbol\\theta} \\log \\mathcal{L}(\\boldsymbol\\theta;\\mathbf{y}).\\tag{2.8}\n\\end{equation}\\]Definition 2.7  (Likelihood equation) necessary condition maximizing likelihood function (regularity assumption, see Hypotheses 2.1) :\n\\[\\begin{equation}\n\\dfrac{\\partial \\log \\mathcal{L}(\\boldsymbol\\theta;\\mathbf{y})}{\\partial \\boldsymbol\\theta} = \\mathbf{0}.\n\\end{equation}\\]Hypothesis 2.1  (Regularity assumptions) :\\(\\boldsymbol\\theta \\\\Theta\\) \\(\\Theta\\) compact.\\(\\boldsymbol\\theta_0\\) identified.log-likelihood function continuous \\(\\boldsymbol\\theta\\).\\(\\mathbb{E}_{\\boldsymbol\\theta_0}(\\log f(Y;\\boldsymbol\\theta))\\) exists.log-likelihood function \\((1/n)\\log\\mathcal{L}(\\boldsymbol\\theta;\\mathbf{y})\\) converges almost surely \\(\\mathbb{E}_{\\boldsymbol\\theta_0}(\\log f(Y;\\boldsymbol\\theta))\\), uniformly \\(\\boldsymbol\\theta \\\\Theta\\).log-likelihood function twice continuously differentiable open neighborood \\(\\boldsymbol\\theta_0\\).matrix \\(\\mathbf{}(\\boldsymbol\\theta_0) = - \\mathbb{E}_0 \\left( \\frac{\\partial^2 \\log \\mathcal{L}(\\boldsymbol\\theta;\\mathbf{y})}{\\partial \\boldsymbol\\theta \\partial \\boldsymbol\\theta'}\\right)\\) —Fisher Information matrix— exists nonsingular.Proposition 2.4  (Properties MLE) regularity conditions (Assumptions 2.1), MLE :Consistent: \\(\\mbox{plim}\\; \\boldsymbol\\theta_{MLE} = {\\boldsymbol\\theta}_0\\) (\\({\\boldsymbol\\theta}_0\\) true vector parameters).Asymptotically normal:\n\\[\\begin{equation}\n\\boxed{\\sqrt{n}(\\boldsymbol\\theta_{MLE} - \\boldsymbol\\theta_{0}) \\overset{d}{\\rightarrow} \\mathcal{N}(0,\\mathcal{}_Y(\\boldsymbol\\theta_0)^{-1}).} \\tag{2.9}\n\\end{equation}\\]Asymptotically efficient: \\(\\boldsymbol\\theta_{MLE}\\) asymptotically efficient achieves Freechet-Darmois-Cramer-Rao lower bound consistent estimators.Invariant: MLE \\(g(\\boldsymbol\\theta_0)\\) \\(g(\\boldsymbol\\theta_{MLE})\\) \\(g\\) continuous continuously differentiable function.Proof. See Appendix 4.5.Since \\(\\mathcal{}_Y(\\boldsymbol\\theta_0)=\\frac{1}{n}\\mathbf{}(\\boldsymbol\\theta_0)\\), asymptotic covariance matrix MLE \\([\\mathbf{}(\\boldsymbol\\theta_0)]^{-1}\\), :\n\\[\n[\\mathbf{}(\\boldsymbol\\theta_0)]^{-1} = \\left[- \\mathbb{E}_0 \\left( \\frac{\\partial^2 \\log \\mathcal{L}(\\boldsymbol\\theta;\\mathbf{y})}{\\partial \\boldsymbol\\theta \\partial \\boldsymbol\\theta'}\\right) \\right]^{-1}.\n\\]\ndirect (analytical) evaluation expectation often reach. can however estimated , either:\n\\[\\begin{eqnarray}\n\\hat{\\mathbf{}}_1^{-1} &=&  \\left( - \\frac{\\partial^2 \\log \\mathcal{L}({\\boldsymbol\\theta_{MLE}};\\mathbf{y})}{\\partial {\\boldsymbol\\theta} \\partial {\\boldsymbol\\theta}'}\\right)^{-1}, \\tag{2.10}\\\\\n\\hat{\\mathbf{}}_2^{-1} &=&  \\left( \\sum_{=1}^n \\frac{\\partial \\log \\mathcal{L}({\\boldsymbol\\theta_{MLE}};y_i)}{\\partial {\\boldsymbol\\theta}} \\frac{\\partial \\log \\mathcal{L}({\\boldsymbol\\theta_{MLE}};y_i)}{\\partial {\\boldsymbol\\theta'}} \\right)^{-1}.  \\tag{2.11}\n\\end{eqnarray}\\]Asymptotically, \\((\\hat{\\mathbf{}}_1^{-1})\\hat{\\mathbf{}}_2=Id\\), , two formulas provide result.case (suspected) misspecification, one can use -called sandwich estimator covariance matrix.3 covariance matrix given :\n\\[\\begin{equation}\n\\hat{\\mathbf{}}_3^{-1} = \\hat{\\mathbf{}}_2^{-1} \\hat{\\mathbf{}}_1 \\hat{\\mathbf{}}_2^{-1}.\\tag{2.12}\n\\end{equation}\\]","code":""},{"path":"estimation-methods.html","id":"to-sum-up-mle-in-practice","chapter":"2 Estimation Methods","heading":"2.2.3 To sum up – MLE in practice","text":"implement MLE, need:parametric model (depending vector parameters \\(\\boldsymbol\\theta\\) whose “true” value \\(\\boldsymbol\\theta_0\\)) specified...d. sources randomness identified.density associated one observation \\(y_i\\) computed analytically (function \\(\\boldsymbol\\theta\\)): \\(f(y;\\boldsymbol\\theta)\\).log-likelihood \\(\\log \\mathcal{L}(\\boldsymbol\\theta;\\mathbf{y}) = \\sum_i \\log f(y_i;\\boldsymbol\\theta)\\).MLE estimator results optimization problem (Eq. (2.8)):\n\\[\\begin{equation}\n\\boldsymbol\\theta_{MLE} = \\arg \\max_{\\boldsymbol\\theta} \\log \\mathcal{L}(\\boldsymbol\\theta;\\mathbf{y}).\n\\end{equation}\\]: \\(\\boldsymbol\\theta_{MLE} \\sim \\mathcal{N}({\\boldsymbol\\theta}_0,\\mathbf{}(\\boldsymbol\\theta_0)^{-1})\\), \\(\\mathbf{}(\\boldsymbol\\theta_0)^{-1}\\) estimated means Eq. (2.10), Eq. (2.11), Eq. (2.12). time, computation numerical.","code":""},{"path":"estimation-methods.html","id":"example-mle-estimation-of-a-mixture-of-gaussian-distribution","chapter":"2 Estimation Methods","heading":"2.2.4 Example: MLE estimation of a mixture of Gaussian distribution","text":"Consider returns Swiss Market Index (SMI). Assume returns independently drawn mixture Gaussian distributions. p.d.f. \\(f(x;\\boldsymbol\\theta)\\), \\(\\boldsymbol\\theta = [\\mu_1,\\sigma_1,\\mu_2,\\sigma_2,p]'\\), given :\n\\[\np \\frac{1}{\\sqrt{2\\pi\\sigma_1^2}}\\exp\\left(-\\frac{(x - \\mu_1)^2}{2\\sigma_1^2}\\right) + (1-p)\\frac{1}{\\sqrt{2\\pi\\sigma_2^2}}\\exp\\left(-\\frac{(x - \\mu_2)^2}{2\\sigma_2^2}\\right).\n\\]\n(See p.d.f. mixtures Gaussian distributions.)\nFigure 2.4: Time series SMI weekly returns (source: Yahoo Finance).\nBuild log-likelihood function (fucntion log.f), use numerical BFGS algorithm maximize (using optim wrapper):Next, compute estimates covariance matrix MLE (using Eqs. (2.10), (2.11), (2.12)), compare three sets resulting standard deviations five estimated paramters:According first (respectively third) type estimate covariance matrix, 95% confidence interval \\(\\mu_1\\) [0.182, 0.42] (resp. [0.151, 0.451]).Note directly estimated parameter \\(p\\) \\(\\nu = \\log(p/(1-p))\\) (way \\(p = \\exp(\\nu)/(1+\\exp(\\nu))\\)). order get estimate standard deviation esitmate \\(p\\), can implement Delta method. method based fact , function \\(g\\) continuous neighborhood \\(\\boldsymbol\\theta_0\\) large \\(n\\), :\n\\[\\begin{equation}\n\\mathbb{V}ar(g(\\hat{\\boldsymbol\\theta}_n)) \\approx \\frac{\\partial g(\\hat{\\boldsymbol\\theta}_n)}{\\partial \\boldsymbol\\theta'}\\mathbb{V}ar(\\hat{\\boldsymbol\\theta}_n)\\frac{\\partial g(\\hat{\\boldsymbol\\theta}_n)'}{\\partial \\boldsymbol\\theta}.\\tag{2.13}\n\\end{equation}\\]previous results show MLE estimate \\(p\\) 0.8749539, standard deviation approximately equal 0.0612573.finish , let us draw estimated parametric p.d.f. (mixture Gaussian distribution), compare non-parametric (kernel-based) estimate p.d.f. (using function density):\nFigure 2.5: Comparison different estimates distribution returns.\n","code":"\nlibrary(AEC);data(smi)\nT <- dim(smi)[1]\nh <- 5 # holding period (one week)\nsmi$r <- c(rep(NaN,h),\n           100*c(log(smi$Close[(1+h):T]/smi$Close[1:(T-h)])))\nindic.dates <- seq(1,T,by=5)  # weekly returns\nsmi <- smi[indic.dates,]\nsmi <- smi[complete.cases(smi),]\npar(mfrow=c(1,1));par(plt=c(.15,.95,.1,.95))\nplot(smi$Date,smi$r,type=\"l\",xlab=\"\",ylab=\"in percent\")\nabline(h=0,col=\"blue\")\nabline(h=mean(smi$r,na.rm = TRUE)+2*sd(smi$r,na.rm = TRUE),lty=3,col=\"blue\")\nabline(h=mean(smi$r,na.rm = TRUE)-2*sd(smi$r,na.rm = TRUE),lty=3,col=\"blue\")\nf <- function(theta,y){ # Likelihood function\n  mu.1 <- theta[1]; mu.2 <- theta[2]\n  sigma.1 <- theta[3]; sigma.2 <- theta[4]\n  p <- exp(theta[5])/(1+exp(theta[5]))\n  res <- p*1/sqrt(2*pi*sigma.1^2)*exp(-(y-mu.1)^2/(2*sigma.1^2)) +\n    (1-p)*1/sqrt(2*pi*sigma.2^2)*exp(-(y-mu.2)^2/(2*sigma.2^2))\n  return(res)\n}\nlog.f <- function(theta,y){ #log-Likelihood function\n  return(-sum(log(f(theta,y))))\n}\nres.optim <- optim(c(0,0,0.5,1.5,.5),\n                   log.f,\n                   y=smi$r,\n                   method=\"BFGS\", # could be \"Nelder-Mead\"\n                   control=list(trace=FALSE,maxit=100),hessian=TRUE)\ntheta <- res.optim$par\ntheta## [1]  0.3012379 -1.3167476  1.7715072  4.8197596  1.9454889\n# Hessian approach:\nI.1 <- solve(res.optim$hessian)\n# Outer-product of gradient approach:\nlog.f.0 <- log(f(theta,smi$r))\nepsilon <- .00000001\nd.log.f <- NULL\nfor(i in 1:length(theta)){\n  theta.i <- theta\n  theta.i[i] <- theta.i[i] + epsilon\n  log.f.i <- log(f(theta.i,smi$r))\n  d.log.f <- cbind(d.log.f,\n                   (log.f.i - log.f.0)/epsilon)\n}\nI.2 <- solve(t(d.log.f) %*% d.log.f)\n# Misspecification-robust approach (sandwich formula):\nI.3 <- I.1 %*% solve(I.2) %*% I.1\ncbind(diag(I.1),diag(I.2),diag(I.3))##             [,1]        [,2]       [,3]\n## [1,] 0.003683422 0.003199481 0.00586160\n## [2,] 0.226892824 0.194283391 0.38653389\n## [3,] 0.005764271 0.002769579 0.01712255\n## [4,] 0.194081311 0.047466419 0.83130838\n## [5,] 0.092114437 0.040366005 0.31347858\ng <- function(theta){\n  mu.1 <- theta[1]; mu.2 <- theta[2]\n  sigma.1 <- theta[3]; sigma.2 <- theta[4]\n  p <- exp(theta[5])/(1+exp(theta[5]))\n  return(c(mu.1,mu.2,sigma.1,sigma.2,p))\n}\n# Computation of g's gradient around estimated theta:\neps <- .00001\ng.theta <- g(theta)\ng.gradient <- NULL\nfor(i in 1:5){\n  theta.perturb <- theta\n  theta.perturb[i] <- theta[i] + eps\n  g.gradient <- cbind(g.gradient,(g(theta.perturb)-g.theta)/eps)\n}\nVar <- g.gradient %*% I.3 %*% t(g.gradient)\nstdv.g.theta <- sqrt(diag(Var))\nstdv.theta <- sqrt(diag(I.3))\ncbind(theta,stdv.theta,g.theta,stdv.g.theta)##           theta stdv.theta    g.theta stdv.g.theta\n## [1,]  0.3012379 0.07656108  0.3012379   0.07656108\n## [2,] -1.3167476 0.62171850 -1.3167476   0.62171850\n## [3,]  1.7715072 0.13085316  1.7715072   0.13085316\n## [4,]  4.8197596 0.91176114  4.8197596   0.91176114\n## [5,]  1.9454889 0.55989158  0.8749539   0.06125726\nx <- seq(-5,5,by=.01)\npar(plt=c(.1,.95,.1,.95))\nplot(x,f(theta,x),type=\"l\",lwd=2,xlab=\"returns, in percent\",ylab=\"\",\n     ylim=c(0,1.4*max(f(theta,x))))\nlines(density(smi$r),type=\"l\",lwd=2,lty=3)\nlines(x,dnorm(x,mean=mean(smi$r),sd = sd(smi$r)),col=\"red\",lty=2,lwd=2)\nrug(smi$r,col=\"blue\")\nlegend(\"topleft\",\n       c(\"Kernel estimate (non-parametric)\",\n         \"Estimated mixture of Gaussian distr. (MLE, parametric)\",\n         \"Normal distribution\"),\n       lty=c(3,1,2),lwd=c(2), # line width\n       col=c(\"black\",\"black\",\"red\"),pt.bg=c(1),pt.cex = c(1),\n       bg=\"white\",seg.len = 4)"},{"path":"estimation-methods.html","id":"TestMLE","chapter":"2 Estimation Methods","heading":"2.2.5 Test procedures","text":"Suppose want test following parameter restrictions:\n\\[\\begin{equation}\n\\boxed{H_0: \\underbrace{h(\\boldsymbol\\theta)}_{r \\times 1}=0.}\n\\end{equation}\\]context MLE, three tests largely used:Likelihood Ratio (LR) test,Wald (W) test,Lagrange Multiplier (LM) test.rationale behind three tests:4LR: \\(h(\\boldsymbol\\theta)=0\\), imposing restriction estimation (restricted estimator) result large decrease likelihood function (w.r.t unrestricted estimation).Wald: \\(h(\\boldsymbol\\theta)=0\\), \\(h(\\hat{\\boldsymbol\\theta})\\) far \\(0\\) (even restrictions imposed MLE).LM: \\(h(\\boldsymbol\\theta)=0\\), gradient likelihood function small evaluated restricted estimator.terms implementation, LR necessitates estimate restricted unrestricted models, Wald test requires estimation unrestricted model , LM tests requires estimation restricted model .shown , three test statistics associated three tests coincide asymptotically. (Therefore, naturally asymptotic distribution, \\(\\chi^2\\).)Proposition 2.5  (Asymptotic distribution Wald statistic) regularity conditions (Assumptions 2.1) \\(H_0: h(\\boldsymbol\\theta)=0\\), Wald statistic, defined :\n\\[\n\\boxed{\\xi^W = h(\\hat{\\boldsymbol\\theta})' \\mathbb{V}ar[h(\\hat{\\boldsymbol\\theta})]^{-1} h(\\hat{\\boldsymbol\\theta}),}\n\\]\n\n\\[\\begin{equation}\n\\mathbb{V}ar[h(\\hat{\\boldsymbol\\theta})] = \\left(\\frac{\\partial h(\\hat{\\boldsymbol\\theta})}{\\partial \\boldsymbol\\theta'} \\right) \\mathbb{V}ar[\\hat{\\boldsymbol\\theta}]\n\\left(\\frac{\\partial h(\\hat{\\boldsymbol\\theta})'}{\\partial \\boldsymbol\\theta} \\right),\\tag{2.14}\n\\end{equation}\\]\nasymptotically \\(\\chi^2(r)\\), number degrees freedom \\(r\\) corresponds dimension \\(h(\\boldsymbol\\theta)\\). (Note Eq. (2.14) one used Delta method, see Eq. (2.13).)Wald test, defined critical region\n\\[\n\\{\\xi^W \\ge \\chi^2_{1-\\alpha}(r)\\},\n\\]\n\\(\\chi^2_{1-\\alpha}(r)\\) denotes quantile level \\(1-\\alpha\\) \\(\\chi^2(r)\\) distribution, asymptotic level \\(\\alpha\\) consistent.5Proof. See Appendix 4.5.practice, Eq. (2.14), \\(\\mathbb{V}ar[\\hat{\\boldsymbol\\theta}]\\) replaced estimate given, e.g., Eq. (2.10), Eq. (2.11), Eq. (2.12).Proposition 2.6  (Asymptotic distribution LM test statistic) regularity conditions (Assumptions 2.1) \\(H_0: h(\\boldsymbol\\theta)=0\\), LM statistic\n\\[\\begin{equation}\n\\boxed{\\xi^{LM} =\n\\left(\\left.\\frac{\\partial \\log \\mathcal{L}(\\boldsymbol\\theta)}{\\partial \\boldsymbol\\theta'}\\right|_{\\boldsymbol\\theta = \\hat{\\boldsymbol\\theta}^0}  \\right)\n[\\mathbf{}(\\hat{\\boldsymbol\\theta}^0)]^{-1}\n\\left(\\left.\\frac{\\partial \\log \\mathcal{L}(\\boldsymbol\\theta)}{\\partial \\boldsymbol\\theta }\\right|_{\\boldsymbol\\theta = \\hat{\\boldsymbol\\theta}^0}  \\right),} \\tag{2.15}\n\\end{equation}\\]\n(\\(\\hat{\\boldsymbol\\theta}^0\\) restricted MLE estimator) \\(\\chi^2(r)\\).test defined critical region:\n\\[\n\\{\\xi^{LM} \\ge \\chi^2_{1-\\alpha}(r)\\}\n\\]\nasymptotic level \\(\\alpha\\) consistent (see Defs. 4.7 4.8). test called Score Lagrange Multiplier (LM) test.Proof. See Appendix 4.5.Definition 2.8  (Likelihood Ratio test statistics) likelihood ratio associated restriction form \\(H_0: h({\\boldsymbol\\theta})=0\\) given :\n\\[\nLR = \\frac{\\mathcal{L}_R(\\boldsymbol\\theta;\\mathbf{y})}{\\mathcal{L}_U(\\boldsymbol\\theta;\\mathbf{y})} \\quad (\\[0,1]),\n\\]\n\\(\\mathcal{L}_R\\) (respectively \\(\\mathcal{L}_U\\)) likelihood function imposes (resp. impose) restriction. likelihood ratio test statistic given \\(-2\\log(LR)\\), :\n\\[\n\\boxed{\\xi^{LR}= 2 (\\log\\mathcal{L}_U(\\boldsymbol\\theta;\\mathbf{y})-\\log\\mathcal{L}_R(\\boldsymbol\\theta;\\mathbf{y})).}\n\\]Proposition 2.7  (Asymptotic equivalence LR, LM, Wald tests) null hypothesis \\(H_0\\), , asymptotically:\n\\[\n\\xi^{LM} = \\xi^{LR} = \\xi^{W}.\n\\]Proof. See Appendix 4.5.","code":""},{"path":"estimation-methods.html","id":"bayesian-approach","chapter":"2 Estimation Methods","heading":"2.3 Bayesian approach","text":"","code":""},{"path":"estimation-methods.html","id":"introduction","chapter":"2 Estimation Methods","heading":"2.3.1 Introduction","text":"excellent introduction Bayesian methods proposed Martin Haugh, 2017.suggested name approach, starting point Bayes formula:\n\\[\n\\mathbb{P}(|B) = \\frac{\\mathbb{P}(\\& B)}{\\mathbb{P}(B)},\n\\]\n\\(\\) \\(B\\) two “events”. instance, \\(\\) may : parameter \\(\\alpha\\) (conceived something stochastic) lies interval \\([,b]\\). Assume interested probability occurrence \\(\\). Without specific information (“unconditionally”), probability \\(\\mathbb{P}()\\). evaluation probability can better provided additional form information. Typically, event \\(B\\) tends occur simultaneously \\(\\), knowledge \\(B\\) can useful. Bayes formula says additional information (\\(B\\)) can used “update” probability event \\(\\).case, intuition work follows: assume know form data-generating process (DGP). , know structure model used draw stochastic data; also know type distributions used generate data. However, know numerical values parameters characterizing DGP. Let us denote \\({\\boldsymbol\\theta}\\) vector unknown parameters. parameters known exactly, assume –even without observed data– priors distribution. , case example (\\(\\) \\(B\\)), observation data generated model can reduce uncertainty associated \\({\\boldsymbol\\theta}\\). Loosely speaking, combining priors observations data generated model result “thinner” distributions components \\({\\boldsymbol\\theta}\\). latter distributions called posterior distributions.6Let us formalize intuition. Define prior \\(f_{\\boldsymbol\\theta}({\\boldsymbol\\theta})\\) model realizations (“data”) vector \\(\\mathbf{y}\\). joint distribution \\((\\mathbf{y},{\\boldsymbol\\theta})\\) given :\n\\[\nf_{Y,{\\boldsymbol\\theta}}(\\mathbf{y},{\\boldsymbol\\theta}) = f_{Y|{\\boldsymbol\\theta}}(\\mathbf{y},{\\boldsymbol\\theta})f_{\\boldsymbol\\theta}({\\boldsymbol\\theta}),\n\\]\n, symmetrically, \n\\[\nf_{Y,{\\boldsymbol\\theta}}(\\mathbf{y},{\\boldsymbol\\theta}) = f_{{\\boldsymbol\\theta}|Y}({\\boldsymbol\\theta},\\mathbf{y})f_Y(\\mathbf{y}),\n\\]\n\\(f_{{\\boldsymbol\\theta}|Y}(\\cdot,\\mathbf{y})\\), distribution parameters conditional observations, posterior distribution.last two equations imply :\n\\[\\begin{equation}\nf_{{\\boldsymbol\\theta}|Y}({\\boldsymbol\\theta},\\mathbf{y}) = \\frac{f_{Y|{\\boldsymbol\\theta}}(\\mathbf{y},{\\boldsymbol\\theta})f_{{\\boldsymbol\\theta}}({\\boldsymbol\\theta})}{f_Y(\\mathbf{y})}.\\tag{2.16}\n\\end{equation}\\]\nNote \\(f_Y\\) marginal (unconditional) distribution \\(\\mathbf{y}\\), can written:\n\\[\\begin{equation}\nf_Y(\\mathbf{y}) = \\int f_{Y|{\\boldsymbol\\theta}}(\\mathbf{y},{\\boldsymbol\\theta})f_{\\boldsymbol\\theta}({\\boldsymbol\\theta}) d {\\boldsymbol\\theta}.\n\\end{equation}\\]Eq. (2.16) sometimes rewritten follows:\n\\[\\begin{equation}\nf_{{\\boldsymbol\\theta}|Y}({\\boldsymbol\\theta},\\mathbf{y}) \\propto f_{{\\boldsymbol\\theta},Y}({\\boldsymbol\\theta},\\mathbf{y}) := f_{Y|{\\boldsymbol\\theta}}(\\mathbf{y},{\\boldsymbol\\theta})f_{\\boldsymbol\\theta}({\\boldsymbol\\theta}), \\tag{2.17}\n\\end{equation}\\]\n\\(\\propto\\) means, loosely speaking, “proportional ”. rare instances, starting given priors, one can analytically compute posterior distribution \\(f_{\\boldsymbol\\theta}({\\boldsymbol\\theta},\\mathbf{y})\\). However, cases, reach. One resort numerical approaches compute posterior distribution. Monte Carlo Markov Chains (MCMC) one .According Bernstein-von Mises Theorem, Bayesian MLE estimators large sample properties. (particular, Bayesian approach also achieve FDCR bound, see Theorem 2.1.) intuition behind result influence prior diminishes increasing sample sizes.","code":""},{"path":"estimation-methods.html","id":"monte-carlo-markov-chains","chapter":"2 Estimation Methods","heading":"2.3.2 Monte-Carlo Markov Chains","text":"MCMC techniques aim using simulations approach distribution whose distribution difficult obtain analytically. Indeed, circumstances, one can draw distribution even know analytical expression.Definition 2.9  (Markov Chain) sequence \\(\\{z_i\\}\\) said (first-order) Markovian process satisfies:\n\\[\nf(z_i|z_{-1},z_{-2},\\dots) = f(z_i|z_{-1}).\n\\]Metropolis-Hastings (MH) algorithm specific MCMC approach allows generate samples \\({\\boldsymbol\\theta}\\)’s whose distribution approximately corresponds posterior distribution Eq. (2.16).MH algorithm recursive algorithm. , one can draw \\(^{th}\\) value \\({\\boldsymbol\\theta}\\), denoted \\({\\boldsymbol\\theta}_i\\), one already drawn \\({\\boldsymbol\\theta}_{-1}\\). Assume \\({\\boldsymbol\\theta}_{-1}\\). obtain value \\({\\boldsymbol\\theta}_i\\) implementing following steps:Draw \\(\\tilde{{\\boldsymbol\\theta}}_i\\) conditional distribution \\(Q_{\\tilde{{\\boldsymbol\\theta}}|{\\boldsymbol\\theta}}(\\cdot,{\\boldsymbol\\theta}_{-1})\\), called proposal distribution.Draw \\(u\\) uniform distribution \\([0,1]\\).Compute\n\\[\\begin{equation}\n\\alpha(\\tilde{{\\boldsymbol\\theta}}_i,{\\boldsymbol\\theta}_{-1}):= \\min\\left(\\frac{f_{{\\boldsymbol\\theta},Y}(\\tilde{{\\boldsymbol\\theta}}_i,\\mathbf{y})}{f_{{\\boldsymbol\\theta},Y}({\\boldsymbol\\theta}_{-1},\\mathbf{y})}\\times\\frac{Q_{\\tilde{{\\boldsymbol\\theta}}|{\\boldsymbol\\theta}}({\\boldsymbol\\theta}_{-1},\\tilde{{\\boldsymbol\\theta}}_i)}{Q_{\\tilde{{\\boldsymbol\\theta}}|{\\boldsymbol\\theta}}(\\tilde{{\\boldsymbol\\theta}}_i,{\\boldsymbol\\theta}_{-1})},1\\right),\\tag{2.18}\n\\end{equation}\\]\n\\(f_{{\\boldsymbol\\theta},Y}\\) given Eq. (2.17).\\(u<\\alpha(\\tilde{{\\boldsymbol\\theta}}_i,{\\boldsymbol\\theta}_{-1})\\), take \\({\\boldsymbol\\theta}_i = \\tilde{{\\boldsymbol\\theta}}_i\\), otherwise leave \\({\\boldsymbol\\theta}_i\\) equal \\({\\boldsymbol\\theta}_{-1}\\).can shown , distribution draws converges posterior distribution. , sufficiently large number iterations, draws can considered drawn posterior distribution.7To get insights algorithm, consider case symmetric proposal distribution, :\n\\[\\begin{equation}\nQ_{\\tilde{{\\boldsymbol\\theta}}|{\\boldsymbol\\theta}}(\\tilde{{\\boldsymbol\\theta}}_i,{\\boldsymbol\\theta}_{-1})=Q_{\\tilde{{\\boldsymbol\\theta}}|{\\boldsymbol\\theta}}({\\boldsymbol\\theta}_{-1},\\tilde{{\\boldsymbol\\theta}}_i).\\tag{2.19}\n\\end{equation}\\]\n:\n\\[\\begin{equation}\n\\alpha(\\tilde{{\\boldsymbol\\theta}},{\\boldsymbol\\theta}_{-1})= \\min\\left(\\frac{q(\\tilde{{\\boldsymbol\\theta}},y)}{q({\\boldsymbol\\theta}_{-1},y)},1\\right). \\tag{2.20}\n\\end{equation}\\]\nRemember , marginal distribution data (\\(f_Y(\\mathbf{y})\\)), \\(f_{{\\boldsymbol\\theta},Y}(\\tilde{{\\boldsymbol\\theta}},\\mathbf{y})\\) probability observing \\(\\mathbf{y}\\) conditional model parameterized \\(\\tilde{\\boldsymbol\\theta}\\). , Eq. (2.20), appears probability larger \\(\\tilde{\\boldsymbol\\theta}\\) \\({\\boldsymbol\\theta}_{-1}\\) (case \\(\\tilde{\\boldsymbol\\theta}\\) seems “consistent observations \\(\\mathbf{y}\\)” \\({\\boldsymbol\\theta}_{-1}\\)), accept \\({\\boldsymbol\\theta}_i\\). contrast, \\(f_{{\\boldsymbol\\theta},Y}(\\tilde{{\\boldsymbol\\theta}},\\mathbf{y})<f_{{\\boldsymbol\\theta},Y}({\\boldsymbol\\theta}_{-1},\\mathbf{y})\\), necessarily accept proposed value \\(\\tilde{{\\boldsymbol\\theta}}\\), especially \\(f_{{\\boldsymbol\\theta},Y}(\\tilde{{\\boldsymbol\\theta}},\\mathbf{y})\\ll f_{{\\boldsymbol\\theta},Y}({\\boldsymbol\\theta}_{-1},\\mathbf{y})\\) (case \\(\\tilde{\\boldsymbol\\theta}\\) seems far less consistent observations \\(\\mathbf{y}\\) \\({\\boldsymbol\\theta}_{-1}\\), , accordingly, acceptance probability, namely \\(\\alpha(\\tilde{{\\boldsymbol\\theta}},{\\boldsymbol\\theta}_{-1})\\), small).choice proposal distribution \\(Q_{\\tilde{\\boldsymbol\\theta}|{\\boldsymbol\\theta}}\\) crucial get rapid convergence algorithm. Looking Eq. (2.18), easily seen optimal choice \\(Q_{\\tilde{\\boldsymbol\\theta}|{\\boldsymbol\\theta}}(\\cdot,{\\boldsymbol\\theta}_i)=f_{{\\boldsymbol\\theta}|Y}(\\cdot,\\mathbf{y})\\). case, \\(\\alpha(\\tilde{{\\boldsymbol\\theta}}_i,{\\boldsymbol\\theta}_{-1})\\equiv 1\\) (see Eq. (2.18)). accept draws proposal distribution, distribution directly posterior distribution. course, situation realistoc objective algorithm precisely approximate posterior distribution.common choice \\(Q\\) multivariate normal distribution. \\({\\boldsymbol\\theta}\\) dimension \\(K\\), can instance use:\n\\[\nQ(\\tilde{\\boldsymbol\\theta},{\\boldsymbol\\theta})= \\frac{1}{\\left(\\sqrt{2\\pi\\sigma^2}\\right)^K}\\exp\\left(-\\frac{1}{2}\\sum_{j=1}^K\\frac{(\\tilde{\\boldsymbol\\theta}_j-{\\boldsymbol\\theta}_j)^2}{\\sigma^2}\\right),\n\\]\nexample symmetric proposal distribution (see Eq. (2.19)). Equivalently, :\n\\[\n\\tilde{\\boldsymbol\\theta} = {\\boldsymbol\\theta} + \\varepsilon,\n\\]\n\\(\\varepsilon\\) \\(K\\)-dimensional vector independent zero-mean normal disturbances variance \\(\\sigma^2\\).8 One determine appropriate value \\(\\sigma\\). low, \\(\\alpha\\) close 1 (\\(\\tilde{{\\boldsymbol\\theta}}_i\\) close \\({\\boldsymbol\\theta}_{-1}\\)), accept often proposed value (\\(\\tilde{{\\boldsymbol\\theta}}_i\\)). seems favourable situation. may . Indeed, means take large number iterations explore whole distribution \\({\\boldsymbol\\theta}\\). \\(\\sigma\\) large? case, likely porposed values (\\(\\tilde{{\\boldsymbol\\theta}}_i\\)) often result poor likelihoods; probability acceptance low Markov chain may blocked initial value. Therefore, intermediate values \\(\\sigma^2\\) determined. acceptance rate (.e., average value \\(\\alpha(\\tilde{{\\boldsymbol\\theta}},{\\boldsymbol\\theta}_{-1})\\)) can used guide . Indeed, literature explores optimal values acceptance rate (order obtain best possible fit posterior minimum number algorithm iterations). particular, following Roberts, Gelman, Gilks (1997), people often target acceptance rate order magnitude 20%.important note , implement approach, one able compute joint p.d.f. \\(q({\\boldsymbol\\theta},\\mathbf{y})=f_{Y|{\\boldsymbol\\theta}}(\\mathbf{y},{\\boldsymbol\\theta})f_{\\boldsymbol\\theta}({\\boldsymbol\\theta})\\) (Eq. (2.17)). , soon one can evaluate likelihood (\\(f_{Y|{\\boldsymbol\\theta}}(\\mathbf{y},{\\boldsymbol\\theta})\\)) prior (\\(f_{\\boldsymbol\\theta}({\\boldsymbol\\theta})\\)), can employ methodology.","code":""},{"path":"estimation-methods.html","id":"example-ar1-specification","chapter":"2 Estimation Methods","heading":"2.3.3 Example: AR(1) specification","text":"following example, employ MCMC order estimate posterior distributions three parameters defining AR(1) model (see Section ??). specification follows:\n\\[\ny_t = \\mu + \\rho y_{t-1} + \\sigma \\varepsilon_{t}, \\quad \\varepsilon_t \\sim \\,..d.\\,\\mathcal{N}(0,1).\n\\]\nHence, \\({\\boldsymbol\\theta} = [\\mu,\\rho,\\sigma]\\). Let us first simulate process \\(T\\) periods:Next, let us write likelihood function, .e. \\(f_{Y|{\\boldsymbol\\theta}}(\\mathbf{y},{\\boldsymbol\\theta})\\). \\(\\rho\\), expected 0 1, use logistic transformation. \\(\\sigma\\), expected positive, use exponential transformation.Next define function rQ draws (Gaussian) proposal distribution, well function Q, computes \\(Q_{\\tilde{\\boldsymbol\\theta}|{\\boldsymbol\\theta}}(\\tilde{\\boldsymbol\\theta},{\\boldsymbol\\theta})\\):consider Gaussian priors:Function p_tilde corresponds \\(f_{{\\boldsymbol\\theta},Y}\\):can now define function \\(\\alpha\\) (Eq. (2.18)):Now, set us write MCMC function:Specify Gaussian priors:\nFigure 2.6: upper line plot compares prior (black) posterior (red) distributions. vertical dashed blue lines indicate true values parameters. second row plots show sequence \\(\\boldsymbol\\theta_i\\)’s generated MCMC algorithm. sequences ones used produce posterior distributions (red lines) upper plots.\n","code":"\nmu <- .6; rho <- .8; sigma <- .5 # true model specification\nT <- 20 # number of observations\ny0 <- mu/(1-rho)\nY <- NULL\nfor(t in 1:T){\n  if(t==1){y <- y0}\n  y <- mu + rho*y + sigma * rnorm(1)\n  Y <- c(Y,y)}\nplot(Y,type=\"l\",xlab=\"time t\",ylab=expression(y[t]))\nlikelihood <- function(param,Y){\n  mu  <- param[1]\n  rho <- exp(param[2])/(1+exp(param[2]))\n  sigma <- exp(param[3])\n  MU <- mu/(1-rho)\n  SIGMA2 <- sigma^2/(1-rho^2)\n  L <- 1/sqrt(2*pi*SIGMA2)*exp(-(Y[1]-MU)^2/(2*SIGMA2))\n  Y1 <- Y[2:length(Y)]\n  Y0 <- Y[1:(length(Y)-1)]\n  aux <- 1/sqrt(2*pi*sigma^2)*exp(-(Y1-mu-rho*Y0)^2/(2*sigma^2))\n  L <- L * prod(aux)\n  return(L)\n}\nrQ <- function(x,a){\n  n <- length(x)\n  y <- x + a * rnorm(n)\n  return(y)}\nQ <- function(y,x,a){\n  q <- 1/sqrt(2*pi*a^2)*exp(-(y - x)^2/(2*a^2))\n  return(prod(q))}\nprior <- function(param,means_prior,stdv_prior){\n  f <- 1/sqrt(2*pi*stdv_prior^2)*exp(-(param - \n                                         means_prior)^2/(2*stdv_prior^2))\n  return(prod(f))}\np_tilde <- function(param,Y,means_prior,stdv_prior){\n  p <- likelihood(param,Y) * prior(param,means_prior,stdv_prior)\n  return(p)}\nalpha <- function(y,x,means_prior,stdv_prior,a){\n  aux <- p_tilde(y,Y,means_prior,stdv_prior)/\n    p_tilde(x,Y,means_prior,stdv_prior) * Q(y,x,a)/Q(x,y,a)\n  alpha_proba <- min(aux,1)\n  return(alpha_proba)}\nMCMC <- function(Y,means_prior,stdv_prior,a,N){\n  x <- means_prior\n  all_theta <- NULL\n  count_accept <- 0\n  for(i in 1:N){\n    y <- rQ(x,a)\n    alph <- alpha(y,x,means_prior,stdv_prior,a)\n    #print(alph)\n    u <- runif(1)\n    if(u < alph){\n      count_accept <- count_accept + 1\n      x <- y}\n    all_theta <- rbind(all_theta,x)}\n  print(paste(\"Acceptance rate:\",toString(round(count_accept/N,3))))\n  return(all_theta)}\ntrue_values <- c(mu,log(rho/(1-rho)),log(sigma))\nmeans_prior <- c(1,0,0) # as if we did not know the true values\nstdv_prior <- rep(2,3)\nresultMCMC <- MCMC(Y,means_prior,stdv_prior,a=.45,N=20000)## [1] \"Acceptance rate: 0.098\"\npar(mfrow=c(2,3))\nfor(i in 1:length(means_prior)){\n  m <- means_prior[i]\n  s <- stdv_prior[i]\n  x <- seq(m-3*s,m+3*s,length.out = 100)\n  par(mfg=c(1,i))\n  aux <- density(resultMCMC[,i])\n  par(plt=c(.15,.95,.15,.85))\n  plot(x,dnorm(x,m,s),type=\"l\",xlab=\"\",ylab=\"\",main=paste(\"Parameter\",i),\n       ylim=c(0,max(aux$y)))\n  lines(aux$x,aux$y,col=\"red\",lwd=2)\n  abline(v=true_values[i],lty=2,col=\"blue\")\n  par(mfg=c(2,i))\n  plot(resultMCMC[,i],1:length(resultMCMC[,i]),xlim=c(min(x),max(x)),\n       type=\"l\",xlab=\"\",ylab=\"\")}"},{"path":"microeconometrics.html","id":"microeconometrics","chapter":"3 Microeconometrics","heading":"3 Microeconometrics","text":"microeconometric models, variables interest often feature restricted distributions —instance discontinuous support—, necessitates specific models. Typical examples discrete-choice models (binary, multinomial, ordered outcomes), sample selection models (censored truncated outcomes), count-data models (integer outcomes). chapter describes estimation interpretation models. also shows discrete-choice models can emerge (structural) random-utility frameworks.","code":""},{"path":"microeconometrics.html","id":"binary-choice-models","chapter":"3 Microeconometrics","heading":"3.1 Binary-choice models","text":"many instances, variables explained (\\(y_i\\)’s) two possible values (\\(0\\) \\(1\\), say). , binary variables. probability equal either 0 1 may depend independent variables, gathered vectors \\(\\mathbf{x}_i\\) (\\(K \\times 1\\)).spectrum applications wide:Binary decisions (e.g. referendums, owner renter, living city countryside, /labour force,…),Contamination (disease default),Success/failure (exams).Without loss generality, model reads:\n\\[\\begin{equation}\\label{eq:binaryBenroulli}\ny_i | \\mathbf{X} \\sim \\mathcal{B}(g(\\mathbf{x}_i;\\boldsymbol\\theta)),\n\\end{equation}\\]\n\\(g(\\mathbf{x}_i;\\boldsymbol\\theta)\\) parameter Bernoulli distribution. words, conditionally \\(\\mathbf{X}\\):\n\\[\\begin{equation}\ny_i = \\left\\{\n\\begin{array}{cl}\n1 & \\mbox{ probability } g(\\mathbf{x}_i;\\boldsymbol\\theta)\\\\\n0 & \\mbox{ probability } 1-g(\\mathbf{x}_i;\\boldsymbol\\theta),\n\\end{array}\n\\right.\\tag{3.1}\n\\end{equation}\\]\n\\(\\boldsymbol\\theta\\) vector parameters estimated.estimation strategy assume \\(g(\\mathbf{x}_i;\\boldsymbol\\theta)\\) can proxied \\(\\tilde{\\boldsymbol\\theta}'\\mathbf{x}_i\\) run linear regression estimate \\(\\tilde{\\boldsymbol\\theta}\\) (situation called Linear Probability Model, LPM):\n\\[\ny_i = \\tilde{\\boldsymbol\\theta}'\\mathbf{x}_i + \\varepsilon_i.\n\\]\nNotwithstanding fact specification exclude negative probabilities probabilities greater one, compatible assumption zero conditional mean (Hypothesis ??) assumption non-correlated residuals (Hypothesis ??), difficultly homoskedasticity assumption (Hypothesis ??). Moreover, \\(\\varepsilon_i\\)’s Gaussian (\\(y_i \\\\{0,1\\}\\)). Hence, using linear regression study relationship \\(\\mathbf{x}_i\\) \\(y_i\\) can consistent inefficient.Figure 3.1 illustrates fit resulting application LPM model binary (dependent) variables.\nFigure 3.1: Fitting binary variable linear model (Linear Probability Model, LPM). model \\(\\mathbb{P}(y_i=1|x_i)=\\Phi(0.5+2x_i)\\), \\(\\Phi\\) c.d.f. normal distribution \\(x_i \\sim \\,..d.\\,\\mathcal{N}(0,1)\\).\nExcept last row (LPM case), Table 3.1 provides examples functions \\(g\\) valued \\([0,1]\\), can therefore used models type: \\(\\mathbb{P}(y_i=1|\\mathbf{x}_i;\\boldsymbol\\theta) = g(\\boldsymbol\\theta'\\mathbf{x}_i)\\) (see Eq. (3.1)). “linear” case given comparison, note satisfy \\(g(\\boldsymbol\\theta'\\mathbf{x}_i) \\[0,1]\\) value \\(\\boldsymbol\\theta'\\mathbf{x}_i\\).Table 3.1:  table provides examples function \\(g\\), s.t. \\(\\mathbb{P}(y_i=1|\\mathbf{x}_i;\\boldsymbol heta) = g(\\boldsymbol\\theta'\\mathbf{x}_i)\\). LPM case (last row) given comparison , , satisfy \\(g(\\boldsymbol\\theta'\\mathbf{x}_i) \\[0,1]\\) value \\(\\boldsymbol\\theta'\\mathbf{x}_i\\).Figure 3.2 displays first three \\(g\\) functions appearing Table 3.1.\nFigure 3.2: Probit, Logit, Log-log functions.\nprobit logit models popular binary-choice models. probit model, :\n\\[\\begin{equation}\ng(z) = \\Phi(z),\\tag{3.2}\n\\end{equation}\\]\n\\(\\Phi\\) c.d.f. normal distribution. logit model:\n\\[\\begin{equation}\ng(z) = \\frac{1}{1+\\exp(-z)}.\\tag{3.3}\n\\end{equation}\\]Figure 3.3 shows conditional probabilities associated (probit) model used generate data Figure 3.1.\nFigure 3.3: model \\(\\mathbb{P}(y_i=1|x_i)=\\Phi(0.5+2x_i)\\), \\(\\Phi\\) c.d.f. normal distribution \\(x_i \\sim \\,..d.\\,\\mathcal{N}(0,1)\\). Crosses give model-implied probabilities \\(y_i=1\\) (conditional \\(x_i\\)).\n","code":""},{"path":"microeconometrics.html","id":"latent","chapter":"3 Microeconometrics","heading":"3.1.1 Interpretation in terms of latent variable, and utility-based models","text":"probit model interpretation terms latent variables, , turn, often exploited structural models, called Random Utility Models (RUM). structural models, assumed agents take decision selecting outcome provides larger utility (agent \\(\\), two possible outcomes: \\(y_i=0\\) \\(y_i=1\\)). Part utility observed econometrician —depends covariates \\(\\mathbf{x}_i\\)— part latent.probit model, :\n\\[\n\\mathbb{P}(y_i=1|\\mathbf{x}_i;\\boldsymbol\\theta) = \\Phi(\\boldsymbol\\theta'\\mathbf{x}_i) = \\mathbb{P}(-\\varepsilon_{}<\\boldsymbol\\theta'\\mathbf{x}_i),\n\\]\n\\(\\varepsilon_{} \\sim \\mathcal{N}(0,1)\\). :\n\\[\n\\mathbb{P}(y_i=1|\\mathbf{x}_i;\\boldsymbol\\theta) = \\mathbb{P}(0< y_i^*),\n\\]\n\\(y_i^* = \\boldsymbol\\theta'\\mathbf{x}_i + \\varepsilon_i\\), \\(\\varepsilon_{} \\sim \\mathcal{N}(0,1)\\). Variable \\(y_i^*\\) can interpreted (latent) variable determines \\(y_i\\) (since \\(y_i = \\mathbb{}_{\\{y_i^*>0\\}}\\)).Figure 3.4 illustrates situation.\nFigure 3.4: Distribution \\(y_i^*\\) conditional \\(\\mathbf{x}_i\\).\nAssume agent (\\(\\)) chooses \\(y_i=1\\) utility associated choice (\\(U_{,1}\\)) higher one associated \\(y_i=0\\) (\\(U_{,0}\\)). Assume utility agent \\(\\), chooses outcome \\(j\\) (\\(\\\\{0,1\\}\\)), given \n\\[\nU_{,j} = V_{,j} + \\varepsilon_{,j},\n\\]\n\\(V_{,j}\\) deterministic component utility associated choice \\(\\varepsilon_{,j}\\) random (agent-specific) component. Moreover, posit \\(V_{,j} = \\boldsymbol\\theta_j'\\mathbf{x}_i\\). :\n\\[\\begin{eqnarray}\n\\mathbb{P}(y_i = 1|\\mathbf{x}_i;\\boldsymbol\\theta) &=& \\mathbb{P}(\\boldsymbol\\theta_1'\\mathbf{x}_i+\\varepsilon_{,1}>\\boldsymbol\\theta_0'\\mathbf{x}_i+\\varepsilon_{,0}) \\nonumber\\\\\n&=& F(\\boldsymbol\\theta_1'\\mathbf{x}_i-\\boldsymbol\\theta_0'\\mathbf{x}_i) = F([\\boldsymbol\\theta_1-\\boldsymbol\\theta_0]'\\mathbf{x}_i),\\tag{3.4}\n\\end{eqnarray}\\]\n\\(F\\) c.d.f. \\(\\varepsilon_{,0}-\\varepsilon_{,1}\\).Note difference \\(\\boldsymbol\\theta_1-\\boldsymbol\\theta_0\\) identifiable (opposed \\(\\boldsymbol\\theta_1\\) \\(\\boldsymbol\\theta_0\\)). Indeed, replacing \\(U\\) \\(aU\\) (\\(>0\\)) gives model. scaling issue can solved fixing variance \\(\\varepsilon_{,0}-\\varepsilon_{,1}\\).Example 3.1  (Migration income) RUM approach used Nakosteen Zimmer (1980) study migration choices. model based comparison marginal costs benefits associated migration. main ingredients approach follows:Wage can earned present location: \\(y_p^* = \\boldsymbol\\theta_p'\\mathbf{x}_p + \\varepsilon_p\\).Migration cost: \\(C^*= \\boldsymbol\\theta_c'\\mathbf{x}_c + \\varepsilon_c\\).Wage earned elsewhere: \\(y_m^* = \\boldsymbol\\theta_m'\\mathbf{x}_m + \\varepsilon_m\\).context, agents decision migrate \\(y_m^* > y_p^* + C^*\\), .e. \n\\[\ny^* = y_m^* -  y_p^* - C^* =  \\boldsymbol\\theta'\\mathbf{x} + \\underbrace{\\varepsilon}_{=\\varepsilon_m - \\varepsilon_c - \\varepsilon_p}>0,\n\\]\n\\(\\mathbf{x}\\) union \\(\\mathbf{x}_i\\)s, \\(\\\\{p,m,c\\}\\).","code":""},{"path":"microeconometrics.html","id":"Avregressors","chapter":"3 Microeconometrics","heading":"3.1.2 Alternative-Varying Regressors","text":"cases, regressors may depend considered alternative (\\(0\\) \\(1\\)). instance:modeling decision participate labour force (), wage depends alternative. Typically, zero considered agent decided work (strictly positive otherwise).context choice transportation mode, “time cost” depends considered transportation mode.terms utility, :\n\\[\nV_{,j} = {\\theta^{(u)}_{j}}'\\mathbf{u}_{,j} + {\\theta^{(v)}_{j}}'\\mathbf{v}_{},\n\\]\n\\(\\mathbf{u}_{,j}\\)’s regressors associated agent \\(\\), taking different values different choices (\\(j=0\\) \\(j=1\\)). case, Eq. (3.4) becomes:\n\\[\\begin{equation}\n\\mathbb{P}(y_i = 1|\\mathbf{x}_i;\\boldsymbol\\theta)  = F\\left({\\theta^{(u)}_{1}}'\\mathbf{u}_{,1}-{\\theta^{(u)}_{0}}'\\mathbf{u}_{,0}+[\\boldsymbol\\theta_1^{(v)}-\\boldsymbol\\theta_0^{(v)}]'\\mathbf{v}_i\\right),\\tag{3.5}\n\\end{equation}\\]\n, \\(\\theta^{(u)}_{1}=\\theta^{(u)}_{0}=\\theta^{(u)}\\) —customary— get:\n\\[\\begin{equation}\n\\mathbb{P}(y_i = 1|\\mathbf{x}_i;\\boldsymbol\\theta)  = F\\left({\\theta^{(u)}_{1}}'(\\mathbf{u}_{,1}-\\mathbf{u}_{,0})+[\\boldsymbol\\theta_1^{(v)}-\\boldsymbol\\theta_0^{(v)}]'\\mathbf{v}_i\\right).\\tag{3.6}\n\\end{equation}\\]Example 3.2  (Fishing-mode dataset) fishing-mode dataset used Cameron Trivedi (2005) (Chapters 14 15) contains alternative-specific variables. Specifically, individual, price catch rate depend fishing model. table reported , lines price catch correspond prices catch rates associated chosen alternative.","code":"\nlibrary(mlogit)\ndata(\"Fishing\",package=\"mlogit\")\nstargazer::stargazer(Fishing,type=\"text\")## \n## ==========================================================\n## Statistic       N     Mean    St. Dev.    Min      Max    \n## ----------------------------------------------------------\n## price.beach   1,182  103.422   103.641   1.290   843.186  \n## price.pier    1,182  103.422   103.641   1.290   843.186  \n## price.boat    1,182  55.257    62.713    2.290   666.110  \n## price.charter 1,182  84.379    63.545   27.290   691.110  \n## catch.beach   1,182   0.241     0.191    0.068    0.533   \n## catch.pier    1,182   0.162     0.160    0.001    0.452   \n## catch.boat    1,182   0.171     0.210   0.0002    0.737   \n## catch.charter 1,182   0.629     0.706    0.002    2.310   \n## income        1,182 4,099.337 2,461.964 416.667 12,500.000\n## ----------------------------------------------------------"},{"path":"microeconometrics.html","id":"estimation","chapter":"3 Microeconometrics","heading":"3.1.3 Estimation","text":"models can estimated Maximum Likelihood approaches (see Section 2.2).simplify exposition, consider \\(\\mathbf{x}_i\\) vectors covariates deterministic. Moreover, assume r.v. independent across entities \\(\\). write likelihood case? easily checked :\n\\[\nf(y_i|\\mathbf{x}_i;\\boldsymbol\\theta) =   g(\\boldsymbol\\theta'\\mathbf{x}_i)^{y_i}(1-g(\\boldsymbol\\theta'\\mathbf{x}_i))^{1-y_i}.\n\\]Therefore, observations \\((\\mathbf{x}_i,y_i)\\) independent across entities \\(\\), obtain:\n\\[\n\\log \\mathcal{L}(\\boldsymbol\\theta;\\mathbf{y},\\mathbf{X}) = \\sum_{=1}^{n}y_i \\log[g(\\boldsymbol\\theta'\\mathbf{x}_i)] + (1-y_i)\\log[1-g(\\boldsymbol\\theta'\\mathbf{x}_i)].\n\\]likelihood equation reads (FOC optimization program, see Def. 2.7):\n\\[\n\\dfrac{\\partial \\log \\mathcal{L}(\\boldsymbol\\theta;\\mathbf{y},\\mathbf{X})}{\\partial \\boldsymbol\\theta} = \\mathbf{0},\n\\]\n:\n\\[\n\\sum_{=1}^{n} y_i \\mathbf{x}_i\\frac{g'(\\boldsymbol\\theta'\\mathbf{x}_i)}{g(\\boldsymbol\\theta'\\mathbf{x}_i)} - (1-y_i) \\mathbf{x}_i \\frac{g'(\\boldsymbol\\theta'\\mathbf{x}_i)}{1-g(\\boldsymbol\\theta'\\mathbf{x}_i)} = \\mathbf{0}.\n\\]nonlinear (multivariate) equation can solved numerically. regularity conditions (Hypotheses 2.1), approximately (Prop. 2.4):\n\\[\n\\boldsymbol\\theta_{MLE} \\sim \\mathcal{N}(\\boldsymbol\\theta_0,\\mathbf{}(\\boldsymbol\\theta_0)^{-1}),\n\\]\n\n\\[\n\\mathbf{}(\\boldsymbol\\theta_0) = - \\mathbb{E}_0 \\left( \\frac{\\partial^2 \\log \\mathcal{L}(\\theta;\\mathbf{y},\\mathbf{X})}{\\partial \\boldsymbol\\theta \\partial \\boldsymbol\\theta'}\\right) = n \\mathcal{}_Y(\\boldsymbol\\theta_0).\n\\]finite samples, can e.g. approximate \\(\\mathbf{}(\\boldsymbol\\theta_0)^{-1}\\) Eq. (2.10):\n\\[\n\\mathbf{}(\\boldsymbol\\theta_0)^{-1} \\approx -\\left(\\frac{\\partial^2 \\log \\mathcal{L}(\\boldsymbol\\theta_{MLE};\\mathbf{y},\\mathbf{X})}{\\partial \\boldsymbol\\theta \\partial \\boldsymbol\\theta'}\\right)^{-1}.\n\\]Probit case (see Table 3.1), can shown :\n\\[\\begin{eqnarray*}\n&&\\frac{\\partial^2 \\log \\mathcal{L}(\\boldsymbol\\theta;\\mathbf{y},\\mathbf{X})}{\\partial \\boldsymbol\\theta \\partial \\boldsymbol\\theta'} = - \\sum_{=1}^{n} g'(\\boldsymbol\\theta'\\mathbf{x}_i) [\\mathbf{x}_i \\mathbf{x}_i'] \\times \\\\\n&&\\left[y_i \\frac{g'(\\boldsymbol\\theta'\\mathbf{x}_i) + \\boldsymbol\\theta'\\mathbf{x}_ig(\\boldsymbol\\theta'\\mathbf{x}_i)}{g(\\boldsymbol\\theta'\\mathbf{x}_i)^2} + (1-y_i) \\frac{g'(\\boldsymbol\\theta'\\mathbf{x}_i) - \\boldsymbol\\theta'\\mathbf{x}_i (1 - g(\\boldsymbol\\theta'\\mathbf{x}_i))}{(1-g(\\boldsymbol\\theta'\\mathbf{x}_i))^2}\\right].\n\\end{eqnarray*}\\]Logit case (see Table 3.1), can shown :\n\\[\n\\frac{\\partial^2 \\log \\mathcal{L}(\\boldsymbol\\theta;\\mathbf{y},\\mathbf{X})}{\\partial \\boldsymbol\\theta \\partial \\boldsymbol\\theta'} = - \\sum_{=1}^{n} g'(\\boldsymbol\\theta'\\mathbf{x}_i) \\mathbf{x}_i\\mathbf{x}_i',\n\\]\n\\(g'(x)=\\dfrac{\\exp(-x)}{(1 + \\exp(-x))^2}\\).Remark , since \\(g'(x)>0\\), \\(-\\partial^2 \\log \\mathcal{L}(\\boldsymbol\\theta;\\mathbf{y},\\mathbf{X})/\\partial \\boldsymbol\\theta \\partial \\boldsymbol\\theta'\\) positive definite.","code":""},{"path":"microeconometrics.html","id":"marginalFX","chapter":"3 Microeconometrics","heading":"3.1.4 Marginal effects","text":"measure marginal effects, .e. effect probability \\(y_i=1\\) marginal increase \\(x_{,k}\\)? object given :\n\\[\n\\frac{\\partial \\mathbb{P}(y_i=1|\\mathbf{x}_i;\\boldsymbol\\theta)}{\\partial x_{,k}} = \\underbrace{g'(\\boldsymbol\\theta'\\mathbf{x}_i)}_{>0}\\theta_k,\n\\]\nsign \\(\\theta_k\\) function \\(g\\) monotonously increasing.agent \\(\\), marginal effect consistently estimated \\(g'(\\boldsymbol\\theta_{MLE}'\\mathbf{x}_i)\\theta_{MLE,k}\\). important see marginal effect depends \\(\\mathbf{x}_i\\): respective increases 1 unit \\(x_{,k}\\) (entity \\(\\)) \\(x_{j,k}\\) (entity \\(j\\)) necessarily effect \\(\\mathbb{P}(y_i=1|\\mathbf{x}_i;\\boldsymbol\\theta)\\) \\(\\mathbb{P}(y_j=1|\\mathbf{x}_j;\\boldsymbol\\theta)\\). address issue, one can compute measures “average” marginal effect. two main solutions. explanatory variable \\(k\\):Denoting \\(\\hat{\\mathbf{x}}\\) sample average \\(\\mathbf{x}_i\\)s, compute \\(g'(\\boldsymbol\\theta_{MLE}'\\hat{\\mathbf{x}})\\theta_{MLE,k}\\).Compute average (across \\(\\)) \\(g'(\\boldsymbol\\theta_{MLE}'\\mathbf{x}_i)\\theta_{MLE,k}\\).","code":""},{"path":"microeconometrics.html","id":"goodness-of-fit","chapter":"3 Microeconometrics","heading":"3.1.5 Goodness of fit","text":"obvious version “\\(R^2\\)” binary-choice models. Existing measures called pseudo-\\(R^2\\) measures.Denoting \\(\\log \\mathcal{L}_0(\\mathbf{y})\\) (maximum) log-likelihood obtained model containing constant term (.e. \\(\\mathbf{x}_i = 1\\) \\(\\)), McFadden’s pseudo-\\(R^2\\) given :\n\\[\nR^2_{MF} = 1 - \\frac{\\log \\mathcal{L}(\\boldsymbol\\theta;\\mathbf{y})}{\\log \\mathcal{L}_0(\\mathbf{y})}.\n\\]\nIntuitively, \\(R^2_{MF}=0\\) explanatory variables convey information outcome \\(y\\). Indeed, case, model better reference model, simply captures fraction \\(y_i\\)’s equal 1.Example 3.3  (Credit defaults (Lending-club dataset)) example makes use credit data package AEC. objective model default probabilities borrowers.Let us first represent relationship fraction households defaulted loan annual income:previous figure suggests effect annual income probability default non-monotonous. therefore include quadratic term one specification (namely eq1 ).consider three specifications. first one (eq0), explanatory variables, trivial. just used compute pseudo-\\(R^2\\). second (eq1), consider covariates (loan amount, ratio amount annual income, number --30 days past-due incidences delinquency borrower’s credit file past 2 years, quadratic function annual income). third model (eq2), add credit rating.Let us compute pseudo R2 last two models:Let us now compute (average) marginal effects, using method ii Section 3.1.4:issue annual_inc variable. Indeed, previous computation realize variable appears twice among explanatory variables (log(annual_inc) (log(annual_inc)^2)). address , one can proceed follows: (1) construct new counterfactual dataset annual incomes increased 1%, (2) use model compute model-implied probabilities default new dataset (3), subtract probabilities resulting original dataset counterfactual probabilities:negative sign means , average across entities considered analysis, 1% increase annual income results decrease default probability. average effect however pretty low. get economic sense size effect, let us compute average effect associated unit increase number delinquencies:can employ likelihood ratio test (see Def. 2.8) see two variables associated annual income jointly statistically significant (context eq1):computation gives p-value 0.0436.Example 3.4  (Replicating Table 14.2 Cameron Trivedi (2005)) following lines codes replicate Table 14.2 Cameron Trivedi (2005) (see Example 3.2).","code":"\nlibrary(AEC)\ncredit$Default <- 0\ncredit$Default[credit$loan_status == \"Charged Off\"] <- 1\ncredit$Default[credit$loan_status ==\n                 \"Does not meet the credit policy. Status:Charged Off\"] <- 1\ncredit$amt2income <- credit$loan_amnt/credit$annual_inc\nplot(as.factor(credit$Default)~log(credit$annual_inc),\n     ylevels=2:1,ylab=\"Default status\",xlab=\"log(annual income)\")\neq0 <- glm(Default ~ 1,data=credit,family=binomial(link=\"probit\"))\neq1 <- glm(Default ~ log(loan_amnt) + amt2income + delinq_2yrs + \n             log(annual_inc)+ I(log(annual_inc)^2),\n           data=credit,family=binomial(link=\"probit\"))\neq2 <- glm(Default ~ grade + log(loan_amnt) + amt2income + delinq_2yrs + \n             log(annual_inc)+ I(log(annual_inc)^2),\n           data=credit,family=binomial(link=\"probit\"))\nstargazer::stargazer(eq0,eq1,eq2,type=\"text\",no.space = TRUE)## \n## ====================================================\n##                           Dependent variable:       \n##                     --------------------------------\n##                                 Default             \n##                        (1)        (2)        (3)    \n## ----------------------------------------------------\n## gradeB                                     0.400*** \n##                                            (0.055)  \n## gradeC                                     0.587*** \n##                                            (0.057)  \n## gradeD                                     0.820*** \n##                                            (0.061)  \n## gradeE                                     0.874*** \n##                                            (0.091)  \n## gradeF                                     1.230*** \n##                                            (0.147)  \n## gradeG                                     1.439*** \n##                                            (0.227)  \n## log(loan_amnt)                  -0.149**  -0.194*** \n##                                 (0.060)    (0.061)  \n## amt2income                      1.266***   1.222*** \n##                                 (0.383)    (0.393)  \n## delinq_2yrs                     0.096***    0.009   \n##                                 (0.034)    (0.035)  \n## log(annual_inc)                 -1.444**    -0.874  \n##                                 (0.569)    (0.586)  \n## I(log(annual_inc)2)             0.064**     0.038   \n##                                 (0.025)    (0.026)  \n## Constant            -1.231***   7.937***    4.749   \n##                      (0.017)    (3.060)    (3.154)  \n## ----------------------------------------------------\n## Observations          9,156      9,156      9,156   \n## Log Likelihood      -3,157.696 -3,120.625 -2,981.343\n## Akaike Inf. Crit.   6,317.392  6,253.250  5,986.686 \n## ====================================================\n## Note:                    *p<0.1; **p<0.05; ***p<0.01\nlogL0 <- logLik(eq0);logL1 <- logLik(eq1);logL2 <- logLik(eq2)\npseudoR2_eq1 <- 1 - logL1/logL0 # pseudo R2\npseudoR2_eq2 <- 1 - logL2/logL0 # pseudo R2\nc(pseudoR2_eq1,pseudoR2_eq2)## [1] 0.01173993 0.05584870\nmean(dnorm(predict(eq2)),na.rm=TRUE)*eq2$coefficients##          (Intercept)               gradeB               gradeC \n##          0.840731198          0.070747353          0.103944305 \n##               gradeD               gradeE               gradeF \n##          0.145089219          0.154773742          0.217702041 \n##               gradeG       log(loan_amnt)           amt2income \n##          0.254722161         -0.034289921          0.216251992 \n##          delinq_2yrs      log(annual_inc) I(log(annual_inc)^2) \n##          0.001574178         -0.154701321          0.006813694\nnew_credit <- credit\nnew_credit$annual_inc <- 1.01 * new_credit$annual_inc\nbas_predict_eq2  <- predict(eq2, newdata = credit, type = \"response\")\n# This is equivalent to pnorm(predict(eq2, newdata = credit))\nnew_predict_eq2  <- predict(eq2, newdata = new_credit, type = \"response\")\nmean(new_predict_eq2 - bas_predict_eq2)## [1] -6.562126e-05\nnew_credit <- credit\nnew_credit$delinq_2yrs <- credit$delinq_2yrs + 1\nnew_predict_eq2  <- predict(eq2, newdata = new_credit, type = \"response\")\nmean(new_predict_eq2 - bas_predict_eq2)## [1] 0.001582332\neq1restr <- glm(Default ~ log(loan_amnt) + amt2income + delinq_2yrs,\n                data=credit,family=binomial(link=\"probit\"))\nLRstat <- 2*(logL1 - logLik(eq1restr))\npvalue <- 1 - c(pchisq(LRstat,df=2))\ndata.reduced <- subset(Fishing,mode %in% c(\"charter\",\"pier\"))\ndata.reduced$lnrelp <- log(data.reduced$price.charter/data.reduced$price.pier)\ndata.reduced$y <- 1*(data.reduced$mode==\"charter\")\n# check first line of Table 14.1:\nprice.charter.y0 <- mean(data.reduced$pcharter[data.reduced$y==0])\nprice.charter.y1 <- mean(data.reduced$pcharter[data.reduced$y==1])\nprice.charter    <- mean(data.reduced$pcharter)\n# Run probit regression:\nreg.probit <- glm(y ~ lnrelp,\n                  data=data.reduced,\n                  family=binomial(link=\"probit\"))\n# Run Logit regression:\nreg.logit <- glm(y ~ lnrelp,\n                 data=data.reduced,\n                 family=binomial(link=\"logit\"))\n# Run OLS regression:\nreg.OLS <- lm(y ~ lnrelp,\n              data=data.reduced)\n# Replicates Table 14.2 of Cameron and Trivedi:\nstargazer::stargazer(reg.logit, reg.probit, reg.OLS,no.space = TRUE,\n                     type=\"text\")## \n## ================================================================\n##                                 Dependent variable:             \n##                     --------------------------------------------\n##                                          y                      \n##                     logistic   probit             OLS           \n##                        (1)       (2)              (3)           \n## ----------------------------------------------------------------\n## lnrelp              -1.823*** -1.056***        -0.243***        \n##                      (0.145)   (0.075)          (0.010)         \n## Constant            2.053***  1.194***          0.784***        \n##                      (0.169)   (0.088)          (0.013)         \n## ----------------------------------------------------------------\n## Observations           630       630              630           \n## R2                                               0.463          \n## Adjusted R2                                      0.462          \n## Log Likelihood      -206.827  -204.411                          \n## Akaike Inf. Crit.    417.654   412.822                          \n## Residual Std. Error                         0.330 (df = 628)    \n## F Statistic                             542.123*** (df = 1; 628)\n## ================================================================\n## Note:                                *p<0.1; **p<0.05; ***p<0.01"},{"path":"microeconometrics.html","id":"predictions-and-roc-curves","chapter":"3 Microeconometrics","heading":"3.1.6 Predictions and ROC curves","text":"compute model-implied predicted outcomes? case \\(y_i\\), predicted outcomes \\(\\hat{y}_i\\) need valued \\(\\{0,1\\}\\). natural choice consists considering \\(\\hat{y}_i=1\\) \\(\\mathbb{P}(y_i=1|\\mathbf{x}_i;\\boldsymbol\\theta) > 0.5\\), .e., taking cutoff \\(c=0.5\\). exist, though, situations relevant. instance, may models predicted probabilities small, less others. context, model-implied probability 10% (say) characterize “high-risk” entity. However, using cutoff 50% identify level riskiness.receiver operating characteristics (ROC) curve consitutes general approach. idea remain agnostic consider possible values cutoff \\(c\\). works follows. potential cutoff \\(c \\[0,1]\\), compute (plot):fraction \\(y = 1\\) values correctly classified (True Positive Rate) againstThe fraction \\(y = 0\\) values incorrectly specified (False Positive Rate).curve mechanically starts (0,0) —corresponds \\(c=1\\)— terminates (1,1) –situation \\(c=0\\).case predictive ability (worst situation), ROC curve straight line (0,0) (1,1).Example 3.5  (ROC fishing-mode dataset) Figure 3.5 shows ROC curve associated probit model estimated Example 3.4.\nFigure 3.5: Application ROC methodology fishing-mode dataset.\n","code":"\nlibrary(pROC)\npredict_model <- predict.glm(reg.probit,type = \"response\")\nroc(data.reduced$y, predict_model, percent=T,\n    boot.n=1000, ci.alpha=0.9, stratified=T, plot=TRUE, grid=TRUE,\n    show.thres=TRUE, legacy.axes = TRUE, reuse.auc = TRUE,\n    print.auc = TRUE, print.thres.col = \"blue\", ci=TRUE,\n    ci.type=\"bars\", print.thres.cex = 0.7, col = 'red',\n    main = paste(\"ROC curve using\",\"(N = \",nrow(data.reduced),\")\") )"},{"path":"microeconometrics.html","id":"multiple-choice-models","chapter":"3 Microeconometrics","heading":"3.2 Multiple Choice Models","text":"now consider cases number possible outcomes (alternatives) larger two. Let us denote \\(J\\) number. \\(y_j \\\\{1,\\dots,J\\}\\). situation arise instance outcome variable reflects:Opinions: strongly opposed / opposed / neutral / support (ranked choices),Occupational field: lawyer / farmer / engineer / doctor / …,Alternative shopping areas,Transportation types.cases, values associated choices meaningful, example, number accidents per day: \\(y = 0, 1,2, \\dots\\) (count data). cases, values meaningless.assume existence covariates, gathered vector \\(\\mathbf{x}_i\\) (\\(K \\times 1\\)), suspected influence probabilities obtaining different outcomes (\\(y_i=j\\), \\(j \\\\{1,\\dots,J\\}\\)).follows, assume \\(y_i\\)’s assumed independently distributed, :\n\\[\\begin{equation}\ny_i = \\left\\{\n\\begin{array}{cl}\n1 & \\mbox{ probability } g_1(\\mathbf{x}_i;\\boldsymbol\\theta)\\\\\n\\vdots \\\\\nJ & \\mbox{ probability } g_J(\\mathbf{x}_i;\\boldsymbol\\theta).\n\\end{array}\n\\right.\\tag{3.7}\n\\end{equation}\\](course, entities (\\(\\)), must \\(\\sum_{j=1}^J g_j(\\mathbf{x}_i;\\boldsymbol\\theta)=1\\).) objective estimate vector population parameters \\(\\boldsymbol\\theta\\) given functional forms \\(g_j\\)’s.","code":""},{"path":"microeconometrics.html","id":"ordered-case","chapter":"3 Microeconometrics","heading":"3.2.1 Ordered case","text":"Sometimes, exists natural order different alternatives. typically case respondents choose level agreement statement, e.g.: (1) Strongly disagree; (2) Disagree; (3) Neither agree disagree; (4) Agree; (5) Strongly agree. Another standard case ratings (F, say).ordered probit model consists extending binary case, considering latent-variable view latter (see Section 3.1.1). Formally, model follows:\n\\[\\begin{equation}\n\\mathbb{P}(y_i = j | \\mathbf{x}_i) = \\mathbb{P}(\\alpha_{j-1} <y^*_i < \\alpha_{j} |\\mathbf{x}_i), \\tag{3.8}\n\\end{equation}\\]\n\n\\[\ny_{}^* = \\boldsymbol\\theta'\\mathbf{x}_i + \\varepsilon_i,\n\\]\n\\(\\varepsilon_i \\sim \\,..d.\\,\\mathcal{N}(0,1)\\). \\(\\alpha_j\\)’s, \\(j \\\\{1,\\dots,J-1\\}\\), (new) parameters estimated, top \\(\\boldsymbol\\theta\\). Naturally, \\(\\alpha_1<\\alpha_2<\\dots<\\alpha_{J-1}\\). Moreover \\(\\alpha_0\\) \\(- \\infty\\) \\(\\alpha_J\\) \\(+ \\infty\\), Eq. (3.8) valid \\(j \\\\{1,\\dots,J\\}\\) (including \\(1\\) \\(J\\)).:\n\\[\\begin{eqnarray*}\ng_j(\\mathbf{x}_i;\\boldsymbol\\theta,\\boldsymbol\\alpha) = \\mathbb{P}(y_i = j | \\mathbf{x}_i) &=& \\mathbb{P}(\\alpha_{j-1} <y^*_i < \\alpha_{j} |\\mathbf{x}_i) \\\\\n&=& \\mathbb{P}(\\alpha_{j-1} - \\boldsymbol\\theta'\\mathbf{x}_i  <\\varepsilon_i < \\alpha_{j} - \\boldsymbol\\theta'\\mathbf{x}_i) \\\\\n&=& \\Phi(\\alpha_{j} - \\boldsymbol\\theta'\\mathbf{x}_i) - \\Phi(\\alpha_{j-1} - \\boldsymbol\\theta'\\mathbf{x}_i),\n\\end{eqnarray*}\\]\n\\(\\Phi\\) c.d.f. \\(\\mathcal{N}(0,1)\\)., \\(\\), one components \\(\\mathbf{x}_i\\) equal 1 (done linear regression introduce intercept specification), one \\(\\alpha_j\\) (\\(j\\\\{1,\\dots,J-1\\}\\)) identified. One can arbitrarily set \\(\\alpha_1=0\\). done binary logit/probit cases.model can estimated maximizing likelihood function (see Section 2.2). function given :\n\\[\\begin{equation}\n\\log \\mathcal{L}(\\boldsymbol\\theta,\\boldsymbol\\alpha;\\mathbf{y},\\mathbf{X}) = \\sum_{=1}^n  \\sum_{j=1}^J \\mathbb{}_{\\{y_i=j\\}} \\log \\left(g_j(\\mathbf{x}_i;\\boldsymbol\\theta,\\boldsymbol\\alpha)\\right). \\tag{3.9}\n\\end{equation}\\]Let us stress two types parameters estimate: included vector \\(\\boldsymbol\\theta\\), \\(\\alpha_j\\)’s, gathered vector \\(\\boldsymbol\\alpha\\).estimated values \\(\\theta_j\\)’s slightly complicated interpret (least term sign) binary case. Indeed, :\n\\[\n\\mathbb{P}(y_i \\le j | \\mathbf{x}_i) = \\Phi(\\alpha_{j} - \\boldsymbol\\theta'\\mathbf{x}_i) \\Rightarrow \\frac{\\partial \\mathbb{P}(y_i \\le j | \\mathbf{x}_i)}{\\mathbf{x}_i} =- \\underbrace{\\Phi'(\\alpha_{j} - \\boldsymbol\\theta'\\mathbf{x}_i)}_{>0}\\boldsymbol\\theta.\n\\]\nHence sign \\(\\theta_k\\) indicates whether \\(\\mathbb{P}(y_i \\le j | \\mathbf{x}_i)\\) increases decreases w.r.t. \\(x_{,k}\\) (\\(k^{th}\\) component \\(\\mathbf{x}_i\\)). contrast:\n\\[\n\\frac{\\partial \\mathbb{P}(y_i = j | \\mathbf{x}_i)}{\\mathbf{x}_i} = \\underbrace{\\left(-F'(\\alpha_{j} + \\boldsymbol\\theta'\\mathbf{x}_i)+F'(\\alpha_{j-1} + \\boldsymbol\\theta'\\mathbf{x}_i)\\right)}_{}\\boldsymbol\\theta.\n\\]\nTherefore signs components \\(\\boldsymbol\\theta\\) necessarily marginal effects. (sign \\(\\) priori unknown.)Example 3.6  (Predicting credit ratings (Lending-club dataset)) Let us use credit dataset (see Example 3.3), let use try model ratings attributed lending-club:Predicted ratings (probabilties given given rating) can computed follows:","code":"\nlibrary(AEC)\nlibrary(MASS)\ncredit$emp_length_low5y   <- credit$emp_length %in%\n  c(\"< 1 year\",\"1 year\",\"2 years\",\"3 years\",\"4 years\")\ncredit$emp_length_high10y <- credit$emp_length==\"10+ years\"\ncredit$annual_inc <- credit$annual_inc/1000\ncredit$loan_amnt  <- credit$loan_amnt/1000\ncredit$income2loan <- credit$annual_inc/credit$loan_amnt\ntraining <- credit[1:20000,] # sample is reduced\ntraining <- subset(training,grade!=c(\"E\",\"F\",\"G\"))\ntraining <- droplevels(training)\ntraining$grade.ordered <- factor(training$grade,ordered=TRUE,\n                                 levels = c(\"D\",\"C\",\"B\",\"A\"))\nmodel1 <- polr(grade.ordered ~ log(loan_amnt) + log(income2loan) + delinq_2yrs,\n               data=training, Hess=TRUE, method=\"probit\")\nmodel2 <- polr(grade.ordered ~ log(loan_amnt) + log(income2loan) + delinq_2yrs +\n                 emp_length_low5y + emp_length_high10y,\n               data=training, Hess=TRUE, method=\"probit\")\nstargazer::stargazer(model1,model2,ord.intercepts = TRUE,type=\"text\",\n                     no.space = TRUE)## \n## ===============================================\n##                        Dependent variable:     \n##                    ----------------------------\n##                           grade.ordered        \n##                         (1)            (2)     \n## -----------------------------------------------\n## log(loan_amnt)         -0.014        -0.040*   \n##                       (0.022)        (0.022)   \n## log(income2loan)      0.115***      0.092***   \n##                       (0.022)        (0.022)   \n## delinq_2yrs          -0.399***      -0.404***  \n##                       (0.025)        (0.025)   \n## emp_length_low5y                    -0.096***  \n##                                      (0.027)   \n## emp_length_high10y                   0.088**   \n##                                      (0.035)   \n## D| C                 -0.937***      -1.073***  \n##                       (0.082)        (0.086)   \n## C| B                  -0.160**      -0.295***  \n##                       (0.082)        (0.085)   \n## B| A                  0.696***      0.564***   \n##                       (0.082)        (0.086)   \n## -----------------------------------------------\n## Observations           8,695          8,695    \n## ===============================================\n## Note:               *p<0.1; **p<0.05; ***p<0.01\npred.grade <- predict(model1,newdata = training)\n# pred.grade = predicted grade, defined as the most likely according model\npred.proba <- predict(model1,newdata = training, type=\"probs\")"},{"path":"microeconometrics.html","id":"MNL","chapter":"3 Microeconometrics","heading":"3.2.2 General multinomial logit model","text":"section introduces general multinomial logit model, natural extension binary logit model (see Table 3.1). general formulation follows:\n\\[\\begin{equation}\ng_j(\\mathbf{x}_i;\\boldsymbol\\theta) = \\frac{\\exp(\\theta_j'\\mathbf{x}_i)}{\\sum_{k=1}^J \\exp(\\theta_k'\\mathbf{x}_i)}.\\tag{3.10}\n\\end{equation}\\]Note , construction, \\(g_j(\\mathbf{x}_i;\\boldsymbol\\theta) \\[0,1]\\) \\(\\sum_{j}g_j(\\mathbf{x}_i;\\boldsymbol\\theta)=1\\).components \\(\\mathbf{x}_i\\) (regressors, covariates) may alternative-specific alternative invariant (see also Section 3.1.2). may, e.g., organize \\(\\mathbf{x}_i\\) follows:\n\\[\\begin{equation}\n\\mathbf{x}_i = [\\mathbf{u}_{,1}',\\dots,\\mathbf{u}_{,J}',\\mathbf{v}_{}']',\\tag{3.11}\n\\end{equation}\\]\nnotations Section 3.1.2, :\\(\\mathbf{u}_{,j}\\) (\\(j \\\\{1,\\dots,J\\}\\)): vector variables associated agent \\(\\) alternative \\(j\\) (alternative-specific regressors). Examples: Travel time per type transportation (transportation choice), wage per type work, cost per type car.\\(\\mathbf{v}_{}\\): vector variables associated agent \\(\\) alternative-invariant. Examples: age gender agent \\(\\),\\(\\mathbf{x}_i\\) Eq. (3.11), obvious notations, \\(\\theta_j\\) form:\n\\[\\begin{equation}\n\\theta_j = [{\\theta^{(u)}_{1,j}}',\\dots,{\\theta^{(u)}_{J,j}}',{\\theta_j^{(v)}}']',\\tag{3.12}\n\\end{equation}\\]\n\\(\\boldsymbol\\theta=[\\theta_1',\\dots,\\theta_J']'\\).literature considered different specific cases general multinomial logit model:9Conditional logit (CL) alternative-varying regressors:\n\\[\\begin{equation}\n\\theta_j = [\\mathbf{0}',\\dots,\\mathbf{0}',\\underbrace{\\boldsymbol\\beta'}_{\\mbox{j$^{th}$ position}},\\mathbf{0}',\\dots]',\\tag{3.13}\n\\end{equation}\\]\n.e., \\(\\boldsymbol\\beta=\\theta^{(u)}_{1,1}=\\dots=\\theta^{(u)}_{J,J}\\) \\(\\theta^{(u)}_{,j}=\\mathbf{0}\\) \\(\\ne j\\).Multinomial logit (MNL) alternative-invariant regressors:\n\\[\\begin{equation}\n\\theta_j = \\left[\\mathbf{0}',\\dots,\\mathbf{0}',{\\theta_j^{(v)}}'\\right]'.\\tag{3.14}\n\\end{equation}\\]Mixed logit:\n\\[\\begin{equation}\n\\theta_j = \\left[\\mathbf{0}',\\dots,\\mathbf{0}',\\boldsymbol\\beta',\\mathbf{0}',\\dots,\\mathbf{0}',{\\theta_j^{(v)}}'\\right]'.\\tag{3.13}\n\\end{equation}\\]Example 3.7  (CL MNL fishing-mode dataset) following lines replicate Table 15.2 Cameron Trivedi (2005) (see also Examples 3.2 3.4):ML estimationGeneral multinomial logit models can estimated Maximum Likelihood techniques (see Section 2.2). Consider general model described Eq. (3.7). can noted :\n\\[\nf(y_i|\\mathbf{x}_i;\\boldsymbol\\theta) = \\prod_{j=1}^J g_j(\\mathbf{x}_i;\\boldsymbol\\theta)^{\\mathbb{}_{\\{y_i=j\\}}},\n\\]\nleads \n\\[\n\\log f(y_i|\\mathbf{x}_i;\\boldsymbol\\theta) = \\sum_{j=1}^J \\mathbb{}_{\\{y_i=j\\}} \\log \\left(g_j(\\mathbf{x}_i;\\boldsymbol\\theta)\\right).\n\\]\nlog-likelihood function therefore given :\n\\[\\begin{equation}\n\\log \\mathcal{L}(\\boldsymbol\\theta;\\mathbf{y},\\mathbf{X}) = \\sum_{=1}^n  \\sum_{j=1}^J \\mathbb{}_{\\{y_i=j\\}} \\log \\left(g_j(\\mathbf{x}_i;\\boldsymbol\\theta)\\right).\\tag{3.9}\n\\end{equation}\\]\nNumerical methods employed order find maximum-likelihood estimate \\(\\boldsymbol\\theta\\). (Standard packages contain fast algorithms.)Marginal EffectsLet us consider computation marginal effects general multinomial logit model (Eq. (3.10)). Using notation \\(p_{,j} \\equiv \\mathbb{P}(y_i=j|\\mathbf{x}_i;\\boldsymbol\\theta)\\), :\n\\[\\begin{eqnarray*}\n\\frac{\\partial p_{,j}}{\\partial x_{,s}} &=& \\frac{\\theta_{j,s}\\exp(\\theta_j'\\mathbf{x}_i)\\sum_{k=1}^J \\exp(\\theta_k'\\mathbf{x}_i)}{(\\sum_{k=1}^J \\exp(\\theta_k'\\mathbf{x}_i))^2} \\\\\n&& - \\frac{\\exp(\\theta_j'\\mathbf{x}_i)\\sum_{k=1}^J \\theta_{k,s} \\exp(\\theta_k'\\mathbf{x}_i)}{(\\sum_{k=1}^J \\exp(\\theta_k'\\mathbf{x}_i))^2}\\\\\n&=& \\theta_{j,s} p_{,j} - \\sum_{k=1}^J \\theta_{k,s} p_{,j}p_{,k}\\\\\n&=&  p_{,j} \\times \\Big(\\theta_{j,s} - \\underbrace{\\sum_{k=1}^J \\theta_{k,s} p_{,k}}_{=\\overline{\\boldsymbol{\\theta}}^{()}_{s}}\\Big),\n\\end{eqnarray*}\\]\n\\(\\overline{\\boldsymbol\\theta}^{()}_{s}\\) depend \\(j\\). Note sign marginal effect necessarily \\(\\theta_{j,s}\\).Random Utility modelsThe general multinomial logit model may arise natural specification arising structural contexts agents compare (random) utilities associated \\(J\\) potential outcomes (see Section 3.1.1 binary situation).Let’s drop \\(\\) subscript simplicity assume utility derived form choosing \\(j\\) given \\(U_j = V_j + \\varepsilon_j\\), \\(V_j\\) deterministic (may depend observed covariates) \\(\\varepsilon_j\\) stochastic. (obvious notations):\n\\[\\begin{eqnarray*}\n\\mathbb{P}(y=j) &=& \\mathbb{P}(U_j>U_k,\\,\\forall k \\ne j)\\\\\n\\mathbb{P}(y=j) &=& \\mathbb{P}(U_k-U_j<0,\\,\\forall k \\ne j)\\\\\n\\mathbb{P}(y=j) &=& \\mathbb{P}(\\underbrace{\\varepsilon_k-\\varepsilon_j}_{=:\\tilde\\varepsilon_{k,j}}<\\underbrace{V_j - V_k}_{=:-\\tilde{V}_{k,j}},\\,\\forall k \\ne j).\n\\end{eqnarray*}\\]last expression \\((J-1)\\)-variate integral. , general, analytical solution, Prop. 3.1 shows case employing Gumbel distributions (see Def. 3.1).Definition 3.1  (Gumbel distribution) c.d.f. Gumbel distribution (\\(\\mathcal{W}\\)) :\n\\[\nF(u) = \\exp(-\\exp(-u)), \\qquad f(u)=\\exp(-u-\\exp(u)).\n\\]Remark: \\(X\\sim\\mathcal{W}\\), \\(\\mathbb{E}(X)=0.577\\) (Euler constant)10 \\(\\mathbb{V}ar(X)=\\pi^2/6\\).\nFigure 3.6: C.d.f. Gumbel distribution (\\(F(x)=\\exp(-\\exp(-x))\\)).\nProposition 3.1  (Weibull) context utility model described , \\(\\varepsilon_j \\sim \\,..d.\\,\\mathcal{W}\\), \n\\[\n\\mathbb{P}(y=j) = \\frac{\\exp(V_j)}{\\sum_{k=1}^J \\exp(V_k)}.\n\\]Proof. :\n\\[\\begin{eqnarray*}\n\\mathbb{P}(y=j) &=& \\mathbb{P}(\\forall\\,k \\ne j,\\,U_k < U_j) =  \\mathbb{P}(\\forall\\,k \\ne j,\\,\\varepsilon_k < V_j - V_k + \\varepsilon_j) \\\\\n&=& \\int \\prod_{k \\ne j} F(V_j - V_k + \\varepsilon) f(\\varepsilon)d\\varepsilon.\n\\end{eqnarray*}\\]\ncomputation, comes \n\\[\n\\prod_{k \\ne j} F(V_j - V_k + \\varepsilon) f(\\varepsilon) = \\exp\\left[-\\varepsilon-\\exp(-\\varepsilon+\\lambda_j)\\right],\n\\]\n\\(\\lambda_j = \\log\\left(1 + \\frac{\\sum_{k \\ne j} \\exp(V_k)}{\\exp(V_j)}\\right)\\). :\n\\[\\begin{eqnarray*}\n\\mathbb{P}(y=j) &=& \\int  \\exp\\left[-\\varepsilon-\\exp(-\\varepsilon+\\lambda_j)\\right] d\\varepsilon\\\\\n&=& \\int  \\exp\\left[-t - \\lambda_j-\\exp(-t)\\right] d\\varepsilon = \\exp(- \\lambda_j),\n\\end{eqnarray*}\\]\nleads result.remarks identification (see Def. 2.5) order.:\n\\[\\begin{eqnarray*}\n\\mathbb{P}(y=j) &=& \\frac{\\exp(V_j)}{\\sum_{k=1}^J \\exp(V_k)}= \\frac{\\exp(V^*_j)}{1 + \\sum_{k=2}^J \\exp(V^*_k)},\n\\end{eqnarray*}\\]\n\\(V^*_j = V_j - V_1\\). can therefore always assume \\(V_{1}=0\\). case \\(V_{,j} = \\theta_j'\\mathbf{x}_i = \\boldsymbol\\beta'\\mathbf{u}_{,j}+{\\theta_j^{(v)}}'\\mathbf{v}_i\\) (see Eqs. (3.11) (3.13)), can instance assume :\n\\[\\begin{eqnarray*}\n&()& \\mathbf{u}_{,1}=0,\\\\\n&(B)& \\theta_1^{(v)} = 0.\n\\end{eqnarray*}\\]\n() hold, can replace \\(\\mathbf{u}_{,j}\\) \\(\\mathbf{u}_{,j}-\\mathbf{u}_{,1}\\).\\(J=2\\) \\(j \\\\{0,1\\}\\) (shift one unit), \\(\\mathbb{P}(y=1|\\mathbf{x})=\\dfrac{\\exp(\\boldsymbol\\theta'\\mathbf{x})}{1+\\exp(\\boldsymbol\\theta'\\mathbf{x})}\\), logit model (Table 3.1).Limitations logit modelsIn Logit model, :\n\\[\\begin{equation}\n\\mathbb{P}(y=j|y \\\\{k,j\\}) = \\frac{\\exp(\\theta_j'\\mathbf{x})}{\\exp(\\theta_j'\\mathbf{x}) + \\exp(\\theta_k'\\mathbf{x})}.\\tag{3.15}\n\\end{equation}\\]\nconditional probability depend alternatives (.e., depend \\(\\theta_m\\), \\(m \\ne j,k\\)). particular, \\(\\mathbf{x} = [\\mathbf{u}_1',\\dots,\\mathbf{u}_J',\\mathbf{v}']'\\), changes \\(\\mathbf{u}_m\\) (\\(m \\ne j,\\,k\\)) impact object shown Eq. (3.15)., Multinomial Logit can seen series pairwise comparisons unaffected characteristics alternatives. model said satisfy independence irrelevant alternatives (IIA) property. , models, individual, ratio probabilities choosing two alternatives independent availability attributes alternatives. may sound alarming, situations like case, instance case want extrapolate results estimated model situation novel outcome highly susbstitutable one previous ones. can illustrated famous “red-blue bus” example:Example 3.8  (Red-blue bus IIA) Assume one logit model capturing decision travel using either car (\\(y=1\\)) (red) bus (\\(y=2\\)). Assume want augment model allow third choice (\\(y=3\\)): travel blue bus. blue bus (\\(y=3\\)) exactly red bus, except color, one expect :\n\\[\n\\mathbb{P}(y=3|y \\\\{2,3\\}) = 0.5,\n\\]\n.e. \\(\\theta_2 = \\theta_3\\).Assume \\(V_1=V_2\\). expect \\(V_2=V_3\\) (hence \\(p_2=p_3\\)). multinomial logit model imply \\(p_1=p_2=p_3=0.33\\). however seem reasonable \\(p_1 = p_2 + p_3 = 0.5\\) \\(p_2=p_3=0.25\\).","code":"\n# Specify data organization:\nlibrary(mlogit)\nlibrary(stargazer)\ndata(\"Fishing\",package=\"mlogit\")\nFish <- mlogit.data(Fishing,\n                    varying = c(2:9),\n                    choice = \"mode\",\n                    shape = \"wide\")\nMNL1 <- mlogit(mode ~ price + catch, data = Fish)\nMNL2 <- mlogit(mode ~ price + catch - 1, data = Fish)\nMNL3 <- mlogit(mode ~ 0 | income, data = Fish)\nMNL4 <- mlogit(mode ~ price + catch | income, data = Fish)\nstargazer(MNL1,MNL2,MNL3,MNL4,type=\"text\",no.space = TRUE,\n          omit.stat = c(\"lr\"))## \n## ===============================================================\n##                                 Dependent variable:            \n##                     -------------------------------------------\n##                                        mode                    \n##                        (1)        (2)        (3)        (4)    \n## ---------------------------------------------------------------\n## (Intercept):boat     0.871***              0.739***   0.527**  \n##                      (0.114)               (0.197)    (0.223)  \n## (Intercept):charter  1.499***              1.341***   1.694*** \n##                      (0.133)               (0.195)    (0.224)  \n## (Intercept):pier     0.307***              0.814***   0.778*** \n##                      (0.115)               (0.229)    (0.220)  \n## price               -0.025***  -0.020***             -0.025*** \n##                      (0.002)    (0.001)               (0.002)  \n## catch                0.377***   0.953***              0.358*** \n##                      (0.110)    (0.089)               (0.110)  \n## income:boat                                0.0001**   0.0001*  \n##                                           (0.00004)   (0.0001) \n## income:charter                             -0.00003   -0.00003 \n##                                           (0.00004)   (0.0001) \n## income:pier                               -0.0001*** -0.0001** \n##                                            (0.0001)   (0.0001) \n## ---------------------------------------------------------------\n## Observations          1,182      1,182      1,182      1,182   \n## R2                    0.178      0.014      0.189              \n## Log Likelihood      -1,230.784 -1,311.980 -1,477.151 -1,215.138\n## ===============================================================\n## Note:                               *p<0.1; **p<0.05; ***p<0.01"},{"path":"microeconometrics.html","id":"nested-logits","chapter":"3 Microeconometrics","heading":"3.2.3 Nested logits","text":"Nested Logits natural extensions logit models choices feature nesting structure. approach relevant makes sense group choices nest, also called limbs. Intuitively, framework consistent idea according , agent, exist unobserved nest-specific variables.setup follows: consider \\(J\\) limbs. limb \\(j\\), \\(K_j\\) branches. Let us denotes \\(y_1\\) limb choice (.e., \\(y_1 \\\\{1,\\dots,J\\}\\)) \\(y_2\\) branch choice (\\(y_2 \\\\{1,\\dots,K_j\\}\\)). utility associated pair choices \\((j,k)\\) given \n\\[\nU_{j,k} = V_{j,k} + \\varepsilon_{j,k}.\n\\]\n:\n\\[\n\\mathbb{P}[(y_1,y_2) = (j,k)|\\mathbf{x}] = \\mathbb{P}(U_{j,k}>U_{l,m},\\,(l,m) \\ne (j,k)|\\mathbf{x}).\n\\]One usually make following two assumptions:deterministic part utility given \\(V_{j,k} = \\mathbf{u}_j'\\boldsymbol\\alpha + \\mathbf{v}_{j,k}'\\boldsymbol\\beta_j\\), \\(\\boldsymbol\\alpha\\) common nests \\(\\boldsymbol\\beta_j\\)’s nest-specific.deterministic part utility given \\(V_{j,k} = \\mathbf{u}_j'\\boldsymbol\\alpha + \\mathbf{v}_{j,k}'\\boldsymbol\\beta_j\\), \\(\\boldsymbol\\alpha\\) common nests \\(\\boldsymbol\\beta_j\\)’s nest-specific.disturbances \\(\\boldsymbol\\varepsilon\\) follow Generalized Extreme Value (GEV) distribution (see Def. 4.15).disturbances \\(\\boldsymbol\\varepsilon\\) follow Generalized Extreme Value (GEV) distribution (see Def. 4.15).following figure displays simulations pairs \\((\\varepsilon_1,\\varepsilon_2)\\) drawn GEV distributions different values \\(\\rho\\). simulation approach based Bhat. code used produce chart provided Appendix 4.6.1.\nFigure 3.7: GEV simulations.\n() (ii), :\n\\[\\begin{eqnarray}\n\\mathbb{P}[(y_1,y_2) = (j,k)|\\mathbf{x}] &=& \\underbrace{\\frac{\\exp(\\mathbf{u}_j'\\boldsymbol\\alpha + \\rho_j I_j)}{\\sum_{m=1}^J \\exp(\\mathbf{u}_m'\\boldsymbol\\alpha + \\rho_m I_m)}}_{= \\mathbb{P}[y_1 = j|\\mathbf{x}]} \\times \\nonumber\\\\\n&& \\underbrace{\\frac{\\exp(\\mathbf{v}_{j,k}'\\boldsymbol\\beta_j/\\rho_j)}{\\sum_{l=1}^{K_j} \\exp(\\mathbf{v}_{j,l}'\\boldsymbol\\beta_j/\\rho_j)}}_{= \\mathbb{P}[y_2 = k|y_1=j,\\mathbf{x}]}, \\tag{3.16}\n\\end{eqnarray}\\]\n\\(I_j\\)’s called inclusive values (log sums), given :\n\\[\nI_j = \\log \\left( \\sum_{l=1}^{K_j} \\exp(\\mathbf{v}_{j,l}'\\boldsymbol\\beta_j/\\rho_j)\\right).\n\\]remarks order:can shown \\(\\rho_j = \\sqrt{1 - \\mathbb{C}(\\varepsilon_{j,k},\\varepsilon_{j,l})}\\), \\(k \\ne l\\).\\(\\rho_j=1\\) implies \\(\\varepsilon_{j,k}\\) \\(\\varepsilon_{j,l}\\) uncorrelated (back multinomial logit case).\\(J=1\\):\n\\[\nF([\\varepsilon_1,\\dots,\\varepsilon_K]',\\rho) = \\exp\\left(-\\left(\\sum_{k=1}^{K} \\exp(-\\varepsilon_k/\\rho)\\right)^{\\rho}\\right).\n\\]:\n\\[\\begin{eqnarray*}\nI_j = \\mathbb{E}(\\max_k(U_{j,k})) &=& \\mathbb{E}(\\max_k(V_{j,k} + \\varepsilon_{j,k})),\n\\end{eqnarray*}\\]\ninclusive values can therefore seen measures relative attractiveness nest.approach allows level correlation across \\(\\varepsilon_{j,k}\\) (given \\(j\\)). can interpreted existence (unobserved) common error component alternatives nest. component contributes making alternatives given nest similar. words, approach can accommodate higher sensitivity (cross-elasticity) alternatives given nest.Note common component reduced zero (.e. \\(\\rho_i=1\\)), model boils multinomial logit model covariance error terms among alternatives.Contrary general multinmial model, nested logits can solve Red-Blue problem described Section 3.2.2 (see Example 3.8). Assume estimated model specifying \\(U_{1} = V_{1} + \\varepsilon_{1}\\) (car choice) \\(U_{2} = V_{2} + \\varepsilon_{2}\\) (red bus choice). can assume blue-bus utility form \\(U_{3} = V_{2} + \\varepsilon_{3}\\) \\(\\varepsilon_{3}\\) perfectly correlated \\(\\varepsilon_{2}\\). done redefining set choices follows:\n\\[\\begin{eqnarray*}\nj=1 &\\Leftrightarrow& (j'=1,k=1) \\\\\nj=2 &\\Leftrightarrow& (j'=2,k=1) \\\\\nj=3 &\\Leftrightarrow& (j'=2,k=2),\n\\end{eqnarray*}\\]\nsetting \\(\\rho_2 \\rightarrow 0\\).IIA holds within nest, considering alternatives different nests. Indeed, using Eq. (3.16):\n\\[\n\\frac{\\mathbb{P}[y_1=j,y_2=k_A|\\mathbf{x}] }{\\mathbb{P}[y_1=j,y_2=k_B|\\mathbf{x}]} = \\frac{\\exp(\\mathbf{v}_{j,k_A}'\\boldsymbol\\beta_j/\\rho_j)}{\\exp(\\mathbf{v}_{j,k_B}'\\boldsymbol\\beta_j/\\rho_j)},\n\\]\n.e. IIA nest \\(j\\).contrast:\n\\[\\begin{eqnarray*}\n\\frac{\\mathbb{P}[y_1=j_A,y_2=k_A|\\mathbf{x}] }{\\mathbb{P}[y_1=j_B,y_2=k_B|\\mathbf{x}]} &=& \\frac{\\exp(\\mathbf{u}_{j_A}'\\boldsymbol\\alpha + \\rho_{j_A} I_{j_A})\\exp(\\mathbf{v}_{{j_A},{k_A}}'\\boldsymbol\\beta_{j_A}/\\rho_{j_A})}{\\exp(\\mathbf{u}_{j_B}'\\boldsymbol\\alpha + \\rho_{j_B} I_{j_B})\\exp(\\mathbf{v}_{{j_B},{k_B}}'\\boldsymbol\\beta_{j_B}/\\rho_{j_B})}\\times\\\\\n&& \\frac{\\sum_{l=1}^{K_{j_B}} \\exp(\\mathbf{v}_{{j_B},l}'\\boldsymbol\\beta_{j_B}/\\rho_{j_B})}{\\sum_{l=1}^{K_{j_A}} \\exp(\\mathbf{v}_{{j_A},l}'\\boldsymbol\\beta_{J_A}/\\rho_{j_A})},\n\\end{eqnarray*}\\]\ndepends expected utilities alternatives nest \\(j_A\\) \\(j_B\\). IIA hold.Example 3.9  (Travel-mode dataset) Let us illustrate nested logits travel-mode dataset used, e.g., Hensher Greene (2002) (see also Heiss (2002)).","code":"\nlibrary(mlogit)\nlibrary(stargazer)\ndata(\"TravelMode\", package = \"AER\")\nPrepared.TravelMode <- mlogit.data(TravelMode,chid.var = \"individual\",\n                                   alt.var = \"mode\",choice = \"choice\",\n                                   shape = \"long\")\n# Fit a multinomial model:\nhl <- mlogit(choice ~ wait + travel + vcost, Prepared.TravelMode,\n             method = \"bfgs\", heterosc = TRUE, tol = 10)\n## Fit a nested logit model:\nTravelMode$avincome <- with(TravelMode, income * (mode == \"air\"))\nTravelMode$time <- with(TravelMode, travel + wait)/60\nTravelMode$timeair <- with(TravelMode, time * I(mode == \"air\"))\nTravelMode$income <- with(TravelMode, income / 10)\n# Hensher and Greene (2002), table 1 p.8-9 model 5\nTravelMode$incomeother <- with(TravelMode,\n                               ifelse(mode %in% c('air', 'car'), income, 0))\nnl1 <- mlogit(choice ~ gcost + wait + incomeother, TravelMode,\n              shape='long', # Indicates how the dataset is organized\n              alt.var='mode', # variable that defines the alternative choices.\n              nests=list(public=c('train', 'bus'),\n                         car='car',air='air'), # defines the \"limbs\".\n              un.nest.el = TRUE)\nnl2 <- mlogit(choice ~ gcost + wait + time, TravelMode,\n              shape='long', # Inidcates how the dataset is organized\n              alt.var='mode', # variable that defines the alternative choices.\n              nests=list(public=c('train', 'bus'),\n                         car='car',air='air'), # defines the \"limbs\".\n              un.nest.el = TRUE)\nstargazer(nl1,nl2,type=\"text\",no.space = TRUE)## \n## ==============================================\n##                       Dependent variable:     \n##                   ----------------------------\n##                              choice           \n##                        (1)            (2)     \n## ----------------------------------------------\n## (Intercept):train     -0.211        -0.284    \n##                      (0.562)        (0.551)   \n## (Intercept):bus       -0.824        -0.712    \n##                      (0.708)        (0.690)   \n## (Intercept):car     -5.237***      -3.845***  \n##                      (0.785)        (0.844)   \n## gcost               -0.013***       -0.004    \n##                      (0.004)        (0.006)   \n## wait                -0.088***      -0.089***  \n##                      (0.011)        (0.011)   \n## incomeother          0.430***                 \n##                      (0.113)                  \n## time                               -0.202***  \n##                                     (0.060)   \n## iv                   0.835***      0.877***   \n##                      (0.192)        (0.198)   \n## ----------------------------------------------\n## Observations           210            210     \n## R2                    0.328          0.313    \n## Log Likelihood       -190.779      -194.841   \n## LR Test (df = 7)    185.959***    177.836***  \n## ==============================================\n## Note:              *p<0.1; **p<0.05; ***p<0.01"},{"path":"microeconometrics.html","id":"tobit","chapter":"3 Microeconometrics","heading":"3.3 Tobit models","text":"situations, dependent variable incompletely observed, may result non-representative sample. Typically, cases, observations dependent variable can lower /upper limit, “true”, underlying, dependent variable . case, OLS regression may lead inconsistent parameter estimates.Tobit models designed address situations. approach named James Tobin, developed model late 50s (see Tobin (1956)).Figure 3.8 illustrates situation. dots (white black) represent “true” observations. Now, assume black observed. ones uses observations OLS regression estimate relatonship \\(x\\) \\(y\\), one gets red line. clear sensitivity \\(y\\) \\(x\\) underestimated. blue line line one obtain white dots also observed; grey line represents model used genrate data (\\(y_i=x_i+\\varepsilon_i\\)).\nFigure 3.8: Bias case sample selection. grey line represents population regression line. model \\(y_i = x_i + \\varepsilon_i\\), \\(\\varepsilon_{,t} \\sim \\mathcal{N}(0,1)\\). red line OLS regression line based black dots .\nAssume (partially) observed dependent variable follows:\n\\[\ny^* = \\boldsymbol\\beta'\\mathbf{x} + \\varepsilon,\n\\]\n\\(\\varepsilon\\) drawn distribution characterized p.d.f. denoted \\(f_{\\boldsymbol\\gamma}^*\\) c.d.f. denoted \\(F_{\\boldsymbol\\gamma}^*\\); functions depend vector parameters \\(\\boldsymbol{\\gamma}\\).observed dependent variable :\n\\[\\begin{eqnarray*}\n\\mbox{Censored case:}&&y = \\left\\{\n\\begin{array}{ccc}\ny^* && y^*>L \\\\\nL && y^*\\le L,\n\\end{array}\n\\right.\\\\\n\\mbox{Truncated case:}&&y = \\left\\{\n\\begin{array}{ccc}\ny^* && y^*>L \\\\\n- && y^*\\le L,\n\\end{array}\n\\right.\n\\end{eqnarray*}\\]\n“\\(-\\)” stands missing observations.formulation easily extended censoring (\\(L \\rightarrow U\\)), censoring .model parameters gathered vector \\(\\theta = [\\boldsymbol\\beta',\\boldsymbol\\gamma']'\\). Let us write conditional p.d.f. observed variable:\n\\[\\begin{eqnarray*}\n\\mbox{Censored case:}&& f(y|\\mathbf{x};\\theta) = \\left\\{\n\\begin{array}{ccc}\nf_{\\boldsymbol\\gamma}^*(y -  \\boldsymbol\\beta'\\mathbf{x}) && y>L \\\\\nF_{\\boldsymbol\\gamma}^*(L-  \\boldsymbol\\beta'\\mathbf{x}) && y = L,\n\\end{array}\n\\right.\\\\\n\\mbox{Truncated case:}&&  f(y|\\mathbf{x};\\theta) =\n\\dfrac{f_{\\boldsymbol\\gamma}^*(y -  \\boldsymbol\\beta'\\mathbf{x})}{1 - F_{\\boldsymbol\\gamma}^*(L-  \\boldsymbol\\beta'\\mathbf{x})} \\quad \\mbox{} \\quad y>L.\n\\end{eqnarray*}\\](conditional) log-likelihood function given :\n\\[\n\\log \\mathcal{L}(\\theta;\\mathbf{y},\\mathbf{X}) = \\sum_{=1}^n \\log f(y_i|\\mathbf{x}_i;\\theta).\n\\]\ncensored case, :\n\\[\\begin{eqnarray*}\n\\log \\mathcal{L}(\\theta;\\mathbf{y},\\mathbf{X}) &=& \\sum_{=1}^n \\left\\{\n\\mathbb{}_{\\{y_i=L\\}}\\log\\left[F_{\\boldsymbol\\gamma}^*(L-  \\boldsymbol\\beta'\\mathbf{x}_i)\\right] + \\right.\\\\\n&& \\left. \\mathbb{}_{\\{y_i>0\\}} \\log \\left[f_{\\boldsymbol\\gamma}^*(y_i -  \\boldsymbol\\beta'\\mathbf{x}_i)\\right]\\right\\}.\n\\end{eqnarray*}\\]Tobit, censored/truncated normal regression model, corresponds case described , Gaussian errors \\(\\varepsilon\\). Specifically:\n\\[\ny^* = \\boldsymbol\\beta'\\mathbf{x} + \\varepsilon,\n\\]\n\\(\\varepsilon \\sim \\,..d.\\,\\mathcal{N}(0,\\sigma^2)\\) (\\(\\Rightarrow\\) \\(\\boldsymbol\\gamma = \\sigma^2\\)).Without loss generality, can assume \\(L=0\\). (One can shift observed data necessary.)censored density (\\(L=0\\)) given :\n\\[\nf(y) = \\left[\n\\frac{1}{\\sqrt{2 \\pi \\sigma^2}}\\exp\\left(-\\frac{1}{2 \\sigma^2}(y - \\boldsymbol\\beta'\\mathbf{x})^2\\right)\n\\right]^{\\mathbb{}_{\\{y>0\\}}}\n\\left[\n1 - \\Phi\\left(\\frac{\\boldsymbol\\beta'\\mathbf{x}}{\\sigma}\\right)\n\\right]^{\\mathbb{}_{\\{y=0\\}}}.\n\\]censored density (\\(L=0\\)) given :\n\\[\nf(y) = \\left[\n\\frac{1}{\\sqrt{2 \\pi \\sigma^2}}\\exp\\left(-\\frac{1}{2 \\sigma^2}(y - \\boldsymbol\\beta'\\mathbf{x})^2\\right)\n\\right]^{\\mathbb{}_{\\{y>0\\}}}\n\\left[\n1 - \\Phi\\left(\\frac{\\boldsymbol\\beta'\\mathbf{x}}{\\sigma}\\right)\n\\right]^{\\mathbb{}_{\\{y=0\\}}}.\n\\]truncated density (\\(L=0\\)) given :\n\\[\nf(y) = \\frac{1}{\\Phi(\\boldsymbol\\beta'\\mathbf{x})}\n\\frac{1}{\\sqrt{2 \\pi \\sigma^2}}\\exp\\left(-\\frac{1}{2 \\sigma^2}(y - \\boldsymbol\\beta'\\mathbf{x})^2\\right).\n\\]truncated density (\\(L=0\\)) given :\n\\[\nf(y) = \\frac{1}{\\Phi(\\boldsymbol\\beta'\\mathbf{x})}\n\\frac{1}{\\sqrt{2 \\pi \\sigma^2}}\\exp\\left(-\\frac{1}{2 \\sigma^2}(y - \\boldsymbol\\beta'\\mathbf{x})^2\\right).\n\\]Results usually heavily rely distributional assumptions (uncensored/untruncated case). framework easy extend heteroskedastic case, instance setting \\(\\sigma_i^2=\\exp(\\alpha'\\mathbf{x}_i)\\). situation illustrated Figure 3.9.\nFigure 3.9: Censored dataset heteroskedasticitiy. model \\(y_i = x_i + \\varepsilon_i\\), \\(\\varepsilon_{,t} \\sim \\mathcal{N}(0,\\sigma_i^2)\\) \\(\\sigma_i = \\exp(-1 + x_i)\\).\nLet us consider conditional means \\(y\\) general case, .e., \\(\\varepsilon\\) distribution. Assume \\(\\mathbf{x}\\) observed, expectations conditional \\(\\mathbf{x}\\).data left-truncated 0, :\n\\[\\begin{eqnarray*}\n\\mathbb{E}(y) &=& \\mathbb{E}(y^*|y^*>0) = \\underbrace{\\boldsymbol\\beta'\\mathbf{x}}_{=\\mathbb{E}(y^*)} + \\underbrace{\\mathbb{E}(\\varepsilon|\\varepsilon>-\\boldsymbol\\beta'\\mathbf{x})}_{>0} > \\mathbb{E}(y^*).\n\\end{eqnarray*}\\]data left-truncated 0, :\n\\[\\begin{eqnarray*}\n\\mathbb{E}(y) &=& \\mathbb{E}(y^*|y^*>0) = \\underbrace{\\boldsymbol\\beta'\\mathbf{x}}_{=\\mathbb{E}(y^*)} + \\underbrace{\\mathbb{E}(\\varepsilon|\\varepsilon>-\\boldsymbol\\beta'\\mathbf{x})}_{>0} > \\mathbb{E}(y^*).\n\\end{eqnarray*}\\]Consider data left-censored 0. Bayes, :\n\\[\nf_{y^*|y^*>0}(u) = \\frac{f_{y^*}(u)}{\\mathbb{P}(y^*>0)}\\mathbb{}_{\\{u>0\\}}.\n\\]\nTherefore:\n\\[\\begin{eqnarray*}\n\\mathbb{E}(y^*|y^*>0) &=& \\frac{1}{\\mathbb{P}(y^*>0)} \\int_{-\\infty}^\\infty u\\, f_{y^*}(u)\\mathbb{}_{\\{u>0\\}} du \\\\\n&=&  \\frac{1}{\\mathbb{P}(y^*>0)} \\mathbb{E}(\\underbrace{y^*\\mathbb{}_{\\{y^*>0\\}}}_{=y}),\n\\end{eqnarray*}\\]\n, :\n\\[\\begin{eqnarray*}\n\\mathbb{E}(y) &=&  \\mathbb{P}(y^*>0)\\mathbb{E}(y^*|y^*>0)\\\\\n&>&  \\mathbb{E}(y^*) =  \\mathbb{P}(y^*>0)\\mathbb{E}(y^*|y^*>0) +  \\mathbb{P}(y^*<0)\\underbrace{\\mathbb{E}(y^*|y^*<0)}_{<0}.\n\\end{eqnarray*}\\]Consider data left-censored 0. Bayes, :\n\\[\nf_{y^*|y^*>0}(u) = \\frac{f_{y^*}(u)}{\\mathbb{P}(y^*>0)}\\mathbb{}_{\\{u>0\\}}.\n\\]\nTherefore:\n\\[\\begin{eqnarray*}\n\\mathbb{E}(y^*|y^*>0) &=& \\frac{1}{\\mathbb{P}(y^*>0)} \\int_{-\\infty}^\\infty u\\, f_{y^*}(u)\\mathbb{}_{\\{u>0\\}} du \\\\\n&=&  \\frac{1}{\\mathbb{P}(y^*>0)} \\mathbb{E}(\\underbrace{y^*\\mathbb{}_{\\{y^*>0\\}}}_{=y}),\n\\end{eqnarray*}\\]\n, :\n\\[\\begin{eqnarray*}\n\\mathbb{E}(y) &=&  \\mathbb{P}(y^*>0)\\mathbb{E}(y^*|y^*>0)\\\\\n&>&  \\mathbb{E}(y^*) =  \\mathbb{P}(y^*>0)\\mathbb{E}(y^*|y^*>0) +  \\mathbb{P}(y^*<0)\\underbrace{\\mathbb{E}(y^*|y^*<0)}_{<0}.\n\\end{eqnarray*}\\]Now, let us come back Tobit (.e., Gaussian case) case.data left-truncated 0:\n\\[\\begin{eqnarray}\n\\mathbb{E}(y) &=& \\boldsymbol\\beta'\\mathbf{x} + \\mathbb{E}(\\varepsilon|\\varepsilon>-\\boldsymbol\\beta'\\mathbf{x}) \\nonumber\\\\\n&=&  \\boldsymbol\\beta'\\mathbf{x} + \\sigma \\underbrace{\\frac{\\phi(\\boldsymbol\\beta'\\mathbf{x}/\\sigma)}{\\Phi(\\boldsymbol\\beta'\\mathbf{x}/\\sigma)}}_{=: \\lambda(\\boldsymbol\\beta'\\mathbf{x}/\\sigma)} = \\sigma \\left( \\frac{\\boldsymbol\\beta'\\mathbf{x}}{\\sigma} + \\lambda\\left(\\frac{\\boldsymbol\\beta'\\mathbf{x}}{\\sigma}\\right)\\right). \\tag{3.17}\n\\end{eqnarray}\\]\npenultimate line obtained using Eq. (4.5).data left-truncated 0:\n\\[\\begin{eqnarray}\n\\mathbb{E}(y) &=& \\boldsymbol\\beta'\\mathbf{x} + \\mathbb{E}(\\varepsilon|\\varepsilon>-\\boldsymbol\\beta'\\mathbf{x}) \\nonumber\\\\\n&=&  \\boldsymbol\\beta'\\mathbf{x} + \\sigma \\underbrace{\\frac{\\phi(\\boldsymbol\\beta'\\mathbf{x}/\\sigma)}{\\Phi(\\boldsymbol\\beta'\\mathbf{x}/\\sigma)}}_{=: \\lambda(\\boldsymbol\\beta'\\mathbf{x}/\\sigma)} = \\sigma \\left( \\frac{\\boldsymbol\\beta'\\mathbf{x}}{\\sigma} + \\lambda\\left(\\frac{\\boldsymbol\\beta'\\mathbf{x}}{\\sigma}\\right)\\right). \\tag{3.17}\n\\end{eqnarray}\\]\npenultimate line obtained using Eq. (4.5).data left-censored 0:\n\\[\\begin{eqnarray*}\n\\mathbb{E}(y) &=&  \\mathbb{P}(y^*>0)\\mathbb{E}(y^*|y^*>0)\\\\\n&=&  \\Phi\\left( \\frac{\\boldsymbol\\beta'\\mathbf{x}}{\\sigma}\\right) \\sigma \\left(\n\\frac{\\boldsymbol\\beta'\\mathbf{x}}{\\sigma} +   \\lambda\\left(\\frac{\\boldsymbol\\beta'\\mathbf{x}}{\\sigma}\\right)\n\\right).\n\\end{eqnarray*}\\]data left-censored 0:\n\\[\\begin{eqnarray*}\n\\mathbb{E}(y) &=&  \\mathbb{P}(y^*>0)\\mathbb{E}(y^*|y^*>0)\\\\\n&=&  \\Phi\\left( \\frac{\\boldsymbol\\beta'\\mathbf{x}}{\\sigma}\\right) \\sigma \\left(\n\\frac{\\boldsymbol\\beta'\\mathbf{x}}{\\sigma} +   \\lambda\\left(\\frac{\\boldsymbol\\beta'\\mathbf{x}}{\\sigma}\\right)\n\\right).\n\\end{eqnarray*}\\]\nFigure 3.10: Conditional means \\(y\\) Tobit models. model \\(y_i = x_i + \\varepsilon_i\\), \\(\\varepsilon_i \\sim \\mathcal{N}(0,1)\\).\nHeckit regressionThe previous formula (Eq. (3.17)) can particular used alternative estimation approach, namely Heckman two-step estimation. approach based two steps:11Using complete sample, fit Probit model \\(\\mathbb{}_{\\{y_i>0\\}}\\) \\(\\mathbf{x}\\). provides consistent estimate \\(\\frac{\\boldsymbol\\beta}{\\sigma}\\), therefore \\(\\lambda(\\boldsymbol\\beta'\\mathbf{x}/\\sigma)\\). (Indeed, \\(z_i \\equiv \\mathbb{}_{\\{y_i>0\\}}\\), \\(\\mathbb{P}(z_i=1|\\mathbf{x}_i;\\boldsymbol\\beta/\\sigma)=\\Phi(\\boldsymbol\\beta'\\mathbf{x}_i/\\sigma)\\).)Using complete sample, fit Probit model \\(\\mathbb{}_{\\{y_i>0\\}}\\) \\(\\mathbf{x}\\). provides consistent estimate \\(\\frac{\\boldsymbol\\beta}{\\sigma}\\), therefore \\(\\lambda(\\boldsymbol\\beta'\\mathbf{x}/\\sigma)\\). (Indeed, \\(z_i \\equiv \\mathbb{}_{\\{y_i>0\\}}\\), \\(\\mathbb{P}(z_i=1|\\mathbf{x}_i;\\boldsymbol\\beta/\\sigma)=\\Phi(\\boldsymbol\\beta'\\mathbf{x}_i/\\sigma)\\).)Using truncated sample : run OLS regression \\(\\mathbf{y}\\) \\(\\left\\{\\mathbf{x},\\lambda(\\boldsymbol\\beta'\\mathbf{x}/\\sigma)\\right\\}\\) (Eq. (3.17) mind). provides consistent estimate \\((\\boldsymbol\\beta,\\sigma)\\).Using truncated sample : run OLS regression \\(\\mathbf{y}\\) \\(\\left\\{\\mathbf{x},\\lambda(\\boldsymbol\\beta'\\mathbf{x}/\\sigma)\\right\\}\\) (Eq. (3.17) mind). provides consistent estimate \\((\\boldsymbol\\beta,\\sigma)\\).underlying specification form:\n\\[\n\\mbox{Conditional mean} + \\mbox{disturbance}.\n\\]\n“Conditional mean” comes Eq. (3.17) “disturbance” error zero conditional mean.approach also applied case sample selection models (Section 3.4).Example 3.10  (Wage prediction) present example based dataset used Mroz (1987) (whicht part sampleSelection package).Figure 3.11 shows , low wages, OLS model tends -predict wages. slope observed Tobit-predicted wages closer one (adjustment line closer 45-degree line.)\nFigure 3.11: Predicted versus observed wages.\nTwo-part modelIn standard Tobit framework, model determining censored —truncated— data censoring mechanism one determining non-censored —observed— data outcome mechanism. two-part model adds flexibility permitting zeros non-zeros generated different densities. second model characterizes outcome conditional outcome observed.seminal paper, Duan et al. (1983) employ methodology account individual annual hospital expenses. two models follows:\\(1^{st}\\) model: \\(\\mathbb{P}(hosp=1|\\mathbf{x}) = \\Phi(\\mathbf{x}_1'\\boldsymbol\\beta_1)\\),\\(2^{nd}\\) model: \\(Expense = \\exp(\\mathbf{x}_2'\\boldsymbol\\beta_2 + \\eta)\\), \\(\\eta \\sim\\,..d.\\, \\mathcal{N}(0,\\sigma_2^2)\\).Specifically:\n\\[\n\\mathbb{E}(Expense|\\mathbf{x}_1,\\mathbf{x}_2) = \\Phi(\\mathbf{x}_1'\\boldsymbol\\beta_1)\\exp\\left(\\mathbf{x}_2'\\boldsymbol\\beta_2+ \\frac{\\sigma_2^2}{2}\\right).\n\\]sample-selection models, studied next section, one specifies joint distribution censoring outcome mechanisms (two parts independent ).","code":"\nlibrary(sampleSelection)\nlibrary(AER)\ndata(\"Mroz87\")\nMroz87$lfp.yesno <- NaN\nMroz87$lfp.yesno[Mroz87$lfp==1] <- \"yes\"\nMroz87$lfp.yesno[Mroz87$lfp==0] <- \"no\"\nMroz87$lfp.yesno <- as.factor(Mroz87$lfp.yesno)\nols <- lm(wage ~ educ + exper + I( exper^2 ) + city,data=subset(Mroz87,lfp==1))\ntobit <- tobit(wage ~ educ + exper + I( exper^2 ) + city,\n               left = 0, right = Inf,\n               data=Mroz87)\nHeckit <- heckit(lfp ~ educ + exper + I( exper^2 ) + city, # selection equation\n                 wage ~ educ + exper + I( exper^2 ) + city, # outcome equation\n                 data=Mroz87 )\n\nstargazer(ols,Heckit,tobit,no.space = TRUE,type=\"text\",omit.stat = \"f\")## \n## ======================================================================\n##                                    Dependent variable:                \n##                     --------------------------------------------------\n##                                            wage                       \n##                           OLS           Heckman           Tobit       \n##                                        selection                      \n##                           (1)             (2)              (3)        \n## ----------------------------------------------------------------------\n## educ                    0.481***       0.759***         0.642***      \n##                         (0.067)         (0.270)          (0.081)      \n## exper                    0.032           0.430          0.461***      \n##                         (0.062)         (0.369)          (0.068)      \n## I(exper2)               -0.0003         -0.008          -0.009***     \n##                         (0.002)         (0.008)          (0.002)      \n## city                     0.449           0.113           -0.087       \n##                         (0.318)         (0.522)          (0.378)      \n## Constant               -2.561***        -12.251        -10.395***     \n##                         (0.929)         (8.853)          (1.095)      \n## ----------------------------------------------------------------------\n## Observations              428             753              753        \n## R2                       0.125           0.128                        \n## Adjusted R2              0.117           0.117                        \n## Log Likelihood                                         -1,462.700     \n## rho                                      1.063                        \n## Inverse Mills Ratio                  5.165 (4.594)                    \n## Residual Std. Error 3.111 (df = 423)                                  \n## Wald Test                                          153.892*** (df = 4)\n## ======================================================================\n## Note:                                      *p<0.1; **p<0.05; ***p<0.01"},{"path":"microeconometrics.html","id":"SSM","chapter":"3 Microeconometrics","heading":"3.4 Sample Selection Models","text":"situation tackled sample-selection models following. dependent variable interest, denoted \\(y_2\\), depends observed variables \\(\\mathbf{x}_2\\). Observing \\(y_2\\), , depends value latent variable (\\(y_1^*\\)) correlated observed variables \\(\\mathbf{x}_1\\). difference w.r.t. two-part model skethed , even conditionally \\((\\mathbf{x}_1,\\mathbf{x}_2)\\), \\(y_1^*\\) \\(y_2\\) may correlated.Tobit case, even simplest case population conditional mean linear regressors (.e. \\(y_2 = \\mathbf{x}_2'\\boldsymbol\\beta_2 + \\varepsilon_2\\)), OLS regression leads inconsistent parameter estimates sample representative population.two latent variables: \\(y_1^*\\) \\(y_2^*\\). observe \\(y_1\\) , considered entity “participates”, also observe \\(y_2\\). specifically:\n\\[\\begin{eqnarray*}\ny_1 &=& \\left\\{\n\\begin{array}{ccc}\n1 &\\mbox{}& y_1^* > 0 \\\\\n0 &\\mbox{}& y_1^* \\le 0\n\\end{array}\n\\right. \\quad \\mbox{(participation equation)}\\\\\ny_2 &=& \\left\\{\n\\begin{array}{ccc}\ny_2^* &\\mbox{}& y_1 = 1 \\\\\n- &\\mbox{}& y_1 = 0\n\\end{array}\n\\right. \\quad \\mbox{(outcome equation).}\n\\end{eqnarray*}\\]Moreover:\n\\[\\begin{eqnarray*}\ny_1^* &=& \\mathbf{x}_1'\\boldsymbol\\beta_1 + \\varepsilon_1 \\\\\ny_2^* &=& \\mathbf{x}_2'\\boldsymbol\\beta_2 + \\varepsilon_2.\n\\end{eqnarray*}\\]Note Tobit model (Section 3.3) special case \\(y_1^*=y_2^*\\).Usually:\n\\[\n\\left[\\begin{array}{c}\\varepsilon_1\\\\\\varepsilon_2\\end{array}\\right] \\sim \\mathcal{N}\\left(\\mathbf{0},\n\\left[\n\\begin{array}{cc}\n1 & \\rho  \\sigma_2 \\\\\n\\rho  \\sigma_2 & \\sigma_2^2\n\\end{array}\n\\right]\n\\right).\n\\]\nLet us derive likelihood associated model. :\n\\[\\begin{eqnarray}\nf(\\underbrace{0}_{=y_1},\\underbrace{-}_{=y_2}|\\mathbf{x};\\theta) &=& \\mathbb{P}(y_1^*\\le 0) = \\Phi(-\\mathbf{x}_1'\\boldsymbol\\beta_1) \\tag{3.18}\\\\\nf(1,y_2|\\mathbf{x};\\theta) &=& f(y_2^*|\\mathbf{x};\\theta) \\mathbb{P}(y_1^*>0|y_2^*,\\mathbf{x};\\theta) \\nonumber \\\\\n&=& \\frac{1}{\\sigma}\\phi\\left(\\frac{y_2 - \\mathbf{x}_2'\\boldsymbol\\beta_2}{\\sigma}\\right)  \\mathbb{P}(y_1^*>0|y_2,\\mathbf{x};\\theta).\\tag{3.19}\n\\end{eqnarray}\\]Let us compute \\(\\mathbb{P}(y_1^*>0|y_2,\\mathbf{x};\\theta)\\). Prop. 4.16 (Appendix 4.4), applied (\\(\\varepsilon_1,\\varepsilon_2\\)), :\n\\[\ny_1^*|y_2 \\sim \\mathcal{N}\\left(\\mathbf{x}_1'\\boldsymbol\\beta_1 + \\frac{\\rho}{\\sigma_2}(y_2-\\mathbf{x}_2'\\boldsymbol\\beta_2),1-\\rho^2\\right).\n\\]\nleads \n\\[\\begin{equation}\n\\mathbb{P}(y_1^*>0|y_2,\\mathbf{x};\\theta) = \\Phi\\left( \\frac{\\mathbf{x}_1'\\boldsymbol\\beta_1 + \\dfrac{\\rho}{\\sigma_2}(y_2-\\mathbf{x}_2'\\boldsymbol\\beta_2)}{\\sqrt{1-\\rho^2}}\\right).\\tag{3.20}\n\\end{equation}\\]Figure 3.12 displays \\(\\mathbb{P}(y_1^*>0|y_2,\\mathbf{x};\\theta)\\) different values \\(y_2\\) \\(\\rho\\), case \\(\\boldsymbol\\beta_1=\\boldsymbol\\beta_2=0\\).\nFigure 3.12: Probability observing \\(y_2\\) depending value, different values conditional correlation \\(y_2\\) \\(y_1^*\\).\nUsing Eqs. (3.18), (3.19) (3.20), one gets log-likelihood function:\n\\[\\begin{eqnarray*}\n\\log \\mathcal{L}(\\theta;\\mathbf{y},\\mathbf{X}) &=& \\sum_{=1}^n  (1 - y_{1,})\\log \\Phi(-\\mathbf{x}_{1,}'\\boldsymbol\\beta_1) + \\\\\n&&  \\sum_{=1}^n y_{1,} \\log \\left(  \\frac{1}{\\sigma}\\phi\\left(\\frac{y_{2,} - \\mathbf{x}_{2,}'\\boldsymbol\\beta_2}{\\sigma}\\right)\\right) + \\\\\n&&  \\sum_{=1}^n y_{1,} \\log \\left(\\Phi\\left( \\frac{\\mathbf{x}_{1,}'\\boldsymbol\\beta_1 + \\dfrac{\\rho}{\\sigma_2}(y_{2,}-\\mathbf{x}_2'\\boldsymbol\\beta_2)}{\\sqrt{1-\\rho^2}}\\right)\\right).\n\\end{eqnarray*}\\]can also compute conditional expectations:\n\\[\\begin{eqnarray}\n\\mathbb{E}(y_2^*|y_1=1,\\mathbf{x}) &=& \\mathbb{E}(\\mathbb{E}(y_2^*|y_1^*,\\mathbf{x})|y_1=1,\\mathbf{x})\\nonumber\\\\\n&=& \\mathbb{E}(\\mathbf{x}_2'\\boldsymbol\\beta_2 + \\rho\\sigma_2(y_1^*-\\mathbf{x}_1'\\boldsymbol\\beta_1)|y_1=1,\\mathbf{x})\\nonumber\\\\\n&=& \\mathbf{x}_2'\\boldsymbol\\beta_2 + \\rho\\sigma_2\\mathbb{E}( \\underbrace{y_1^*-\\mathbf{x}_1'\\boldsymbol\\beta_1}_{=\\varepsilon_1 \\sim\\mathcal{N}(0,1)}|\\varepsilon_1>-\\mathbf{x}_1'\\boldsymbol\\beta_1,\\mathbf{x})\\nonumber\\\\\n&=& \\mathbf{x}_2'\\boldsymbol\\beta_2 + \\rho\\sigma_2\\frac{\\phi(-\\mathbf{x}_1'\\boldsymbol\\beta_1)}{1 - \\Phi(-\\mathbf{x}_1'\\boldsymbol\\beta_1)}\\nonumber\\\\\n&=& \\mathbf{x}_2'\\boldsymbol\\beta_2 + \\rho\\sigma_2\\frac{\\phi(\\mathbf{x}_1'\\boldsymbol\\beta_1)}{\\Phi(\\mathbf{x}_1'\\boldsymbol\\beta_1)}=\\mathbf{x}_2'\\boldsymbol\\beta_2 + \\rho\\sigma_2\\lambda(\\mathbf{x}_1'\\boldsymbol\\beta_1),\\tag{3.21}\n\\end{eqnarray}\\]\n:\n\\[\\begin{eqnarray*}\n\\mathbb{E}(y_2^*|y_1=0,\\mathbf{x}) &=&  \\mathbf{x}_2'\\boldsymbol\\beta_2 + \\rho\\sigma_2\\mathbb{E}(y_1^*-\\mathbf{x}_1'\\boldsymbol\\beta_1|\\varepsilon_1\\le-\\mathbf{x}_1'\\boldsymbol\\beta_1,\\mathbf{x})\\\\\n&=& \\mathbf{x}_2'\\boldsymbol\\beta_2 + \\rho\\sigma_2\\frac{\\phi(-\\mathbf{x}_1'\\boldsymbol\\beta_1)}{1 - \\Phi(-\\mathbf{x}_1'\\boldsymbol\\beta_1)}\\\\\n&=& \\mathbf{x}_2'\\boldsymbol\\beta_2 - \\rho\\sigma_2\\frac{\\phi(-\\mathbf{x}_1'\\boldsymbol\\beta_1)}{\\Phi(-\\mathbf{x}_1'\\boldsymbol\\beta_1)}=\\mathbf{x}_2'\\boldsymbol\\beta_2 - \\rho\\sigma_2\\lambda(-\\mathbf{x}_1'\\boldsymbol\\beta_1).\n\\end{eqnarray*}\\]Heckman procedureAs tobit models (Section 3.3), can use Heckman procedure estimate model. Eq. (3.21) shows \\(\\mathbb{E}(y_2^*|y_1=1,\\mathbf{x}) \\ne \\mathbf{x}_2'\\boldsymbol\\beta_2\\) \\(\\rho \\ne 0\\). Therefore, OLS approach yields biased estimates based employed sub-sample \\(y_1=1\\).Heckman two-step procedure (“Heckit”) consists replacing \\(\\lambda(\\mathbf{x}_1'\\boldsymbol\\beta_1)\\) appearing Eq. (3.21) consistent estimate . precisely:Get estimate \\(\\widehat{\\boldsymbol\\beta_1}\\) \\(\\boldsymbol\\beta_1\\) (probit regression \\(y_1\\) \\(\\mathbf{x}_1\\)).Run OLS regression (using data associated \\(y_1=1\\)):\n\\(\\lambda(\\mathbf{x}_1'\\widehat{\\boldsymbol\\beta_1})\\) regressor.estimate \\(\\sigma_2^2\\)? Eq. (4.6), :\n\\[\n\\mathbb{V}ar(y_2|y_1^*>0,\\mathbf{x}) = \\mathbb{V}ar(\\varepsilon_2|\\varepsilon_1>-\\mathbf{x}_1'\\boldsymbol\\beta_1,\\mathbf{x}).\n\\]\nUsing \\(\\varepsilon_2\\) can decomposed \\(\\rho\\sigma_2\\varepsilon_1 + \\xi\\), \\(\\xi \\sim \\mathcal{N}(0,\\sigma_2^2(1-\\rho^2))\\) independent \\(\\varepsilon_1\\), get:\n\\[\n\\mathbb{V}ar(y_2|y_1^*>0,\\mathbf{x}) = \\sigma_2^2(1-\\rho^2) + \\rho^2\\sigma_2^2 \\mathbb{V}ar(\\varepsilon_1|\\varepsilon_1>-\\mathbf{x}_1'\\boldsymbol\\beta_1,\\mathbf{x}).\n\\]\nUsing Eq. (4.7), get:\n\\[\n\\mathbb{V}ar(\\varepsilon_1|\\varepsilon_1>-\\mathbf{x}_1'\\boldsymbol\\beta_1,\\mathbf{x}) = 1 - \\mathbf{x}_1'\\boldsymbol\\beta_1 \\lambda(\\mathbf{x}_1'\\boldsymbol\\beta_1) - \\lambda(\\mathbf{x}_1'\\boldsymbol\\beta_1)^2,\n\\]\ngives\n\\[\\begin{eqnarray*}\n\\mathbb{V}ar(y_2|y_1^*>0,\\mathbf{x}) &=& \\sigma_2^2(1-\\rho^2) + \\rho^2\\sigma_2^2 (1 - \\mathbf{x}_1'\\boldsymbol\\beta_1 \\lambda(\\mathbf{x}_1'\\boldsymbol\\beta_1) - \\lambda(\\mathbf{x}_1'\\boldsymbol\\beta_1)^2)\\\\\n&=& \\sigma_2^2 - \\rho^2\\sigma_2^2 \\left(\\mathbf{x}_1'\\boldsymbol\\beta_1 \\lambda(\\mathbf{x}_1'\\boldsymbol\\beta_1) + \\lambda(\\mathbf{x}_1'\\boldsymbol\\beta_1)^2\\right),\n\\end{eqnarray*}\\]\n, finally:\n\\[\n\\sigma_2^2 \\approx \\widehat{\\mathbb{V}ar}(y_2|y_1^*>0,\\mathbf{x}) + \\widehat{\\rho \\sigma_2}^2 \\left(\\mathbf{x}_1'\\widehat{\\boldsymbol\\beta_1} \\lambda(\\mathbf{x}_1'\\widehat{\\boldsymbol\\beta_1}) + \\lambda(\\mathbf{x}_1'\\widehat{\\boldsymbol\\beta_1})^2\\right).\n\\]Heckman procedure computationally simple. Although computational costs longer issue, two-step solution allows certain generalisations easily ML, robust certain circumstances. computation parameter standard errors fairly complicated two steps (see Cameron Trivedi (2005), Subsection 16.10.2). Bootstrap can resorted .Example 3.11  (Wage prediction) Example 3.10, let us use Mroz (1987) dataset , objective explaining wage setting.","code":"\nlibrary(sampleSelection)\nlibrary(AER)\ndata(\"Mroz87\")\nMroz87$lfp.yesno <- NaN\nMroz87$lfp.yesno[Mroz87$lfp==1] <- \"yes\"\nMroz87$lfp.yesno[Mroz87$lfp==0] <- \"no\"\nMroz87$lfp.yesno <- as.factor(Mroz87$lfp.yesno)\n#Logit & Probit (selection equation)\nlogitW <- glm(lfp ~ age + I( age^2 ) + kids5 + huswage + educ,\n              family = binomial(link = \"logit\"), data = Mroz87) \nprobitW <- glm(lfp ~ age + I( age^2 ) + kids5 + huswage + educ,\n               family = binomial(link = \"probit\"), data = Mroz87) \n# OLS for outcome:\nols1 <- lm(log(wage) ~ educ+exper+I( exper^2 )+city,data=subset(Mroz87,lfp==1))\n# Two-step Heckman estimation\nheckvan <- \n  heckit( lfp ~ age + I( age^2 ) + kids5 + huswage + educ, # selection equation\n          log(wage) ~ educ + exper + I( exper^2 ) + city, # outcome equation\n          data=Mroz87 )\n# Maximun likelihood estimation of selection model:\nml <- selection(lfp~age+I(age^2)+kids5+huswage+educ, \n                log(wage)~educ+exper+I(exper^2)+city, data = Mroz87)\n# Print selection-equation estimates:\nstargazer(logitW,probitW,heckvan,ml,type = \"text\",no.space = TRUE,\n          selection.equation = TRUE)## \n## ===================================================================\n##                                   Dependent variable:              \n##                     -----------------------------------------------\n##                                           lfp                      \n##                     logistic   probit      Heckman      selection  \n##                                           selection                \n##                        (1)       (2)         (3)           (4)     \n## -------------------------------------------------------------------\n## age                   0.012     0.010       0.010         0.010    \n##                      (0.114)   (0.069)     (0.069)       (0.069)   \n## I(age2)              -0.001    -0.0005     -0.0005       -0.0005   \n##                      (0.001)   (0.001)     (0.001)       (0.001)   \n## kids5               -1.409*** -0.855***   -0.855***     -0.854***  \n##                      (0.198)   (0.116)     (0.115)       (0.116)   \n## huswage             -0.069*** -0.042***   -0.042***     -0.042***  \n##                      (0.020)   (0.012)     (0.012)       (0.013)   \n## educ                0.244***  0.148***    0.148***      0.148***   \n##                      (0.040)   (0.024)     (0.023)       (0.024)   \n## Constant             -0.938    -0.620      -0.620        -0.615    \n##                      (2.508)   (1.506)     (1.516)       (1.518)   \n## -------------------------------------------------------------------\n## Observations           753       753         753           753     \n## R2                                          0.158                  \n## Adjusted R2                                 0.148                  \n## Log Likelihood      -459.955  -459.901                  -891.177   \n## Akaike Inf. Crit.    931.910   931.802                             \n## rho                                         0.018     0.014 (0.203)\n## Inverse Mills Ratio                     0.012 (0.152)              \n## ===================================================================\n## Note:                                   *p<0.1; **p<0.05; ***p<0.01\n# Print outcome-equation estimates:\nstargazer(ols1,heckvan,ml,type = \"text\",no.space = TRUE,omit.stat = \"f\")## \n## ================================================================\n##                                 Dependent variable:             \n##                     --------------------------------------------\n##                                      log(wage)                  \n##                           OLS           Heckman      selection  \n##                                        selection                \n##                           (1)             (2)           (3)     \n## ----------------------------------------------------------------\n## educ                    0.106***       0.106***      0.106***   \n##                         (0.014)         (0.017)       (0.017)   \n## exper                   0.041***       0.041***      0.041***   \n##                         (0.013)         (0.013)       (0.013)   \n## I(exper2)               -0.001**       -0.001**      -0.001**   \n##                         (0.0004)       (0.0004)      (0.0004)   \n## city                     0.054           0.053         0.053    \n##                         (0.068)         (0.069)       (0.069)   \n## Constant               -0.531***        -0.547*      -0.544**   \n##                         (0.199)         (0.289)       (0.272)   \n## ----------------------------------------------------------------\n## Observations              428             753           753     \n## R2                       0.158           0.158                  \n## Adjusted R2              0.150           0.148                  \n## Log Likelihood                                       -891.177   \n## rho                                      0.018     0.014 (0.203)\n## Inverse Mills Ratio                  0.012 (0.152)              \n## Residual Std. Error 0.667 (df = 423)                            \n## ================================================================\n## Note:                                *p<0.1; **p<0.05; ***p<0.01"},{"path":"microeconometrics.html","id":"models-of-count-data","chapter":"3 Microeconometrics","heading":"3.5 Models of Count Data","text":"Count-data models aim explaining dependent variables \\(y_i\\) take integer values. Typically, one may want account number doctor visits, customers, hospital stays, borrowers’ defaults, recreational trips, accidents. Quite often, data feature large proportion zeros (see, e.g., Table 20.1 Cameron Trivedi (2005)), /skewed right.","code":""},{"path":"microeconometrics.html","id":"poisson-model","chapter":"3 Microeconometrics","heading":"3.5.1 Poisson model","text":"basic count-data model Poisson model. model, \\(y \\sim \\mathcal{P}(\\mu)\\), .e.\n\\[\n\\mathbb{P}(y=k) = \\frac{\\mu^k e^{-\\mu}}{k!},\n\\]\nimplying \\(\\mathbb{E}(y) = \\mathbb{V}ar(y) = \\mu\\).Poisson parameter, \\(\\mu\\), assumed depend observed variables, gathered vector \\(\\mathbf{x}_i\\) entity \\(\\). ensure \\(\\mu_i \\ge 0\\), common take \\(\\mu_i = \\exp(\\boldsymbol\\beta'\\mathbf{x}_i)\\), gives:\n\\[\ny_i \\sim \\mathcal{P}(\\exp[\\boldsymbol\\beta'\\mathbf{x}_i]).\n\\]Poisson regression intrinsically heteroskedastic (since \\(\\mathbb{V}ar(y_i) = \\mu_i = \\exp(\\boldsymbol\\beta'\\mathbf{x}_i)\\)).assumption independence across entities, log-likelihood given :\n\\[\n\\log \\mathcal{L}(\\boldsymbol\\beta;\\mathbf{y},\\mathbf{X}) = \\sum_{=1}^n (y_i \\boldsymbol\\beta'\\mathbf{x}_i - \\exp[\\boldsymbol\\beta'\\mathbf{x}_i] - \\ln[y_i!]).\n\\]\nfirst-order condition get MLE :\n\\[\\begin{equation}\n\\sum_{=1}^n (y_i - \\exp[\\boldsymbol\\beta'\\mathbf{x}_i])\\mathbf{x}_i = \\underbrace{\\mathbf{0}}_{K \\times 1}. \\tag{3.23}\n\\end{equation}\\]Eq. (3.23) equivalent define Pseudo Maximum-Likelihood estimator \\(\\boldsymbol\\beta\\) (misspecified) model\n\\[\ny_i \\sim ..d.\\,\\mathcal{N}(\\exp[\\boldsymbol\\beta'\\mathbf{x}_i],\\sigma^2).\n\\]\n, Eq. (3.23) also characterizes (true) ML estimator \\(\\boldsymbol\\beta\\) previous model.Since \\(\\mathbb{E}(y_i|\\mathbf{x}_i) = \\exp(\\boldsymbol\\beta'\\mathbf{x}_i)\\), :\n\\[\ny_i = \\exp(\\boldsymbol\\beta'\\mathbf{x}_i) + \\varepsilon_i,\n\\]\n\\(\\mathbb{E}(\\varepsilon_i|\\mathbf{x}_i) = 0\\). notably implies (N)LS estimator \\(\\boldsymbol\\beta\\) consistent.interpret regression coefficients (components \\(\\boldsymbol\\beta\\))? :\n\\[\n\\frac{\\partial \\mathbb{E}(y_i|\\mathbf{x}_i)}{\\partial x_{,j}} = \\beta_j \\exp(\\boldsymbol\\beta'\\mathbf{x}_i),\n\\]\ndepends considered individual.average estimated response :\n\\[\n\\widehat{\\beta}_j \\frac{1}{n}\\sum_{=1}^n  \\exp(\\widehat{\\boldsymbol\\beta}'\\mathbf{x}_i),\n\\]\nequal \\(\\widehat{\\beta}_j \\overline{y}\\) model includes constant (e.g., \\(x_{1,}=1\\) entities \\(\\)).limitation standard Poisson model distribution \\(y_i\\) conditional \\(\\mathbf{x}_i\\) depends single parameter (\\(\\mu_i\\)). Besides, often tension fitting fraction zeros, .e. \\(\\mathbb{P}(y_i=0|\\mathbf{x}_i)=\\exp[-\\exp(\\boldsymbol\\beta'\\mathbf{x}_i)]\\), distribution \\(y_i|\\mathbf{x}_i,y_i>0\\). following models (negative binomial, NB model, Hurdle model, Zero-Inflated model) designed address points.","code":""},{"path":"microeconometrics.html","id":"negative-binomial-model","chapter":"3 Microeconometrics","heading":"3.5.2 Negative binomial model","text":"negative binomial model, :\n\\[\ny_i|\\lambda_i \\sim \\mathcal{P}(\\lambda_i),\n\\]\n\\(\\lambda_i\\) now random. Specifically, takes form:\n\\[\n\\lambda_i = \\nu_i \\times \\exp(\\boldsymbol\\beta'\\mathbf{x}_i),\n\\]\n\\(\\nu_i \\sim \\,..d.\\,\\Gamma(\\underbrace{\\delta}_{\\mbox{shape}},\\underbrace{1/\\delta}_{\\mbox{scale}})\\). , p.d.f. \\(\\nu_i\\) :\n\\[\ng(\\nu) = \\frac{\\nu^{\\delta - 1}e^{-\\nu\\delta}\\delta^\\delta}{\\Gamma(\\delta)},\n\\]\n\\(\\Gamma:\\,z \\mapsto \\int_0^{+\\infty}t^{z-1}e^{-t}dt\\) (\\(\\Gamma(k+1)=k!\\)).notably implies :\n\\[\n\\mathbb{E}(\\nu_i) = 1 \\quad \\mbox{} \\quad \\mathbb{V}ar(\\nu) = \\frac{1}{\\delta}.\n\\]Hence, p.d.f. \\(y_i\\) conditional \\(\\mu\\) \\(\\delta\\) (\\(\\mu=\\exp(\\boldsymbol\\beta'\\mathbf{x}_i)\\)) obtained mixture densities:\n\\[\n\\mathbb{P}(y_i=k|\\exp(\\boldsymbol\\beta'\\mathbf{x}_i)=\\mu;\\delta)=\\int_0^\\infty \\frac{e^{-\\mu \\nu}(\\mu \\nu)^k}{k!} \\frac{\\nu^{\\delta - 1}e^{-\\nu\\delta}\\delta^\\delta}{\\Gamma(\\delta)} d \\nu.\n\\]can shown :\n\\[\n\\mathbb{E}(y|\\mathbf{x}) = \\mu \\quad \\mbox{}\\quad \\mathbb{V}ar(y|\\mathbf{x}) = \\mu\\left(1+\\alpha \\mu\\right),\n\\]\n\\(\\exp(\\boldsymbol\\beta'\\mathbf{x}_i)=\\mu\\) \\(\\alpha = 1/\\delta\\).one additional degree freedom w.r.t. Poisson model (\\(\\alpha\\)).Note \\(\\mathbb{V}ar(y|\\mathbf{x}) > \\mathbb{E}(y|\\mathbf{x})\\) (often called data). Moreover, conditional variance quadratic mean:\n\\[\n\\mathbb{V}ar(y|\\mathbf{x}) = \\mu+\\alpha \\mu^2.\n\\]\nprevious expression basis -called NB2 specification. \\(\\delta\\) replaced \\(\\mu/\\gamma\\), get NB1 model:\n\\[\n\\mathbb{V}ar(y|\\mathbf{x}) = \\mu(1+\\gamma).\n\\]Example 3.12  (Number doctor visits) following example compares different specifications, namely linear regression model, Poisson model, NB model, account number doctor visits. dataset (`randdata) one used Chapter 20 Cameron Trivedi (2005) (available page).\nFigure 3.13: Distribution number doctor visits.\nModels’ predictions can obtained follows:Let us now compute model-implied probabilities, let’s compare frequencies observed data.appears NB model better capturing relatively large number zeros Poisson model. also case Hurdle Zero-Inflation models:","code":"\nlibrary(AEC)\nlibrary(COUNT)\nlibrary(pscl) # for predprob function and hurdle model\npar(plt=c(.15,.95,.1,.95))\nplot(table(randdata$mdvis))\nranddata$LC <- log(1 + randdata$coins)\nmodel.OLS <- lm(mdvis ~ LC + idp + lpi + fmde + physlm + disea + hlthg + \n                  hlthf + hlthp - 1,data=randdata)\nmodel.poisson <- glm(mdvis ~ LC + idp + lpi + fmde + physlm + disea + \n                       hlthg + hlthf + hlthp - 1,data=randdata,family = poisson)\nmodel.neg.bin <- glm.nb(mdvis ~ LC + idp + lpi + fmde + physlm + disea +\n                          hlthg + hlthf + hlthp - 1,data=randdata)\nmodel.neg.bin.with.intercept <- \n  glm.nb(mdvis ~ LC + idp + lpi + fmde + physlm + disea + hlthg + \n           hlthf + hlthp,data=randdata)\nstargazer::stargazer(model.OLS,model.poisson,model.neg.bin,\n                     model.neg.bin.with.intercept,type=\"text\",\n                     no.space = TRUE,omit.stat = c(\"f\",\"ser\"))## \n## =========================================================================\n##                                     Dependent variable:                  \n##                   -------------------------------------------------------\n##                                            mdvis                         \n##                      OLS      Poisson               negative             \n##                                                     binomial             \n##                      (1)        (2)           (3)              (4)       \n## -------------------------------------------------------------------------\n## LC                -0.155***  -0.051***     -0.057***        -0.058***    \n##                    (0.020)    (0.003)       (0.006)          (0.006)     \n## idp               -0.546***  -0.183***     -0.212***        -0.268***    \n##                    (0.075)    (0.011)       (0.023)          (0.023)     \n## lpi               0.230***   0.095***       0.088***         0.041***    \n##                    (0.012)    (0.002)       (0.004)          (0.004)     \n## fmde              -0.073***  -0.029***     -0.030***        -0.038***    \n##                    (0.012)    (0.002)       (0.004)          (0.003)     \n## physlm            0.945***   0.217***       0.229***         0.269***    \n##                    (0.104)    (0.013)       (0.031)          (0.030)     \n## disea             0.177***   0.050***       0.062***         0.038***    \n##                    (0.004)   (0.0005)       (0.001)          (0.001)     \n## hlthg             0.270***   0.126***       0.068***         -0.044**    \n##                    (0.066)    (0.009)       (0.020)          (0.020)     \n## hlthf             0.455***   0.149***       0.084**           0.017      \n##                    (0.123)    (0.016)       (0.037)          (0.036)     \n## hlthp             1.537***   0.197***       0.185**          0.178**     \n##                    (0.263)    (0.027)       (0.076)          (0.074)     \n## Constant                                                     0.664***    \n##                                                              (0.025)     \n## -------------------------------------------------------------------------\n## Observations       20,190     20,190         20,190           20,190     \n## R2                  0.322                                                \n## Adjusted R2         0.322                                                \n## Log Likelihood              -64,221.340   -43,745.860      -43,384.660   \n## theta                                   0.732*** (0.010) 0.773*** (0.011)\n## Akaike Inf. Crit.           128,460.700    87,509.710       86,789.320   \n## =========================================================================\n## Note:                                         *p<0.1; **p<0.05; ***p<0.01\n# prediction of beta'x, equivalent to \"model.poisson$fitted.values\":\npredict_poisson.beta.x <- predict(model.poisson)\n# prediction of the number of events (exp(beta'x)):\npredict_poisson <- predict(model.poisson,type=\"response\")\npredict_NB <- model.neg.bin$fitted.values\nprop.table.data  <- prop.table(table(randdata$mdvis))\npredprob.poisson <- predprob(model.poisson) # part of pscl package\npredprob.nb      <- predprob(model.neg.bin)\nprint(rbind(prop.table.data[1:6],\n            apply(predprob.poisson[,1:6],2,mean),\n            apply(predprob.nb[,1:6],2,mean)))##              0         1         2          3          4          5\n## [1,] 0.3124319 0.1890540 0.1385339 0.09331352 0.06661714 0.04794453\n## [2,] 0.1220592 0.2230328 0.2271621 0.17173478 0.10888940 0.06244353\n## [3,] 0.3486824 0.1884640 0.1219340 0.08385899 0.05968603 0.04350209"},{"path":"microeconometrics.html","id":"hurdle-model","chapter":"3 Microeconometrics","heading":"3.5.3 Hurdle model","text":"main objective model, w.r.t. Poisson model, generate zeros data predicted previous count models. idea separate modeling number zeros number non-zero counts. Specifically, frequency zeros determined \\(f_1\\), (relative) frequencies non-zero counts determined \\(f_2\\):\n\\[\nf(y) = \\left\\{\n\\begin{array}{lll}\nf_1(0) & \\mbox{$y=0$},\\\\\n\\dfrac{1-f_1(0)}{1-f_2(0)}f_2(y) & \\mbox{$y>0$}.\n\\end{array}\n\\right.\n\\]Note back standard Poisson model \\(f_1 \\equiv f_2\\). model straightforwardly estimated ML.","code":""},{"path":"microeconometrics.html","id":"zero-inflated-model","chapter":"3 Microeconometrics","heading":"3.5.4 Zero-inflated model","text":"objective Hurdle model, modeling slightly different. based mixture binary process \\(B\\) (p.d.f. \\(f_1\\)) process \\(Z\\) (p.d.f. \\(f_2\\)). \\(B\\) \\(Z\\) independent. Formally:\n\\[\ny = B Z,\n\\]\nimplying:\n\\[\nf(y) = \\left\\{\n\\begin{array}{lll}\nf_1(0) + (1-f_1(0))f_2(0) & \\mbox{$y=0$},\\\\\n(1-f_1(0))f_2(y) & \\mbox{$y>0$}.\n\\end{array}\n\\right.\n\\]\nTypically, \\(f_1\\) corresponds logit model \\(f_2\\) Poisson negative binomial density. model easily estimated ML techniques.Example 3.13  (Number doctor visits) Let us come back data used Example 3.12, estimate Hurdle zero-inflation models:Let us test importance LC model using Wald test:Finally, compare average model-implied probabilities frequencies observed data:","code":"\nmodel.hurdle <- \n  hurdle(mdvis ~ LC + idp + lpi + fmde + physlm + disea + hlthg + hlthf + \n           hlthp, data=randdata,\n         dist = \"poisson\", zero.dist = \"binomial\", link = \"logit\")\nmodel.zeroinfl <- zeroinfl(mdvis ~ LC + idp + lpi + fmde + physlm +\n                             disea + hlthg + hlthf + hlthp, data=randdata,\n                           dist = \"poisson\", link = \"logit\")\nstargazer(model.hurdle,model.zeroinfl,zero.component=FALSE,\n          no.space=TRUE,type=\"text\")## \n## ===========================================\n##                    Dependent variable:     \n##                ----------------------------\n##                           mdvis            \n##                    hurdle     zero-inflated\n##                                count data  \n##                     (1)            (2)     \n## -------------------------------------------\n## LC               -0.015***      -0.015***  \n##                   (0.003)        (0.003)   \n## idp              -0.085***      -0.086***  \n##                   (0.011)        (0.011)   \n## lpi               0.010***      0.010***   \n##                   (0.002)        (0.002)   \n## fmde             -0.021***      -0.021***  \n##                   (0.002)        (0.002)   \n## physlm            0.231***      0.231***   \n##                   (0.012)        (0.012)   \n## disea             0.022***      0.022***   \n##                   (0.001)        (0.001)   \n## hlthg             0.027***      0.026***   \n##                   (0.010)        (0.010)   \n## hlthf             0.147***      0.146***   \n##                   (0.016)        (0.016)   \n## hlthp             0.304***      0.303***   \n##                   (0.026)        (0.026)   \n## Constant          1.133***      1.133***   \n##                   (0.012)        (0.012)   \n## -------------------------------------------\n## Observations       20,190        20,190    \n## Log Likelihood  -54,772.100    -54,772.550 \n## ===========================================\n## Note:           *p<0.1; **p<0.05; ***p<0.01\nstargazer(model.hurdle,model.zeroinfl,zero.component=TRUE,\n          no.space=TRUE,type=\"text\")## \n## ===========================================\n##                    Dependent variable:     \n##                ----------------------------\n##                           mdvis            \n##                    hurdle     zero-inflated\n##                                count data  \n##                     (1)            (2)     \n## -------------------------------------------\n## LC               -0.150***      0.154***   \n##                   (0.010)        (0.011)   \n## idp              -0.631***      0.637***   \n##                   (0.038)        (0.040)   \n## lpi               0.102***      -0.105***  \n##                   (0.007)        (0.007)   \n## fmde             -0.062***      0.060***   \n##                   (0.006)        (0.006)   \n## physlm            0.239***      -0.203***  \n##                   (0.056)        (0.058)   \n## disea             0.062***      -0.059***  \n##                   (0.003)        (0.003)   \n## hlthg            -0.142***      0.158***   \n##                   (0.034)        (0.036)   \n## hlthf            -0.352***      0.396***   \n##                   (0.062)        (0.064)   \n## hlthp              -0.181         0.233    \n##                   (0.149)        (0.151)   \n## Constant          0.411***      -0.528***  \n##                   (0.044)        (0.047)   \n## -------------------------------------------\n## Observations       20,190        20,190    \n## Log Likelihood  -54,772.100    -54,772.550 \n## ===========================================\n## Note:           *p<0.1; **p<0.05; ***p<0.01\n# Test whether LC is important in the model:\nmodel.hurdle.reduced <- update(model.hurdle,.~.-LC)\nlmtest::waldtest(model.hurdle, model.hurdle.reduced)## Wald test\n## \n## Model 1: mdvis ~ LC + idp + lpi + fmde + physlm + disea + hlthg + hlthf + \n##     hlthp\n## Model 2: mdvis ~ idp + lpi + fmde + physlm + disea + hlthg + hlthf + hlthp\n##   Res.Df Df  Chisq Pr(>Chisq)    \n## 1  20170                         \n## 2  20172 -2 247.64  < 2.2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\npredprob.hurdle   <- predprob(model.hurdle)\npredprob.zeroinfl <- predprob(model.zeroinfl)\nprint(rbind(prop.table.data[1:6],\n  apply(predprob.poisson[,1:6],2,mean),\n  apply(predprob.nb[,1:6],2,mean),\n  apply(predprob.hurdle[,1:6],2,mean),\n  apply(predprob.zeroinfl[,1:6],2,mean)))##              0          1         2          3          4          5\n## [1,] 0.3124319 0.18905399 0.1385339 0.09331352 0.06661714 0.04794453\n## [2,] 0.1220592 0.22303277 0.2271621 0.17173478 0.10888940 0.06244353\n## [3,] 0.3486824 0.18846395 0.1219340 0.08385899 0.05968603 0.04350209\n## [4,] 0.3124319 0.06056959 0.1083120 0.13262624 0.12553899 0.09847017\n## [5,] 0.3124684 0.06053026 0.1082799 0.13262562 0.12556531 0.09850218"},{"path":"append.html","id":"append","chapter":"4 Appendix","heading":"4 Appendix","text":"","code":""},{"path":"append.html","id":"PCAapp","chapter":"4 Appendix","heading":"4.1 Principal component analysis (PCA)","text":"Principal component analysis (PCA) classical easy--use statistical method reduce dimension large datasets containing variables linearly driven relatively small number factors. approach widely used data analysis image compression.Suppose \\(T\\) observations \\(n\\)-dimensional random vector \\(x\\), denoted \\(x_{1},x_{2},\\ldots,x_{T}\\). suppose component \\(x\\) mean zero.Denote \\(X\\) matrix given \\(\\left[\\begin{array}{cccc} x_{1} & x_{2} & \\ldots & x_{T}\\end{array}\\right]'\\). Denote \\(j^{th}\\) column \\(X\\) \\(X_{j}\\).want find linear combination \\(x_{}\\)’s (\\(x.u\\)), \\(\\left\\Vert u\\right\\Vert =1\\), “maximum variance.” , want solve:\n\\[\\begin{equation}\n\\begin{array}{clll}\n\\underset{u}{\\arg\\max} & u'X'Xu. \\\\\n\\mbox{s.t. } & \\left| u\\right| =1\n\\end{array}\\tag{4.1}\n\\end{equation}\\]Since \\(X'X\\) positive definite matrix, admits following decomposition:\n\\[\\begin{eqnarray*}\nX'X & = & PDP'\\\\\n& = & P\\left[\\begin{array}{ccc}\n\\lambda_{1}\\\\\n& \\ddots\\\\\n&  & \\lambda_{n}\n\\end{array}\\right]P',\n\\end{eqnarray*}\\]\n\\(P\\) orthogonal matrix whose columns eigenvectors \\(X'X\\).can order eigenvalues \\(\\lambda_{1}\\geq\\ldots\\geq\\lambda_{n}\\). (Since \\(X'X\\) positive definite, eigenvalues positive.)Since \\(P\\) orthogonal, \\(u'X'Xu=u'PDP'u=y'Dy\\) \\(\\left\\Vert y\\right\\Vert =1\\). Therefore, \\(y_{}^{2}\\leq 1\\) \\(\\leq n\\).consequence:\n\\[\ny'Dy=\\sum_{=1}^{n}y_{}^{2}\\lambda_{}\\leq\\lambda_{1}\\sum_{=1}^{n}y_{}^{2}=\\lambda_{1}.\n\\]easily seen maximum reached \\(y=\\left[1,0,\\cdots,0\\right]'\\). Therefore, maximum optimization program (Eq. (4.1)) obtained \\(u=P\\left[1,0,\\cdots,0\\right]'\\). , \\(u\\) eigenvector \\(X'X\\) associated larger eigenvalue (first column \\(P\\)).Let us denote \\(F\\) vector given matrix product \\(XP\\) (note last column equal \\(Xu\\)). columns \\(F\\), denoted \\(F_{j}\\), called factors. :\n\\[\nF'F=P'X'XP=D.\n\\]\nTherefore, particular, \\(F_{j}\\)’s orthogonal.Since \\(X=FP'\\), \\(X_{j}\\)’s linear combinations factors. Let us denote \\(\\hat{X}_{,j}\\) part \\(X_{}\\) explained factor \\(F_{j}\\), :\n\\[\\begin{eqnarray*}\n\\hat{X}_{,j} & = & p_{ij}F_{j}\\\\\nX_{} & = & \\sum_{j}\\hat{X}_{,j}=\\sum_{j}p_{ij}F_{j}.\n\\end{eqnarray*}\\]Consider share variance explained –\\(n\\) variables (\\(X_{1},\\ldots,X_{n}\\))– first factor \\(F_{1}\\):\n\\[\\begin{eqnarray*}\n\\frac{\\sum_{}\\hat{X}_{,1}\\hat{X}'_{,1}}{\\sum_{}X_{}X'_{}} & = & \\frac{\\sum_{}p_{i1}F_{1}F'_{1}p_{i1}}{tr(X'X)} = \\frac{\\sum_{}p_{i1}^{2}\\lambda_{1}}{tr(X'X)} = \\frac{\\lambda_{1}}{\\sum_{}\\lambda_{}}.\n\\end{eqnarray*}\\]Intuitively, first eigenvalue large, means first factor embed large share fluctutaions \\(n\\) \\(X_{}\\)’s.Let us illustrate PCA term structure yields. term strucutre yields (yield curve) know driven small number factors (e.g., Litterman Scheinkman (1991)). One can typically employ PCA recover factors. data used example taken Fred database (tickers: “DGS6MO”,“DGS1”, …). second plot shows factor loardings, indicate first factor level factor (loadings = black line), second factor slope factor (loadings = blue line), third factor curvature factor (loadings = red line).run PCA, one simply apply function prcomp matrix data:Let us know visualize results. first plot Figure 4.1 shows share total variance explained different principal components (PCs). second plot shows facotr loadings. two bottom plots show yields (black) fitted linear combinations first two PCs .\nFigure 4.1: PCA results. dataset contains 8 time series U.S. interest rates different maturities.\n","code":"\nlibrary(AEC)\nUSyields <- USyields[complete.cases(USyields),]\nyds <- USyields[c(\"Y1\",\"Y2\",\"Y3\",\"Y5\",\"Y7\",\"Y10\",\"Y20\",\"Y30\")]\nPCA.yds <- prcomp(yds,center=TRUE,scale. = TRUE)\npar(mfrow=c(2,2))\npar(plt=c(.1,.95,.2,.8))\nbarplot(PCA.yds$sdev^2/sum(PCA.yds$sdev^2),\n        main=\"Share of variance expl. by PC's\")\naxis(1, at=1:dim(yds)[2], labels=colnames(PCA.yds$x))\nnb.PC <- 2\nplot(-PCA.yds$rotation[,1],type=\"l\",lwd=2,ylim=c(-1,1),\n     main=\"Factor loadings (1st 3 PCs)\",xaxt=\"n\",xlab=\"\")\naxis(1, at=1:dim(yds)[2], labels=colnames(yds))\nlines(PCA.yds$rotation[,2],type=\"l\",lwd=2,col=\"blue\")\nlines(PCA.yds$rotation[,3],type=\"l\",lwd=2,col=\"red\")\nY1.hat <- PCA.yds$x[,1:nb.PC] %*% PCA.yds$rotation[\"Y1\",1:2]\nY1.hat <- mean(USyields$Y1) + sd(USyields$Y1) * Y1.hat\nplot(USyields$date,USyields$Y1,type=\"l\",lwd=2,\n     main=\"Fit of 1-year yields (2 PCs)\",\n     ylab=\"Obs (black) / Fitted by 2PCs (dashed blue)\")\nlines(USyields$date,Y1.hat,col=\"blue\",lty=2,lwd=2)\nY10.hat <- PCA.yds$x[,1:nb.PC] %*% PCA.yds$rotation[\"Y10\",1:2]\nY10.hat <- mean(USyields$Y10) + sd(USyields$Y10) * Y10.hat\nplot(USyields$date,USyields$Y10,type=\"l\",lwd=2,\n     main=\"Fit of 10-year yields (2 PCs)\",\n     ylab=\"Obs (black) / Fitted by 2PCs (dashed blue)\")\nlines(USyields$date,Y10.hat,col=\"blue\",lty=2,lwd=2)"},{"path":"append.html","id":"LinAlgebra","chapter":"4 Appendix","heading":"4.2 Linear algebra: definitions and results","text":"Definition 4.1  (Eigenvalues) eigenvalues matrix \\(M\\) numbers \\(\\lambda\\) :\n\\[\n|M - \\lambda | = 0,\n\\]\n\\(| \\bullet |\\) determinant operator.Proposition 4.1  (Properties determinant) :\\(|MN|=|M|\\times|N|\\).\\(|M^{-1}|=|M|^{-1}\\).\\(M\\) admits diagonal representation \\(M=TDT^{-1}\\), \\(D\\) diagonal matrix whose diagonal entries \\(\\{\\lambda_i\\}_{=1,\\dots,n}\\), :\n\\[\n|M - \\lambda |=\\prod_{=1}^n (\\lambda_i - \\lambda).\n\\]Definition 4.2  (Moore-Penrose inverse) \\(M \\\\mathbb{R}^{m \\times n}\\), Moore-Penrose pseudo inverse (exists ) unique matrix \\(M^* \\\\mathbb{R}^{n \\times m}\\) satisfies:\\(M M^* M = M\\)\\(M^* M M^* = M^*\\)\\((M M^*)'=M M^*\\)\n.iv \\((M^* M)'=M^* M\\).Proposition 4.2  (Properties Moore-Penrose inverse) \\(M\\) invertible \\(M^* = M^{-1}\\).pseudo-inverse zero matrix transpose.\n*\npseudo-inverse pseudo-inverse original matrix.Definition 4.3  (Idempotent matrix) Matrix \\(M\\) idempotent \\(M^2=M\\).\\(M\\) symmetric idempotent matrix, \\(M'M=M\\).Proposition 4.3  (Roots idempotent matrix) eigenvalues idempotent matrix either 1 0.Proof. \\(\\lambda\\) eigenvalue idempotent matrix \\(M\\) \\(\\exists x \\ne 0\\) s.t. \\(Mx=\\lambda x\\). Hence \\(M^2x=\\lambda M x \\Rightarrow (1-\\lambda)Mx=0\\). Either element \\(Mx\\) zero, case \\(\\lambda=0\\) least one element \\(Mx\\) nonzero, case \\(\\lambda=1\\).Proposition 4.4  (Idempotent matrix chi-square distribution) rank symmetric idempotent matrix equal trace.Proof. result follows Prop. 4.3, combined fact rank symmetric matrix equal number nonzero eigenvalues.Proposition 4.5  (Constrained least squares) solution following optimisation problem:\n\\[\\begin{eqnarray*}\n\\underset{\\boldsymbol\\beta}{\\min} && || \\mathbf{y} - \\mathbf{X}\\boldsymbol\\beta ||^2 \\\\\n&& \\mbox{subject } \\mathbf{R}\\boldsymbol\\beta = \\mathbf{q}\n\\end{eqnarray*}\\]\ngiven :\n\\[\n\\boxed{\\boldsymbol\\beta^r = \\boldsymbol\\beta_0 - (\\mathbf{X}'\\mathbf{X})^{-1} \\mathbf{R}'\\{\\mathbf{R}(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{R}'\\}^{-1}(\\mathbf{R}\\boldsymbol\\beta_0 - \\mathbf{q}),}\n\\]\n\\(\\boldsymbol\\beta_0=(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\mathbf{y}\\).Proof. See instance Jackman, 2007.Proposition 4.6  (Inverse partitioned matrix) :\n\\[\\begin{eqnarray*}\n&&\\left[ \\begin{array}{cc} \\mathbf{}_{11} & \\mathbf{}_{12} \\\\ \\mathbf{}_{21} & \\mathbf{}_{22} \\end{array}\\right]^{-1} = \\\\\n&&\\left[ \\begin{array}{cc} (\\mathbf{}_{11} - \\mathbf{}_{12}\\mathbf{}_{22}^{-1}\\mathbf{}_{21})^{-1} & - \\mathbf{}_{11}^{-1}\\mathbf{}_{12}(\\mathbf{}_{22} - \\mathbf{}_{21}\\mathbf{}_{11}^{-1}\\mathbf{}_{12})^{-1} \\\\\n-(\\mathbf{}_{22} - \\mathbf{}_{21}\\mathbf{}_{11}^{-1}\\mathbf{}_{12})^{-1}\\mathbf{}_{21}\\mathbf{}_{11}^{-1} & (\\mathbf{}_{22} - \\mathbf{}_{21}\\mathbf{}_{11}^{-1}\\mathbf{}_{12})^{-1} \\end{array} \\right].\n\\end{eqnarray*}\\]Definition 4.4  (Matrix derivatives) Consider fonction \\(f: \\mathbb{R}^K \\rightarrow \\mathbb{R}\\). first-order derivative :\n\\[\n\\frac{\\partial f}{\\partial \\mathbf{b}}(\\mathbf{b}) =\n\\left[\\begin{array}{c}\n\\frac{\\partial f}{\\partial b_1}(\\mathbf{b})\\\\\n\\vdots\\\\\n\\frac{\\partial f}{\\partial b_K}(\\mathbf{b})\n\\end{array}\n\\right].\n\\]\nuse notation:\n\\[\n\\frac{\\partial f}{\\partial \\mathbf{b}'}(\\mathbf{b}) = \\left(\\frac{\\partial f}{\\partial \\mathbf{b}}(\\mathbf{b})\\right)'.\n\\]Proposition 4.7  :\\(f(\\mathbf{b}) = ' \\mathbf{b}\\) \\(\\) \\(K \\times 1\\) vector \\(\\frac{\\partial f}{\\partial \\mathbf{b}}(\\mathbf{b}) = \\).\\(f(\\mathbf{b}) = \\mathbf{b}'\\mathbf{b}\\) \\(\\) \\(K \\times K\\) matrix, \\(\\frac{\\partial f}{\\partial \\mathbf{b}}(\\mathbf{b}) = 2A\\mathbf{b}\\).Proposition 4.8  (Square absolute summability) :\n\\[\n\\underbrace{\\sum_{=0}^{\\infty}|\\theta_i| < + \\infty}_{\\mbox{Absolute summability}} \\Rightarrow \\underbrace{\\sum_{=0}^{\\infty} \\theta_i^2 < + \\infty}_{\\mbox{Square summability}}.\n\\]Proof. See Appendix 3.Hamilton. Idea: Absolute summability implies exist \\(N\\) , \\(j>N\\), \\(|\\theta_j| < 1\\) (deduced Cauchy criterion, Theorem 4.2 therefore \\(\\theta_j^2 < |\\theta_j|\\).","code":""},{"path":"append.html","id":"variousResults","chapter":"4 Appendix","heading":"4.3 Statistical analysis: definitions and results","text":"","code":""},{"path":"append.html","id":"moments-and-statistics","chapter":"4 Appendix","heading":"4.3.1 Moments and statistics","text":"Definition 4.5  (Partial correlation) partial correlation \\(y\\) \\(z\\), controlling variables \\(\\mathbf{X}\\) sample correlation \\(y^*\\) \\(z^*\\), latter two variables residuals regressions \\(y\\) \\(\\mathbf{X}\\) \\(z\\) \\(\\mathbf{X}\\), respectively.correlation denoted \\(r_{yz}^\\mathbf{X}\\). definition, :\n\\[\\begin{equation}\nr_{yz}^\\mathbf{X} = \\frac{\\mathbf{z^*}'\\mathbf{y^*}}{\\sqrt{(\\mathbf{z^*}'\\mathbf{z^*})(\\mathbf{y^*}'\\mathbf{y^*})}}.\\tag{4.2}\n\\end{equation}\\]Definition 4.6  (Skewness kurtosis) Let \\(Y\\) random variable whose fourth moment exists. expectation \\(Y\\) denoted \\(\\mu\\).skewness \\(Y\\) given :\n\\[\n\\frac{\\mathbb{E}[(Y-\\mu)^3]}{\\{\\mathbb{E}[(Y-\\mu)^2]\\}^{3/2}}.\n\\]kurtosis \\(Y\\) given :\n\\[\n\\frac{\\mathbb{E}[(Y-\\mu)^4]}{\\{\\mathbb{E}[(Y-\\mu)^2]\\}^{2}}.\n\\]Theorem 4.1  (Cauchy-Schwarz inequality) :\n\\[\n|\\mathbb{C}ov(X,Y)| \\le \\sqrt{\\mathbb{V}ar(X)\\mathbb{V}ar(Y)}\n\\]\n, \\(X \\ne =\\) \\(Y \\ne 0\\), equality holds iff \\(X\\) \\(Y\\) affine transformation.Proof. \\(\\mathbb{V}ar(X)=0\\), trivial. case, let’s define \\(Z\\) \\(Z = Y - \\frac{\\mathbb{C}ov(X,Y)}{\\mathbb{V}ar(X)}X\\). easily seen \\(\\mathbb{C}ov(X,Z)=0\\). , variance \\(Y=Z+\\frac{\\mathbb{C}ov(X,Y)}{\\mathbb{V}ar(X)}X\\) equal sum variance \\(Z\\) variance \\(\\frac{\\mathbb{C}ov(X,Y)}{\\mathbb{V}ar(X)}X\\), :\n\\[\n\\mathbb{V}ar(Y) = \\mathbb{V}ar(Z) + \\left(\\frac{\\mathbb{C}ov(X,Y)}{\\mathbb{V}ar(X)}\\right)^2\\mathbb{V}ar(X) \\ge \\left(\\frac{\\mathbb{C}ov(X,Y)}{\\mathbb{V}ar(X)}\\right)^2\\mathbb{V}ar(X).\n\\]\nequality holds iff \\(\\mathbb{V}ar(Z)=0\\), .e. iff \\(Y = \\frac{\\mathbb{C}ov(X,Y)}{\\mathbb{V}ar(X)}X+cst\\).Definition 4.7  (Asymptotic level) asymptotic test critical region \\(\\Omega_n\\) asymptotic level equal \\(\\alpha\\) :\n\\[\n\\underset{\\theta \\\\Theta}{\\mbox{sup}} \\quad \\underset{n \\rightarrow \\infty}{\\mbox{lim}} \\mathbb{P}_\\theta (S_n \\\\Omega_n) = \\alpha,\n\\]\n\\(S_n\\) test statistic \\(\\Theta\\) null hypothesis \\(H_0\\) equivalent \\(\\theta \\\\Theta\\).Definition 4.8  (Asymptotically consistent test) asymptotic test critical region \\(\\Omega_n\\) consistent :\n\\[\n\\forall \\theta \\\\Theta^c, \\quad \\mathbb{P}_\\theta (S_n \\\\Omega_n) \\rightarrow 1,\n\\]\n\\(S_n\\) test statistic \\(\\Theta^c\\) null hypothesis \\(H_0\\) equivalent \\(\\theta \\notin \\Theta^c\\).Definition 4.9  (Kullback discrepancy) Given two p.d.f. \\(f\\) \\(f^*\\), Kullback discrepancy defined :\n\\[\n(f,f^*) = \\mathbb{E}^* \\left( \\log \\frac{f^*(Y)}{f(Y)} \\right) = \\int \\log \\frac{f^*(y)}{f(y)} f^*(y) dy.\n\\]Proposition 4.9  (Properties Kullback discrepancy) :\\((f,f^*) \\ge 0\\)\\((f,f^*) = 0\\) iff \\(f \\equiv f^*\\).Proof. \\(x \\rightarrow -\\log(x)\\) convex function. Therefore \\(\\mathbb{E}^*(-\\log f(Y)/f^*(Y)) \\ge -\\log \\mathbb{E}^*(f(Y)/f^*(Y)) = 0\\) (proves ()). Since \\(x \\rightarrow -\\log(x)\\) strictly convex, equality () holds \\(f(Y)/f^*(Y)\\) constant (proves (ii)).Definition 4.10  (Characteristic function) real-valued random variable \\(X\\), characteristic function defined :\n\\[\n\\phi_X: u \\rightarrow \\mathbb{E}[\\exp(iuX)].\n\\]","code":""},{"path":"append.html","id":"standard-distributions","chapter":"4 Appendix","heading":"4.3.2 Standard distributions","text":"Definition 4.11  (F distribution) Consider \\(n=n_1+n_2\\) ..d. \\(\\mathcal{N}(0,1)\\) r.v. \\(X_i\\). r.v. \\(F\\) defined :\n\\[\nF = \\frac{\\sum_{=1}^{n_1} X_i^2}{\\sum_{j=n_1+1}^{n_1+n_2} X_j^2}\\frac{n_2}{n_1}\n\\]\n\\(F \\sim \\mathcal{F}(n_1,n_2)\\). (See Table 4.4 quantiles.)Definition 4.12  (Student-t distribution) \\(Z\\) follows Student-t (\\(t\\)) distribution \\(\\nu\\) degrees freedom (d.f.) :\n\\[\nZ = X_0 \\bigg/ \\sqrt{\\frac{\\sum_{=1}^{\\nu}X_i^2}{\\nu}}, \\quad X_i \\sim ..d. \\mathcal{N}(0,1).\n\\]\n\\(\\mathbb{E}(Z)=0\\), \\(\\mathbb{V}ar(Z)=\\frac{\\nu}{\\nu-2}\\) \\(\\nu>2\\). (See Table 4.2 quantiles.)Definition 4.13  (Chi-square distribution) \\(Z\\) follows \\(\\chi^2\\) distribution \\(\\nu\\) d.f. \\(Z = \\sum_{=1}^{\\nu}X_i^2\\) \\(X_i \\sim ..d. \\mathcal{N}(0,1)\\).\n\\(\\mathbb{E}(Z)=\\nu\\). (See Table 4.3 quantiles.)\nFigure 4.2: Pdf Cauchy distribution (\\(\\mu=0\\), \\(\\gamma=1\\)).\nProposition 4.10  (Inner product multivariate Gaussian variable) Let \\(X\\) \\(n\\)-dimensional multivariate Gaussian variable: \\(X \\sim \\mathcal{N}(0,\\Sigma)\\). :\n\\[\nX' \\Sigma^{-1}X \\sim \\chi^2(n).\n\\]Proof. \\(\\Sigma\\) symmetrical definite positive matrix, admits spectral decomposition \\(PDP'\\) \\(P\\) orthogonal matrix (.e. \\(PP'=Id\\)) D diagonal matrix non-negative entries. Denoting \\(\\sqrt{D^{-1}}\\) diagonal matrix whose diagonal entries inverse \\(D\\), easily checked covariance matrix \\(Y:=\\sqrt{D^{-1}}P'X\\) \\(Id\\). Therefore \\(Y\\) vector uncorrelated Gaussian variables. properties Gaussian variables imply components \\(Y\\) also independent. Hence \\(Y'Y=\\sum_i Y_i^2 \\sim \\chi^2(n)\\).remains note \\(Y'Y=X'PD^{-1}P'X=X'\\mathbb{V}ar(X)^{-1}X\\) conclude.Definition 4.15  (Generalized Extreme Value (GEV) distribution) vector disturbances \\(\\boldsymbol\\varepsilon=[\\varepsilon_{1,1},\\dots,\\varepsilon_{1,K_1},\\dots,\\varepsilon_{J,1},\\dots,\\varepsilon_{J,K_J}]'\\) follows Generalized Extreme Value (GEV) distribution c.d.f. :\n\\[\nF(\\boldsymbol\\varepsilon,\\boldsymbol\\rho) = \\exp(-G(e^{-\\varepsilon_{1,1}},\\dots,e^{-\\varepsilon_{J,K_J}};\\boldsymbol\\rho))\n\\]\n\n\\[\\begin{eqnarray*}\nG(\\mathbf{Y};\\boldsymbol\\rho) &\\equiv&  G(Y_{1,1},\\dots,Y_{1,K_1},\\dots,Y_{J,1},\\dots,Y_{J,K_J};\\boldsymbol\\rho) \\\\\n&=& \\sum_{j=1}^J\\left(\\sum_{k=1}^{K_j} Y_{jk}^{1/\\rho_j}\n\\right)^{\\rho_j}\n\\end{eqnarray*}\\]","code":""},{"path":"append.html","id":"StochConvergences","chapter":"4 Appendix","heading":"4.3.3 Stochastic convergences","text":"Proposition 4.11  (Chebychev's inequality) \\(\\mathbb{E}(|X|^r)\\) finite \\(r>0\\) :\n\\[\n\\forall \\varepsilon > 0, \\quad \\mathbb{P}(|X - c|>\\varepsilon) \\le \\frac{\\mathbb{E}[|X - c|^r]}{\\varepsilon^r}.\n\\]\nparticular, \\(r=2\\):\n\\[\n\\forall \\varepsilon > 0, \\quad \\mathbb{P}(|X - c|>\\varepsilon) \\le \\frac{\\mathbb{E}[(X - c)^2]}{\\varepsilon^2}.\n\\]Proof. Remark \\(\\varepsilon^r \\mathbb{}_{\\{|X| \\ge \\varepsilon\\}} \\le |X|^r\\) take expectation sides.Definition 4.16  (Convergence probability) random variable sequence \\(x_n\\) converges probability constant \\(c\\) \\(\\forall \\varepsilon\\), \\(\\lim_{n \\rightarrow \\infty} \\mathbb{P}(|x_n - c|>\\varepsilon) = 0\\).denoted : \\(\\mbox{plim } x_n = c\\).Definition 4.17  (Convergence Lr norm) \\(x_n\\) converges \\(r\\)-th mean (\\(L^r\\)-norm) towards \\(x\\), \\(\\mathbb{E}(|x_n|^r)\\) \\(\\mathbb{E}(|x|^r)\\) exist \n\\[\n\\lim_{n \\rightarrow \\infty} \\mathbb{E}(|x_n - x|^r) = 0.\n\\]\ndenoted : \\(x_n \\overset{L^r}{\\rightarrow} c\\).\\(r=2\\), convergence called mean square convergence.Definition 4.18  (Almost sure convergence) random variable sequence \\(x_n\\) converges almost surely \\(c\\) \\(\\mathbb{P}(\\lim_{n \\rightarrow \\infty} x_n = c) = 1\\).denoted : \\(x_n \\overset{.s.}{\\rightarrow} c\\).Definition 4.19  (Convergence distribution) \\(x_n\\) said converge distribution (law) \\(x\\) \n\\[\n\\lim_{n \\rightarrow \\infty} F_{x_n}(s) = F_{x}(s)\n\\]\n\\(s\\) \\(F_X\\) –cumulative distribution \\(X\\)– continuous.denoted : \\(x_n \\overset{d}{\\rightarrow} x\\).Proposition 4.12  (Rules limiting distributions (Slutsky)) :Slutsky’s theorem: \\(x_n \\overset{d}{\\rightarrow} x\\) \\(y_n \\overset{p}{\\rightarrow} c\\) \n\\[\\begin{eqnarray*}\nx_n y_n &\\overset{d}{\\rightarrow}& x c \\\\\nx_n + y_n &\\overset{d}{\\rightarrow}& x + c \\\\\nx_n/y_n &\\overset{d}{\\rightarrow}& x / c \\quad (\\mbox{}c \\ne 0)\n\\end{eqnarray*}\\]Slutsky’s theorem: \\(x_n \\overset{d}{\\rightarrow} x\\) \\(y_n \\overset{p}{\\rightarrow} c\\) \n\\[\\begin{eqnarray*}\nx_n y_n &\\overset{d}{\\rightarrow}& x c \\\\\nx_n + y_n &\\overset{d}{\\rightarrow}& x + c \\\\\nx_n/y_n &\\overset{d}{\\rightarrow}& x / c \\quad (\\mbox{}c \\ne 0)\n\\end{eqnarray*}\\]Continuous mapping theorem: \\(x_n \\overset{d}{\\rightarrow} x\\) \\(g\\) continuous function \\(g(x_n) \\overset{d}{\\rightarrow} g(x).\\)Continuous mapping theorem: \\(x_n \\overset{d}{\\rightarrow} x\\) \\(g\\) continuous function \\(g(x_n) \\overset{d}{\\rightarrow} g(x).\\)Proposition 4.13  (Implications stochastic convergences) :\n\\[\\begin{align*}\n&\\boxed{\\overset{L^s}{\\rightarrow}}& &\\underset{1 \\le r \\le s}{\\Rightarrow}& &\\boxed{\\overset{L^r}{\\rightarrow}}&\\\\\n&& && &\\Downarrow&\\\\\n&\\boxed{\\overset{.s.}{\\rightarrow}}& &\\Rightarrow& &\\boxed{\\overset{p}{\\rightarrow}}& \\Rightarrow \\qquad \\boxed{\\overset{d}{\\rightarrow}}.\n\\end{align*}\\]Proof. (fact \\(\\left(\\overset{p}{\\rightarrow}\\right) \\Rightarrow \\left( \\overset{d}{\\rightarrow}\\right)\\)). Assume \\(X_n \\overset{p}{\\rightarrow} X\\). Denoting \\(F\\) \\(F_n\\) c.d.f. \\(X\\) \\(X_n\\), respectively:\n\\[\\begin{eqnarray*}\nF_n(x) &=& \\mathbb{P}(X_n \\le x,X\\le x+\\varepsilon) + \\mathbb{P}(X_n \\le x,X > x+\\varepsilon)\\\\\n&\\le& F(x+\\varepsilon) + \\mathbb{P}(|X_n - X|>\\varepsilon).\\tag{4.3}\n\\end{eqnarray*}\\]\nBesides,\n\\[\\begin{eqnarray*}\nF(x-\\varepsilon) &=& \\mathbb{P}(X \\le x-\\varepsilon,X_n \\le x) + \\mathbb{P}(X \\le x-\\varepsilon,X_n > x)\\\\\n&\\le& F_n(x) + \\mathbb{P}(|X_n - X|>\\varepsilon),\n\\end{eqnarray*}\\]\nimplies:\n\\[\\begin{equation}\nF(x-\\varepsilon) - \\mathbb{P}(|X_n - X|>\\varepsilon) \\le F_n(x).\\tag{4.4}\n\\end{equation}\\]\nEqs. (4.3) (4.4) imply:\n\\[\nF(x-\\varepsilon) - \\mathbb{P}(|X_n - X|>\\varepsilon) \\le F_n(x)  \\le F(x+\\varepsilon) + \\mathbb{P}(|X_n - X|>\\varepsilon).\n\\]\nTaking limits \\(n \\rightarrow \\infty\\) yields\n\\[\nF(x-\\varepsilon) \\le \\underset{n \\rightarrow \\infty}{\\mbox{lim inf}}\\; F_n(x) \\le \\underset{n \\rightarrow \\infty}{\\mbox{lim sup}}\\; F_n(x)  \\le F(x+\\varepsilon).\n\\]\nresult obtained taking limits \\(\\varepsilon \\rightarrow 0\\) (\\(F\\) continuous \\(x\\)).Proposition 4.14  (Convergence distribution constant) \\(X_n\\) converges distribution constant \\(c\\), \\(X_n\\) converges probability \\(c\\).Proof. \\(\\varepsilon>0\\), \\(\\mathbb{P}(X_n < c - \\varepsilon) \\underset{n \\rightarrow \\infty}{\\rightarrow} 0\\) .e. \\(\\mathbb{P}(X_n \\ge c - \\varepsilon) \\underset{n \\rightarrow \\infty}{\\rightarrow} 1\\) \\(\\mathbb{P}(X_n < c + \\varepsilon) \\underset{n \\rightarrow \\infty}{\\rightarrow} 1\\). Therefore \\(\\mathbb{P}(c - \\varepsilon \\le X_n < c + \\varepsilon) \\underset{n \\rightarrow \\infty}{\\rightarrow} 1\\),\ngives result.Example 4.1  (Convergence probability $L^r$) Let \\(\\{x_n\\}_{n \\\\mathbb{N}}\\) series random variables defined :\n\\[\nx_n = n u_n,\n\\]\n\\(u_n\\) independent random variables s.t. \\(u_n \\sim \\mathcal{B}(1/n)\\).\\(x_n \\overset{p}{\\rightarrow} 0\\) \\(x_n \\overset{L^r}{\\nrightarrow} 0\\) \\(\\mathbb{E}(|X_n-0|)=\\mathbb{E}(X_n)=1\\).Theorem 4.2  (Cauchy criterion (non-stochastic case)) \\(\\sum_{=0}^{T} a_i\\) converges (\\(T \\rightarrow \\infty\\)) iff, \\(\\eta > 0\\), exists integer \\(N\\) , \\(M\\ge N\\),\n\\[\n\\left|\\sum_{=N+1}^{M} a_i\\right| < \\eta.\n\\]Theorem 4.3  (Cauchy criterion (stochastic case)) \\(\\sum_{=0}^{T} \\theta_i \\varepsilon_{t-}\\) converges mean square (\\(T \\rightarrow \\infty\\)) random variable iff, \\(\\eta > 0\\), exists integer \\(N\\) , \\(M\\ge N\\),\n\\[\n\\mathbb{E}\\left[\\left(\\sum_{=N+1}^{M} \\theta_i \\varepsilon_{t-}\\right)^2\\right] < \\eta.\n\\]","code":""},{"path":"append.html","id":"CLTappend","chapter":"4 Appendix","heading":"4.3.4 Central limit theorem","text":"Theorem 4.4  (Law large numbers) sample mean consistent estimator population mean.Proof. Let’s denote \\(\\phi_{X_i}\\) characteristic function r.v. \\(X_i\\). mean \\(X_i\\) \\(\\mu\\) Talyor expansion characteristic function :\n\\[\n\\phi_{X_i}(u) = \\mathbb{E}(\\exp(iuX)) = 1 + iu\\mu + o(u).\n\\]\nproperties characteristic function (see Def. 4.10) imply :\n\\[\n\\phi_{\\frac{1}{n}(X_1+\\dots+X_n)}(u) = \\prod_{=1}^{n} \\left(1 + \\frac{u}{n}\\mu + o\\left(\\frac{u}{n}\\right) \\right) \\rightarrow e^{iu\\mu}.\n\\]\nfacts () \\(e^{iu\\mu}\\) characteristic function constant \\(\\mu\\) (b) characteristic function uniquely characterises distribution imply sample mean converges distribution constant \\(\\mu\\), implies converges probability \\(\\mu\\).Theorem 4.5  (Lindberg-Levy Central limit theorem, CLT) \\(x_n\\) ..d. sequence random variables mean \\(\\mu\\) variance \\(\\sigma^2\\) (\\(\\]0,+\\infty[\\)), :\n\\[\n\\boxed{\\sqrt{n} (\\bar{x}_n - \\mu) \\overset{d}{\\rightarrow} \\mathcal{N}(0,\\sigma^2), \\quad \\mbox{} \\quad \\bar{x}_n = \\frac{1}{n} \\sum_{=1}^{n} x_i.}\n\\]Proof. Let us introduce r.v. \\(Y_n:= \\sqrt{n}(\\bar{X}_n - \\mu)\\). \\(\\phi_{Y_n}(u) = \\left[ \\mathbb{E}\\left( \\exp(\\frac{1}{\\sqrt{n}} u (X_1 - \\mu)) \\right) \\right]^n\\). :\n\\[\\begin{eqnarray*}\n&&\\left[ \\mathbb{E}\\left( \\exp\\left(\\frac{1}{\\sqrt{n}} u (X_1 - \\mu)\\right) \\right) \\right]^n\\\\\n&=& \\left[ \\mathbb{E}\\left( 1 + \\frac{1}{\\sqrt{n}} u (X_1 - \\mu) - \\frac{1}{2n} u^2 (X_1 - \\mu)^2 + o(u^2) \\right) \\right]^n \\\\\n&=& \\left( 1 - \\frac{1}{2n}u^2\\sigma^2 + o(u^2)\\right)^n.\n\\end{eqnarray*}\\]\nTherefore \\(\\phi_{Y_n}(u) \\underset{n \\rightarrow \\infty}{\\rightarrow} \\exp \\left( - \\frac{1}{2}u^2\\sigma^2 \\right)\\), characteristic function \\(\\mathcal{N}(0,\\sigma^2)\\).","code":""},{"path":"append.html","id":"GaussianVar","chapter":"4 Appendix","heading":"4.4 Some properties of Gaussian variables","text":"Proposition 4.15  \\(\\mathbf{}\\) idempotent \\(\\mathbf{x}\\) Gaussian, \\(\\mathbf{L}\\mathbf{x}\\) \\(\\mathbf{x}'\\mathbf{}\\mathbf{x}\\) independent \\(\\mathbf{L}\\mathbf{}=\\mathbf{0}\\).Proof. \\(\\mathbf{L}\\mathbf{}=\\mathbf{0}\\), two Gaussian vectors \\(\\mathbf{L}\\mathbf{x}\\) \\(\\mathbf{}\\mathbf{x}\\) independent. implies independence function \\(\\mathbf{L}\\mathbf{x}\\) function \\(\\mathbf{}\\mathbf{x}\\). results follows observation \\(\\mathbf{x}'\\mathbf{}\\mathbf{x}=(\\mathbf{}\\mathbf{x})'(\\mathbf{}\\mathbf{x})\\), function \\(\\mathbf{}\\mathbf{x}\\).Proposition 4.16  (Bayesian update vector Gaussian variables) \n\\[\n\\left[\n\\begin{array}{c}\nY_1\\\\\nY_2\n\\end{array}\n\\right]\n\\sim \\mathcal{N}\n\\left(0,\n\\left[\\begin{array}{cc}\n\\Omega_{11} & \\Omega_{12}\\\\\n\\Omega_{21} & \\Omega_{22}\n\\end{array}\\right]\n\\right),\n\\]\n\n\\[\nY_{2}|Y_{1} \\sim \\mathcal{N}\n\\left(\n\\Omega_{21}\\Omega_{11}^{-1}Y_{1},\\Omega_{22}-\\Omega_{21}\\Omega_{11}^{-1}\\Omega_{12}\n\\right).\n\\]\n\\[\nY_{1}|Y_{2} \\sim \\mathcal{N}\n\\left(\n\\Omega_{12}\\Omega_{22}^{-1}Y_{2},\\Omega_{11}-\\Omega_{12}\\Omega_{22}^{-1}\\Omega_{21}\n\\right).\n\\]Proposition 4.17  (Truncated distributions) \\(X\\) random variable distributed according p.d.f. \\(f\\), c.d.f. \\(F\\), infinite support. p.d.f. \\(X|\\le X < b\\) \n\\[\ng(x) = \\frac{f(x)}{F(b)-F()}\\mathbb{}_{\\{\\le x < b\\}},\n\\]\n\\(<b\\).partiucular, Gaussian variable \\(X \\sim \\mathcal{N}(\\mu,\\sigma^2)\\), \n\\[\nf(X=x|\\le X<b) = \\dfrac{\\dfrac{1}{\\sigma}\\phi\\left(\\dfrac{x - \\mu}{\\sigma}\\right)}{Z}.\n\\]\n\\(Z = \\Phi(\\beta)-\\Phi(\\alpha)\\), \\(\\alpha = \\dfrac{- \\mu}{\\sigma}\\) \\(\\beta = \\dfrac{b - \\mu}{\\sigma}\\).Moreover:\n\\[\\begin{eqnarray}\n\\mathbb{E}(X|\\le X<b) &=& \\mu - \\frac{\\phi\\left(\\beta\\right)-\\phi\\left(\\alpha\\right)}{Z}\\sigma. \\tag{4.5}\n\\end{eqnarray}\\]also :\n\\[\\begin{eqnarray}\n&& \\mathbb{V}ar(X|\\le X<b) \\nonumber\\\\\n&=& \\sigma^2\\left[\n1 -  \\frac{\\beta\\phi\\left(\\beta\\right)-\\alpha\\phi\\left(\\alpha\\right)}{Z} -  \\left(\\frac{\\phi\\left(\\beta\\right)-\\phi\\left(\\alpha\\right)}{Z}\\right)^2 \\right] \\tag{4.6}\n\\end{eqnarray}\\]particular, \\(b \\rightarrow \\infty\\), get:\n\\[\\begin{equation}\n\\mathbb{V}ar(X|< X) = \\sigma^2\\left[1 + \\alpha\\lambda(-\\alpha) - \\lambda(-\\alpha)^2 \\right], \\tag{4.7}\n\\end{equation}\\]\n\\(\\lambda(x)=\\dfrac{\\phi(x)}{\\Phi(x)}\\) called inverse Mills ratio.Consider case \\(\\rightarrow - \\infty\\) (.e. conditioning set \\(X<b\\)) \\(\\mu=0\\), \\(\\sigma=1\\). Eq. (4.5) gives \\(\\mathbb{E}(X|X<b) = - \\lambda(b) = - \\dfrac{\\phi(b)}{\\Phi(b)}\\), \\(\\lambda\\) function computing inverse Mills ratio.\nFigure 4.3: \\(\\mathbb{E}(X|X<b)\\) function \\(b\\) \\(X\\sim \\mathcal{N}(0,1)\\) (black).\nProposition 4.18  (p.d.f. multivariate Gaussian variable) \\(Y \\sim \\mathcal{N}(\\mu,\\Omega)\\) \\(Y\\) \\(n\\)-dimensional vector, density function \\(Y\\) :\n\\[\n\\frac{1}{(2 \\pi)^{n/2}|\\Omega|^{1/2}}\\exp\\left[-\\frac{1}{2}\\left(Y-\\mu\\right)'\\Omega^{-1}\\left(Y-\\mu\\right)\\right].\n\\]","code":""},{"path":"append.html","id":"AppendixProof","chapter":"4 Appendix","heading":"4.5 Proofs","text":"Proof Proposition 2.4Proof. Assumptions () (ii) (set Assumptions 2.1) imply \\(\\boldsymbol\\theta_{MLE}\\) exists (\\(=\\mbox{argmax}_\\theta (1/n)\\log \\mathcal{L}(\\boldsymbol\\theta;\\mathbf{y})\\)).\\((1/n)\\log \\mathcal{L}(\\boldsymbol\\theta;\\mathbf{y})\\) can interpreted sample mean r.v. \\(\\log f(Y_i;\\boldsymbol\\theta)\\) ..d. Therefore \\((1/n)\\log \\mathcal{L}(\\boldsymbol\\theta;\\mathbf{y})\\) converges \\(\\mathbb{E}_{\\boldsymbol\\theta_0}(\\log f(Y;\\boldsymbol\\theta))\\) – exists (Assumption iv).latter convergence uniform (Assumption v), solution \\(\\boldsymbol\\theta_{MLE}\\) almost surely converges solution limit problem:\n\\[\n\\mbox{argmax}_\\theta \\mathbb{E}_{\\boldsymbol\\theta_0}(\\log f(Y;\\boldsymbol\\theta)) = \\mbox{argmax}_\\theta \\int_{\\mathcal{Y}} \\log f(y;\\boldsymbol\\theta)f(y;\\boldsymbol\\theta_0) dy.\n\\]Properties Kullback information measure (see Prop. 4.9), together identifiability assumption (ii) implies solution limit problem unique equal \\(\\boldsymbol\\theta_0\\).Consider r.v. sequence \\(\\boldsymbol\\theta\\) converges \\(\\boldsymbol\\theta_0\\). Taylor expansion score neighborood \\(\\boldsymbol\\theta_0\\) yields :\n\\[\n\\frac{\\partial \\log \\mathcal{L}(\\boldsymbol\\theta;\\mathbf{y})}{\\partial \\boldsymbol\\theta} = \\frac{\\partial \\log \\mathcal{L}(\\boldsymbol\\theta_0;\\mathbf{y})}{\\partial \\boldsymbol\\theta} + \\frac{\\partial^2 \\log \\mathcal{L}(\\boldsymbol\\theta_0;\\mathbf{y})}{\\partial \\boldsymbol\\theta \\partial \\boldsymbol\\theta'}(\\boldsymbol\\theta - \\boldsymbol\\theta_0) + o_p(\\boldsymbol\\theta - \\boldsymbol\\theta_0)\n\\]\\(\\boldsymbol\\theta_{MLE}\\) converges \\(\\boldsymbol\\theta_0\\) satisfies likelihood equation \\(\\frac{\\partial \\log \\mathcal{L}(\\boldsymbol\\theta;\\mathbf{y})}{\\partial \\boldsymbol\\theta} = \\mathbf{0}\\). Therefore:\n\\[\n\\frac{\\partial \\log \\mathcal{L}(\\boldsymbol\\theta_0;\\mathbf{y})}{\\partial \\boldsymbol\\theta} \\approx - \\frac{\\partial^2 \\log \\mathcal{L}(\\boldsymbol\\theta_0;\\mathbf{y})}{\\partial \\boldsymbol\\theta \\partial \\boldsymbol\\theta'}(\\boldsymbol\\theta_{MLE} - \\boldsymbol\\theta_0),\n\\]\nequivalently:\n\\[\n\\frac{1}{\\sqrt{n}} \\frac{\\partial \\log \\mathcal{L}(\\boldsymbol\\theta_0;\\mathbf{y})}{\\partial \\boldsymbol\\theta} \\approx\n\\left(- \\frac{1}{n} \\sum_{=1}^n \\frac{\\partial^2 \\log f(y_i;\\boldsymbol\\theta_0)}{\\partial \\boldsymbol\\theta \\partial \\boldsymbol\\theta'} \\right)\\sqrt{n}(\\boldsymbol\\theta_{MLE} - \\boldsymbol\\theta_0),\n\\]law large numbers, : \\(\\left(- \\frac{1}{n} \\sum_{=1}^n \\frac{\\partial^2 \\log f(y_i;\\boldsymbol\\theta_0)}{\\partial \\boldsymbol\\theta \\partial \\boldsymbol\\theta'} \\right) \\overset{}\\rightarrow \\frac{1}{n} \\mathbf{}(\\boldsymbol\\theta_0) = \\mathcal{}_Y(\\boldsymbol\\theta_0)\\).Besides, :\n\\[\\begin{eqnarray*}\n\\frac{1}{\\sqrt{n}} \\frac{\\partial \\log \\mathcal{L}(\\boldsymbol\\theta_0;\\mathbf{y})}{\\partial \\boldsymbol\\theta} &=& \\sqrt{n} \\left( \\frac{1}{n} \\sum_i \\frac{\\partial \\log f(y_i;\\boldsymbol\\theta_0)}{\\partial \\boldsymbol\\theta} \\right) \\\\\n&=& \\sqrt{n} \\left( \\frac{1}{n} \\sum_i \\left\\{ \\frac{\\partial \\log f(y_i;\\boldsymbol\\theta_0)}{\\partial \\boldsymbol\\theta} - \\mathbb{E}_{\\boldsymbol\\theta_0} \\frac{\\partial \\log f(Y_i;\\boldsymbol\\theta_0)}{\\partial \\boldsymbol\\theta} \\right\\} \\right)\n\\end{eqnarray*}\\]\nconverges \\(\\mathcal{N}(0,\\mathcal{}_Y(\\boldsymbol\\theta_0))\\) CLT.Collecting preceding results leads (b). fact \\(\\boldsymbol\\theta_{MLE}\\) achieves FDCR bound proves (c).Proof Proposition 2.5Proof. \\(\\sqrt{n}(\\hat{\\boldsymbol\\theta}_{n} - \\boldsymbol\\theta_{0}) \\overset{d}{\\rightarrow} \\mathcal{N}(0,\\mathcal{}(\\boldsymbol\\theta_0)^{-1})\\) (Eq. (2.9)). Taylor expansion around \\(\\boldsymbol\\theta_0\\) yields :\n\\[\\begin{equation}\n\\sqrt{n}(h(\\hat{\\boldsymbol\\theta}_{n}) - h(\\boldsymbol\\theta_{0})) \\overset{d}{\\rightarrow} \\mathcal{N}\\left(0,\\frac{\\partial h(\\boldsymbol\\theta_{0})}{\\partial \\boldsymbol\\theta'}\\mathcal{}(\\boldsymbol\\theta_0)^{-1}\\frac{\\partial h(\\boldsymbol\\theta_{0})'}{\\partial \\boldsymbol\\theta}\\right). \\tag{4.8}\n\\end{equation}\\]\n\\(H_0\\), \\(h(\\boldsymbol\\theta_{0})=0\\) therefore:\n\\[\\begin{equation}\n\\sqrt{n} h(\\hat{\\boldsymbol\\theta}_{n}) \\overset{d}{\\rightarrow} \\mathcal{N}\\left(0,\\frac{\\partial h(\\boldsymbol\\theta_{0})}{\\partial \\boldsymbol\\theta'}\\mathcal{}(\\boldsymbol\\theta_0)^{-1}\\frac{\\partial h(\\boldsymbol\\theta_{0})'}{\\partial \\boldsymbol\\theta}\\right). \\tag{4.9}\n\\end{equation}\\]\nHence\n\\[\n\\sqrt{n} \\left(\n\\frac{\\partial h(\\boldsymbol\\theta_{0})}{\\partial \\boldsymbol\\theta'}\\mathcal{}(\\boldsymbol\\theta_0)^{-1}\\frac{\\partial h(\\boldsymbol\\theta_{0})'}{\\partial \\boldsymbol\\theta}\n\\right)^{-1/2} h(\\hat{\\boldsymbol\\theta}_{n}) \\overset{d}{\\rightarrow} \\mathcal{N}\\left(0,Id\\right).\n\\]\nTaking quadratic form, obtain:\n\\[\nn h(\\hat{\\boldsymbol\\theta}_{n})'  \\left(\n\\frac{\\partial h(\\boldsymbol\\theta_{0})}{\\partial \\boldsymbol\\theta'}\\mathcal{}(\\boldsymbol\\theta_0)^{-1}\\frac{\\partial h(\\boldsymbol\\theta_{0})'}{\\partial \\boldsymbol\\theta}\n\\right)^{-1} h(\\hat{\\boldsymbol\\theta}_{n}) \\overset{d}{\\rightarrow} \\chi^2(r).\n\\]fact test asymptotic level \\(\\alpha\\) directly stems precedes. Consistency test: Consider \\(\\theta_0 \\\\Theta\\). MLE consistent, \\(h(\\hat{\\boldsymbol\\theta}_{n})\\) converges \\(h(\\boldsymbol\\theta_0) \\ne 0\\). Eq. (4.8) still valid. implies \\(\\xi^W_n\\) converges \\(+\\infty\\) therefore \\(\\mathbb{P}_{\\boldsymbol\\theta}(\\xi^W_n \\ge \\chi^2_{1-\\alpha}(r)) \\rightarrow 1\\).Proof Proposition 2.6Proof. Notations: “\\(\\approx\\)” means “equal term converges 0 probability”. \\(H_0\\). \\(\\hat{\\boldsymbol\\theta}^0\\) constrained ML estimator; \\(\\hat{\\boldsymbol\\theta}\\) denotes unconstrained one.combine two Taylor expansion: \\(h(\\hat{\\boldsymbol\\theta}_n) \\approx \\dfrac{\\partial h(\\boldsymbol\\theta_0)}{\\partial \\boldsymbol\\theta'}(\\hat{\\boldsymbol\\theta}_n - \\boldsymbol\\theta_0)\\) \\(h(\\hat{\\boldsymbol\\theta}_n^0) \\approx \\dfrac{\\partial h(\\boldsymbol\\theta_0)}{\\partial \\boldsymbol\\theta'}(\\hat{\\boldsymbol\\theta}_n^0 - \\boldsymbol\\theta_0)\\) use \\(h(\\hat{\\boldsymbol\\theta}_n^0)=0\\) (definition) get:\n\\[\\begin{equation}\n\\sqrt{n}h(\\hat{\\boldsymbol\\theta}_n) \\approx \\dfrac{\\partial h(\\boldsymbol\\theta_0)}{\\partial \\boldsymbol\\theta'}\\sqrt{n}(\\hat{\\boldsymbol\\theta}_n - \\hat{\\boldsymbol\\theta}^0_n). \\tag{4.10}\n\\end{equation}\\]\nBesides, (using definition information matrix):\n\\[\\begin{equation}\n\\frac{1}{\\sqrt{n}}\\frac{\\partial \\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}^0_n;\\mathbf{y})}{\\partial \\boldsymbol\\theta} \\approx\n\\frac{1}{\\sqrt{n}}\\frac{\\partial \\log \\mathcal{L}(\\boldsymbol\\theta_0;\\mathbf{y})}{\\partial \\boldsymbol\\theta} - \\mathcal{}(\\boldsymbol\\theta_0)\\sqrt{n}(\\hat{\\boldsymbol\\theta}^0_n-\\boldsymbol\\theta_0) \\tag{4.11}\n\\end{equation}\\]\n:\n\\[\\begin{equation}\n0=\\frac{1}{\\sqrt{n}}\\frac{\\partial \\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}_n;\\mathbf{y})}{\\partial \\boldsymbol\\theta} \\approx\n\\frac{1}{\\sqrt{n}}\\frac{\\partial \\log \\mathcal{L}(\\boldsymbol\\theta_0;\\mathbf{y})}{\\partial \\boldsymbol\\theta} - \\mathcal{}(\\boldsymbol\\theta_0)\\sqrt{n}(\\hat{\\boldsymbol\\theta}_n-\\boldsymbol\\theta_0).\\tag{4.12}\n\\end{equation}\\]\nTaking difference multiplying \\(\\mathcal{}(\\boldsymbol\\theta_0)^{-1}\\):\n\\[\\begin{equation}\n\\sqrt{n}(\\hat{\\boldsymbol\\theta}_n-\\hat{\\boldsymbol\\theta}_n^0) \\approx\n\\mathcal{}(\\boldsymbol\\theta_0)^{-1}\\frac{1}{\\sqrt{n}}\\frac{\\partial \\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}^0_n;\\mathbf{y})}{\\partial \\boldsymbol\\theta}\n\\mathcal{}(\\boldsymbol\\theta_0).\\tag{4.13}\n\\end{equation}\\]\nEqs. (4.10) (4.13) yield :\n\\[\\begin{equation}\n\\sqrt{n}h(\\hat{\\boldsymbol\\theta}_n) \\approx \\dfrac{\\partial h(\\boldsymbol\\theta_0)}{\\partial \\boldsymbol\\theta'} \\mathcal{}(\\boldsymbol\\theta_0)^{-1}\\frac{1}{\\sqrt{n}}\\frac{\\partial \\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}^0_n;\\mathbf{y})}{\\partial \\boldsymbol\\theta}.\\tag{4.14}\n\\end{equation}\\]Recall \\(\\hat{\\boldsymbol\\theta}^0_n\\) MLE \\(\\boldsymbol\\theta_0\\) constraint \\(h(\\boldsymbol\\theta)=0\\). vector Lagrange multipliers \\(\\hat\\lambda_n\\) associated program satisfies:\n\\[\\begin{equation}\n\\frac{\\partial \\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}^0_n;\\mathbf{y})}{\\partial \\boldsymbol\\theta}+ \\frac{\\partial h'(\\hat{\\boldsymbol\\theta}^0_n;\\mathbf{y})}{\\partial \\boldsymbol\\theta}\\hat\\lambda_n = 0.\\tag{4.15}\n\\end{equation}\\]\nSubstituting latter equation Eq. (4.14) gives:\n\\[\\begin{eqnarray*}\n\\sqrt{n}h(\\hat{\\boldsymbol\\theta}_n) &\\approx&\n- \\dfrac{\\partial h(\\boldsymbol\\theta_0)}{\\partial \\boldsymbol\\theta'} \\mathcal{}(\\boldsymbol\\theta_0)^{-1}\n\\frac{\\partial h'(\\hat{\\boldsymbol\\theta}^0_n;\\mathbf{y})}{\\partial \\boldsymbol\\theta} \\frac{\\hat\\lambda_n}{\\sqrt{n}} \\\\\n&\\approx&\n- \\dfrac{\\partial h(\\boldsymbol\\theta_0)}{\\partial \\boldsymbol\\theta'} \\mathcal{}(\\boldsymbol\\theta_0)^{-1}\n\\frac{\\partial h'(\\boldsymbol\\theta_0;\\mathbf{y})}{\\partial \\boldsymbol\\theta} \\frac{\\hat\\lambda_n}{\\sqrt{n}},\n\\end{eqnarray*}\\]\nyields:\n\\[\\begin{equation}\n\\frac{\\hat\\lambda_n}{\\sqrt{n}} \\approx - \\left(\n\\dfrac{\\partial h(\\boldsymbol\\theta_0)}{\\partial \\boldsymbol\\theta'} \\mathcal{}(\\boldsymbol\\theta_0)^{-1}\n\\frac{\\partial h'(\\boldsymbol\\theta_0;\\mathbf{y})}{\\partial \\boldsymbol\\theta}\n\\right)^{-1}\n\\sqrt{n}h(\\hat{\\boldsymbol\\theta}_n).\\tag{4.16}\n\\end{equation}\\]\nfollows, Eq. (4.9), :\n\\[\n\\frac{\\hat\\lambda_n}{\\sqrt{n}} \\overset{d}{\\rightarrow} \\mathcal{N}\\left(0,\\left(\n\\dfrac{\\partial h(\\boldsymbol\\theta_0)}{\\partial \\boldsymbol\\theta'} \\mathcal{}(\\boldsymbol\\theta_0)^{-1}\n\\frac{\\partial h'(\\boldsymbol\\theta_0;\\mathbf{y})}{\\partial \\boldsymbol\\theta}\n\\right)^{-1}\\right).\n\\]\nTaking quadratic form last equation gives:\n\\[\n\\frac{1}{n}\\hat\\lambda_n' \\dfrac{\\partial h(\\hat{\\boldsymbol\\theta}^0_n)}{\\partial \\boldsymbol\\theta'} \\mathcal{}(\\hat{\\boldsymbol\\theta}^0_n)^{-1}\n\\frac{\\partial h'(\\hat{\\boldsymbol\\theta}^0_n;\\mathbf{y})}{\\partial \\boldsymbol\\theta} \\hat\\lambda_n \\overset{d}{\\rightarrow} \\chi^2(r).\n\\]\nUsing Eq. (4.15), appears left-hand side term last equation \\(\\xi^{LM}\\) defined Eq. (2.15). Consistency: see Remark 17.3 Gouriéroux Monfort (1995).Proof Proposition 2.7Proof. Let us first demonstrate asymptotic equivalence \\(\\xi^{LM}\\) \\(\\xi^{LR}\\).second-order Taylor expansions \\(\\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}^0_n,\\mathbf{y})\\) \\(\\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}_n,\\mathbf{y})\\) :\n\\[\\begin{eqnarray*}\n\\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}_n,\\mathbf{y}) &\\approx& \\log \\mathcal{L}(\\boldsymbol\\theta_0,\\mathbf{y})\n+ \\frac{\\partial \\log \\mathcal{L}(\\boldsymbol\\theta_0,\\mathbf{y})}{\\partial \\boldsymbol\\theta'}(\\hat{\\boldsymbol\\theta}_n-\\boldsymbol\\theta_0) \\\\\n&& - \\frac{n}{2} (\\hat{\\boldsymbol\\theta}_n-\\boldsymbol\\theta_0)' \\mathcal{}(\\boldsymbol\\theta_0) (\\hat{\\boldsymbol\\theta}_n-\\boldsymbol\\theta_0)\\\\\n\\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}^0_n,\\mathbf{y}) &\\approx& \\log \\mathcal{L}(\\boldsymbol\\theta_0,\\mathbf{y})\n+ \\frac{\\partial \\log \\mathcal{L}(\\boldsymbol\\theta_0,\\mathbf{y})}{\\partial \\boldsymbol\\theta'}(\\hat{\\boldsymbol\\theta}^0_n-\\boldsymbol\\theta_0) \\\\\n&& - \\frac{n}{2} (\\hat{\\boldsymbol\\theta}^0_n-\\boldsymbol\\theta_0)' \\mathcal{}(\\boldsymbol\\theta_0) (\\hat{\\boldsymbol\\theta}^0_n-\\boldsymbol\\theta_0).\n\\end{eqnarray*}\\]\nTaking difference, obtain:\n\\[\\begin{eqnarray*}\n\\xi_n^{LR} &\\approx& 2\\frac{\\partial \\log \\mathcal{L}(\\boldsymbol\\theta_0,\\mathbf{y})}{\\partial \\boldsymbol\\theta'}\n(\\hat{\\boldsymbol\\theta}_n-\\hat{\\boldsymbol\\theta}^0_n) + n (\\hat{\\boldsymbol\\theta}^0_n-\\boldsymbol\\theta_0)' \\mathcal{}(\\boldsymbol\\theta_0) (\\hat{\\boldsymbol\\theta}^0_n-\\boldsymbol\\theta_0)\\\\\n&& - n (\\hat{\\boldsymbol\\theta}_n-\\boldsymbol\\theta_0)' \\mathcal{}(\\boldsymbol\\theta_0) (\\hat{\\boldsymbol\\theta}_n-\\boldsymbol\\theta_0).\n\\end{eqnarray*}\\]\nUsing \\(\\dfrac{1}{\\sqrt{n}}\\frac{\\partial \\log \\mathcal{L}(\\boldsymbol\\theta_0;\\mathbf{y})}{\\partial \\boldsymbol\\theta} \\approx \\mathcal{}(\\boldsymbol\\theta_0)\\sqrt{n}(\\hat{\\boldsymbol\\theta}_n-\\boldsymbol\\theta_0)\\) (Eq. (4.12)), :\n\\[\\begin{eqnarray*}\n\\xi_n^{LR} &\\approx&\n2n(\\hat{\\boldsymbol\\theta}_n-\\boldsymbol\\theta_0)'\\mathcal{}(\\boldsymbol\\theta_0)\n(\\hat{\\boldsymbol\\theta}_n-\\hat{\\boldsymbol\\theta}^0_n)\n+ n (\\hat{\\boldsymbol\\theta}^0_n-\\boldsymbol\\theta_0)' \\mathcal{}(\\boldsymbol\\theta_0) (\\hat{\\boldsymbol\\theta}^0_n-\\boldsymbol\\theta_0) \\\\\n&& - n (\\hat{\\boldsymbol\\theta}_n-\\boldsymbol\\theta_0)' \\mathcal{}(\\boldsymbol\\theta_0) (\\hat{\\boldsymbol\\theta}_n-\\boldsymbol\\theta_0).\n\\end{eqnarray*}\\]\nsecond three terms sum, replace \\((\\hat{\\boldsymbol\\theta}^0_n-\\boldsymbol\\theta_0)\\) \\((\\hat{\\boldsymbol\\theta}^0_n-\\hat{\\boldsymbol\\theta}_n+\\hat{\\boldsymbol\\theta}_n-\\boldsymbol\\theta_0)\\) develop associated product. leads :\n\\[\\begin{equation}\n\\xi_n^{LR} \\approx n (\\hat{\\boldsymbol\\theta}^0_n-\\hat{\\boldsymbol\\theta}_n)' \\mathcal{}(\\boldsymbol\\theta_0)^{-1} (\\hat{\\boldsymbol\\theta}^0_n-\\hat{\\boldsymbol\\theta}_n). \\tag{4.17}\n\\end{equation}\\]\ndifference Eqs. (4.11) (4.12) implies:\n\\[\n\\frac{1}{\\sqrt{n}}\\frac{\\partial \\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}^0_n;\\mathbf{y})}{\\partial \\boldsymbol\\theta} \\approx\n\\mathcal{}(\\boldsymbol\\theta_0)\\sqrt{n}(\\hat{\\boldsymbol\\theta}_n-\\hat{\\boldsymbol\\theta}^0_n),\n\\]\n, associated Eq. @(eq:lr10), gives:\n\\[\n\\xi_n^{LR} \\approx \\frac{1}{n} \\frac{\\partial \\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}^0_n;\\mathbf{y})}{\\partial \\boldsymbol\\theta'} \\mathcal{}(\\boldsymbol\\theta_0)^{-1} \\frac{\\partial \\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}^0_n;\\mathbf{y})}{\\partial \\boldsymbol\\theta} \\approx \\xi_n^{LM}.\n\\]\nHence \\(\\xi_n^{LR}\\) asymptotic distribution \\(\\xi_n^{LM}\\).Let’s show LR test consistent. , note :\n\\[\\begin{eqnarray*}\n\\frac{\\log \\mathcal{L}(\\hat{\\boldsymbol\\theta},\\mathbf{y}) - \\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}^0,\\mathbf{y})}{n} &=& \\frac{1}{n} \\sum_{=1}^n[\\log f(y_i;\\hat{\\boldsymbol\\theta}_n) - \\log f(y_i;\\hat{\\boldsymbol\\theta}_n^0)]\\\\\n&\\rightarrow& \\mathbb{E}_0[\\log f(Y;\\boldsymbol\\theta_0) - \\log f(Y;\\boldsymbol\\theta_\\infty)],\n\\end{eqnarray*}\\]\n\\(\\boldsymbol\\theta_\\infty\\), pseudo true value, \\(h(\\boldsymbol\\theta_\\infty) \\ne 0\\) (definition \\(H_1\\)). Kullback inequality asymptotic identifiability \\(\\boldsymbol\\theta_0\\), follows \\(\\mathbb{E}_0[\\log f(Y;\\boldsymbol\\theta_0) - \\log f(Y;\\boldsymbol\\theta_\\infty)] >0\\). Therefore \\(\\xi_n^{LR} \\rightarrow + \\infty\\) \\(H_1\\).Let us now demonstrate equivalence \\(\\xi^{LM} \\xi^{W}\\).(using Eq. (eq:multiplier)):\n\\[\n\\xi^{LM}_n = \\frac{1}{n}\\hat\\lambda_n' \\dfrac{\\partial h(\\hat{\\boldsymbol\\theta}^0_n)}{\\partial \\boldsymbol\\theta'} \\mathcal{}(\\hat{\\boldsymbol\\theta}^0_n)^{-1}\n\\frac{\\partial h'(\\hat{\\boldsymbol\\theta}^0_n;\\mathbf{y})}{\\partial \\boldsymbol\\theta} \\hat\\lambda_n.\n\\]\nSince, \\(H_0\\), \\(\\hat{\\boldsymbol\\theta}_n^0\\approx\\hat{\\boldsymbol\\theta}_n \\approx {\\boldsymbol\\theta}_0\\), Eq. (4.16) therefore implies :\n\\[\n\\xi^{LM} \\approx n h(\\hat{\\boldsymbol\\theta}_n)' \\left(\n\\dfrac{\\partial h(\\hat{\\boldsymbol\\theta}_n)}{\\partial \\boldsymbol\\theta'} \\mathcal{}(\\hat{\\boldsymbol\\theta}_n)^{-1}\n\\frac{\\partial h'(\\hat{\\boldsymbol\\theta}_n;\\mathbf{y})}{\\partial \\boldsymbol\\theta}\n\\right)^{-1}\nh(\\hat{\\boldsymbol\\theta}_n) = \\xi^{W},\n\\]\ngives result.Proof Eq. (??)Proof. :\n\\[\\begin{eqnarray*}\n&&T\\mathbb{E}\\left[(\\bar{y}_T - \\mu)^2\\right]\\\\\n&=& T\\mathbb{E}\\left[\\left(\\frac{1}{T}\\sum_{t=1}^T(y_t - \\mu)\\right)^2\\right] = \\frac{1}{T} \\mathbb{E}\\left[\\sum_{t=1}^T(y_t - \\mu)^2+2\\sum_{s<t\\le T}(y_t - \\mu)(y_s - \\mu)\\right]\\\\\n&=& \\gamma_0 +\\frac{2}{T}\\left(\\sum_{t=2}^{T}\\mathbb{E}\\left[(y_t - \\mu)(y_{t-1} - \\mu)\\right]\\right) +\\frac{2}{T}\\left(\\sum_{t=3}^{T}\\mathbb{E}\\left[(y_t - \\mu)(y_{t-2} - \\mu)\\right]\\right) + \\dots \\\\\n&&+ \\frac{2}{T}\\left(\\sum_{t=T-1}^{T}\\mathbb{E}\\left[(y_t - \\mu)(y_{t-(T-2)} - \\mu)\\right]\\right) + \\frac{2}{T}\\mathbb{E}\\left[(y_t - \\mu)(y_{t-(T-1)} - \\mu)\\right]\\\\\n&=&  \\gamma_0 + 2 \\frac{T-1}{T}\\gamma_1 + \\dots + 2 \\frac{1}{T}\\gamma_{T-1} .\n\\end{eqnarray*}\\]\nTherefore:\n\\[\\begin{eqnarray*}\n&& T\\mathbb{E}\\left[(\\bar{y}_T - \\mu)^2\\right] - \\sum_{j=-\\infty}^{+\\infty} \\gamma_j \\\\\n&=& - 2\\frac{1}{T}\\gamma_1 - 2\\frac{2}{T}\\gamma_2 - \\dots - 2\\frac{T-1}{T}\\gamma_{T-1} - 2\\gamma_T - 2 \\gamma_{T+1} + \\dots\n\\end{eqnarray*}\\]\n:\n\\[\\begin{eqnarray*}\n&& \\left|T\\mathbb{E}\\left[(\\bar{y}_T - \\mu)^2\\right] - \\sum_{j=-\\infty}^{+\\infty} \\gamma_j\\right|\\\\\n&\\le& 2\\frac{1}{T}|\\gamma_1| + 2\\frac{2}{T}|\\gamma_2| + \\dots + 2\\frac{T-1}{T}|\\gamma_{T-1}| + 2|\\gamma_T| + 2 |\\gamma_{T+1}| + \\dots\n\\end{eqnarray*}\\]\\(q \\le T\\), :\n\\[\\begin{eqnarray*}\n\\left|T\\mathbb{E}\\left[(\\bar{y}_T - \\mu)^2\\right] - \\sum_{j=-\\infty}^{+\\infty} \\gamma_j\\right| &\\le& 2\\frac{1}{T}|\\gamma_1| + 2\\frac{2}{T}|\\gamma_2| + \\dots + 2\\frac{q-1}{T}|\\gamma_{q-1}| +2\\frac{q}{T}|\\gamma_q| +\\\\\n&&2\\frac{q+1}{T}|\\gamma_{q+1}| + \\dots  + 2\\frac{T-1}{T}|\\gamma_{T-1}| + 2|\\gamma_T| + 2 |\\gamma_{T+1}| + \\dots\\\\\n&\\le& \\frac{2}{T}\\left(|\\gamma_1| + 2|\\gamma_2| + \\dots + (q-1)|\\gamma_{q-1}| +q|\\gamma_q|\\right) +\\\\\n&&2|\\gamma_{q+1}| + \\dots  + 2|\\gamma_{T-1}| + 2|\\gamma_T| + 2 |\\gamma_{T+1}| + \\dots\n\\end{eqnarray*}\\]Consider \\(\\varepsilon > 0\\). fact autocovariances absolutely summable implies exists \\(q_0\\) (Cauchy criterion, Theorem 4.2):\n\\[\n2|\\gamma_{q_0+1}|+2|\\gamma_{q_0+2}|+2|\\gamma_{q_0+3}|+\\dots < \\varepsilon/2.\n\\]\n, \\(T > q_0\\), comes :\n\\[\n\\left|T \\mathbb{E}\\left[(\\bar{y}_T - \\mu)^2\\right] - \\sum_{j=-\\infty}^{+\\infty} \\gamma_j\\right|\\le \\frac{2}{T}\\left(|\\gamma_1| + 2|\\gamma_2| + \\dots + (q_0-1)|\\gamma_{q_0-1}| +q_0|\\gamma_{q_0}|\\right) + \\varepsilon/2.\n\\]\n\\(T \\ge 2\\left(|\\gamma_1| + 2|\\gamma_2| + \\dots + (q_0-1)|\\gamma_{q_0-1}| +q_0|\\gamma_{q_0}|\\right)/(\\varepsilon/2)\\) (\\(= f(q_0)\\), say) \n\\[\n\\frac{2}{T}\\left(|\\gamma_1| + 2|\\gamma_2| + \\dots + (q_0-1)|\\gamma_{q_0-1}| +q_0|\\gamma_{q_0}|\\right) \\le \\varepsilon/2.\n\\]\n, \\(T>f(q_0)\\) \\(T>q_0\\), .e. \\(T>\\max(f(q_0),q_0)\\), :\n\\[\n\\left|T \\mathbb{E}\\left[(\\bar{y}_T - \\mu)^2\\right] - \\sum_{j=-\\infty}^{+\\infty} \\gamma_j\\right|\\le \\varepsilon.\n\\]Proof Proposition ??Proof. :\n\\[\\begin{eqnarray}\n\\mathbb{E}([y_{t+1} - y^*_{t+1}]^2) &=& \\mathbb{E}\\left([\\color{blue}{\\{y_{t+1} - \\mathbb{E}(y_{t+1}|x_t)\\}} + \\color{red}{\\{\\mathbb{E}(y_{t+1}|x_t)  - y^*_{t+1}\\}}]^2\\right)\\nonumber\\\\\n&=&  \\mathbb{E}\\left(\\color{blue}{[y_{t+1} - \\mathbb{E}(y_{t+1}|x_t)]}^2\\right) + \\mathbb{E}\\left(\\color{red}{[\\mathbb{E}(y_{t+1}|x_t)  - y^*_{t+1}]}^2\\right)\\nonumber\\\\\n&& + 2\\mathbb{E}\\left( \\color{blue}{[y_{t+1} - \\mathbb{E}(y_{t+1}|x_t)]}\\color{red}{ [\\mathbb{E}(y_{t+1}|x_t)  - y^*_{t+1}]}\\right). \\tag{4.18}\n\\end{eqnarray}\\]\nLet us focus last term. :\n\\[\\begin{eqnarray*}\n&&\\mathbb{E}\\left(\\color{blue}{[y_{t+1} - \\mathbb{E}(y_{t+1}|x_t)]}\\color{red}{ [\\mathbb{E}(y_{t+1}|x_t)  - y^*_{t+1}]}\\right)\\\\\n&=& \\mathbb{E}( \\mathbb{E}( \\color{blue}{[y_{t+1} - \\mathbb{E}(y_{t+1}|x_t)]}\\color{red}{ \\underbrace{[\\mathbb{E}(y_{t+1}|x_t)  - y^*_{t+1}]}_{\\mbox{function $x_t$}}}|x_t))\\\\\n&=& \\mathbb{E}( \\color{red}{ [\\mathbb{E}(y_{t+1}|x_t)  - y^*_{t+1}]} \\mathbb{E}( \\color{blue}{[y_{t+1} - \\mathbb{E}(y_{t+1}|x_t)]}|x_t))\\\\\n&=& \\mathbb{E}( \\color{red}{ [\\mathbb{E}(y_{t+1}|x_t)  - y^*_{t+1}]} \\color{blue}{\\underbrace{[\\mathbb{E}(y_{t+1}|x_t) - \\mathbb{E}(y_{t+1}|x_t)]}_{=0}})=0.\n\\end{eqnarray*}\\]Therefore, Eq. (4.18) becomes:\n\\[\\begin{eqnarray*}\n&&\\mathbb{E}([y_{t+1} - y^*_{t+1}]^2) \\\\\n&=&  \\underbrace{\\mathbb{E}\\left(\\color{blue}{[y_{t+1} - \\mathbb{E}(y_{t+1}|x_t)]}^2\\right)}_{\\mbox{$\\ge 0$ depend $y^*_{t+1}$}} + \\underbrace{\\mathbb{E}\\left(\\color{red}{[\\mathbb{E}(y_{t+1}|x_t)  - y^*_{t+1}]}^2\\right)}_{\\mbox{$\\ge 0$ depends $y^*_{t+1}$}}.\n\\end{eqnarray*}\\]\nimplies \\(\\mathbb{E}([y_{t+1} - y^*_{t+1}]^2)\\) always larger \\(\\color{blue}{\\mathbb{E}([y_{t+1} - \\mathbb{E}(y_{t+1}|x_t)]^2)}\\), therefore minimized second term equal zero, \\(\\mathbb{E}(y_{t+1}|x_t) = y^*_{t+1}\\).Proof Proposition ??Proof. Using Proposition 4.18, obtain , conditionally \\(x_1\\), log-likelihood given \n\\[\\begin{eqnarray*}\n\\log\\mathcal{L}(Y_{T};\\theta) & = & -(Tn/2)\\log(2\\pi)+(T/2)\\log\\left|\\Omega^{-1}\\right|\\\\\n&  & -\\frac{1}{2}\\sum_{t=1}^{T}\\left[\\left(y_{t}-\\Pi'x_{t}\\right)'\\Omega^{-1}\\left(y_{t}-\\Pi'x_{t}\\right)\\right].\n\\end{eqnarray*}\\]\nLet’s rewrite last term log-likelihood:\n\\[\\begin{eqnarray*}\n\\sum_{t=1}^{T}\\left[\\left(y_{t}-\\Pi'x_{t}\\right)'\\Omega^{-1}\\left(y_{t}-\\Pi'x_{t}\\right)\\right] & =\\\\\n\\sum_{t=1}^{T}\\left[\\left(y_{t}-\\hat{\\Pi}'x_{t}+\\hat{\\Pi}'x_{t}-\\Pi'x_{t}\\right)'\\Omega^{-1}\\left(y_{t}-\\hat{\\Pi}'x_{t}+\\hat{\\Pi}'x_{t}-\\Pi'x_{t}\\right)\\right] & =\\\\\n\\sum_{t=1}^{T}\\left[\\left(\\hat{\\varepsilon}_{t}+(\\hat{\\Pi}-\\Pi)'x_{t}\\right)'\\Omega^{-1}\\left(\\hat{\\varepsilon}_{t}+(\\hat{\\Pi}-\\Pi)'x_{t}\\right)\\right],\n\\end{eqnarray*}\\]\n\\(j^{th}\\) element \\((n\\times1)\\) vector \\(\\hat{\\varepsilon}_{t}\\) sample residual, observation \\(t\\), OLS regression \\(y_{j,t}\\) \\(x_{t}\\). Expanding previous equation, get:\n\\[\\begin{eqnarray*}\n&&\\sum_{t=1}^{T}\\left[\\left(y_{t}-\\Pi'x_{t}\\right)'\\Omega^{-1}\\left(y_{t}-\\Pi'x_{t}\\right)\\right]  = \\sum_{t=1}^{T}\\hat{\\varepsilon}_{t}'\\Omega^{-1}\\hat{\\varepsilon}_{t}\\\\\n&&+2\\sum_{t=1}^{T}\\hat{\\varepsilon}_{t}'\\Omega^{-1}(\\hat{\\Pi}-\\Pi)'x_{t}+\\sum_{t=1}^{T}x'_{t}(\\hat{\\Pi}-\\Pi)\\Omega^{-1}(\\hat{\\Pi}-\\Pi)'x_{t}.\n\\end{eqnarray*}\\]\nLet’s apply trace operator second term (scalar):\n\\[\\begin{eqnarray*}\n\\sum_{t=1}^{T}\\hat{\\varepsilon}_{t}'\\Omega^{-1}(\\hat{\\Pi}-\\Pi)'x_{t} & = & Tr\\left(\\sum_{t=1}^{T}\\hat{\\varepsilon}_{t}'\\Omega^{-1}(\\hat{\\Pi}-\\Pi)'x_{t}\\right)\\\\\n=  Tr\\left(\\sum_{t=1}^{T}\\Omega^{-1}(\\hat{\\Pi}-\\Pi)'x_{t}\\hat{\\varepsilon}_{t}'\\right) & = & Tr\\left(\\Omega^{-1}(\\hat{\\Pi}-\\Pi)'\\sum_{t=1}^{T}x_{t}\\hat{\\varepsilon}_{t}'\\right).\n\\end{eqnarray*}\\]\nGiven , construction (property OLS estimates), sample residuals orthogonal explanatory variables, term zero. Introducing \\(\\tilde{x}_{t}=(\\hat{\\Pi}-\\Pi)'x_{t}\\), \n\\[\\begin{eqnarray*}\n\\sum_{t=1}^{T}\\left[\\left(y_{t}-\\Pi'x_{t}\\right)'\\Omega^{-1}\\left(y_{t}-\\Pi'x_{t}\\right)\\right] =\\sum_{t=1}^{T}\\hat{\\varepsilon}_{t}'\\Omega^{-1}\\hat{\\varepsilon}_{t}+\\sum_{t=1}^{T}\\tilde{x}'_{t}\\Omega^{-1}\\tilde{x}_{t}.\n\\end{eqnarray*}\\]\nSince \\(\\Omega\\) positive definite matrix, \\(\\Omega^{-1}\\) well. Consequently, smallest value last term can take obtained \\(\\tilde{x}_{t}=0\\), .e. \\(\\Pi=\\hat{\\Pi}.\\)MLE \\(\\Omega\\) matrix \\(\\hat{\\Omega}\\) maximizes \\(\\Omega\\overset{\\ell}{\\rightarrow}L(Y_{T};\\hat{\\Pi},\\Omega)\\). :\n\\[\\begin{eqnarray*}\n\\log\\mathcal{L}(Y_{T};\\hat{\\Pi},\\Omega) & = & -(Tn/2)\\log(2\\pi)+(T/2)\\log\\left|\\Omega^{-1}\\right| -\\frac{1}{2}\\sum_{t=1}^{T}\\left[\\hat{\\varepsilon}_{t}'\\Omega^{-1}\\hat{\\varepsilon}_{t}\\right].\n\\end{eqnarray*}\\]Matrix \\(\\hat{\\Omega}\\) symmetric positive definite. easily checked (unrestricted) matrix maximizes latter expression symmetric positive definite matrix. Indeed:\n\\[\n\\frac{\\partial \\log\\mathcal{L}(Y_{T};\\hat{\\Pi},\\Omega)}{\\partial\\Omega}=\\frac{T}{2}\\Omega'-\\frac{1}{2}\\sum_{t=1}^{T}\\hat{\\varepsilon}_{t}\\hat{\\varepsilon}'_{t}\\Rightarrow\\hat{\\Omega}'=\\frac{1}{T}\\sum_{t=1}^{T}\\hat{\\varepsilon}_{t}\\hat{\\varepsilon}'_{t},\n\\]\nleads result.Proof Proposition ??Proof. Let us drop \\(\\) subscript. Rearranging Eq. (??), :\n\\[\n\\sqrt{T}(\\mathbf{b}-\\boldsymbol{\\beta}) =  (X'X/T)^{-1}\\sqrt{T}(X'\\boldsymbol\\varepsilon/T).\n\\]\nLet us consider autocovariances \\(\\mathbf{v}_t = x_t \\varepsilon_t\\), denoted \\(\\gamma^v_j\\). Using fact \\(x_t\\) linear combination past \\(\\varepsilon_t\\)s \\(\\varepsilon_t\\) white noise, get \\(\\mathbb{E}(\\varepsilon_t x_t)=0\\). Therefore\n\\[\n\\gamma^v_j = \\mathbb{E}(\\varepsilon_t\\varepsilon_{t-j}x_tx_{t-j}').\n\\]\n\\(j>0\\), \\(\\mathbb{E}(\\varepsilon_t\\varepsilon_{t-j}x_tx_{t-j}')=\\mathbb{E}(\\mathbb{E}[\\varepsilon_t\\varepsilon_{t-j}x_tx_{t-j}'|\\varepsilon_{t-j},x_t,x_{t-j}])=\\) \\(\\mathbb{E}(\\varepsilon_{t-j}x_tx_{t-j}'\\mathbb{E}[\\varepsilon_t|\\varepsilon_{t-j},x_t,x_{t-j}])=0\\). Note \\(\\mathbb{E}[\\varepsilon_t|\\varepsilon_{t-j},x_t,x_{t-j}]=0\\) \\(\\{\\varepsilon_t\\}\\) ..d. white noise sequence. \\(j=0\\), :\n\\[\n\\gamma^v_0 = \\mathbb{E}(\\varepsilon_t^2x_tx_{t}')= \\mathbb{E}(\\varepsilon_t^2) \\mathbb{E}(x_tx_{t}')=\\sigma^2\\mathbf{Q}.\n\\]\nconvergence distribution \\(\\sqrt{T}(X'\\boldsymbol\\varepsilon/T)=\\sqrt{T}\\frac{1}{T}\\sum_{t=1}^Tv_t\\) results Central Limit Theorem covariance-stationary processes, using \\(\\gamma_j^v\\) computed .","code":""},{"path":"append.html","id":"additional-codes","chapter":"4 Appendix","heading":"4.6 Additional codes","text":"","code":""},{"path":"append.html","id":"App:GEV","chapter":"4 Appendix","heading":"4.6.1 Simulating GEV distributions","text":"following lines code used generate Figure 3.7.","code":"\nn.sim <- 4000\npar(mfrow=c(1,3),\n    plt=c(.2,.95,.2,.85))\nall.rhos <- c(.3,.6,.95)\nfor(j in 1:length(all.rhos)){\n  theta <- 1/all.rhos[j]\n  v1 <- runif(n.sim)\n  v2 <- runif(n.sim)\n  w <- rep(.000001,n.sim)\n  # solve for f(w) = w*(1 - log(w)/theta) - v2 = 0\n  for(i in 1:20){\n    f.i <- w * (1 - log(w)/theta) - v2\n    f.prime <- 1 - log(w)/theta - 1/theta\n    w <- w - f.i/f.prime\n  }\n  u1 <- exp(v1^(1/theta) * log(w))\n  u2 <- exp((1-v1)^(1/theta) * log(w))\n\n  # Get eps1 and eps2 using the inverse of\n  # the Gumbel distribution's cdf:\n  eps1 <- -log(-log(u1))\n  eps2 <- -log(-log(u2))\n  cbind(cor(eps1,eps2),1-all.rhos[j]^2)\n  plot(eps1,eps2,pch=19,col=\"#FF000044\",\n       main=paste(\"rho = \",toString(all.rhos[j]),sep=\"\"),\n       xlab=expression(epsilon[1]),\n       ylab=expression(epsilon[2]),\n       cex.lab=2,cex.main=1.5)\n}"},{"path":"append.html","id":"IRFDELTA","chapter":"4 Appendix","heading":"4.6.2 Computing the covariance matrix of IRF using the delta method","text":"","code":"\nirf.function <- function(THETA){\n  c <- THETA[1]\n  phi <- THETA[2:(p+1)]\n  if(q>0){\n    theta <- c(1,THETA[(1+p+1):(1+p+q)])\n  }else{\n    theta <- 1\n  }\n  sigma <- THETA[1+p+q+1]\n  r <- dim(Matrix.of.Exog)[2] - 1\n  beta <- THETA[(1+p+q+1+1):(1+p+q+1+(r+1))]\n  \n  irf <- sim.arma(0,phi,beta,sigma=sd(Ramey$ED3_TC,na.rm=TRUE),T=60,\n                  y.0=rep(0,length(x$phi)),nb.sim=1,make.IRF=1,\n                  X=NaN,beta=NaN)\n  return(irf)\n}\n\nIRF.0 <- 100*irf.function(x$THETA)\neps <- .00000001\nd.IRF <- NULL\nfor(i in 1:length(x$THETA)){\n  THETA.i <- x$THETA\n  THETA.i[i] <- THETA.i[i] + eps\n  IRF.i <- 100*irf.function(THETA.i)\n  d.IRF <- cbind(d.IRF,\n                 (IRF.i - IRF.0)/eps\n                 )\n}\nmat.var.cov.IRF <- d.IRF %*% x$I %*% t(d.IRF)"},{"path":"append.html","id":"statistical-tables","chapter":"4 Appendix","heading":"4.7 Statistical Tables","text":"Table 4.1: Quantiles \\(\\mathcal{N}(0,1)\\) distribution. \\(\\) \\(b\\) respectively row column number; corresponding cell gives \\(\\mathbb{P}(0<X\\le +b)\\), \\(X \\sim \\mathcal{N}(0,1)\\).Table 4.2: Quantiles Student-\\(t\\) distribution. rows correspond different degrees freedom (\\(\\nu\\), say); columns correspond different probabilities (\\(z\\), say). cell gives \\(q\\) s.t. \\(\\mathbb{P}(-q<X<q)=z\\), \\(X \\sim t(\\nu)\\).Table 4.3: Quantiles \\(\\chi^2\\) distribution. rows correspond different degrees freedom; columns correspond different probabilities.Table 4.4: Quantiles \\(\\mathcal{F}\\) distribution. columns rows correspond different degrees freedom (resp. \\(n_1\\) \\(n_2\\)). different panels correspond different probabilities (\\(\\alpha\\)) corresponding cell gives \\(z\\) s.t. \\(\\mathbb{P}(X \\le z)=\\alpha\\), \\(X \\sim \\mathcal{F}(n_1,n_2)\\).","code":""},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]
